
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://pytorch.apachecn.org/0.2/package_references/torch-nn/">
      
      
        <link rel="prev" href="../Storage/">
      
      
        <link rel="next" href="../functional/">
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.4.3, mkdocs-material-9.1.17">
    
    
      
        <title>torch.nn - 【布客】PyTorch 中文翻译</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.26e3688c.min.css">
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  <link rel="stylesheet" href="../../../assets/stylesheets/custom.bea7efe8.min.css">
  <!-- google ads -->
  <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-3565452474788507" crossorigin="anonymous"></script>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-8DP4GX97XY"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-8DP4GX97XY');
  </script>
  <!-- google webmaster -->
  <meta name="google-site-verification" content="pyo9N70ZWyh8JB43bIu633mhxesJ1IcwWCZlM3jUfFo" />

  <!-- wwads-cn union -->
  <meta name="wwads-cn-verify" content="03c6b06952c750899bb03d998e631860" />
  <script type="text/javascript" charset="UTF-8" src="https://cdn.wwads.cn/js/makemoney.js" async></script>

  </head>
  
  
    <body dir="ltr">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#torchnn" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
        <aside class="md-banner">
          <div class="md-banner__inner md-grid md-typeset">
            
            
  => 组织无偿提供 中文版本（免费，秒级响应）
  <a target="_blank" href="https://chat.ibooker.org.cn/chat" style="color: red;">
    <span class="twemoji mastodon">
      <img src="https://data.apachecn.org/img/icon/ROBOT_TXT.svg" alt="ChatGPT - ailake.top">
    </span>
    <strong>ChatGPT - ailake.top</strong>
  </a> 一起来白嫖叭～！

          </div>
          
        </aside>
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="【布客】PyTorch 中文翻译" class="md-header__button md-logo" aria-label="【布客】PyTorch 中文翻译" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            【布客】PyTorch 中文翻译
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              torch.nn
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/apachecn/pytorch-doc-zh" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    apachecn/pytorch-doc-zh
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="【布客】PyTorch 中文翻译" class="md-nav__button md-logo" aria-label="【布客】PyTorch 中文翻译" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    【布客】PyTorch 中文翻译
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/apachecn/pytorch-doc-zh" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    apachecn/pytorch-doc-zh
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        PyTorch 中文文档 & 教程
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
      
      
      
        <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
          PyTorch 2.0 中文文档 & 教程
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 2.0 中文文档 & 教程
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1" >
      
      
      
        <label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="0">
          中文教程
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_1">
          <span class="md-nav__icon md-icon"></span>
          中文教程
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../2.0/tutorials/README.md" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_2" >
      
      
      
        <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="0">
          中文文档
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_2">
          <span class="md-nav__icon md-icon"></span>
          中文文档
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../2.0/docs/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
      
      
      
        <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
          PyTorch 1.7 中文文档
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 1.7 中文文档
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" >
      
      
      
        <label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="0">
          学习 PyTorch
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_1">
          <span class="md-nav__icon md-icon"></span>
          学习 PyTorch
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_1" >
      
      
      
        <label class="md-nav__link" for="__nav_3_1_1" id="__nav_3_1_1_label" tabindex="0">
          PyTorch 深度学习：60 分钟的突击
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_1_1">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 深度学习：60 分钟的突击
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/02/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/03/" class="md-nav__link">
        张量
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/04/" class="md-nav__link">
        torch.autograd的简要介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/05/" class="md-nav__link">
        神经网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/06/" class="md-nav__link">
        训练分类器
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_2" >
      
      
      
        <label class="md-nav__link" for="__nav_3_1_2" id="__nav_3_1_2_label" tabindex="0">
          通过示例学习 PyTorch
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_1_2">
          <span class="md-nav__icon md-icon"></span>
          通过示例学习 PyTorch
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/07/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/08/" class="md-nav__link">
        热身：NumPy
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/09/" class="md-nav__link">
        PyTorch：张量
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/10/" class="md-nav__link">
        PyTorch：张量和 Autograd
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/11/" class="md-nav__link">
        PyTorch：定义新的 Autograd 函数
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/12/" class="md-nav__link">
        PyTorch：nn
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/13/" class="md-nav__link">
        PyTorch：optim
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/14/" class="md-nav__link">
        PyTorch：自定义nn模块
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/15/" class="md-nav__link">
        PyTorch：控制流 - 权重共享
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/16/" class="md-nav__link">
        torch.nn到底是什么？
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/17/" class="md-nav__link">
        使用 TensorBoard 可视化模型，数据和训练
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
      
      
      
        <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
          图片/视频
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_2">
          <span class="md-nav__icon md-icon"></span>
          图片/视频
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/19/" class="md-nav__link">
        torchvision对象检测微调教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/20/" class="md-nav__link">
        计算机视觉的迁移学习教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/21/" class="md-nav__link">
        对抗示例生成
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/22/" class="md-nav__link">
        DCGAN 教程
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_3" >
      
      
      
        <label class="md-nav__link" for="__nav_3_3" id="__nav_3_3_label" tabindex="0">
          音频
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_3">
          <span class="md-nav__icon md-icon"></span>
          音频
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/24/" class="md-nav__link">
        音频 I/O 和torchaudio的预处理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/25/" class="md-nav__link">
        使用torchaudio的语音命令识别
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4" id="__nav_3_4_label" tabindex="0">
          文本
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4">
          <span class="md-nav__icon md-icon"></span>
          文本
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/27/" class="md-nav__link">
        使用nn.Transformer和torchtext的序列到序列建模
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/28/" class="md-nav__link">
        从零开始的 NLP：使用字符级 RNN 分类名称
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/29/" class="md-nav__link">
        从零开始的 NLP：使用字符级 RNN 生成名称
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/30/" class="md-nav__link">
        从零开始的 NLP：使用序列到序列网络和注意力的翻译
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/31/" class="md-nav__link">
        使用torchtext的文本分类
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/32/" class="md-nav__link">
        torchtext语言翻译
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_5" >
      
      
      
        <label class="md-nav__link" for="__nav_3_5" id="__nav_3_5_label" tabindex="0">
          强化学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_5">
          <span class="md-nav__icon md-icon"></span>
          强化学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/34/" class="md-nav__link">
        强化学习（DQN）教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/35/" class="md-nav__link">
        训练玩马里奥的 RL 智能体
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_6" >
      
      
      
        <label class="md-nav__link" for="__nav_3_6" id="__nav_3_6_label" tabindex="0">
          在生产中部署 PyTorch 模型
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_6">
          <span class="md-nav__icon md-icon"></span>
          在生产中部署 PyTorch 模型
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/37/" class="md-nav__link">
        通过使用 Flask 的 REST API 在 Python 中部署 PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/38/" class="md-nav__link">
        TorchScript 简介
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/39/" class="md-nav__link">
        在 C-- 中加载 TorchScript 模型
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/40/" class="md-nav__link">
        将模型从 PyTorch 导出到 ONNX 并使用 ONNX 运行时运行它（可选）
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_7" >
      
      
      
        <label class="md-nav__link" for="__nav_3_7" id="__nav_3_7_label" tabindex="0">
          前端 API
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_7_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_7">
          <span class="md-nav__icon md-icon"></span>
          前端 API
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/42/" class="md-nav__link">
        PyTorch 中的命名张量简介（原型）
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/43/" class="md-nav__link">
        PyTorch 中通道在最后的内存格式（beta）
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/44/" class="md-nav__link">
        使用 PyTorch C-- 前端
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/45/" class="md-nav__link">
        自定义 C-- 和 CUDA 扩展
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/46/" class="md-nav__link">
        使用自定义 C-- 运算符扩展 TorchScript
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/47/" class="md-nav__link">
        使用自定义 C-- 类扩展 TorchScript
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/48/" class="md-nav__link">
        TorchScript 中的动态并行性
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/49/" class="md-nav__link">
        C-- 前端中的 Autograd
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/50/" class="md-nav__link">
        在 C-- 中注册调度运算符
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_8" >
      
      
      
        <label class="md-nav__link" for="__nav_3_8" id="__nav_3_8_label" tabindex="0">
          模型优化
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_8_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_8">
          <span class="md-nav__icon md-icon"></span>
          模型优化
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/52/" class="md-nav__link">
        分析您的 PyTorch 模块
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/53/" class="md-nav__link">
        使用 Ray Tune 的超参数调整
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/54/" class="md-nav__link">
        模型剪裁教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/55/" class="md-nav__link">
        LSTM 单词语言模型上的动态量化（beta）
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/56/" class="md-nav__link">
        BERT 上的动态量化（Beta）
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/57/" class="md-nav__link">
        PyTorch 中使用 Eager 模式的静态量化（beta）
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/58/" class="md-nav__link">
        计算机视觉的量化迁移学习教程（beta）
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_9" >
      
      
      
        <label class="md-nav__link" for="__nav_3_9" id="__nav_3_9_label" tabindex="0">
          并行和分布式训练
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_9_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_9">
          <span class="md-nav__icon md-icon"></span>
          并行和分布式训练
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/60/" class="md-nav__link">
        PyTorch 分布式概述
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/61/" class="md-nav__link">
        单机模型并行最佳实践
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/62/" class="md-nav__link">
        分布式数据并行入门
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/63/" class="md-nav__link">
        用 PyTorch 编写分布式应用
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/64/" class="md-nav__link">
        分布式 RPC 框架入门
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/65/" class="md-nav__link">
        使用分布式 RPC 框架实现参数服务器
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/66/" class="md-nav__link">
        使用 RPC 的分布式管道并行化
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/67/" class="md-nav__link">
        使用异步执行实现批量 RPC 处理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.7/68/" class="md-nav__link">
        将分布式DataParallel与分布式 RPC 框架相结合
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
      
      
      
        <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
          PyTorch 1.4 中文文档 & 教程
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 1.4 中文文档 & 教程
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1" >
      
      
      
        <label class="md-nav__link" for="__nav_4_1" id="__nav_4_1_label" tabindex="0">
          入门
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_1">
          <span class="md-nav__icon md-icon"></span>
          入门
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1_1" >
      
      
      
        <label class="md-nav__link" for="__nav_4_1_1" id="__nav_4_1_1_label" tabindex="0">
          使用 PyTorch 进行深度学习：60 分钟的闪电战
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_4_1_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_1_1">
          <span class="md-nav__icon md-icon"></span>
          使用 PyTorch 进行深度学习：60 分钟的闪电战
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/4/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/blitz/tensor_tutorial/" class="md-nav__link">
        什么是PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/blitz/autograd_tutorial/" class="md-nav__link">
        Autograd：自动求导
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/blitz/neural_networks_tutorial/" class="md-nav__link">
        神经网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/blitz/cifar10_tutorial/" class="md-nav__link">
        训练分类器
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/blitz/data_parallel_tutorial/" class="md-nav__link">
        可选：数据并行
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/5/" class="md-nav__link">
        编写自定义数据集，数据加载器和转换
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/6/" class="md-nav__link">
        使用 TensorBoard 可视化模型，数据和训练
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_2" >
      
      
      
        <label class="md-nav__link" for="__nav_4_2" id="__nav_4_2_label" tabindex="0">
          图片
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_2">
          <span class="md-nav__icon md-icon"></span>
          图片
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/8/" class="md-nav__link">
        TorchVision 对象检测微调教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/9/" class="md-nav__link">
        转移学习的计算机视觉教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/10/" class="md-nav__link">
        空间变压器网络教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/11/" class="md-nav__link">
        使用 PyTorch 进行神经传递
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/12/" class="md-nav__link">
        对抗示例生成
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/13/" class="md-nav__link">
        DCGAN 教程
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_3" >
      
      
      
        <label class="md-nav__link" for="__nav_4_3" id="__nav_4_3_label" tabindex="0">
          音频
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_3">
          <span class="md-nav__icon md-icon"></span>
          音频
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/15/" class="md-nav__link">
        torchaudio 教程
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_4" >
      
      
      
        <label class="md-nav__link" for="__nav_4_4" id="__nav_4_4_label" tabindex="0">
          文本
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_4">
          <span class="md-nav__icon md-icon"></span>
          文本
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/17/" class="md-nav__link">
        NLP From Scratch: 使用char-RNN对姓氏进行分类
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/18/" class="md-nav__link">
        NLP From Scratch: 生成名称与字符级RNN
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/19/" class="md-nav__link">
        NLP From Scratch: 基于注意力机制的 seq2seq 神经网络翻译
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/20/" class="md-nav__link">
        使用 TorchText 进行文本分类
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/21/" class="md-nav__link">
        使用 TorchText 进行语言翻译
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/22/" class="md-nav__link">
        使用 nn.Transformer 和 TorchText 进行序列到序列建模
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_5" >
      
      
      
        <label class="md-nav__link" for="__nav_4_5" id="__nav_4_5_label" tabindex="0">
          命名为 Tensor(实验性）
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_5">
          <span class="md-nav__icon md-icon"></span>
          命名为 Tensor(实验性）
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/24/" class="md-nav__link">
        (实验性)PyTorch 中的命名张量简介
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_6" >
      
      
      
        <label class="md-nav__link" for="__nav_4_6" id="__nav_4_6_label" tabindex="0">
          强化学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_6">
          <span class="md-nav__icon md-icon"></span>
          强化学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/26/" class="md-nav__link">
        强化学习(DQN)教程
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_7" >
      
      
      
        <label class="md-nav__link" for="__nav_4_7" id="__nav_4_7_label" tabindex="0">
          在生产中部署 PyTorch 模型
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_7_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_7">
          <span class="md-nav__icon md-icon"></span>
          在生产中部署 PyTorch 模型
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/28/" class="md-nav__link">
        通过带有 Flask 的 REST API 在 Python 中部署 PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/29/" class="md-nav__link">
        TorchScript 简介
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/30/" class="md-nav__link">
        在 C --中加载 TorchScript 模型
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/31/" class="md-nav__link">
        (可选）将模型从 PyTorch 导出到 ONNX 并使用 ONNX Runtime 运行
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_8" >
      
      
      
        <label class="md-nav__link" for="__nav_4_8" id="__nav_4_8_label" tabindex="0">
          并行和分布式训练
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_8_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_8">
          <span class="md-nav__icon md-icon"></span>
          并行和分布式训练
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/33/" class="md-nav__link">
        单机模型并行最佳实践
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/34/" class="md-nav__link">
        分布式数据并行入门
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/35/" class="md-nav__link">
        用 PyTorch 编写分布式应用程序
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/36/" class="md-nav__link">
        分布式 RPC 框架入门
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/37/" class="md-nav__link">
        (高级）带有 Amazon AWS 的 PyTorch 1.0 分布式训练师
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_9" >
      
      
      
        <label class="md-nav__link" for="__nav_4_9" id="__nav_4_9_label" tabindex="0">
          扩展 PyTorch
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_9_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_9">
          <span class="md-nav__icon md-icon"></span>
          扩展 PyTorch
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/39/" class="md-nav__link">
        使用自定义 C --运算符扩展 TorchScript
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/40/" class="md-nav__link">
        使用自定义 C --类扩展 TorchScript
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/41/" class="md-nav__link">
        使用 numpy 和 scipy 创建扩展
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/42/" class="md-nav__link">
        自定义 C --和 CUDA 扩展
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_10" >
      
      
      
        <label class="md-nav__link" for="__nav_4_10" id="__nav_4_10_label" tabindex="0">
          模型优化
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_10_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_10">
          <span class="md-nav__icon md-icon"></span>
          模型优化
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/44/" class="md-nav__link">
        LSTM Word 语言模型上的(实验）动态量化
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/45/" class="md-nav__link">
        (实验性）在 PyTorch 中使用 Eager 模式进行静态量化
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/46/" class="md-nav__link">
        (实验性）计算机视觉教程的量化转移学习
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/47/" class="md-nav__link">
        (实验）BERT 上的动态量化
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/48/" class="md-nav__link">
        修剪教程
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_11" >
      
      
      
        <label class="md-nav__link" for="__nav_4_11" id="__nav_4_11_label" tabindex="0">
          PyTorch 用其他语言
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_11_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_11">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 用其他语言
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/50/" class="md-nav__link">
        使用 PyTorch C --前端
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_12" >
      
      
      
        <label class="md-nav__link" for="__nav_4_12" id="__nav_4_12_label" tabindex="0">
          PyTorch 基础知识
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_12_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_12">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 基础知识
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/52/" class="md-nav__link">
        通过示例学习 PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/53/" class="md-nav__link">
        torch.nn 到底是什么？
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_13" >
      
      
      
        <label class="md-nav__link" for="__nav_4_13" id="__nav_4_13_label" tabindex="0">
          笔记
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_13_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_13">
          <span class="md-nav__icon md-icon"></span>
          笔记
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/56/" class="md-nav__link">
        自动求导机制
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/57/" class="md-nav__link">
        广播语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/58/" class="md-nav__link">
        CPU 线程和 TorchScript 推断
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/59/" class="md-nav__link">
        CUDA 语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/60/" class="md-nav__link">
        分布式 Autograd 设计
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/61/" class="md-nav__link">
        扩展 PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/62/" class="md-nav__link">
        经常问的问题
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/63/" class="md-nav__link">
        大规模部署的功能
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/64/" class="md-nav__link">
        并行处理最佳实践
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/65/" class="md-nav__link">
        重现性
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/66/" class="md-nav__link">
        远程参考协议
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/67/" class="md-nav__link">
        序列化语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/68/" class="md-nav__link">
        Windows 常见问题
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/69/" class="md-nav__link">
        XLA 设备上的 PyTorch
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_14" >
      
      
      
        <label class="md-nav__link" for="__nav_4_14" id="__nav_4_14_label" tabindex="0">
          语言绑定
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_14_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_14">
          <span class="md-nav__icon md-icon"></span>
          语言绑定
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/71/" class="md-nav__link">
        PyTorch C -- API
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/72/" class="md-nav__link">
        PyTorch Java API
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_15" >
      
      
      
        <label class="md-nav__link" for="__nav_4_15" id="__nav_4_15_label" tabindex="0">
          Python API
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_15_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_15">
          <span class="md-nav__icon md-icon"></span>
          Python API
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/74/" class="md-nav__link">
        torch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/75/" class="md-nav__link">
        torch.nn
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/76/" class="md-nav__link">
        torch功能
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/77/" class="md-nav__link">
        torch张量
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/78/" class="md-nav__link">
        张量属性
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/79/" class="md-nav__link">
        自动差分包-Torch.Autograd
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/80/" class="md-nav__link">
        torch.cuda
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/81/" class="md-nav__link">
        分布式通讯包-Torch.Distributed
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/82/" class="md-nav__link">
        概率分布-torch分布
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/83/" class="md-nav__link">
        torch.hub
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/84/" class="md-nav__link">
        torch脚本
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/85/" class="md-nav__link">
        torch.nn.init
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/86/" class="md-nav__link">
        torch.onnx
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/87/" class="md-nav__link">
        torch.optim
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/88/" class="md-nav__link">
        量化
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/89/" class="md-nav__link">
        分布式 RPC 框架
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/90/" class="md-nav__link">
        torch随机
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/91/" class="md-nav__link">
        torch稀疏
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/92/" class="md-nav__link">
        torch存储
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/93/" class="md-nav__link">
        torch.utils.bottleneck
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/94/" class="md-nav__link">
        torch.utils.checkpoint
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/95/" class="md-nav__link">
        torch.utils.cpp_extension
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/96/" class="md-nav__link">
        torch.utils.data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/97/" class="md-nav__link">
        torch.utils.dlpack
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/98/" class="md-nav__link">
        torch.utils.model_zoo
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/99/" class="md-nav__link">
        torch.utils.tensorboard
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/100/" class="md-nav__link">
        类型信息
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/101/" class="md-nav__link">
        命名张量
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/102/" class="md-nav__link">
        命名为 Tensors 操作员范围
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/103/" class="md-nav__link">
        糟糕！
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_16" >
      
      
      
        <label class="md-nav__link" for="__nav_4_16" id="__nav_4_16_label" tabindex="0">
          torchvision参考
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_16_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_16">
          <span class="md-nav__icon md-icon"></span>
          torchvision参考
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/105/" class="md-nav__link">
        torchvision
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_17" >
      
      
      
        <label class="md-nav__link" for="__nav_4_17" id="__nav_4_17_label" tabindex="0">
          音频参考
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_17_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_17">
          <span class="md-nav__icon md-icon"></span>
          音频参考
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/107/" class="md-nav__link">
        torchaudio
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_18" >
      
      
      
        <label class="md-nav__link" for="__nav_4_18" id="__nav_4_18_label" tabindex="0">
          torchtext参考
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_18_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_18">
          <span class="md-nav__icon md-icon"></span>
          torchtext参考
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/109/" class="md-nav__link">
        torchtext
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_19" >
      
      
      
        <label class="md-nav__link" for="__nav_4_19" id="__nav_4_19_label" tabindex="0">
          社区
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_19_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_19">
          <span class="md-nav__icon md-icon"></span>
          社区
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/111/" class="md-nav__link">
        PyTorch 贡献指南
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/112/" class="md-nav__link">
        PyTorch 治理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.4/113/" class="md-nav__link">
        PyTorch 治理| 感兴趣的人
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
      
      
      
        <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
          PyTorch 1.0 中文文档 & 教程
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 1.0 中文文档 & 教程
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/" class="md-nav__link">
        目录
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2" id="__nav_5_2_label" tabindex="0">
          中文教程
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2">
          <span class="md-nav__icon md-icon"></span>
          中文教程
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2_1" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2_1" id="__nav_5_2_1_label" tabindex="0">
          入门
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_2_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2_1">
          <span class="md-nav__icon md-icon"></span>
          入门
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2_1_1" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2_1_1" id="__nav_5_2_1_1_label" tabindex="0">
          PyTorch 深度学习: 60 分钟极速入门
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_5_2_1_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2_1_1">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 深度学习: 60 分钟极速入门
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/deep_learning_60min_blitz/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/blitz_tensor_tutorial/" class="md-nav__link">
        什么是 PyTorch？
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/blitz_autograd_tutorial/" class="md-nav__link">
        Autograd：自动求导
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/blitz_neural_networks_tutorial/" class="md-nav__link">
        神经网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/blitz_cifar10_tutorial/" class="md-nav__link">
        训练分类器
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/blitz_data_parallel_tutorial/" class="md-nav__link">
        可选：数据并行处理
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/data_loading_tutorial/" class="md-nav__link">
        数据加载和处理教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/pytorch_with_examples/" class="md-nav__link">
        用例子学习 PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/transfer_learning_tutorial/" class="md-nav__link">
        迁移学习教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/deploy_seq2seq_hybrid_frontend_tutorial/" class="md-nav__link">
        混合前端的 seq2seq 模型部署
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/saving_loading_models/" class="md-nav__link">
        Saving and Loading Models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/nn_tutorial/" class="md-nav__link">
        What is torch.nn really?
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2_2" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2_2" id="__nav_5_2_2_label" tabindex="0">
          图像
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_2_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2_2">
          <span class="md-nav__icon md-icon"></span>
          图像
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/finetuning_torchvision_models_tutorial/" class="md-nav__link">
        Torchvision 模型微调
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/spatial_transformer_tutorial/" class="md-nav__link">
        空间变换器网络教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/neural_style_tutorial/" class="md-nav__link">
        使用 PyTorch 进行图像风格转换
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/fgsm_tutorial/" class="md-nav__link">
        对抗性示例生成
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/super_resolution_with_caffe2/" class="md-nav__link">
        使用 ONNX 将模型从 PyTorch 传输到 Caffe2 和移动端
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2_3" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2_3" id="__nav_5_2_3_label" tabindex="0">
          文本
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_2_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2_3">
          <span class="md-nav__icon md-icon"></span>
          文本
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/chatbot_tutorial/" class="md-nav__link">
        聊天机器人教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/char_rnn_generation_tutorial/" class="md-nav__link">
        使用字符级别特征的 RNN 网络生成姓氏
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/char_rnn_classification_tutorial/" class="md-nav__link">
        使用字符级别特征的 RNN 网络进行姓氏分类
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2_3_4" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2_3_4" id="__nav_5_2_3_4_label" tabindex="0">
          Deep Learning for NLP with Pytorch
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_5_2_3_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2_3_4">
          <span class="md-nav__icon md-icon"></span>
          Deep Learning for NLP with Pytorch
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/deep_learning_nlp_tutorial/" class="md-nav__link">
        在深度学习和 NLP 中使用 Pytorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/nlp_pytorch_tutorial/" class="md-nav__link">
        PyTorch 介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/nlp_deep_learning_tutorial/" class="md-nav__link">
        使用 PyTorch 进行深度学习
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/nlp_word_embeddings_tutorial/" class="md-nav__link">
        Word Embeddings: Encoding Lexical Semantics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/nlp_sequence_models_tutorial/" class="md-nav__link">
        序列模型和 LSTM 网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/nlp_advanced_tutorial/" class="md-nav__link">
        Advanced: Making Dynamic Decisions and the Bi-LSTM CRF
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/seq2seq_translation_tutorial/" class="md-nav__link">
        基于注意力机制的 seq2seq 神经网络翻译
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2_4" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2_4" id="__nav_5_2_4_label" tabindex="0">
          生成
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_2_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2_4">
          <span class="md-nav__icon md-icon"></span>
          生成
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/dcgan_faces_tutorial/" class="md-nav__link">
        DCGAN Tutorial
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2_5" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2_5" id="__nav_5_2_5_label" tabindex="0">
          强化学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_2_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2_5">
          <span class="md-nav__icon md-icon"></span>
          强化学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/reinforcement_q_learning/" class="md-nav__link">
        Reinforcement Learning (DQN) Tutorial
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2_6" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2_6" id="__nav_5_2_6_label" tabindex="0">
          扩展 PyTorch
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_2_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2_6">
          <span class="md-nav__icon md-icon"></span>
          扩展 PyTorch
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/numpy_extensions_tutorial/" class="md-nav__link">
        用 numpy 和 scipy 创建扩展
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/cpp_extension/" class="md-nav__link">
        Custom C-- and CUDA Extensions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/torch_script_custom_ops/" class="md-nav__link">
        Extending TorchScript with Custom C-- Operators
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2_7" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2_7" id="__nav_5_2_7_label" tabindex="0">
          生产性使用
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_2_7_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2_7">
          <span class="md-nav__icon md-icon"></span>
          生产性使用
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/dist_tuto/" class="md-nav__link">
        Writing Distributed Applications with PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/aws_distributed_training_tutorial/" class="md-nav__link">
        使用 Amazon AWS 进行分布式训练
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/ONNXLive/" class="md-nav__link">
        ONNX 现场演示教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/cpp_export/" class="md-nav__link">
        在 C-- 中加载 PYTORCH 模型
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2_8" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2_8" id="__nav_5_2_8_label" tabindex="0">
          其它语言中的 PyTorch
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_2_8_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2_8">
          <span class="md-nav__icon md-icon"></span>
          其它语言中的 PyTorch
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/cpp_frontend/" class="md-nav__link">
        使用 PyTorch C-- 前端
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3" >
      
      
      
        <label class="md-nav__link" for="__nav_5_3" id="__nav_5_3_label" tabindex="0">
          中文文档
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_3">
          <span class="md-nav__icon md-icon"></span>
          中文文档
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3_1" >
      
      
      
        <label class="md-nav__link" for="__nav_5_3_1" id="__nav_5_3_1_label" tabindex="0">
          注解
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_3_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_3_1">
          <span class="md-nav__icon md-icon"></span>
          注解
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/notes_autograd/" class="md-nav__link">
        自动求导机制
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/notes_broadcasting/" class="md-nav__link">
        广播语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/notes_cuda/" class="md-nav__link">
        CUDA 语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/notes_extending/" class="md-nav__link">
        Extending PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/notes_faq/" class="md-nav__link">
        Frequently Asked Questions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/notes_multiprocessing/" class="md-nav__link">
        Multiprocessing best practices
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/notes_randomness/" class="md-nav__link">
        Reproducibility
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/notes_serialization/" class="md-nav__link">
        Serialization semantics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/notes_windows/" class="md-nav__link">
        Windows FAQ
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3_2" >
      
      
      
        <label class="md-nav__link" for="__nav_5_3_2" id="__nav_5_3_2_label" tabindex="0">
          包参考
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_3_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_3_2">
          <span class="md-nav__icon md-icon"></span>
          包参考
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3_2_1" >
      
      
      
        <label class="md-nav__link" for="__nav_5_3_2_1" id="__nav_5_3_2_1_label" tabindex="0">
          torch
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_5_3_2_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_3_2_1">
          <span class="md-nav__icon md-icon"></span>
          torch
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/torch/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/torch_tensors/" class="md-nav__link">
        Tensors
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/torch_random_sampling/" class="md-nav__link">
        Random sampling
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/torch_serialization_parallelism_utilities/" class="md-nav__link">
        Serialization, Parallelism, Utilities
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3_2_1_5" >
      
      
      
        <label class="md-nav__link" for="__nav_5_3_2_1_5" id="__nav_5_3_2_1_5_label" tabindex="0">
          Math operations
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="5" aria-labelledby="__nav_5_3_2_1_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_3_2_1_5">
          <span class="md-nav__icon md-icon"></span>
          Math operations
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/torch_math_operations_pointwise_ops/" class="md-nav__link">
        Pointwise Ops
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/torch_math_operations_reduction_ops/" class="md-nav__link">
        Reduction Ops
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/torch_math_operations_comparison_ops/" class="md-nav__link">
        Comparison Ops
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/torch_math_operations_spectral_ops/" class="md-nav__link">
        Spectral Ops
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/torch_math_operations_other_ops/" class="md-nav__link">
        Other Operations
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/torch_math_operations_blas_lapack_ops/" class="md-nav__link">
        BLAS and LAPACK Operations
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/tensors/" class="md-nav__link">
        torch.Tensor
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/tensor_attributes/" class="md-nav__link">
        Tensor Attributes
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/type_info/" class="md-nav__link">
        数据类型信息
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/sparse/" class="md-nav__link">
        torch.sparse
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/cuda/" class="md-nav__link">
        torch.cuda
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/storage/" class="md-nav__link">
        torch.Storage
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/nn/" class="md-nav__link">
        torch.nn
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/nn_functional/" class="md-nav__link">
        torch.nn.functional
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/nn_init/" class="md-nav__link">
        torch.nn.init
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/optim/" class="md-nav__link">
        torch.optim
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/autograd/" class="md-nav__link">
        Automatic differentiation package - torch.autograd
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/distributed/" class="md-nav__link">
        Distributed communication package - torch.distributed
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/distributions/" class="md-nav__link">
        Probability distributions - torch.distributions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/jit/" class="md-nav__link">
        Torch Script
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/multiprocessing/" class="md-nav__link">
        多进程包 - torch.multiprocessing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/bottleneck/" class="md-nav__link">
        torch.utils.bottleneck
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/checkpoint/" class="md-nav__link">
        torch.utils.checkpoint
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/docs_cpp_extension/" class="md-nav__link">
        torch.utils.cpp_extension
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/data/" class="md-nav__link">
        torch.utils.data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/dlpack/" class="md-nav__link">
        torch.utils.dlpack
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/hub/" class="md-nav__link">
        torch.hub
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/model_zoo/" class="md-nav__link">
        torch.utils.model_zoo
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/onnx/" class="md-nav__link">
        torch.onnx
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/distributed_deprecated/" class="md-nav__link">
        Distributed communication package (deprecated) - torch.distributed.deprecated
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3_3" >
      
      
      
        <label class="md-nav__link" for="__nav_5_3_3" id="__nav_5_3_3_label" tabindex="0">
          torchvision 参考
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_3_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_3_3">
          <span class="md-nav__icon md-icon"></span>
          torchvision 参考
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/docs_torchvision_ref/" class="md-nav__link">
        目录
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/torchvision_datasets/" class="md-nav__link">
        torchvision.datasets
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/torchvision_models/" class="md-nav__link">
        torchvision.models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/torchvision_transforms/" class="md-nav__link">
        torchvision.transforms
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1.0/torchvision_utils/" class="md-nav__link">
        torchvision.utils
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
      
      
      
        <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
          PyTorch 0.4 中文文档
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 0.4 中文文档
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_1" >
      
      
      
        <label class="md-nav__link" for="__nav_6_1" id="__nav_6_1_label" tabindex="0">
          笔记
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_6_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_6_1">
          <span class="md-nav__icon md-icon"></span>
          笔记
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.4/1/" class="md-nav__link">
        自动求导机制
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.4/2/" class="md-nav__link">
        广播语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.4/3/" class="md-nav__link">
        CUDA 语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.4/4/" class="md-nav__link">
        扩展 PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.4/5/" class="md-nav__link">
        常见问题
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.4/6/" class="md-nav__link">
        多进程最佳实践
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.4/7/" class="md-nav__link">
        序列化语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.4/8/" class="md-nav__link">
        Windows 常见问题
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_2" >
      
      
      
        <label class="md-nav__link" for="__nav_6_2" id="__nav_6_2_label" tabindex="0">
          包参考
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_6_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_6_2">
          <span class="md-nav__icon md-icon"></span>
          包参考
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.4/10/" class="md-nav__link">
        Torch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.4/11/" class="md-nav__link">
        torch.Tensor
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.4/12/" class="md-nav__link">
        Tensor Attributes
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.4/13/" class="md-nav__link">
        torch.sparse
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.4/14/" class="md-nav__link">
        torch.cuda
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.4/15/" class="md-nav__link">
        torch.Storage
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.4/16/" class="md-nav__link">
        torch.nn
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.4/17/" class="md-nav__link">
        torch.nn.functional
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.4/18/" class="md-nav__link">
        自动差异化包 - torch.autograd
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.4/19/" class="md-nav__link">
        torch.optim
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.4/20/" class="md-nav__link">
        torch.nn.init
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.4/21/" class="md-nav__link">
        torch.distributions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.4/22/" class="md-nav__link">
        Multiprocessing 包 - torch.multiprocessing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.4/23/" class="md-nav__link">
        分布式通讯包 - torch.distributed
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.4/24/" class="md-nav__link">
        torch.utils.bottleneck
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.4/25/" class="md-nav__link">
        torch.utils.checkpoint
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.4/26/" class="md-nav__link">
        torch.utils.cpp_extension
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.4/27/" class="md-nav__link">
        torch.utils.data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.4/28/" class="md-nav__link">
        torch.utils.ffi
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.4/29/" class="md-nav__link">
        torch.utils.model_zoo
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.4/30/" class="md-nav__link">
        torch.onnx
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.4/31/" class="md-nav__link">
        遗留包 - torch.legacy
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_3" >
      
      
      
        <label class="md-nav__link" for="__nav_6_3" id="__nav_6_3_label" tabindex="0">
          torchvision 参考
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_6_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_6_3">
          <span class="md-nav__icon md-icon"></span>
          torchvision 参考
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.4/33/" class="md-nav__link">
        torchvision
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.4/34/" class="md-nav__link">
        torchvision.datasets
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.4/35/" class="md-nav__link">
        torchvision.models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.4/36/" class="md-nav__link">
        torchvision.transform
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.4/37/" class="md-nav__link">
        torchvision.utils
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
      
      
      
        <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
          PyTorch 0.3 中文文档 & 教程
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 0.3 中文文档 & 教程
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/" class="md-nav__link">
        目录
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_2" >
      
      
      
        <label class="md-nav__link" for="__nav_7_2" id="__nav_7_2_label" tabindex="0">
          中文教程
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_7_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_2">
          <span class="md-nav__icon md-icon"></span>
          中文教程
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_2_1" >
      
      
      
        <label class="md-nav__link" for="__nav_7_2_1" id="__nav_7_2_1_label" tabindex="0">
          初学者教程
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_7_2_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_2_1">
          <span class="md-nav__icon md-icon"></span>
          初学者教程
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_2_1_1" >
      
      
      
        <label class="md-nav__link" for="__nav_7_2_1_1" id="__nav_7_2_1_1_label" tabindex="0">
          PyTorch 深度学习: 60 分钟极速入门教程
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_7_2_1_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_2_1_1">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 深度学习: 60 分钟极速入门教程
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/deep_learning_60min_blitz/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/blitz_tensor_tutorial/" class="md-nav__link">
        PyTorch 是什么？
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/blitz_autograd_tutorial/" class="md-nav__link">
        自动求导: 自动微分
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/blitz_neural_networks_tutorial/" class="md-nav__link">
        神经网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/blitz_cifar10_tutorial/" class="md-nav__link">
        训练一个分类器
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/blitz_data_parallel_tutorial/" class="md-nav__link">
        可选: 数据并行
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_2_1_2" >
      
      
      
        <label class="md-nav__link" for="__nav_7_2_1_2" id="__nav_7_2_1_2_label" tabindex="0">
          PyTorch for former Torch users
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_7_2_1_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_2_1_2">
          <span class="md-nav__icon md-icon"></span>
          PyTorch for former Torch users
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/former_torchies_tutorial/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/former_torchies_tensor_tutorial/" class="md-nav__link">
        Tensors
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/former_torchies_autograd_tutorial/" class="md-nav__link">
        Autograd (自动求导)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/former_torchies_nn_tutorial/" class="md-nav__link">
        nn package
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/former_torchies_parallelism_tutorial/" class="md-nav__link">
        Multi-GPU examples
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_2_1_3" >
      
      
      
        <label class="md-nav__link" for="__nav_7_2_1_3" id="__nav_7_2_1_3_label" tabindex="0">
          跟着例子学习 PyTorch
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_7_2_1_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_2_1_3">
          <span class="md-nav__icon md-icon"></span>
          跟着例子学习 PyTorch
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/pytorch_with_examples/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/pytorch_with_examples_warm-up-numpy/" class="md-nav__link">
        Warm-up: numpy
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/pytorch_with_examples_pytorch-tensors/" class="md-nav__link">
        PyTorch: Tensors
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/pytorch_with_examples_pytorch-variables-and-autograd/" class="md-nav__link">
        PyTorch: 变量和autograd
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/pytorch_with_examples_pytorch-defining-new-autograd-functions/" class="md-nav__link">
        PyTorch: 定义新的autograd函数
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/pytorch_with_examples_tensorflow-static-graphs/" class="md-nav__link">
        TensorFlow: 静态图
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/pytorch_with_examples_pytorch-nn/" class="md-nav__link">
        PyTorch: nn包
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/pytorch_with_examples_pytorch-optim/" class="md-nav__link">
        PyTorch: optim包
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/pytorch_with_examples_pytorch-custom-nn-modules/" class="md-nav__link">
        PyTorch: 定制化nn模块
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/pytorch_with_examples_pytorch-control-flow-weight-sharing/" class="md-nav__link">
        PyTorch: 动态控制流程 - 权重共享
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/transfer_learning_tutorial/" class="md-nav__link">
        迁移学习教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/data_loading_tutorial/" class="md-nav__link">
        数据加载和处理教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_2_1_6" >
      
      
      
        <label class="md-nav__link" for="__nav_7_2_1_6" id="__nav_7_2_1_6_label" tabindex="0">
          针对NLP的Pytorch深度学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_7_2_1_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_2_1_6">
          <span class="md-nav__icon md-icon"></span>
          针对NLP的Pytorch深度学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/deep_learning_nlp_tutorial/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/nlp_pytorch_tutorial/" class="md-nav__link">
        PyTorch介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/nlp_deep_learning_tutorial/" class="md-nav__link">
        PyTorch深度学习
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/nlp_word_embeddings_tutorial/" class="md-nav__link">
        词汇嵌入:编码词汇语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/nlp_sequence_models_tutorial/" class="md-nav__link">
        序列模型和 LSTM 网络(长短记忆网络）
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/nlp_advanced_tutorial/" class="md-nav__link">
        高级教程: 作出动态决策和 Bi-LSTM CRF
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_2_2" >
      
      
      
        <label class="md-nav__link" for="__nav_7_2_2" id="__nav_7_2_2_label" tabindex="0">
          中级教程
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_7_2_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_2_2">
          <span class="md-nav__icon md-icon"></span>
          中级教程
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/char_rnn_classification_tutorial/" class="md-nav__link">
        用字符级RNN分类名称
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/char_rnn_generation_tutorial/" class="md-nav__link">
        基与字符级RNN(Char-RNN）的人名生成
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/seq2seq_translation_tutorial/" class="md-nav__link">
        用基于注意力机制的seq2seq神经网络进行翻译
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/reinforcement_q_learning/" class="md-nav__link">
        强化学习(DQN）教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/dist_tuto/" class="md-nav__link">
        Writing Distributed Applications with PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/spatial_transformer_tutorial/" class="md-nav__link">
        空间转换网络 (Spatial Transformer Networks) 教程
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_2_3" >
      
      
      
        <label class="md-nav__link" for="__nav_7_2_3" id="__nav_7_2_3_label" tabindex="0">
          高级教程
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_7_2_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_2_3">
          <span class="md-nav__icon md-icon"></span>
          高级教程
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/neural_style_tutorial/" class="md-nav__link">
        用 PyTorch 做 神经转换 (Neural Transfer)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/numpy_extensions_tutorial/" class="md-nav__link">
        使用 numpy 和 scipy 创建扩展
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/super_resolution_with_caffe2/" class="md-nav__link">
        使用 ONNX 将模型从 PyTorch 迁移到 Caffe2 和 Mobile
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/c_extension/" class="md-nav__link">
        为 pytorch 自定义 C 扩展
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_3" >
      
      
      
        <label class="md-nav__link" for="__nav_7_3" id="__nav_7_3_label" tabindex="0">
          中文文档
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_7_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_3">
          <span class="md-nav__icon md-icon"></span>
          中文文档
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_3_1" >
      
      
      
        <label class="md-nav__link" for="__nav_7_3_1" id="__nav_7_3_1_label" tabindex="0">
          介绍
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_7_3_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_3_1">
          <span class="md-nav__icon md-icon"></span>
          介绍
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/notes_autograd/" class="md-nav__link">
        自动求导机制
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/notes_broadcasting/" class="md-nav__link">
        广播语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/notes_cuda/" class="md-nav__link">
        CUDA 语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/notes_extending/" class="md-nav__link">
        扩展 PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/notes_multiprocessing/" class="md-nav__link">
        多进程的最佳实践
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/notes_serialization/" class="md-nav__link">
        序列化语义
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_3_2" >
      
      
      
        <label class="md-nav__link" for="__nav_7_3_2" id="__nav_7_3_2_label" tabindex="0">
          Package 参考
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_7_3_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_3_2">
          <span class="md-nav__icon md-icon"></span>
          Package 参考
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/torch/" class="md-nav__link">
        torch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/tensors/" class="md-nav__link">
        torch.Tensor
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/sparse/" class="md-nav__link">
        torch.sparse
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/storage/" class="md-nav__link">
        torch.Storage
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/nn/" class="md-nav__link">
        torch.nn
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/optim/" class="md-nav__link">
        torch.optim
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/autograd/" class="md-nav__link">
        Automatic differentiation package - torch.autograd
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/distributions/" class="md-nav__link">
        Probability distributions - torch.distributions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/multiprocessing/" class="md-nav__link">
        Multiprocessing package - torch.multiprocessing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/distributed/" class="md-nav__link">
        Distributed communication package - torch.distributed
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/legacy/" class="md-nav__link">
        Legacy package - torch.legacy
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/cuda/" class="md-nav__link">
        torch.cuda
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/ffi/" class="md-nav__link">
        torch.utils.ffi
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/data/" class="md-nav__link">
        torch.utils.data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/model_zoo/" class="md-nav__link">
        torch.utils.model_zoo
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/onnx/" class="md-nav__link">
        torch.onnx
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_3_3" >
      
      
      
        <label class="md-nav__link" for="__nav_7_3_3" id="__nav_7_3_3_label" tabindex="0">
          torchvision 参考
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_7_3_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_3_3">
          <span class="md-nav__icon md-icon"></span>
          torchvision 参考
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/torchvision/" class="md-nav__link">
        torchvision
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/datasets/" class="md-nav__link">
        torchvision.datasets
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/models/" class="md-nav__link">
        torchvision.models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/transforms/" class="md-nav__link">
        torchvision.transforms
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../0.3/utils/" class="md-nav__link">
        torchvision.utils
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8" checked>
      
      
      
        <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0">
          PyTorch 0.2 中文文档
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="true">
        <label class="md-nav__title" for="__nav_8">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 0.2 中文文档
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8_2" >
      
      
      
        <label class="md-nav__link" for="__nav_8_2" id="__nav_8_2_label" tabindex="0">
          说明
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_8_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_8_2">
          <span class="md-nav__icon md-icon"></span>
          说明
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../notes/autograd/" class="md-nav__link">
        自动求导机制
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../notes/cuda/" class="md-nav__link">
        CUDA语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../notes/extending/" class="md-nav__link">
        扩展PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../notes/multiprocessing/" class="md-nav__link">
        多进程最佳实践
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../notes/serialization/" class="md-nav__link">
        序列化语义
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8_3" checked>
      
      
      
        <label class="md-nav__link" for="__nav_8_3" id="__nav_8_3_label" tabindex="0">
          PACKAGE参考
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_8_3_label" aria-expanded="true">
        <label class="md-nav__title" for="__nav_8_3">
          <span class="md-nav__icon md-icon"></span>
          PACKAGE参考
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../torch/" class="md-nav__link">
        torch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Tensor/" class="md-nav__link">
        torch.Tensor
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Storage/" class="md-nav__link">
        torch.Storage
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          torch.nn
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        torch.nn
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#parameters" class="md-nav__link">
    Parameters
  </a>
  
    <nav class="md-nav" aria-label="Parameters">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#class-torchnnparameter" class="md-nav__link">
    class torch.nn.Parameter()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#containers" class="md-nav__link">
    Containers(容器）：
  </a>
  
    <nav class="md-nav" aria-label="Containers(容器）：">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#class-torchnnmodule" class="md-nav__link">
    class torch.nn.Module
  </a>
  
    <nav class="md-nav" aria-label="class torch.nn.Module">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#add_modulename-module" class="md-nav__link">
    add_module(name, module)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#children" class="md-nav__link">
    children()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cpudevice_idnone" class="md-nav__link">
    cpu(device_id=None)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cudadevice_idnone" class="md-nav__link">
    cuda(device_id=None)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#double" class="md-nav__link">
    double()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#eval" class="md-nav__link">
    eval()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#float" class="md-nav__link">
    float()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#forward-input" class="md-nav__link">
    forward(* input)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#half" class="md-nav__link">
    half()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#load_state_dictstate_dict" class="md-nav__link">
    load_state_dict(state_dict)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#modules" class="md-nav__link">
    modules()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#named_children" class="md-nav__link">
    named_children()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#named_modulesmemonone-prefixsource" class="md-nav__link">
    named_modules(memo=None, prefix='')[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#parametersmemonone" class="md-nav__link">
    parameters(memo=None)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#register_backward_hookhook" class="md-nav__link">
    register_backward_hook(hook)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#register_buffername-tensor" class="md-nav__link">
    register_buffer(name, tensor)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#register_forward_hookhook" class="md-nav__link">
    register_forward_hook(hook)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#register_parametername-param" class="md-nav__link">
    register_parameter(name, param)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#state_dictdestinationnone-prefixsource" class="md-nav__link">
    state_dict(destination=None, prefix='')[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#trainmodetrue" class="md-nav__link">
    train(mode=True)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#zero_grad" class="md-nav__link">
    zero_grad()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnsequential-args" class="md-nav__link">
    class torch.nn.Sequential(* args)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnmodulelistmodulesnonesource" class="md-nav__link">
    class torch.nn.ModuleList(modules=None)[source]
  </a>
  
    <nav class="md-nav" aria-label="class torch.nn.ModuleList(modules=None)[source]">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#appendmodulesource" class="md-nav__link">
    append(module)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#extendmodulessource" class="md-nav__link">
    extend(modules)[source]
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnparameterlistparametersnone" class="md-nav__link">
    class torch.nn.ParameterList(parameters=None)
  </a>
  
    <nav class="md-nav" aria-label="class torch.nn.ParameterList(parameters=None)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#appendparametersource" class="md-nav__link">
    append(parameter)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#extendparameterssource" class="md-nav__link">
    extend(parameters)[source]
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    卷积层
  </a>
  
    <nav class="md-nav" aria-label="卷积层">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#class-torchnnconv1din_channels-out_channels-kernel_size-stride1-padding0-dilation1-groups1-biastrue" class="md-nav__link">
    class torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnconv2din_channels-out_channels-kernel_size-stride1-padding0-dilation1-groups1-biastrue" class="md-nav__link">
    class torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnconv3din_channels-out_channels-kernel_size-stride1-padding0-dilation1-groups1-biastrue" class="md-nav__link">
    class torch.nn.Conv3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnconvtranspose1din_channels-out_channels-kernel_size-stride1-padding0-output_padding0-groups1-biastrue" class="md-nav__link">
    class torch.nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnconvtranspose2din_channels-out_channels-kernel_size-stride1-padding0-output_padding0-groups1-biastrue" class="md-nav__link">
    class torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchnnconvtranspose3din_channels-out_channels-kernel_size-stride1-padding0-output_padding0-groups1-biastrue" class="md-nav__link">
    torch.nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    池化层
  </a>
  
    <nav class="md-nav" aria-label="池化层">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#class-torchnnmaxpool1dkernel_size-stridenone-padding0-dilation1-return_indicesfalse-ceil_modefalse" class="md-nav__link">
    class torch.nn.MaxPool1d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnmaxpool2dkernel_size-stridenone-padding0-dilation1-return_indicesfalse-ceil_modefalse" class="md-nav__link">
    class torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnmaxpool3dkernel_size-stridenone-padding0-dilation1-return_indicesfalse-ceil_modefalse" class="md-nav__link">
    class torch.nn.MaxPool3d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)
  </a>
  
    <nav class="md-nav" aria-label="class torch.nn.MaxPool3d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#class-torchnnmaxunpool1dkernel_size-stridenone-padding0" class="md-nav__link">
    class torch.nn.MaxUnpool1d(kernel_size, stride=None, padding=0)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnmaxunpool2dkernel_size-stridenone-padding0" class="md-nav__link">
    class torch.nn.MaxUnpool2d(kernel_size, stride=None, padding=0)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnmaxunpool3dkernel_size-stridenone-padding0" class="md-nav__link">
    class torch.nn.MaxUnpool3d(kernel_size, stride=None, padding=0)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnavgpool1dkernel_size-stridenone-padding0-ceil_modefalse-count_include_padtrue" class="md-nav__link">
    class torch.nn.AvgPool1d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnavgpool2dkernel_size-stridenone-padding0-ceil_modefalse-count_include_padtrue" class="md-nav__link">
    class torch.nn.AvgPool2d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnavgpool3dkernel_size-stridenone" class="md-nav__link">
    class torch.nn.AvgPool3d(kernel_size, stride=None)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnfractionalmaxpool2dkernel_size-output_sizenone-output_rationone-return_indicesfalse-_random_samplesnone" class="md-nav__link">
    class torch.nn.FractionalMaxPool2d(kernel_size, output_size=None, output_ratio=None, return_indices=False, _random_samples=None)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnlppool2dnorm_type-kernel_size-stridenone-ceil_modefalse" class="md-nav__link">
    class torch.nn.LPPool2d(norm_type, kernel_size, stride=None, ceil_mode=False)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnadaptivemaxpool1doutput_size-return_indicesfalse" class="md-nav__link">
    class torch.nn.AdaptiveMaxPool1d(output_size, return_indices=False)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnadaptivemaxpool2doutput_size-return_indicesfalse" class="md-nav__link">
    class torch.nn.AdaptiveMaxPool2d(output_size, return_indices=False)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnadaptiveavgpool1doutput_size" class="md-nav__link">
    class torch.nn.AdaptiveAvgPool1d(output_size)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnadaptiveavgpool2doutput_size" class="md-nav__link">
    class torch.nn.AdaptiveAvgPool2d(output_size)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#non-linear-activations-source" class="md-nav__link">
    Non-Linear Activations [source]
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#normalization-layers-source" class="md-nav__link">
    Normalization layers [source]
  </a>
  
    <nav class="md-nav" aria-label="Normalization layers [source]">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#class-torchnnbatchnorm1dnum_features-eps1e-05-momentum01-affinetrue-source" class="md-nav__link">
    class torch.nn.BatchNorm1d(num_features, eps=1e-05, momentum=0.1, affine=True) [source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnbatchnorm2dnum_features-eps1e-05-momentum01-affinetruesource" class="md-nav__link">
    class torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnbatchnorm3dnum_features-eps1e-05-momentum01-affinetruesource" class="md-nav__link">
    class torch.nn.BatchNorm3d(num_features, eps=1e-05, momentum=0.1, affine=True)[source]
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#recurrent-layers" class="md-nav__link">
    Recurrent layers
  </a>
  
    <nav class="md-nav" aria-label="Recurrent layers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#class-torchnnrnn-args-kwargssource" class="md-nav__link">
    class torch.nn.RNN( args, * kwargs)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnlstm-args-kwargssource" class="md-nav__link">
    class torch.nn.LSTM( args, * kwargs)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnngru-args-kwargssource" class="md-nav__link">
    class torch.nn.GRU( args, * kwargs)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnrnncellinput_size-hidden_size-biastrue-nonlinearitytanhsource" class="md-nav__link">
    class torch.nn.RNNCell(input_size, hidden_size, bias=True, nonlinearity='tanh')[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnlstmcellinput_size-hidden_size-biastruesource" class="md-nav__link">
    class torch.nn.LSTMCell(input_size, hidden_size, bias=True)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnngrucellinput_size-hidden_size-biastruesource" class="md-nav__link">
    class torch.nn.GRUCell(input_size, hidden_size, bias=True)[source]
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#linear-layers" class="md-nav__link">
    Linear layers
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#dropout-layers" class="md-nav__link">
    Dropout layers
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse-layers" class="md-nav__link">
    Sparse layers
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#distance-functions" class="md-nav__link">
    Distance functions
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#loss-functions" class="md-nav__link">
    Loss functions
  </a>
  
    <nav class="md-nav" aria-label="Loss functions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#class-torchnnl1losssize_averagetruesource" class="md-nav__link">
    class torch.nn.L1Loss(size_average=True)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnmselosssize_averagetruesource" class="md-nav__link">
    class torch.nn.MSELoss(size_average=True)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnncrossentropylossweightnone-size_averagetruesource" class="md-nav__link">
    class torch.nn.CrossEntropyLoss(weight=None, size_average=True)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnnlllossweightnone-size_averagetruesource" class="md-nav__link">
    class torch.nn.NLLLoss(weight=None, size_average=True)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnnllloss2dweightnone-size_averagetruesource" class="md-nav__link">
    class torch.nn.NLLLoss2d(weight=None, size_average=True)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnkldivlossweightnone-size_averagetruesource" class="md-nav__link">
    class torch.nn.KLDivLoss(weight=None, size_average=True)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnbcelossweightnone-size_averagetruesource" class="md-nav__link">
    class torch.nn.BCELoss(weight=None, size_average=True)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnmarginrankinglossmargin0-size_averagetruesource" class="md-nav__link">
    class torch.nn.MarginRankingLoss(margin=0, size_average=True)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnhingeembeddinglosssize_averagetruesource" class="md-nav__link">
    class torch.nn.HingeEmbeddingLoss(size_average=True)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnmultilabelmarginlosssize_averagetruesource" class="md-nav__link">
    class torch.nn.MultiLabelMarginLoss(size_average=True)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnsmoothl1losssize_averagetruesource" class="md-nav__link">
    class torch.nn.SmoothL1Loss(size_average=True)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnsoftmarginlosssize_averagetruesource" class="md-nav__link">
    class torch.nn.SoftMarginLoss(size_average=True)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnmultilabelsoftmarginlossweightnone-size_averagetruesource" class="md-nav__link">
    class torch.nn.MultiLabelSoftMarginLoss(weight=None, size_average=True)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnncosineembeddinglossmargin0-size_averagetruesource" class="md-nav__link">
    class torch.nn.CosineEmbeddingLoss(margin=0, size_average=True)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnmultimarginlossp1-margin1-weightnone-size_averagetruesource" class="md-nav__link">
    class torch.nn.MultiMarginLoss(p=1, margin=1, weight=None, size_average=True)[source]
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vision-layers" class="md-nav__link">
    Vision layers
  </a>
  
    <nav class="md-nav" aria-label="Vision layers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#class-torchnnpixelshuffleupscale_factorsource" class="md-nav__link">
    class torch.nn.PixelShuffle(upscale_factor)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnupsamplingnearest2dsizenone-scale_factornonesource" class="md-nav__link">
    class torch.nn.UpsamplingNearest2d(size=None, scale_factor=None)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnupsamplingbilinear2dsizenone-scale_factornonesource" class="md-nav__link">
    class torch.nn.UpsamplingBilinear2d(size=None, scale_factor=None)[source]
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#multi-gpu-layers" class="md-nav__link">
    Multi-GPU layers
  </a>
  
    <nav class="md-nav" aria-label="Multi-GPU layers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#class-torchnndataparallelmodule-device_idsnone-output_devicenone-dim0source" class="md-nav__link">
    class torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0)[source]
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#utilities" class="md-nav__link">
    Utilities
  </a>
  
    <nav class="md-nav" aria-label="Utilities">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#torchnnutilsclip_grad_normparameters-max_norm-norm_type2source" class="md-nav__link">
    torch.nn.utils.clip_grad_norm(parameters, max_norm, norm_type=2)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchnnutilsrnnpackedsequence_cls-data-batch_sizessource" class="md-nav__link">
    torch.nn.utils.rnn.PackedSequence(_cls, data, batch_sizes)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchnnutilsrnnpack_padded_sequenceinput-lengths-batch_firstfalsesource" class="md-nav__link">
    torch.nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=False)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchnnutilsrnnpad_packed_sequencesequence-batch_firstfalsesource" class="md-nav__link">
    torch.nn.utils.rnn.pad_packed_sequence(sequence, batch_first=False)[source]
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../functional/" class="md-nav__link">
        torch.nn.functional
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../torch-autograd/" class="md-nav__link">
        torch.autograd
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../torch-optim/" class="md-nav__link">
        torch.optim
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../nn_init/" class="md-nav__link">
        torch.nn.init
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../torch-multiprocessing/" class="md-nav__link">
        torch.multiprocessing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../legacy/" class="md-nav__link">
        torch.legacy
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../torch-cuda/" class="md-nav__link">
        torch.cuda
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../ffi/" class="md-nav__link">
        torch.utils.ffi
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../data/" class="md-nav__link">
        torch.utils.data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../model_zoo/" class="md-nav__link">
        torch.utils.model_zoo
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8_4" >
      
      
      
        <label class="md-nav__link" for="__nav_8_4" id="__nav_8_4_label" tabindex="0">
          TORCHVISION参考
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_8_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_8_4">
          <span class="md-nav__icon md-icon"></span>
          TORCHVISION参考
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../torchvision/torchvision/" class="md-nav__link">
        torchvision
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../torchvision/torchvision-datasets/" class="md-nav__link">
        torchvision.datasets
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../torchvision/torchvision-models/" class="md-nav__link">
        torchvision.models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../torchvision/torchvision-transform/" class="md-nav__link">
        torchvision.transforms
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../torchvision/torchvision-utils/" class="md-nav__link">
        torchvision.utils
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../acknowledgement/" class="md-nav__link">
        致谢
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../contrib/" class="md-nav__link">
        贡献者
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="https://www.apachecn.org/about/" class="md-nav__link">
        关于我们
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="https://docs.apachecn.org" class="md-nav__link">
        中文资源合集
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#parameters" class="md-nav__link">
    Parameters
  </a>
  
    <nav class="md-nav" aria-label="Parameters">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#class-torchnnparameter" class="md-nav__link">
    class torch.nn.Parameter()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#containers" class="md-nav__link">
    Containers(容器）：
  </a>
  
    <nav class="md-nav" aria-label="Containers(容器）：">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#class-torchnnmodule" class="md-nav__link">
    class torch.nn.Module
  </a>
  
    <nav class="md-nav" aria-label="class torch.nn.Module">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#add_modulename-module" class="md-nav__link">
    add_module(name, module)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#children" class="md-nav__link">
    children()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cpudevice_idnone" class="md-nav__link">
    cpu(device_id=None)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cudadevice_idnone" class="md-nav__link">
    cuda(device_id=None)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#double" class="md-nav__link">
    double()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#eval" class="md-nav__link">
    eval()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#float" class="md-nav__link">
    float()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#forward-input" class="md-nav__link">
    forward(* input)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#half" class="md-nav__link">
    half()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#load_state_dictstate_dict" class="md-nav__link">
    load_state_dict(state_dict)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#modules" class="md-nav__link">
    modules()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#named_children" class="md-nav__link">
    named_children()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#named_modulesmemonone-prefixsource" class="md-nav__link">
    named_modules(memo=None, prefix='')[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#parametersmemonone" class="md-nav__link">
    parameters(memo=None)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#register_backward_hookhook" class="md-nav__link">
    register_backward_hook(hook)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#register_buffername-tensor" class="md-nav__link">
    register_buffer(name, tensor)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#register_forward_hookhook" class="md-nav__link">
    register_forward_hook(hook)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#register_parametername-param" class="md-nav__link">
    register_parameter(name, param)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#state_dictdestinationnone-prefixsource" class="md-nav__link">
    state_dict(destination=None, prefix='')[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#trainmodetrue" class="md-nav__link">
    train(mode=True)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#zero_grad" class="md-nav__link">
    zero_grad()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnsequential-args" class="md-nav__link">
    class torch.nn.Sequential(* args)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnmodulelistmodulesnonesource" class="md-nav__link">
    class torch.nn.ModuleList(modules=None)[source]
  </a>
  
    <nav class="md-nav" aria-label="class torch.nn.ModuleList(modules=None)[source]">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#appendmodulesource" class="md-nav__link">
    append(module)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#extendmodulessource" class="md-nav__link">
    extend(modules)[source]
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnparameterlistparametersnone" class="md-nav__link">
    class torch.nn.ParameterList(parameters=None)
  </a>
  
    <nav class="md-nav" aria-label="class torch.nn.ParameterList(parameters=None)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#appendparametersource" class="md-nav__link">
    append(parameter)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#extendparameterssource" class="md-nav__link">
    extend(parameters)[source]
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    卷积层
  </a>
  
    <nav class="md-nav" aria-label="卷积层">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#class-torchnnconv1din_channels-out_channels-kernel_size-stride1-padding0-dilation1-groups1-biastrue" class="md-nav__link">
    class torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnconv2din_channels-out_channels-kernel_size-stride1-padding0-dilation1-groups1-biastrue" class="md-nav__link">
    class torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnconv3din_channels-out_channels-kernel_size-stride1-padding0-dilation1-groups1-biastrue" class="md-nav__link">
    class torch.nn.Conv3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnconvtranspose1din_channels-out_channels-kernel_size-stride1-padding0-output_padding0-groups1-biastrue" class="md-nav__link">
    class torch.nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnconvtranspose2din_channels-out_channels-kernel_size-stride1-padding0-output_padding0-groups1-biastrue" class="md-nav__link">
    class torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchnnconvtranspose3din_channels-out_channels-kernel_size-stride1-padding0-output_padding0-groups1-biastrue" class="md-nav__link">
    torch.nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    池化层
  </a>
  
    <nav class="md-nav" aria-label="池化层">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#class-torchnnmaxpool1dkernel_size-stridenone-padding0-dilation1-return_indicesfalse-ceil_modefalse" class="md-nav__link">
    class torch.nn.MaxPool1d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnmaxpool2dkernel_size-stridenone-padding0-dilation1-return_indicesfalse-ceil_modefalse" class="md-nav__link">
    class torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnmaxpool3dkernel_size-stridenone-padding0-dilation1-return_indicesfalse-ceil_modefalse" class="md-nav__link">
    class torch.nn.MaxPool3d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)
  </a>
  
    <nav class="md-nav" aria-label="class torch.nn.MaxPool3d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#class-torchnnmaxunpool1dkernel_size-stridenone-padding0" class="md-nav__link">
    class torch.nn.MaxUnpool1d(kernel_size, stride=None, padding=0)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnmaxunpool2dkernel_size-stridenone-padding0" class="md-nav__link">
    class torch.nn.MaxUnpool2d(kernel_size, stride=None, padding=0)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnmaxunpool3dkernel_size-stridenone-padding0" class="md-nav__link">
    class torch.nn.MaxUnpool3d(kernel_size, stride=None, padding=0)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnavgpool1dkernel_size-stridenone-padding0-ceil_modefalse-count_include_padtrue" class="md-nav__link">
    class torch.nn.AvgPool1d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnavgpool2dkernel_size-stridenone-padding0-ceil_modefalse-count_include_padtrue" class="md-nav__link">
    class torch.nn.AvgPool2d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnavgpool3dkernel_size-stridenone" class="md-nav__link">
    class torch.nn.AvgPool3d(kernel_size, stride=None)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnfractionalmaxpool2dkernel_size-output_sizenone-output_rationone-return_indicesfalse-_random_samplesnone" class="md-nav__link">
    class torch.nn.FractionalMaxPool2d(kernel_size, output_size=None, output_ratio=None, return_indices=False, _random_samples=None)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnlppool2dnorm_type-kernel_size-stridenone-ceil_modefalse" class="md-nav__link">
    class torch.nn.LPPool2d(norm_type, kernel_size, stride=None, ceil_mode=False)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnadaptivemaxpool1doutput_size-return_indicesfalse" class="md-nav__link">
    class torch.nn.AdaptiveMaxPool1d(output_size, return_indices=False)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnadaptivemaxpool2doutput_size-return_indicesfalse" class="md-nav__link">
    class torch.nn.AdaptiveMaxPool2d(output_size, return_indices=False)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnadaptiveavgpool1doutput_size" class="md-nav__link">
    class torch.nn.AdaptiveAvgPool1d(output_size)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnadaptiveavgpool2doutput_size" class="md-nav__link">
    class torch.nn.AdaptiveAvgPool2d(output_size)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#non-linear-activations-source" class="md-nav__link">
    Non-Linear Activations [source]
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#normalization-layers-source" class="md-nav__link">
    Normalization layers [source]
  </a>
  
    <nav class="md-nav" aria-label="Normalization layers [source]">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#class-torchnnbatchnorm1dnum_features-eps1e-05-momentum01-affinetrue-source" class="md-nav__link">
    class torch.nn.BatchNorm1d(num_features, eps=1e-05, momentum=0.1, affine=True) [source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnbatchnorm2dnum_features-eps1e-05-momentum01-affinetruesource" class="md-nav__link">
    class torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnbatchnorm3dnum_features-eps1e-05-momentum01-affinetruesource" class="md-nav__link">
    class torch.nn.BatchNorm3d(num_features, eps=1e-05, momentum=0.1, affine=True)[source]
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#recurrent-layers" class="md-nav__link">
    Recurrent layers
  </a>
  
    <nav class="md-nav" aria-label="Recurrent layers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#class-torchnnrnn-args-kwargssource" class="md-nav__link">
    class torch.nn.RNN( args, * kwargs)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnlstm-args-kwargssource" class="md-nav__link">
    class torch.nn.LSTM( args, * kwargs)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnngru-args-kwargssource" class="md-nav__link">
    class torch.nn.GRU( args, * kwargs)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnrnncellinput_size-hidden_size-biastrue-nonlinearitytanhsource" class="md-nav__link">
    class torch.nn.RNNCell(input_size, hidden_size, bias=True, nonlinearity='tanh')[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnlstmcellinput_size-hidden_size-biastruesource" class="md-nav__link">
    class torch.nn.LSTMCell(input_size, hidden_size, bias=True)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnngrucellinput_size-hidden_size-biastruesource" class="md-nav__link">
    class torch.nn.GRUCell(input_size, hidden_size, bias=True)[source]
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#linear-layers" class="md-nav__link">
    Linear layers
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#dropout-layers" class="md-nav__link">
    Dropout layers
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse-layers" class="md-nav__link">
    Sparse layers
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#distance-functions" class="md-nav__link">
    Distance functions
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#loss-functions" class="md-nav__link">
    Loss functions
  </a>
  
    <nav class="md-nav" aria-label="Loss functions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#class-torchnnl1losssize_averagetruesource" class="md-nav__link">
    class torch.nn.L1Loss(size_average=True)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnmselosssize_averagetruesource" class="md-nav__link">
    class torch.nn.MSELoss(size_average=True)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnncrossentropylossweightnone-size_averagetruesource" class="md-nav__link">
    class torch.nn.CrossEntropyLoss(weight=None, size_average=True)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnnlllossweightnone-size_averagetruesource" class="md-nav__link">
    class torch.nn.NLLLoss(weight=None, size_average=True)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnnllloss2dweightnone-size_averagetruesource" class="md-nav__link">
    class torch.nn.NLLLoss2d(weight=None, size_average=True)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnkldivlossweightnone-size_averagetruesource" class="md-nav__link">
    class torch.nn.KLDivLoss(weight=None, size_average=True)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnbcelossweightnone-size_averagetruesource" class="md-nav__link">
    class torch.nn.BCELoss(weight=None, size_average=True)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnmarginrankinglossmargin0-size_averagetruesource" class="md-nav__link">
    class torch.nn.MarginRankingLoss(margin=0, size_average=True)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnhingeembeddinglosssize_averagetruesource" class="md-nav__link">
    class torch.nn.HingeEmbeddingLoss(size_average=True)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnmultilabelmarginlosssize_averagetruesource" class="md-nav__link">
    class torch.nn.MultiLabelMarginLoss(size_average=True)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnsmoothl1losssize_averagetruesource" class="md-nav__link">
    class torch.nn.SmoothL1Loss(size_average=True)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnsoftmarginlosssize_averagetruesource" class="md-nav__link">
    class torch.nn.SoftMarginLoss(size_average=True)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnmultilabelsoftmarginlossweightnone-size_averagetruesource" class="md-nav__link">
    class torch.nn.MultiLabelSoftMarginLoss(weight=None, size_average=True)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnncosineembeddinglossmargin0-size_averagetruesource" class="md-nav__link">
    class torch.nn.CosineEmbeddingLoss(margin=0, size_average=True)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnmultimarginlossp1-margin1-weightnone-size_averagetruesource" class="md-nav__link">
    class torch.nn.MultiMarginLoss(p=1, margin=1, weight=None, size_average=True)[source]
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vision-layers" class="md-nav__link">
    Vision layers
  </a>
  
    <nav class="md-nav" aria-label="Vision layers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#class-torchnnpixelshuffleupscale_factorsource" class="md-nav__link">
    class torch.nn.PixelShuffle(upscale_factor)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnupsamplingnearest2dsizenone-scale_factornonesource" class="md-nav__link">
    class torch.nn.UpsamplingNearest2d(size=None, scale_factor=None)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-torchnnupsamplingbilinear2dsizenone-scale_factornonesource" class="md-nav__link">
    class torch.nn.UpsamplingBilinear2d(size=None, scale_factor=None)[source]
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#multi-gpu-layers" class="md-nav__link">
    Multi-GPU layers
  </a>
  
    <nav class="md-nav" aria-label="Multi-GPU layers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#class-torchnndataparallelmodule-device_idsnone-output_devicenone-dim0source" class="md-nav__link">
    class torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0)[source]
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#utilities" class="md-nav__link">
    Utilities
  </a>
  
    <nav class="md-nav" aria-label="Utilities">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#torchnnutilsclip_grad_normparameters-max_norm-norm_type2source" class="md-nav__link">
    torch.nn.utils.clip_grad_norm(parameters, max_norm, norm_type=2)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchnnutilsrnnpackedsequence_cls-data-batch_sizessource" class="md-nav__link">
    torch.nn.utils.rnn.PackedSequence(_cls, data, batch_sizes)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchnnutilsrnnpack_padded_sequenceinput-lengths-batch_firstfalsesource" class="md-nav__link">
    torch.nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=False)[source]
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchnnutilsrnnpad_packed_sequencesequence-batch_firstfalsesource" class="md-nav__link">
    torch.nn.utils.rnn.pad_packed_sequence(sequence, batch_first=False)[source]
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
    <a href="https://github.com/apachecn/pytorch-doc-zh/edit/master/docs/0.2/package_references/torch-nn.md" title="Edit this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4v-2m10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1 2.1 2.1Z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/apachecn/pytorch-doc-zh/raw/master/docs/0.2/package_references/torch-nn.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.15 8.15 0 0 1-1.23-2Z"/></svg>
    </a>
  


<div class="wwads-cn wwads-horizontal" data-id="206" style="max-width:680px"></div>
<h1 id="torchnn">torch.nn</h1>
<h2 id="parameters">Parameters</h2>
<h3 id="class-torchnnparameter">class torch.nn.Parameter()</h3>
<p><code>Variable</code>的一种，常被用于模块参数(<code>module parameter</code>)。</p>
<p><code>Parameters</code> 是 <code>Variable</code> 的子类。<code>Paramenters</code>和<code>Modules</code>一起使用的时候会有一些特殊的属性，即：当<code>Paramenters</code>赋值给<code>Module</code>的属性的时候，他会自动的被加到 <code>Module</code>的 参数列表中(即：会出现在 <code>parameters() 迭代器中</code>)。将<code>Varibale</code>赋值给<code>Module</code>属性则不会有这样的影响。
这样做的原因是：我们有时候会需要缓存一些临时的状态(<code>state</code>), 比如：模型中<code>RNN</code>的最后一个隐状态。如果没有<code>Parameter</code>这个类的话，那么这些临时变量也会注册成为模型变量。</p>
<p><code>Variable</code> 与 <code>Parameter</code>的另一个不同之处在于，<code>Parameter</code>不能被 <code>volatile</code>(即：无法设置<code>volatile=True</code>)而且默认<code>requires_grad=True</code>。<code>Variable</code>默认<code>requires_grad=False</code>。</p>
<p>参数说明:</p>
<ul>
<li>
<p>data (Tensor) – parameter tensor.</p>
</li>
<li>
<p>requires_grad (bool, optional) – 默认为<code>True</code>，在<code>BP</code>的过程中会对其求微分。</p>
</li>
</ul>
<h2 id="containers">Containers(容器）：</h2>
<h3 id="class-torchnnmodule">class torch.nn.Module</h3>
<p>所有网络的基类。</p>
<p>你的模型也应该继承这个类。</p>
<p><code>Modules</code>也可以包含其它<code>Modules</code>,允许使用树结构嵌入他们。你可以将子模块赋值给模型属性。</p>
<pre><code class="language-python">import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)# submodule: Conv2d
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
       x = F.relu(self.conv1(x))
       return F.relu(self.conv2(x))
</code></pre>
<p>通过上面方式赋值的<code>submodule</code>会被注册。当调用 <code>.cuda()</code> 的时候，<code>submodule</code>的参数也会转换为<code>cuda Tensor</code>。</p>
<h4 id="add_modulename-module">add_module(name, module)</h4>
<p>将一个 <code>child module</code> 添加到当前 <code>modle</code>。
被添加的<code>module</code>可以通过 <code>name</code>属性来获取。
例：</p>
<pre><code class="language-python">import torch.nn as nn
class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.add_module(&quot;conv&quot;, nn.Conv2d(10, 20, 4))
        #self.conv = nn.Conv2d(10, 20, 4) 和上面这个增加module的方式等价
model = Model()
print(model.conv)
</code></pre>
<p>输出：</p>
<pre><code>Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))
</code></pre>
<h4 id="children">children()</h4>
<p>Returns an iterator over immediate children modules.
返回当前模型 子模块的迭代器。</p>
<pre><code class="language-python">import torch.nn as nn
class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.add_module(&quot;conv&quot;, nn.Conv2d(10, 20, 4))
        self.add_module(&quot;conv1&quot;, nn.Conv2d(20 ,10, 4))
model = Model()

for sub_module in model.children():
    print(sub_module)
</code></pre>
<pre><code>Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))
Conv2d(20, 10, kernel_size=(4, 4), stride=(1, 1))
</code></pre>
<h4 id="cpudevice_idnone">cpu(device_id=None)</h4>
<p>将所有的模型参数(<code>parameters</code>)和<code>buffers</code>复制到<code>CPU</code></p>
<p><code>NOTE</code>：官方文档用的move，但我觉着<code>copy</code>更合理。</p>
<h4 id="cudadevice_idnone">cuda(device_id=None)</h4>
<p>将所有的模型参数(<code>parameters</code>)和<code>buffers</code>赋值<code>GPU</code></p>
<p>参数说明:</p>
<ul>
<li>device_id (int, optional) – 如果指定的话，所有的模型参数都会复制到指定的设备上。</li>
</ul>
<h4 id="double">double()</h4>
<p>将<code>parameters</code>和<code>buffers</code>的数据类型转换成<code>double</code>。</p>
<h4 id="eval">eval()</h4>
<p>将模型设置成<code>evaluation</code>模式</p>
<p>仅仅当模型中有<code>Dropout</code>和<code>BatchNorm</code>是才会有影响。</p>
<h4 id="float">float()</h4>
<p>将<code>parameters</code>和<code>buffers</code>的数据类型转换成<code>float</code>。</p>
<h4 id="forward-input">forward(* input)</h4>
<p>定义了每次执行的 计算步骤。
在所有的子类中都需要重写这个函数。</p>
<h4 id="half">half()</h4>
<p>将<code>parameters</code>和<code>buffers</code>的数据类型转换成<code>half</code>。</p>
<h4 id="load_state_dictstate_dict">load_state_dict(state_dict)</h4>
<p>将<code>state_dict</code>中的<code>parameters</code>和<code>buffers</code>复制到此<code>module</code>和它的后代中。<code>state_dict</code>中的<code>key</code>必须和 <code>model.state_dict()</code>返回的<code>key</code>一致。
<code>NOTE</code>：用来加载模型参数。</p>
<p>参数说明:</p>
<ul>
<li>state_dict (dict) – 保存<code>parameters</code>和<code>persistent buffers</code>的字典。</li>
</ul>
<h4 id="modules">modules()</h4>
<p>返回一个包含 当前模型 所有模块的迭代器。</p>
<pre><code class="language-python">import torch.nn as nn
class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.add_module(&quot;conv&quot;, nn.Conv2d(10, 20, 4))
        self.add_module(&quot;conv1&quot;, nn.Conv2d(20 ,10, 4))
model = Model()

for module in model.modules():
    print(module)
</code></pre>
<pre><code>Model (
  (conv): Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))
  (conv1): Conv2d(20, 10, kernel_size=(4, 4), stride=(1, 1))
)
Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))
Conv2d(20, 10, kernel_size=(4, 4), stride=(1, 1))
</code></pre>
<p>可以看出，<code>modules()</code>返回的<code>iterator</code>不止包含 子模块。这是和<code>children()</code>的不同。</p>
<p><strong><code>NOTE：</code></strong>
重复的模块只被返回一次(<code>children()也是</code>)。 在下面的例子中, <code>submodule</code> 只会被返回一次：</p>
<pre><code class="language-python">import torch.nn as nn

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        submodule = nn.Conv2d(10, 20, 4)
        self.add_module(&quot;conv&quot;, submodule)
        self.add_module(&quot;conv1&quot;, submodule)
model = Model()

for module in model.modules():
    print(module)
</code></pre>
<pre><code>Model (
  (conv): Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))
  (conv1): Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))
)
Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))
</code></pre>
<h4 id="named_children">named_children()</h4>
<p>返回 包含 模型当前子模块 的迭代器，<code>yield</code> 模块名字和模块本身。</p>
<p>例子：</p>
<pre><code class="language-python">for name, module in model.named_children():
    if name in ['conv4', 'conv5']:
        print(module)
</code></pre>
<h4 id="named_modulesmemonone-prefixsource">named_modules(memo=None, prefix='')[source]</h4>
<p>返回包含网络中所有模块的迭代器, <code>yielding</code>  模块名和模块本身。</p>
<p><strong><code>注意：</code></strong></p>
<p>重复的模块只被返回一次(<code>children()也是</code>)。 在下面的例子中, <code>submodule</code> 只会被返回一次。</p>
<h4 id="parametersmemonone">parameters(memo=None)</h4>
<p>返回一个 包含模型所有参数 的迭代器。</p>
<p>一般用来当作<code>optimizer</code>的参数。</p>
<p>例子：</p>
<pre><code class="language-python">for param in model.parameters():
    print(type(param.data), param.size())

&lt;class 'torch.FloatTensor'&gt; (20L,)
&lt;class 'torch.FloatTensor'&gt; (20L, 1L, 5L, 5L)
</code></pre>
<h4 id="register_backward_hookhook">register_backward_hook(hook)</h4>
<p>在<code>module</code>上注册一个<code>bachward hook</code>。</p>
<p>每次计算<code>module</code>的<code>inputs</code>的梯度的时候，这个<code>hook</code>会被调用。<code>hook</code>应该拥有下面的<code>signature</code>。</p>
<p><code>hook(module, grad_input, grad_output) -&gt; Variable or None</code></p>
<p>如果<code>module</code>有多个输入输出的话，那么<code>grad_input</code> <code>grad_output</code>将会是个<code>tuple</code>。
<code>hook</code>不应该修改它的<code>arguments</code>，但是它可以选择性的返回关于输入的梯度，这个返回的梯度在后续的计算中会替代<code>grad_input</code>。</p>
<p>这个函数返回一个 句柄(<code>handle</code>)。它有一个方法 <code>handle.remove()</code>，可以用这个方法将<code>hook</code>从<code>module</code>移除。</p>
<h4 id="register_buffername-tensor">register_buffer(name, tensor)</h4>
<p>给<code>module</code>添加一个<code>persistent buffer</code>。</p>
<p><code>persistent buffer</code>通常被用在这么一种情况：我们需要保存一个状态，但是这个状态不能看作成为模型参数。
例如：, <code>BatchNorm's</code> running_mean 不是一个 <code>parameter</code>, 但是它也是需要保存的状态之一。</p>
<p><code>Buffers</code>可以通过注册时候的<code>name</code>获取。</p>
<p><strong><code>NOTE</code>:我们可以用 buffer 保存 <code>moving average</code></strong></p>
<p>例子：</p>
<pre><code class="language-python">self.register_buffer('running_mean', torch.zeros(num_features))

self.running_mean
</code></pre>
<h4 id="register_forward_hookhook">register_forward_hook(hook)</h4>
<p>在<code>module</code>上注册一个<code>forward hook</code>。
每次调用<code>forward()</code>计算输出的时候，这个<code>hook</code>就会被调用。它应该拥有以下签名：</p>
<p><code>hook(module, input, output) -&gt; None</code></p>
<p><code>hook</code>不应该修改 <code>input</code>和<code>output</code>的值。 这个函数返回一个 句柄(<code>handle</code>)。它有一个方法 <code>handle.remove()</code>，可以用这个方法将<code>hook</code>从<code>module</code>移除。</p>
<h4 id="register_parametername-param">register_parameter(name, param)</h4>
<p>向<code>module</code>添加 <code>parameter</code></p>
<p><code>parameter</code>可以通过注册时候的<code>name</code>获取。</p>
<h4 id="state_dictdestinationnone-prefixsource">state_dict(destination=None, prefix='')[source]</h4>
<p>返回一个字典，保存着<code>module</code>的所有状态(<code>state</code>）。</p>
<p><code>parameters</code>和<code>persistent buffers</code>都会包含在字典中，字典的<code>key</code>就是<code>parameter</code>和<code>buffer</code>的 <code>names</code>。</p>
<p>例子：</p>
<pre><code class="language-python">import torch
from torch.autograd import Variable
import torch.nn as nn

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv2 = nn.Linear(1, 2)
        self.vari = Variable(torch.rand([1]))
        self.par = nn.Parameter(torch.rand([1]))
        self.register_buffer(&quot;buffer&quot;, torch.randn([2,3]))

model = Model()
print(model.state_dict().keys())

</code></pre>
<pre><code>odict_keys(['par', 'buffer', 'conv2.weight', 'conv2.bias'])
</code></pre>
<h4 id="trainmodetrue">train(mode=True)</h4>
<p>将<code>module</code>设置为 <code>training mode</code>。</p>
<p>仅仅当模型中有<code>Dropout</code>和<code>BatchNorm</code>是才会有影响。</p>
<h4 id="zero_grad">zero_grad()</h4>
<p>将<code>module</code>中的所有模型参数的梯度设置为0.</p>
<h3 id="class-torchnnsequential-args">class torch.nn.Sequential(* args)</h3>
<p>一个时序容器。<code>Modules</code> 会以他们传入的顺序被添加到容器中。当然，也可以传入一个<code>OrderedDict</code>。</p>
<p>为了更容易的理解如何使用<code>Sequential</code>, 下面给出了一个例子:</p>
<pre><code class="language-python"># Example of using Sequential

model = nn.Sequential(
          nn.Conv2d(1,20,5),
          nn.ReLU(),
          nn.Conv2d(20,64,5),
          nn.ReLU()
        )
# Example of using Sequential with OrderedDict
model = nn.Sequential(OrderedDict([
          ('conv1', nn.Conv2d(1,20,5)),
          ('relu1', nn.ReLU()),
          ('conv2', nn.Conv2d(20,64,5)),
          ('relu2', nn.ReLU())
        ]))
</code></pre>
<h3 id="class-torchnnmodulelistmodulesnonesource">class torch.nn.ModuleList(modules=None)[source]</h3>
<p>将<code>submodules</code>保存在一个<code>list</code>中。</p>
<p><code>ModuleList</code>可以像一般的<code>Python list</code>一样被<code>索引</code>。而且<code>ModuleList</code>中包含的<code>modules</code>已经被正确的注册，对所有的<code>module method</code>可见。</p>
<p>参数说明:</p>
<ul>
<li>modules (list, optional) – 将要被添加到<code>MuduleList</code>中的 <code>modules</code> 列表</li>
</ul>
<p>例子:</p>
<pre><code class="language-python">class MyModule(nn.Module):
    def __init__(self):
        super(MyModule, self).__init__()
        self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])

    def forward(self, x):
        # ModuleList can act as an iterable, or be indexed         using ints
        for i, l in enumerate(self.linears):
            x = self.linears[i // 2](x) + l(x)
        return x
</code></pre>
<h4 id="appendmodulesource">append(module)[source]</h4>
<p>等价于 list 的 <code>append()</code></p>
<p>参数说明:</p>
<ul>
<li>module (nn.Module) – 要 append 的<code>module</code></li>
</ul>
<h4 id="extendmodulessource">extend(modules)[source]</h4>
<p>等价于 <code>list</code> 的 <code>extend()</code> 方法</p>
<p>参数说明:</p>
<ul>
<li>modules (list) – list of modules to append</li>
</ul>
<h3 id="class-torchnnparameterlistparametersnone">class torch.nn.ParameterList(parameters=None)</h3>
<p>将<code>submodules</code>保存在一个<code>list</code>中。</p>
<p><code>ParameterList</code>可以像一般的<code>Python list</code>一样被<code>索引</code>。而且<code>ParameterList</code>中包含的<code>parameters</code>已经被正确的注册，对所有的<code>module method</code>可见。</p>
<p>参数说明:</p>
<ul>
<li>modules (list, optional) – a list of nn.Parameter</li>
</ul>
<p>例子:</p>
<pre><code class="language-python">class MyModule(nn.Module):
    def __init__(self):
        super(MyModule, self).__init__()
        self.params = nn.ParameterList([nn.Parameter(torch.randn(10, 10)) for i in range(10)])

    def forward(self, x):
        # ModuleList can act as an iterable, or be indexed using ints
        for i, p in enumerate(self.params):
            x = self.params[i // 2].mm(x) + p.mm(x)
        return x
</code></pre>
<h4 id="appendparametersource">append(parameter)[source]</h4>
<p>等价于<code>python list</code> 的 <code>append</code> 方法。</p>
<p>参数说明:</p>
<ul>
<li>parameter (nn.Parameter) – parameter to append</li>
</ul>
<h4 id="extendparameterssource">extend(parameters)[source]</h4>
<p>等价于<code>python list</code> 的 <code>extend</code> 方法。</p>
<p>参数说明:</p>
<ul>
<li>parameters (list) – list of parameters to append</li>
</ul>
<h2 id="_1">卷积层</h2>
<h3 id="class-torchnnconv1din_channels-out_channels-kernel_size-stride1-padding0-dilation1-groups1-biastrue">class torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)</h3>
<p>一维卷积层，输入的尺度是(N, C_in,L)，输出尺度 (N,C_out,L_out）的计算方式：  </p>
<div class="arithmatex">\[
out(N_i, C_{out_j})=bias(C _{out_j})+\sum^{C_{in}-1}_{k=0}weight(C_{out_j},k)\bigotimes input(N_i,k)
\]</div>
<p><strong>说明</strong>   </p>
<p><code>bigotimes</code>: 表示相关系数计算   <br />
<code>stride</code>: 控制相关系数的计算步长  <br />
<code>dilation</code>: 用于控制内核点之间的距离，详细描述在<a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">这里</a>      <br />
<code>groups</code>: 控制输入和输出之间的连接，    <code>group=1</code>，输出是所有的输入的卷积；<code>group=2</code>，此时相当于有并排的两个卷积层，每个卷积层计算输入通道的一半，并且产生的输出是输出通道的一半，随后将这两个输出连接起来。</p>
<p><strong>Parameters：</strong>  </p>
<ul>
<li>in_channels(<code>int</code>) – 输入信号的通道</li>
<li>out_channels(<code>int</code>) – 卷积产生的通道</li>
<li>kerner_size(<code>int</code> or <code>tuple</code>) - 卷积核的尺寸</li>
<li>stride(<code>int</code> or <code>tuple</code>, <code>optional</code>) - 卷积步长</li>
<li>padding (<code>int</code> or <code>tuple</code>, <code>optional</code>)- 输入的每一条边补充0的层数   </li>
<li>dilation(<code>int</code> or <code>tuple</code>, `optional``) – 卷积核元素之间的间距</li>
<li>groups(<code>int</code>, <code>optional</code>) – 从输入通道到输出通道的阻塞连接数</li>
<li>bias(<code>bool</code>, <code>optional</code>) - 如果<code>bias=True</code>，添加偏置</li>
</ul>
<p><strong>shape:</strong><br />
输入: (N,C_in,L_in)  <br />
输出: (N,C_out,L_out)   <br />
输入输出的计算方式： <br />
<span class="arithmatex">\(<span class="arithmatex">\(L_{out}=floor((L_{in}+2*padding-dilation*(kernerl\_size-1)-1)/stride+1)\)</span>\)</span></p>
<p><strong>变量:</strong><br />
weight(<code>tensor</code>) - 卷积的权重，大小是(<code>out_channels</code>, <code>in_channels</code>, <code>kernel_size</code>)  <br />
bias(<code>tensor</code>) - 卷积的偏置系数，大小是(<code>out_channel</code>）  </p>
<p><strong>example:</strong></p>
<pre><code class="language-python">&gt;&gt;&gt; m = nn.Conv1d(16, 33, 3, stride=2)
&gt;&gt;&gt; input = autograd.Variable(torch.randn(20, 16, 50))
&gt;&gt;&gt; output = m(input)
</code></pre>
<h3 id="class-torchnnconv2din_channels-out_channels-kernel_size-stride1-padding0-dilation1-groups1-biastrue">class torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)</h3>
<p>二维卷积层, 输入的尺度是(N, C_in,H,W)，输出尺度(N,C_out,H_out,W_out）的计算方式：  </p>
<div class="arithmatex">\[out(N_i, C_{out_j})=bias(C_{out_j})+\sum^{C_{in}-1}_{k=0}weight(C_{out_j},k)\bigotimes input(N_i,k)\]</div>
<p><strong>说明</strong><br />
<code>bigotimes</code>: 表示二维的相关系数计算
<code>stride</code>: 控制相关系数的计算步长 <br />
<code>dilation</code>: 用于控制内核点之间的距离，详细描述在<a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">这里</a><br />
<code>groups</code>: 控制输入和输出之间的连接： <code>group=1</code>，输出是所有的输入的卷积；<code>group=2</code>，此时相当于有并排的两个卷积层，每个卷积层计算输入通道的一半，并且产生的输出是输出通道的一半，随后将这两个输出连接起来。      </p>
<p>参数<code>kernel_size</code>，<code>stride,padding</code>，<code>dilation</code>也可以是一个<code>int</code>的数据，此时卷积height和width值相同;也可以是一个<code>tuple</code>数组，<code>tuple</code>的第一维度表示height的数值，tuple的第二维度表示width的数值</p>
<p><strong>Parameters：</strong>  </p>
<ul>
<li>in_channels(<code>int</code>) – 输入信号的通道</li>
<li>out_channels(<code>int</code>) – 卷积产生的通道</li>
<li>kerner_size(<code>int</code> or <code>tuple</code>) - 卷积核的尺寸</li>
<li>stride(<code>int</code> or <code>tuple</code>, <code>optional</code>) - 卷积步长</li>
<li>padding(<code>int</code> or <code>tuple</code>, <code>optional</code>) - 输入的每一条边补充0的层数</li>
<li>dilation(<code>int</code> or <code>tuple</code>, <code>optional</code>) – 卷积核元素之间的间距</li>
<li>groups(<code>int</code>, <code>optional</code>) – 从输入通道到输出通道的阻塞连接数</li>
<li>bias(<code>bool</code>, <code>optional</code>) - 如果<code>bias=True</code>，添加偏置</li>
</ul>
<p><strong>shape:</strong><br />
input: (N,C_in,H_in,W_in)  <br />
output: (N,C_out,H_out,W_out)<br />
<span class="arithmatex">\(<span class="arithmatex">\(H_{out}=floor((H_{in}+2*padding[0]-dilation[0]*(kernerl\_size[0]-1)-1)/stride[0]+1)\)</span>\)</span>    </p>
<div class="arithmatex">\[W_{out}=floor((W_{in}+2*padding[1]-dilation[1]*(kernerl\_size[1]-1)-1)/stride[1]+1)\]</div>
<p><strong>变量:</strong><br />
weight(<code>tensor</code>) - 卷积的权重，大小是(<code>out_channels</code>, <code>in_channels</code>,<code>kernel_size</code>)<br />
bias(<code>tensor</code>) - 卷积的偏置系数，大小是(<code>out_channel</code>）  </p>
<p><strong>example:</strong></p>
<pre><code class="language-python">&gt;&gt;&gt; # With square kernels and equal stride
&gt;&gt;&gt; m = nn.Conv2d(16, 33, 3, stride=2)
&gt;&gt;&gt; # non-square kernels and unequal stride and with padding
&gt;&gt;&gt; m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))
&gt;&gt;&gt; # non-square kernels and unequal stride and with padding and dilation
&gt;&gt;&gt; m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))
&gt;&gt;&gt; input = autograd.Variable(torch.randn(20, 16, 50, 100))
&gt;&gt;&gt; output = m(input)
</code></pre>
<h3 id="class-torchnnconv3din_channels-out_channels-kernel_size-stride1-padding0-dilation1-groups1-biastrue">class torch.nn.Conv3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)</h3>
<p>三维卷积层, 输入的尺度是(N, C_in,D,H,W)，输出尺度(N,C_out,D_out,H_out,W_out）的计算方式：<br />
<span class="arithmatex">\(<span class="arithmatex">\(out(N_i, C_{out_j})=bias(C_{out_j})+\sum^{C_{in}-1}_{k=0}weight(C_{out_j},k)\bigotimes input(N_i,k)\)</span>\)</span></p>
<p><strong>说明</strong><br />
<code>bigotimes</code>: 表示二维的相关系数计算
<code>stride</code>: 控制相关系数的计算步长 <br />
<code>dilation</code>: 用于控制内核点之间的距离，详细描述在<a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">这里</a>  <br />
<code>groups</code>: 控制输入和输出之间的连接： <code>group=1</code>，输出是所有的输入的卷积；<code>group=2</code>，此时相当于有并排的两个卷积层，每个卷积层计算输入通道的一半，并且产生的输出是输出通道的一半，随后将这两个输出连接起来。<br />
参数<code>kernel_size</code>，<code>stride</code>，<code>padding</code>，<code>dilation</code>可以是一个<code>int</code>的数据 - 卷积height和width值相同，也可以是一个有三个<code>int</code>数据的<code>tuple</code>数组，<code>tuple</code>的第一维度表示depth的数值，<code>tuple</code>的第二维度表示height的数值，<code>tuple</code>的第三维度表示width的数值</p>
<p><strong>Parameters：</strong>  </p>
<ul>
<li>in_channels(<code>int</code>) – 输入信号的通道</li>
<li>out_channels(<code>int</code>) – 卷积产生的通道</li>
<li>kernel_size(<code>int</code> or <code>tuple</code>) - 卷积核的尺寸</li>
<li>stride(<code>int</code> or <code>tuple</code>, <code>optional</code>) - 卷积步长</li>
<li>padding(<code>int</code> or <code>tuple</code>, <code>optional</code>) - 输入的每一条边补充0的层数</li>
<li>dilation(<code>int</code> or <code>tuple</code>, <code>optional</code>) – 卷积核元素之间的间距</li>
<li>groups(<code>int</code>, <code>optional</code>) – 从输入通道到输出通道的阻塞连接数</li>
<li>bias(<code>bool</code>, <code>optional</code>) - 如果<code>bias=True</code>，添加偏置</li>
</ul>
<p><strong>shape:</strong><br />
<code>input</code>: (N,C_in,D_in,H_in,W_in)  <br />
<code>output</code>: (N,C_out,D_out,H_out,W_out)  <br />
<span class="arithmatex">\(<span class="arithmatex">\(D_{out}=floor((D_{in}+2*padding[0]-dilation[0]*(kernerl\_size[0]-1)-1)/stride[0]+1)\)</span>\)</span>      </p>
<div class="arithmatex">\[H_{out}=floor((H_{in}+2*padding[1]-dilation[2]*(kernerl\_size[1]-1)-1)/stride[1]+1)\]</div>
<div class="arithmatex">\[W_{out}=floor((W_{in}+2*padding[2]-dilation[2]*(kernerl\_size[2]-1)-1)/stride[2]+1)\]</div>
<p><strong>变量:</strong>  </p>
<ul>
<li>weight(<code>tensor</code>) - 卷积的权重，shape是(<code>out_channels</code>, <code>in_channels</code>,<code>kernel_size</code>)`</li>
<li>bias(<code>tensor</code>) - 卷积的偏置系数，shape是(<code>out_channel</code>）  </li>
</ul>
<p><strong>example:</strong></p>
<pre><code class="language-python">&gt;&gt;&gt; # With square kernels and equal stride
&gt;&gt;&gt; m = nn.Conv3d(16, 33, 3, stride=2)
&gt;&gt;&gt; # non-square kernels and unequal stride and with padding
&gt;&gt;&gt; m = nn.Conv3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(4, 2, 0))
&gt;&gt;&gt; input = autograd.Variable(torch.randn(20, 16, 10, 50, 100))
&gt;&gt;&gt; output = m(input)
</code></pre>
<h3 id="class-torchnnconvtranspose1din_channels-out_channels-kernel_size-stride1-padding0-output_padding0-groups1-biastrue">class torch.nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True)</h3>
<p>1维的解卷积操作(<code>transposed convolution operator</code>，注意改视作操作可视作解卷积操作，但并不是真正的解卷积操作）
该模块可以看作是<code>Conv1d</code>相对于其输入的梯度，有时(但不正确地）被称为解卷积操作。</p>
<p><strong>注意</strong><br />
由于内核的大小，输入的最后的一些列的数据可能会丢失。因为输入和输出是不是完全的互相关。因此，用户可以进行适当的填充(padding操作）。  </p>
<p><strong>参数</strong></p>
<ul>
<li>in_channels(<code>int</code>) – 输入信号的通道数</li>
<li>out_channels(<code>int</code>) – 卷积产生的通道</li>
<li>kernel_size(<code>int</code> or <code>tuple</code>) - 卷积核的大小</li>
<li>stride(<code>int</code> or <code>tuple</code>, <code>optional</code>) - 卷积步长</li>
<li>padding(<code>int</code> or <code>tuple</code>, <code>optional</code>) - 输入的每一条边补充0的层数</li>
<li>output_padding(<code>int</code> or <code>tuple</code>, <code>optional</code>) - 输出的每一条边补充0的层数</li>
<li>dilation(<code>int</code> or <code>tuple</code>, <code>optional</code>) – 卷积核元素之间的间距</li>
<li>groups(<code>int</code>, <code>optional</code>) – 从输入通道到输出通道的阻塞连接数</li>
<li>bias(<code>bool</code>, <code>optional</code>) - 如果<code>bias=True</code>，添加偏置</li>
</ul>
<p><strong>shape:</strong> <br />
输入: (N,C_in,L_in)  <br />
输出: (N,C_out,L_out)  <br />
<span class="arithmatex">\(<span class="arithmatex">\(L_{out}=(L_{in}-1)*stride-2*padding+kernel\_size+output\_padding\)</span>\)</span>  </p>
<p><strong>变量:</strong><br />
- weight(<code>tensor</code>) - 卷积的权重，大小是(<code>in_channels</code>, <code>in_channels</code>,<code>kernel_size</code>)<br />
- bias(<code>tensor</code>) - 卷积的偏置系数，大小是(<code>out_channel</code>)</p>
<h3 id="class-torchnnconvtranspose2din_channels-out_channels-kernel_size-stride1-padding0-output_padding0-groups1-biastrue">class torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True)</h3>
<p>2维的转置卷积操作(<code>transposed convolution operator</code>，注意改视作操作可视作解卷积操作，但并不是真正的解卷积操作）
该模块可以看作是<code>Conv2d</code>相对于其输入的梯度，有时(但不正确地）被称为解卷积操作。</p>
<p><strong>说明</strong></p>
<p><code>stride</code>: 控制相关系数的计算步长 <br />
<code>dilation</code>: 用于控制内核点之间的距离，详细描述在<a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">这里</a>  <br />
<code>groups</code>: 控制输入和输出之间的连接： <code>group=1</code>，输出是所有的输入的卷积；<code>group=2</code>，此时相当于有并排的两个卷积层，每个卷积层计算输入通道的一半，并且产生的输出是输出通道的一半，随后将这两个输出连接起来。     </p>
<p>参数<code>kernel_size</code>，<code>stride</code>，<code>padding</code>，<code>dilation</code>数据类型：
可以是一个<code>int</code>类型的数据，此时卷积height和width值相同;
也可以是一个<code>tuple</code>数组(包含来两个<code>int</code>类型的数据），第一个<code>int</code>数据表示<code>height</code>的数值，第二个<code>int</code>类型的数据表示width的数值</p>
<p><strong>注意</strong><br />
由于内核的大小，输入的最后的一些列的数据可能会丢失。因为输入和输出是不是完全的互相关。因此，用户可以进行适当的填充(<code>padding</code>操作）。</p>
<p><strong>参数：</strong></p>
<ul>
<li>in_channels(<code>int</code>) – 输入信号的通道数</li>
<li>out_channels(<code>int</code>) – 卷积产生的通道数</li>
<li>kerner_size(<code>int</code> or <code>tuple</code>) - 卷积核的大小</li>
<li>stride(<code>int</code> or <code>tuple</code>,<code>optional</code>) - 卷积步长</li>
<li>padding(<code>int</code> or <code>tuple</code>, <code>optional</code>) - 输入的每一条边补充0的层数</li>
<li>output_padding(<code>int</code> or <code>tuple</code>, <code>optional</code>) - 输出的每一条边补充0的层数</li>
<li>dilation(<code>int</code> or <code>tuple</code>, <code>optional</code>) – 卷积核元素之间的间距</li>
<li>groups(<code>int</code>, <code>optional</code>) – 从输入通道到输出通道的阻塞连接数</li>
<li>bias(<code>bool</code>, <code>optional</code>) - 如果<code>bias=True</code>，添加偏置</li>
</ul>
<p><strong>shape:</strong> <br />
输入: (N,C_in,H_in，W_in)  <br />
输出: (N,C_out,H_out,W_out)   <br />
<span class="arithmatex">\(<span class="arithmatex">\(H_{out}=(H_{in}-1)*stride[0]-2*padding[0]+kernel\_size[0]+output\_padding[0]\)</span>\)</span>    </p>
<div class="arithmatex">\[W_{out}=(W_{in}-1)*stride[1]-2*padding[1]+kernel\_size[1]+output\_padding[1]\]</div>
<p><strong>变量:</strong>   <br />
- weight(<code>tensor</code>) - 卷积的权重，大小是(<code>in_channels</code>, <code>in_channels</code>,<code>kernel_size</code>)  <br />
- bias(<code>tensor</code>) - 卷积的偏置系数，大小是(<code>out_channel</code>）  </p>
<p><strong>Example</strong></p>
<pre><code class="language-python">&gt;&gt;&gt; # With square kernels and equal stride
&gt;&gt;&gt; m = nn.ConvTranspose2d(16, 33, 3, stride=2)
&gt;&gt;&gt; # non-square kernels and unequal stride and with padding
&gt;&gt;&gt; m = nn.ConvTranspose2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))
&gt;&gt;&gt; input = autograd.Variable(torch.randn(20, 16, 50, 100))
&gt;&gt;&gt; output = m(input)
&gt;&gt;&gt; # exact output size can be also specified as an argument
&gt;&gt;&gt; input = autograd.Variable(torch.randn(1, 16, 12, 12))
&gt;&gt;&gt; downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1)
&gt;&gt;&gt; upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1)
&gt;&gt;&gt; h = downsample(input)
&gt;&gt;&gt; h.size()
torch.Size([1, 16, 6, 6])
&gt;&gt;&gt; output = upsample(h, output_size=input.size())
&gt;&gt;&gt; output.size()
torch.Size([1, 16, 12, 12])
</code></pre>
<h3 id="torchnnconvtranspose3din_channels-out_channels-kernel_size-stride1-padding0-output_padding0-groups1-biastrue">torch.nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True)</h3>
<p>3维的转置卷积操作(<code>transposed convolution operator</code>，注意改视作操作可视作解卷积操作，但并不是真正的解卷积操作）
转置卷积操作将每个输入值和一个可学习权重的卷积核相乘，输出所有输入通道的求和</p>
<p>该模块可以看作是<code>Conv3d</code>相对于其输入的梯度，有时(但不正确地）被称为解卷积操作。</p>
<p><strong>说明</strong></p>
<p><code>stride</code>: 控制相关系数的计算步长 <br />
<code>dilation</code>: 用于控制内核点之间的距离，详细描述在<a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">这里</a>  <br />
<code>groups</code>: 控制输入和输出之间的连接： <code>group=1</code>，输出是所有的输入的卷积；<code>group=2</code>，此时相当于有并排的两个卷积层，每个卷积层计算输入通道的一半，并且产生的输出是输出通道的一半，随后将这两个输出连接起来。     </p>
<p>参数<code>kernel\_size</code>，<code>stride</code>, <code>padding</code>，<code>dilation</code>数据类型：
一个<code>int</code>类型的数据，此时卷积height和width值相同;
也可以是一个<code>tuple</code>数组(包含来两个<code>int</code>类型的数据），第一个<code>int</code>数据表示height的数值，tuple的第二个int类型的数据表示width的数值</p>
<p><strong>注意</strong><br />
由于内核的大小，输入的最后的一些列的数据可能会丢失。因为输入和输出是不是完全的互相关。因此，用户可以进行适当的填充(padding操作）。</p>
<p><strong>参数：</strong></p>
<ul>
<li>in_channels(<code>int</code>) – 输入信号的通道数</li>
<li>out_channels(<code>int</code>) – 卷积产生的通道数</li>
<li>kernel_size(<code>int</code> or <code>tuple</code>) - 卷积核的大小</li>
<li>stride(<code>int</code> or <code>tuple</code>, <code>optional</code>) - 卷积步长</li>
<li>padding(<code>int</code> or <code>tuple</code>, <code>optional</code>) - 输入的每一条边补充0的层数</li>
<li>output_padding(<code>int</code> or <code>tuple</code>, <code>optional</code>) - 输出的每一条边补充0的层数</li>
<li>dilation(<code>int</code> or <code>tuple</code>, <code>optional</code>) – 卷积核元素之间的间距</li>
<li>groups(<code>int</code>, <code>optional</code>) – 从输入通道到输出通道的阻塞连接数</li>
<li>bias(<code>bool</code>, <code>optional</code>) - 如果<code>bias=True</code>，添加偏置</li>
</ul>
<p><strong>shape:</strong><br />
输入: (N,C_in,H_in，W_in)      <br />
输出: (N,C_out,H_out,W_out)    <br />
<span class="arithmatex">\(<span class="arithmatex">\(D_{out}=(D_{in}-1)*stride[0]-2*padding[0]+kernel\_size[0]+output\_padding[0]\)</span>\)</span>  </p>
<div class="arithmatex">\[H_{out}=(H_{in}-1)*stride[1]-2*padding[1]+kernel\_size[1]+output\_padding[0]\]</div>
<div class="arithmatex">\[W_{out}=(W_{in}-1)*stride[2]-2*padding[2]+kernel\_size[2]+output\_padding[2]\]</div>
<p><strong>变量:</strong><br />
- weight(<code>tensor</code>) - 卷积的权重，大小是(<code>in_channels</code>, <code>in_channels</code>,<code>kernel_size</code>)<br />
- bias(<code>tensor</code>) - 卷积的偏置系数，大小是(<code>out_channel</code>）</p>
<p><strong>Example</strong></p>
<pre><code class="language-python">&gt;&gt;&gt; # With square kernels and equal stride
&gt;&gt;&gt; m = nn.ConvTranspose3d(16, 33, 3, stride=2)
&gt;&gt;&gt; # non-square kernels and unequal stride and with padding
&gt;&gt;&gt; m = nn.Conv3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(0, 4, 2))
&gt;&gt;&gt; input = autograd.Variable(torch.randn(20, 16, 10, 50, 100))
&gt;&gt;&gt; output = m(input)
</code></pre>
<h2 id="_2">池化层</h2>
<h3 id="class-torchnnmaxpool1dkernel_size-stridenone-padding0-dilation1-return_indicesfalse-ceil_modefalse">class torch.nn.MaxPool1d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)</h3>
<p>对于输入信号的输入通道，提供1维最大池化(<code>max pooling</code>）操作</p>
<p>如果输入的大小是(N,C,L)，那么输出的大小是(N,C,L_out)的计算方式是：     <br />
<span class="arithmatex">\(<span class="arithmatex">\(out(N_i, C_j,k)=max^{kernel\_size-1}_{m=0}input(N_{i},C_j,stride*k+m)\)</span>\)</span>    </p>
<p>如果<code>padding</code>不是0，会在输入的每一边添加相应数目0 <br />
<code>dilation</code>用于控制内核点之间的距离，详细描述在<a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">这里</a>    </p>
<p><strong>参数：</strong></p>
<ul>
<li>kernel_size(<code>int</code> or <code>tuple</code>) - max pooling的窗口大小</li>
<li>stride(<code>int</code> or <code>tuple</code>, <code>optional</code>) - max pooling的窗口移动的步长。默认值是<code>kernel_size</code></li>
<li>padding(<code>int</code> or <code>tuple</code>, <code>optional</code>) - 输入的每一条边补充0的层数</li>
<li>dilation(<code>int</code> or <code>tuple</code>, <code>optional</code>) – 一个控制窗口中元素步幅的参数</li>
<li>return_indices - 如果等于<code>True</code>，会返回输出最大值的序号，对于上采样操作会有帮助</li>
<li>ceil_mode - 如果等于<code>True</code>，计算输出信号大小的时候，会使用向上取整，代替默认的向下取整的操作</li>
</ul>
<p><strong>shape:</strong><br />
输入: (N,C_in,L_in)     <br />
输出: (N,C_out,L_out)    <br />
<span class="arithmatex">\(<span class="arithmatex">\(L_{out}=floor((L_{in} + 2*padding - dilation*(kernel\_size - 1) - 1)/stride + 1\)</span>\)</span>  </p>
<p><strong>example:</strong></p>
<pre><code class="language-python">&gt;&gt;&gt; # pool of size=3, stride=2
&gt;&gt;&gt; m = nn.MaxPool1d(3, stride=2)
&gt;&gt;&gt; input = autograd.Variable(torch.randn(20, 16, 50))
&gt;&gt;&gt; output = m(input)
</code></pre>
<h3 id="class-torchnnmaxpool2dkernel_size-stridenone-padding0-dilation1-return_indicesfalse-ceil_modefalse">class torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)</h3>
<p>对于输入信号的输入通道，提供2维最大池化(<code>max pooling</code>）操作</p>
<p>如果输入的大小是(N,C,H,W)，那么输出的大小是(N,C,H_out,W_out)和池化窗口大小(kH,kW)的关系是： <br />
<span class="arithmatex">\(<span class="arithmatex">\(out(N_i, C_j,k)=max^{kH-1}_{m=0}max^{kW-1}_{m=0}input(N_{i},C_j,stride[0]*h+m,stride[1]*w+n)\)</span>\)</span>   </p>
<p>如果<code>padding</code>不是0，会在输入的每一边添加相应数目0 <br />
<code>dilation</code>用于控制内核点之间的距离，详细描述在<a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">这里</a>    </p>
<p>参数<code>kernel_size</code>，<code>stride</code>, <code>padding</code>，<code>dilation</code>数据类型：
可以是一个<code>int</code>类型的数据，此时卷积height和width值相同;
也可以是一个<code>tuple</code>数组(包含来两个int类型的数据），第一个<code>int</code>数据表示height的数值，<code>tuple</code>的第二个int类型的数据表示width的数值</p>
<p><strong>参数：</strong></p>
<ul>
<li>kernel_size(<code>int</code> or <code>tuple</code>) - max pooling的窗口大小</li>
<li>stride(<code>int</code> or <code>tuple</code>, <code>optional</code>) - max pooling的窗口移动的步长。默认值是<code>kernel_size</code></li>
<li>padding(<code>int</code> or <code>tuple</code>, <code>optional</code>) - 输入的每一条边补充0的层数</li>
<li>dilation(<code>int</code> or <code>tuple</code>, <code>optional</code>) – 一个控制窗口中元素步幅的参数</li>
<li>return_indices - 如果等于<code>True</code>，会返回输出最大值的序号，对于上采样操作会有帮助</li>
<li>ceil_mode - 如果等于<code>True</code>，计算输出信号大小的时候，会使用向上取整，代替默认的向下取整的操作</li>
</ul>
<p><strong>shape:</strong>  <br />
输入: (N,C,H_{in},W_in)    <br />
输出: (N,C,H_out,W_out)  <br />
<span class="arithmatex">\(<span class="arithmatex">\(H_{out}=floor((H_{in} + 2*padding[0] - dilation[0]*(kernel\_size[0] - 1) - 1)/stride[0] + 1\)</span>\)</span>   </p>
<div class="arithmatex">\[W_{out}=floor((W_{in} + 2*padding[1] - dilation[1]*(kernel\_size[1] - 1) - 1)/stride[1] + 1\]</div>
<p><strong>example:</strong>  </p>
<pre><code class="language-python">&gt;&gt;&gt; # pool of square window of size=3, stride=2
&gt;&gt;&gt; m = nn.MaxPool2d(3, stride=2)
&gt;&gt;&gt; # pool of non-square window
&gt;&gt;&gt; m = nn.MaxPool2d((3, 2), stride=(2, 1))
&gt;&gt;&gt; input = autograd.Variable(torch.randn(20, 16, 50, 32))
&gt;&gt;&gt; output = m(input)
</code></pre>
<h3 id="class-torchnnmaxpool3dkernel_size-stridenone-padding0-dilation1-return_indicesfalse-ceil_modefalse">class torch.nn.MaxPool3d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)</h3>
<p>对于输入信号的输入通道，提供3维最大池化(max pooling）操作</p>
<p>如果输入的大小是(N,C,D,H,W)，那么输出的大小是(N,C,D,H_out,W_out)和池化窗口大小(kD,kH,kW)的关系是：  <br />
<span class="arithmatex">\(<span class="arithmatex">\(out(N_i,C_j,d,h,w)=max^{kD-1}_{m=0}max^{kH-1}_{m=0}max^{kW-1}_{m=0}\)</span>\)</span>   </p>
<div class="arithmatex">\[input(N_{i},C_j,stride[0]*k+d,stride[1]*h+m,stride[2]*w+n)\]</div>
<p>如果<code>padding</code>不是0，会在输入的每一边添加相应数目0   <br />
<code>dilation</code>用于控制内核点之间的距离，详细描述在<a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">这里</a>      </p>
<p>参数<code>kernel_size</code>，<code>stride</code>, <code>padding</code>，<code>dilation</code>数据类型：
可以是<code>int</code>类型的数据，此时卷积height和width值相同;
也可以是一个<code>tuple</code>数组(包含来两个<code>int</code>类型的数据），第一个<code>int</code>数据表示height的数值，<code>tuple</code>的第二个<code>int</code>类型的数据表示width的数值</p>
<p><strong>参数：</strong></p>
<ul>
<li>kernel_size(<code>int</code> or <code>tuple</code>) - max pooling的窗口大小</li>
<li>stride(<code>int</code> or <code>tuple</code>, <code>optional</code>) - max pooling的窗口移动的步长。默认值是kernel_size</li>
<li>padding(<code>int</code> or <code>tuple</code>, <code>optional</code>) - 输入的每一条边补充0的层数</li>
<li>dilation(<code>int</code> or <code>tuple</code>, <code>optional</code>) – 一个控制窗口中元素步幅的参数</li>
<li>return_indices - 如果等于<code>True</code>，会返回输出最大值的序号，对于上采样操作会有帮助</li>
<li>ceil_mode - 如果等于<code>True</code>，计算输出信号大小的时候，会使用向上取整，代替默认的向下取整的操作</li>
</ul>
<p><strong>shape:</strong><br />
输入: (N,C,H_in,W_in)  <br />
输出: (N,C,H_out,W_out)<br />
<span class="arithmatex">\(<span class="arithmatex">\(D_{out}=floor((D_{in} + 2*padding[0] - dilation[0]*(kernel\_size[0] - 1) - 1)/stride[0] + 1)\)</span>\)</span>   </p>
<div class="arithmatex">\[H_{out}=floor((H_{in} + 2*padding[1] - dilation[1]*(kernel\_size[0] - 1) - 1)/stride[1] + 1)\]</div>
<div class="arithmatex">\[W_{out}=floor((W_{in} + 2*padding[2] - dilation[2]*(kernel\_size[2] - 1) - 1)/stride[2] + 1)\]</div>
<p><strong>example:</strong>  </p>
<pre><code class="language-python">&gt;&gt;&gt; # pool of square window of size=3, stride=2
&gt;&gt;&gt;m = nn.MaxPool3d(3, stride=2)
&gt;&gt;&gt; # pool of non-square window
&gt;&gt;&gt; m = nn.MaxPool3d((3, 2, 2), stride=(2, 1, 2))
&gt;&gt;&gt; input = autograd.Variable(torch.randn(20, 16, 50,44, 31))  
&gt;&gt;&gt; output = m(input)
</code></pre>
<h4 id="class-torchnnmaxunpool1dkernel_size-stridenone-padding0">class torch.nn.MaxUnpool1d(kernel_size, stride=None, padding=0)</h4>
<p><code>Maxpool1d</code>的逆过程，不过并不是完全的逆过程，因为在<code>maxpool1d</code>的过程中，一些最大值的已经丢失。
<code>MaxUnpool1d</code>输入<code>MaxPool1d</code>的输出，包括最大值的索引，并计算所有<code>maxpool1d</code>过程中非最大值被设置为零的部分的反向。</p>
<p><strong>注意：</strong><br />
<code>MaxPool1d</code>可以将多个输入大小映射到相同的输出大小。因此，反演过程可能会变得模棱两可。 为了适应这一点，可以在调用中将输出大小(<code>output_size</code>）作为额外的参数传入。 具体用法，请参阅下面的输入和示例</p>
<p><strong>参数：</strong></p>
<ul>
<li>kernel_size(<code>int</code> or <code>tuple</code>) - max pooling的窗口大小</li>
<li>stride(<code>int</code> or <code>tuple</code>, <code>optional</code>) - max pooling的窗口移动的步长。默认值是<code>kernel_size</code></li>
<li>padding(<code>int</code> or <code>tuple</code>, <code>optional</code>) - 输入的每一条边补充0的层数</li>
</ul>
<p><strong>输入：</strong><br />
<code>input</code>:需要转换的<code>tensor</code>
<code>indices</code>：Maxpool1d的索引号
<code>output_size</code>:一个指定输出大小的<code>torch.Size</code></p>
<p><strong>shape:</strong><br />
<code>input</code>: (N,C,H_in)<br />
<code>output</code>:(N,C,H_out)   <br />
<span class="arithmatex">\(<span class="arithmatex">\(H_{out}=(H_{in}-1)*stride[0]-2*padding[0]+kernel\_size[0]\)</span>\)</span>   <br />
也可以使用<code>output_size</code>指定输出的大小</p>
<p><strong>Example：</strong></p>
<pre><code class="language-python">&gt;&gt;&gt; pool = nn.MaxPool1d(2, stride=2, return_indices=True)
&gt;&gt;&gt; unpool = nn.MaxUnpool1d(2, stride=2)
&gt;&gt;&gt; input = Variable(torch.Tensor([[[1, 2, 3, 4, 5, 6, 7, 8]]]))
&gt;&gt;&gt; output, indices = pool(input)
&gt;&gt;&gt; unpool(output, indices)
    Variable containing:
    (0 ,.,.) =
       0   2   0   4   0   6   0   8
    [torch.FloatTensor of size 1x1x8]

&gt;&gt;&gt; # Example showcasing the use of output_size
&gt;&gt;&gt; input = Variable(torch.Tensor([[[1, 2, 3, 4, 5, 6, 7, 8, 9]]]))
&gt;&gt;&gt; output, indices = pool(input)
&gt;&gt;&gt; unpool(output, indices, output_size=input.size())
    Variable containing:
    (0 ,.,.) =
       0   2   0   4   0   6   0   8   0
    [torch.FloatTensor of size 1x1x9]
&gt;&gt;&gt; unpool(output, indices)
    Variable containing:
    (0 ,.,.) =
       0   2   0   4   0   6   0   8
    [torch.FloatTensor of size 1x1x8]
</code></pre>
<h4 id="class-torchnnmaxunpool2dkernel_size-stridenone-padding0">class torch.nn.MaxUnpool2d(kernel_size, stride=None, padding=0)</h4>
<p><code>Maxpool2d</code>的逆过程，不过并不是完全的逆过程，因为在maxpool2d的过程中，一些最大值的已经丢失。
<code>MaxUnpool2d</code>的输入是<code>MaxPool2d</code>的输出，包括最大值的索引，并计算所有<code>maxpool2d</code>过程中非最大值被设置为零的部分的反向。</p>
<p><strong>注意：</strong><br />
<code>MaxPool2d</code>可以将多个输入大小映射到相同的输出大小。因此，反演过程可能会变得模棱两可。 为了适应这一点，可以在调用中将输出大小(<code>output_size</code>）作为额外的参数传入。具体用法，请参阅下面示例</p>
<p><strong>参数：</strong></p>
<ul>
<li>kernel_size(<code>int</code> or <code>tuple</code>) - max pooling的窗口大小</li>
<li>stride(<code>int</code> or <code>tuple</code>, <code>optional</code>) - max pooling的窗口移动的步长。默认值是<code>kernel_size</code></li>
<li>padding(<code>int</code> or <code>tuple</code>, <code>optional</code>) - 输入的每一条边补充0的层数</li>
</ul>
<p><strong>输入：</strong><br />
<code>input</code>:需要转换的<code>tensor</code> <br />
<code>indices</code>：Maxpool1d的索引号  <br />
<code>output_size</code>:一个指定输出大小的<code>torch.Size</code></p>
<p><strong>大小：</strong> <br />
<code>input</code>: (N,C,H_in,W_in)     <br />
<code>output</code>:(N,C,H_out,W_out)        </p>
<div class="arithmatex">\[H_{out}=(H_{in}-1)*stride[0]-2*padding[0]+kernel\_size[0]\]</div>
<div class="arithmatex">\[W_{out}=(W_{in}-1)*stride[1]-2*padding[1]+kernel\_size[1]\]</div>
<p>也可以使用<code>output_size</code>指定输出的大小</p>
<p><strong>Example：</strong></p>
<pre><code class="language-python">&gt;&gt;&gt; pool = nn.MaxPool2d(2, stride=2, return_indices=True)
&gt;&gt;&gt; unpool = nn.MaxUnpool2d(2, stride=2)
&gt;&gt;&gt; input = Variable(torch.Tensor([[[[ 1,  2,  3,  4],
    ...                                  [ 5,  6,  7,  8],
    ...                                  [ 9, 10, 11, 12],
    ...                                  [13, 14, 15, 16]]]]))
&gt;&gt;&gt; output, indices = pool(input)
&gt;&gt;&gt; unpool(output, indices)
    Variable containing:
    (0 ,0 ,.,.) =
       0   0   0   0
       0   6   0   8
       0   0   0   0
       0  14   0  16
    [torch.FloatTensor of size 1x1x4x4]

&gt;&gt;&gt; # specify a different output size than input size
&gt;&gt;&gt; unpool(output, indices, output_size=torch.Size([1, 1, 5, 5]))
    Variable containing:
    (0 ,0 ,.,.) =
       0   0   0   0   0
       6   0   8   0   0
       0   0   0  14   0
      16   0   0   0   0
       0   0   0   0   0
    [torch.FloatTensor of size 1x1x5x5]
</code></pre>
<h4 id="class-torchnnmaxunpool3dkernel_size-stridenone-padding0">class torch.nn.MaxUnpool3d(kernel_size, stride=None, padding=0)</h4>
<p><code>Maxpool3d</code>的逆过程，不过并不是完全的逆过程，因为在<code>maxpool3d</code>的过程中，一些最大值的已经丢失。
<code>MaxUnpool3d</code>的输入就是<code>MaxPool3d</code>的输出，包括最大值的索引，并计算所有<code>maxpool3d</code>过程中非最大值被设置为零的部分的反向。</p>
<p><strong>注意：</strong>  <br />
<code>MaxPool3d</code>可以将多个输入大小映射到相同的输出大小。因此，反演过程可能会变得模棱两可。为了适应这一点，可以在调用中将输出大小(<code>output_size</code>）作为额外的参数传入。具体用法，请参阅下面的输入和示例</p>
<p><strong>参数：</strong></p>
<ul>
<li>kernel_size(<code>int</code> or <code>tuple</code>) - Maxpooling窗口大小</li>
<li>stride(<code>int</code> or <code>tuple</code>, <code>optional</code>) - max pooling的窗口移动的步长。默认值是<code>kernel_size</code></li>
<li>padding(<code>int</code> or <code>tuple</code>, <code>optional</code>) - 输入的每一条边补充0的层数</li>
</ul>
<p><strong>输入：</strong><br />
<code>input</code>:需要转换的<code>tensor</code> <br />
<code>indices</code>：<code>Maxpool1d</code>的索引序数 <br />
<code>output_size</code>:一个指定输出大小的<code>torch.Size</code></p>
<p><strong>大小:</strong>   <br />
<code>input</code>: (N,C,D_in,H_in,W_in)  <br />
<code>outpu</code>t:(N,C,D_out,H_out,W_out)  </p>
<div class="arithmatex">\[
\begin{aligned}
&amp;D_{out}=(D_{in}-1)*stride[0]-2*padding[0]+kernel\_size[0]\\
&amp;H_{out}=(H_{in}-1)*stride[1]-2*padding[0]+kernel\_size[1]\\ 
&amp;W_{out}=(W_{in}-1)*stride[2]-2*padding[2]+kernel\_size[2]\\
\end{aligned}
\]</div>
<p>也可以使用<code>output_size</code>指定输出的大小</p>
<p><strong>Example：</strong></p>
<pre><code class="language-python">&gt;&gt;&gt; # pool of square window of size=3, stride=2
&gt;&gt;&gt; pool = nn.MaxPool3d(3, stride=2, return_indices=True)
&gt;&gt;&gt; unpool = nn.MaxUnpool3d(3, stride=2)
&gt;&gt;&gt; output, indices = pool(Variable(torch.randn(20, 16, 51, 33, 15)))
&gt;&gt;&gt; unpooled_output = unpool(output, indices)
&gt;&gt;&gt; unpooled_output.size()
torch.Size([20, 16, 51, 33, 15])
</code></pre>
<h3 id="class-torchnnavgpool1dkernel_size-stridenone-padding0-ceil_modefalse-count_include_padtrue">class torch.nn.AvgPool1d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True)</h3>
<p>对信号的输入通道，提供1维平均池化(average pooling )
输入信号的大小(N,C,L)，输出大小(N,C,L_out)和池化窗口大小k的关系是：  <br />
<span class="arithmatex">\(<span class="arithmatex">\(out(N_i,C_j,l)=1/k*\sum^{k}_{m=0}input(N_{i},C_{j},stride*l+m)\)</span>\)</span>   <br />
如果<code>padding</code>不是0，会在输入的每一边添加相应数目0</p>
<p><strong>参数：</strong></p>
<ul>
<li>kernel_size(<code>int</code> or <code>tuple</code>) - 池化窗口大小</li>
<li>stride(<code>int</code> or <code>tuple</code>, <code>optional</code>) - max pooling的窗口移动的步长。默认值是<code>kernel_size</code></li>
<li>padding(<code>int</code> or <code>tuple</code>, <code>optional</code>) - 输入的每一条边补充0的层数</li>
<li>dilation(<code>int</code> or <code>tuple</code>, <code>optional</code>) – 一个控制窗口中元素步幅的参数</li>
<li>return_indices - 如果等于<code>True</code>，会返回输出最大值的序号，对于上采样操作会有帮助</li>
<li>ceil_mode - 如果等于<code>True</code>，计算输出信号大小的时候，会使用向上取整，代替默认的向下取整的操作</li>
</ul>
<p><strong>大小：</strong><br />
<code>input</code>:(N,C,L_in)<br />
<code>output</code>:(N,C,L_out)<br />
<span class="arithmatex">\(<span class="arithmatex">\(L_{out}=floor((L_{in}+2*padding-kernel\_size)/stride+1)\)</span>\)</span>      </p>
<p><strong>Example:</strong></p>
<pre><code class="language-python">&gt;&gt;&gt; # pool with window of size=3, stride=2
&gt;&gt;&gt; m = nn.AvgPool1d(3, stride=2)
&gt;&gt;&gt; m(Variable(torch.Tensor([[[1,2,3,4,5,6,7]]])))
Variable containing:
    (0 ,.,.) =
    2  4  6
    [torch.FloatTensor of size 1x1x3]
</code></pre>
<h3 id="class-torchnnavgpool2dkernel_size-stridenone-padding0-ceil_modefalse-count_include_padtrue">class torch.nn.AvgPool2d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True)</h3>
<p>对信号的输入通道，提供2维的平均池化(average pooling )<br />
输入信号的大小(N,C,H,W)，输出大小(N,C,H_out,W_out)和池化窗口大小(kH,kW)的关系是：        <br />
$$
out(N_i,C_j,h,w)=1/(kH<em>kW)</em>\sum^{kH-1}<em>{m=0}\sum^{kW-1}</em>{n=0}input(N_{i},C_{j},stride[0]<em>h+m,stride[1]</em>w+n)$$       </p>
<p>如果<code>padding</code>不是0，会在输入的每一边添加相应数目0</p>
<p><strong>参数：</strong></p>
<ul>
<li>kernel_size(<code>int</code> or <code>tuple</code>) - 池化窗口大小</li>
<li>stride(<code>int</code> or <code>tuple</code>, <code>optional</code>) - max pooling的窗口移动的步长。默认值是<code>kernel_size</code></li>
<li>padding(<code>int</code> or <code>tuple</code>, <code>optional</code>) - 输入的每一条边补充0的层数</li>
<li>dilation(<code>int</code> or <code>tuple</code>, <code>optional</code>) – 一个控制窗口中元素步幅的参数</li>
<li>ceil_mode - 如果等于<code>True</code>，计算输出信号大小的时候，会使用向上取整，代替默认的向下取整的操作</li>
<li>count_include_pad - 如果等于<code>True</code>，计算平均池化时，将包括<code>padding</code>填充的0</li>
</ul>
<p><strong>shape：</strong><br />
<code>input</code>: (N,C,H_in,W_in)<br />
<code>output</code>: (N,C,H_out,W_out)  </p>
<div class="arithmatex">\[
\begin{aligned}
&amp;H_{out}=floor((H_{in}+2*padding[0]-kernel\_size[0])/stride[0]+1)\\  
&amp;W_{out}=floor((W_{in}+2*padding[1]-kernel\_size[1])/stride[1]+1)\\
\end{aligned}
\]</div>
<p><strong>Example:</strong></p>
<pre><code class="language-python">&gt;&gt;&gt; # pool of square window of size=3, stride=2
&gt;&gt;&gt; m = nn.AvgPool2d(3, stride=2)
&gt;&gt;&gt; # pool of non-square window
&gt;&gt;&gt; m = nn.AvgPool2d((3, 2), stride=(2, 1))
&gt;&gt;&gt; input = autograd.Variable(torch.randn(20, 16, 50, 32))
&gt;&gt;&gt; output = m(input)
</code></pre>
<h3 id="class-torchnnavgpool3dkernel_size-stridenone">class torch.nn.AvgPool3d(kernel_size, stride=None)</h3>
<p>对信号的输入通道，提供3维的平均池化(<code>average pooling</code>）
输入信号的大小(N,C,D,H,W)，输出大小(N,C,D_out,H_out,W_out)和池化窗口大小(kD,kH,kW)的关系是：    </p>
<p>$$
\begin{aligned}
out(N_i,C_j,d,h,w)=1/(kD<em>kH</em>kW)<em>\sum^{kD-1}<em>{k=0}\sum^{kH-1}</em>{m=0}\sum^{kW-1}<em>{n=0}input(N</em>{i},C_{j},stride[0]</em>d+k,stride[1]<em>h+m,stride[2]</em>w+n)
\end{aligned}
$$
如果<code>padding</code>不是0，会在输入的每一边添加相应数目0</p>
<p><strong>参数：</strong></p>
<ul>
<li>kernel_size(<code>int</code> or <code>tuple</code>) - 池化窗口大小</li>
<li>stride(<code>int</code> or <code>tuple</code>, <code>optional</code>) - max <code>pooling</code>的窗口移动的步长。默认值是<code>kernel_size</code></li>
</ul>
<p><strong>shape：</strong>   <br />
输入大小:(N,C,D_in,H_in,W_in)<br />
输出大小:(N,C,D_out,H_out,W_out)</p>
<div class="arithmatex">\[
\begin{aligned}
D_{out}=floor((D_{in}+2*padding[0]-kernel\_size[0])/stride[0]+1)\\  
H_{out}=floor((H_{in}+2*padding[1]-kernel\_size[1])/stride[1]+1)\\  
W_{out}=floor((W_{in}+2*padding[2]-kernel\_size[2])/stride[2]+1)  
\end{aligned}
\]</div>
<p><strong>Example:</strong></p>
<pre><code class="language-python">&gt;&gt;&gt; # pool of square window of size=3, stride=2
&gt;&gt;&gt; m = nn.AvgPool3d(3, stride=2)
&gt;&gt;&gt; # pool of non-square window
&gt;&gt;&gt; m = nn.AvgPool3d((3, 2, 2), stride=(2, 1, 2))
&gt;&gt;&gt; input = autograd.Variable(torch.randn(20, 16, 50,44, 31))
&gt;&gt;&gt; output = m(input)
</code></pre>
<h3 id="class-torchnnfractionalmaxpool2dkernel_size-output_sizenone-output_rationone-return_indicesfalse-_random_samplesnone">class torch.nn.FractionalMaxPool2d(kernel_size, output_size=None, output_ratio=None, return_indices=False, _random_samples=None)</h3>
<p>对输入的信号，提供2维的分数最大化池化操作
分数最大化池化的细节请阅读<a href="https://arxiv.org/abs/1412.6071">论文</a>
由目标输出大小确定的随机步长,在<span class="arithmatex">\(kH*kW\)</span>区域进行最大池化操作。输出特征和输入特征的数量相同。</p>
<p><strong>参数：</strong></p>
<ul>
<li>kernel_size(<code>int</code> or <code>tuple</code>) - 最大池化操作时的窗口大小。可以是一个数字(表示<code>K*K</code>的窗口），也可以是一个元组(<code>kh*kw</code>）</li>
<li>output_size - 输出图像的尺寸。可以使用一个<code>tuple</code>指定(oH,oW)，也可以使用一个数字oH指定一个oH*oH的输出。</li>
<li>output_ratio – 将输入图像的大小的百分比指定为输出图片的大小，使用一个范围在(0,1)之间的数字指定   </li>
<li>return_indices - 默认值<code>False</code>，如果设置为<code>True</code>，会返回输出的索引，索引对  <code>nn.MaxUnpool2d</code>有用。</li>
</ul>
<p><strong>Example：</strong></p>
<pre><code class="language-python">&gt;&gt;&gt; # pool of square window of size=3, and target output size 13x12
&gt;&gt;&gt; m = nn.FractionalMaxPool2d(3, output_size=(13, 12))
&gt;&gt;&gt; # pool of square window and target output size being half of input image size
&gt;&gt;&gt; m = nn.FractionalMaxPool2d(3, output_ratio=(0.5, 0.5))
&gt;&gt;&gt; input = autograd.Variable(torch.randn(20, 16, 50, 32))
&gt;&gt;&gt; output = m(input)
</code></pre>
<h3 id="class-torchnnlppool2dnorm_type-kernel_size-stridenone-ceil_modefalse">class torch.nn.LPPool2d(norm_type, kernel_size, stride=None, ceil_mode=False)</h3>
<p>对输入信号提供2维的幂平均池化操作。
输出的计算方式：   <br />
<span class="arithmatex">\(<span class="arithmatex">\(f(x)=pow(sum(X,p),1/p)\)</span>\)</span>  </p>
<ul>
<li>当p为无穷大的时候时，等价于最大池化操作</li>
<li>当<code>p=1</code>时，等价于平均池化操作</li>
</ul>
<p>参数<code>kernel_size</code>, <code>stride</code>的数据类型：</p>
<ul>
<li><code>int</code>，池化窗口的宽和高相等</li>
<li><code>tuple</code>数组(两个数字的），一个元素是池化窗口的高，另一个是宽</li>
</ul>
<p><strong>参数</strong></p>
<ul>
<li>kernel_size: 池化窗口的大小</li>
<li>stride：池化窗口移动的步长。<code>kernel_size</code>是默认值</li>
<li>ceil_mode: <code>ceil_mode=True</code>时，将使用向下取整代替向上取整</li>
</ul>
<p><strong>shape</strong></p>
<ul>
<li>输入：(N,C,H_in,W_in)  </li>
<li>输出：(N,C,H_out,W_out) </li>
</ul>
<div class="arithmatex">\[
\begin{aligned}
H_{out} = floor((H_{in}+2*padding[0]-dilation[0]*(kernel\_size[0]-1)-1)/stride[0]+1)\\
W_{out} = floor((W_{in}+2*padding[1]-dilation[1]*(kernel\_size[1]-1)-1)/stride[1]+1)
\end{aligned}
\]</div>
<p><strong>Example:</strong></p>
<pre><code class="language-python">&gt;&gt;&gt; # power-2 pool of square window of size=3, stride=2
&gt;&gt;&gt; m = nn.LPPool2d(2, 3, stride=2)
&gt;&gt;&gt; # pool of non-square window of power 1.2
&gt;&gt;&gt; m = nn.LPPool2d(1.2, (3, 2), stride=(2, 1))
&gt;&gt;&gt; input = autograd.Variable(torch.randn(20, 16, 50, 32))
&gt;&gt;&gt; output = m(input)
</code></pre>
<h3 id="class-torchnnadaptivemaxpool1doutput_size-return_indicesfalse">class torch.nn.AdaptiveMaxPool1d(output_size, return_indices=False)</h3>
<p>对输入信号，提供1维的自适应最大池化操作
对于任何输入大小的输入，可以将输出尺寸指定为H，但是输入和输出特征的数目不会变化。</p>
<p><strong>参数：</strong></p>
<ul>
<li>output_size: 输出信号的尺寸</li>
<li>return_indices: 如果设置为<code>True</code>，会返回输出的索引。对  <code>nn.MaxUnpool1d</code>有用，默认值是<code>False</code></li>
</ul>
<p><strong>Example：</strong></p>
<pre><code class="language-python">&gt;&gt;&gt; # target output size of 5
&gt;&gt;&gt; m = nn.AdaptiveMaxPool1d(5)
&gt;&gt;&gt; input = autograd.Variable(torch.randn(1, 64, 8))
&gt;&gt;&gt; output = m(input)
</code></pre>
<h3 id="class-torchnnadaptivemaxpool2doutput_size-return_indicesfalse">class torch.nn.AdaptiveMaxPool2d(output_size, return_indices=False)</h3>
<p>对输入信号，提供2维的自适应最大池化操作
对于任何输入大小的输入，可以将输出尺寸指定为H*W，但是输入和输出特征的数目不会变化。</p>
<p><strong>参数：</strong></p>
<ul>
<li>output_size: 输出信号的尺寸,可以用(H,W）表示<code>H*W</code>的输出，也可以使用数字<code>H</code>表示<code>H*H</code>大小的输出</li>
<li>return_indices: 如果设置为<code>True</code>，会返回输出的索引。对  <code>nn.MaxUnpool2d</code>有用，默认值是<code>False</code></li>
</ul>
<p><strong>Example：</strong></p>
<pre><code class="language-python">&gt;&gt;&gt; # target output size of 5x7
&gt;&gt;&gt; m = nn.AdaptiveMaxPool2d((5,7))
&gt;&gt;&gt; input = autograd.Variable(torch.randn(1, 64, 8, 9))
&gt;&gt;&gt; # target output size of 7x7 (square)
&gt;&gt;&gt; m = nn.AdaptiveMaxPool2d(7)
&gt;&gt;&gt; input = autograd.Variable(torch.randn(1, 64, 10, 9))
&gt;&gt;&gt; output = m(input)
</code></pre>
<h3 id="class-torchnnadaptiveavgpool1doutput_size">class torch.nn.AdaptiveAvgPool1d(output_size)</h3>
<p>对输入信号，提供1维的自适应平均池化操作
对于任何输入大小的输入，可以将输出尺寸指定为H*W，但是输入和输出特征的数目不会变化。</p>
<p><strong>参数：</strong></p>
<ul>
<li>output_size: 输出信号的尺寸  </li>
</ul>
<p><strong>Example：</strong></p>
<pre><code class="language-python">&gt;&gt;&gt; # target output size of 5
&gt;&gt;&gt; m = nn.AdaptiveAvgPool1d(5)
&gt;&gt;&gt; input = autograd.Variable(torch.randn(1, 64, 8))
&gt;&gt;&gt; output = m(input)
</code></pre>
<h3 id="class-torchnnadaptiveavgpool2doutput_size">class torch.nn.AdaptiveAvgPool2d(output_size)</h3>
<p>对输入信号，提供2维的自适应平均池化操作
对于任何输入大小的输入，可以将输出尺寸指定为<code>H*W</code>，但是输入和输出特征的数目不会变化。</p>
<p><strong>参数：</strong></p>
<ul>
<li>output_size: 输出信号的尺寸,可以用(H,W)表示<code>H*W</code>的输出，也可以使用耽搁数字H表示H*H大小的输出</li>
</ul>
<p><strong>Example：</strong></p>
<pre><code class="language-python">&gt;&gt;&gt; # target output size of 5x7
&gt;&gt;&gt; m = nn.AdaptiveAvgPool2d((5,7))
&gt;&gt;&gt; input = autograd.Variable(torch.randn(1, 64, 8, 9))
&gt;&gt;&gt; # target output size of 7x7 (square)
&gt;&gt;&gt; m = nn.AdaptiveAvgPool2d(7)
&gt;&gt;&gt; input = autograd.Variable(torch.randn(1, 64, 10, 9))
&gt;&gt;&gt; output = m(input)
</code></pre>
<h2 id="non-linear-activations-source">Non-Linear Activations <a href="https://pytorch.org/docs/nn.html#non-linear-activations"><font size=2>[source]</font></a></h2>
<blockquote>
<p>class torch.nn.ReLU(inplace=False) <a href="https://pytorch.org/docs/_modules/torch/nn/modules/activation.html#ReLU"><font size=2>[source]</font></a></p>
</blockquote>
<p>对输入运用修正线性单元函数<span class="arithmatex">\({ReLU}(x)= max(0, x)\)</span>，</p>
<p>参数： inplace-选择是否进行覆盖运算</p>
<p>shape：</p>
<ul>
<li>输入：<span class="arithmatex">\((N, *)\)</span>，*代表任意数目附加维度</li>
<li>输出：<span class="arithmatex">\((N, *)\)</span>，与输入拥有同样的shape属性</li>
</ul>
<p>例子：</p>
<pre><code class="language-python">&gt;&gt;&gt; m = nn.ReLU()
&gt;&gt;&gt; input = autograd.Variable(torch.randn(2))
&gt;&gt;&gt; print(input)
&gt;&gt;&gt; print(m(input))
</code></pre>
<blockquote>
<p>class torch.nn.ReLU6(inplace=False) <a href="https://pytorch.org/docs/_modules/torch/nn/modules/activation.html#ReLU6"><font size=2>[source]</font></a></p>
</blockquote>
<p>对输入的每一个元素运用函数<span class="arithmatex">\({ReLU6}(x) = min(max(0,x), 6)\)</span>，</p>
<p>参数： inplace-选择是否进行覆盖运算</p>
<p>shape：</p>
<ul>
<li>输入：<span class="arithmatex">\((N, *)\)</span>，*代表任意数目附加维度</li>
<li>输出：<span class="arithmatex">\((N, *)\)</span>，与输入拥有同样的shape属性</li>
</ul>
<p>例子：</p>
<pre><code class="language-python">&gt;&gt;&gt; m = nn.ReLU6()
&gt;&gt;&gt; input = autograd.Variable(torch.randn(2))
&gt;&gt;&gt; print(input)
&gt;&gt;&gt; print(m(input))
</code></pre>
<blockquote>
<p>class torch.nn.ELU(alpha=1.0,   inplace=False) <a href="https://pytorch.org/docs/_modules/torch/nn/modules/activation.html#ELU"><font size=2>[source]</font></a></p>
</blockquote>
<p>对输入的每一个元素运用函数<span class="arithmatex">\(f(x) = max(0,x) + min(0, alpha * (e^x - 1))\)</span>，</p>
<p>shape：</p>
<ul>
<li>输入：<span class="arithmatex">\((N, *)\)</span>，星号代表任意数目附加维度</li>
<li>输出：<span class="arithmatex">\((N, *)\)</span>与输入拥有同样的shape属性</li>
</ul>
<p>例子：</p>
<pre><code class="language-python">&gt;&gt;&gt; m = nn.ELU()
&gt;&gt;&gt; input = autograd.Variable(torch.randn(2))
&gt;&gt;&gt; print(input)
&gt;&gt;&gt; print(m(input))
</code></pre>
<blockquote>
<p>class torch.nn.PReLU(num_parameters=1, init=0.25)<a href="https://pytorch.org/docs/_modules/torch/nn/modules/activation.html#PReLU"><font size=2>[source]</font></a></p>
</blockquote>
<p>对输入的每一个元素运用函数<span class="arithmatex">\(PReLU(x) = max(0,x) + a * min(0,x)\)</span>，<code>a</code>是一个可学习参数。当没有声明时，<code>nn.PReLU()</code>在所有的输入中只有一个参数<code>a</code>；如果是<code>nn.PReLU(nChannels)</code>，<code>a</code>将应用到每个输入。</p>
<p>注意：当为了表现更佳的模型而学习参数<code>a</code>时不要使用权重衰减(weight decay）</p>
<p>参数：</p>
<ul>
<li>num_parameters：需要学习的<code>a</code>的个数，默认等于1</li>
<li>init：<code>a</code>的初始值，默认等于0.25</li>
</ul>
<p>shape：</p>
<ul>
<li>输入：<span class="arithmatex">\((N, *)\)</span>，*代表任意数目附加维度</li>
<li>输出：<span class="arithmatex">\((N, *)\)</span>，与输入拥有同样的shape属性</li>
</ul>
<p>例子：</p>
<pre><code class="language-python">&gt;&gt;&gt; m = nn.PReLU()
&gt;&gt;&gt; input = autograd.Variable(torch.randn(2))
&gt;&gt;&gt; print(input)
&gt;&gt;&gt; print(m(input))
</code></pre>
<blockquote>
<p>class torch.nn.LeakyReLU(negative_slope=0.01, inplace=False) <a href="https://pytorch.org/docs/_modules/torch/nn/modules/activation.html#LeakyReLU"><font size=2>[source]</font></a></p>
</blockquote>
<p>对输入的每一个元素运用<span class="arithmatex">\(f(x) = max(0, x) + {negative\_slope} * min(0, x)\)</span></p>
<p>参数：</p>
<ul>
<li>negative_slope：控制负斜率的角度，默认等于0.01</li>
<li>inplace-选择是否进行覆盖运算</li>
</ul>
<p>shape：</p>
<ul>
<li>输入：<span class="arithmatex">\((N, *)\)</span>，*代表任意数目附加维度</li>
<li>输出：<span class="arithmatex">\((N, *)\)</span>，与输入拥有同样的shape属性</li>
</ul>
<p>例子：</p>
<pre><code class="language-python">&gt;&gt;&gt; m = nn.LeakyReLU(0.1)
&gt;&gt;&gt; input = autograd.Variable(torch.randn(2))
&gt;&gt;&gt; print(input)
&gt;&gt;&gt; print(m(input))
</code></pre>
<blockquote>
<p>class torch.nn.Threshold(threshold, value, inplace=False) <a href="https://pytorch.org/docs/_modules/torch/nn/modules/activation.html#Threshold"><font size=2>[source]</font></a></p>
</blockquote>
<p>Threshold定义：</p>
<pre><code>y = x, if x &gt;= threshold
y = value, if x &lt; threshold
</code></pre>
<p>参数：</p>
<ul>
<li>threshold：阈值</li>
<li>value：输入值小于阈值则会被value代替</li>
<li>inplace：选择是否进行覆盖运算</li>
</ul>
<p>shape：</p>
<ul>
<li>输入：<span class="arithmatex">\((N, *)\)</span>，*代表任意数目附加维度</li>
<li>输出：<span class="arithmatex">\((N, *)\)</span>，与输入拥有同样的shape属性</li>
</ul>
<p>例子：</p>
<pre><code class="language-python">&gt;&gt;&gt; m = nn.Threshold(0.1, 20)
&gt;&gt;&gt; input = Variable(torch.randn(2))
&gt;&gt;&gt; print(input)
&gt;&gt;&gt; print(m(input))
</code></pre>
<blockquote>
<p>class torch.nn.Hardtanh(min_value=-1, max_value=1, inplace=False) <a href="https://pytorch.org/docs/_modules/torch/nn/modules/activation.html#Hardtanh"><font size=2>[source]</font></a></p>
</blockquote>
<p>对每个元素，</p>
<pre><code>f(x) = +1, if\ x  &gt;  1
f(x) = -1, if\ x  &lt; -1
f(x) =  x,  otherwise
</code></pre>
<p>线性区域的范围[-1,1]可以被调整</p>
<p>参数：</p>
<ul>
<li>min_value：线性区域范围最小值</li>
<li>max_value：线性区域范围最大值</li>
<li>inplace：选择是否进行覆盖运算</li>
</ul>
<p>shape：</p>
<ul>
<li>输入：(N, *)，*表示任意维度组合</li>
<li>输出：(N, *)，与输入有相同的shape属性</li>
</ul>
<p>例子：</p>
<pre><code class="language-python">&gt;&gt;&gt; m = nn.Hardtanh()
&gt;&gt;&gt; input = autograd.Variable(torch.randn(2))
&gt;&gt;&gt; print(input)
&gt;&gt;&gt; print(m(input))
</code></pre>
<blockquote>
<p>class torch.nn.Sigmoid <a href="https://pytorch.org/docs/_modules/torch/nn/modules/activation.html#Sigmoid"><font size=2>[source]</font></a></p>
</blockquote>
<p>对每个元素运用Sigmoid函数，Sigmoid 定义如下：</p>
<div class="arithmatex">\[f(x) = 1 / ( 1 + e^{-x})\]</div>
<p>shape：</p>
<ul>
<li>输入：(N, *)，*表示任意维度组合</li>
<li>输出：(N, *)，与输入有相同的shape属性</li>
</ul>
<p>例子：</p>
<pre><code class="language-python">&gt;&gt;&gt; m = nn.Sigmoid()
&gt;&gt;&gt; input = autograd.Variable(torch.randn(2))
&gt;&gt;&gt; print(input)
&gt;&gt;&gt; print(m(input))
</code></pre>
<blockquote>
<p>class torch.nn.Tanh <a href="https://pytorch.org/docs/_modules/torch/nn/modules/activation.html#Tanh"><font size=2>[source]</font></a></p>
</blockquote>
<p>对输入的每个元素，</p>
<div class="arithmatex">\[f(x) = \frac{e^{x} - e^{-x}} {e^{x} + e^{x}}\]</div>
<p>shape：</p>
<ul>
<li>输入：(N, *)，*表示任意维度组合</li>
<li>输出：(N, *)，与输入有相同的shape属性</li>
</ul>
<p>例子：</p>
<pre><code class="language-python">&gt;&gt;&gt; m = nn.Tanh()
&gt;&gt;&gt; input = autograd.Variable(torch.randn(2))
&gt;&gt;&gt; print(input)
&gt;&gt;&gt; print(m(input))
</code></pre>
<blockquote>
<p>class torch.nn.LogSigmoid <a href="https://pytorch.org/docs/_modules/torch/nn/modules/activation.html#LogSigmoid"><font size=2>[source]</font></a></p>
</blockquote>
<p>对输入的每个元素，<span class="arithmatex">\(LogSigmoid(x) = log( 1 / ( 1 + e^{-x}))\)</span></p>
<p>shape：</p>
<ul>
<li>输入：(N, *)，*表示任意维度组合</li>
<li>输出：(N, *)，与输入有相同的shape属性</li>
</ul>
<p>例子：</p>
<pre><code class="language-python">&gt;&gt;&gt; m = nn.LogSigmoid()
&gt;&gt;&gt; input = autograd.Variable(torch.randn(2))
&gt;&gt;&gt; print(input)
&gt;&gt;&gt; print(m(input))
</code></pre>
<blockquote>
<p>class torch.nn.Softplus(beta=1, threshold=20)<a href="https://pytorch.org/docs/_modules/torch/nn/modules/activation.html#Softplus"><font size=2>[source]</font></a></p>
</blockquote>
<p>对每个元素运用Softplus函数，Softplus 定义如下：</p>
<div class="arithmatex">\[f(x) = \frac{1}{beta} * log(1 + e^{(beta * x_i)})\]</div>
<p>Softplus函数是ReLU函数的平滑逼近，Softplus函数可以使得输出值限定为正数。</p>
<p>为了保证数值稳定性，线性函数的转换可以使输出大于某个值。</p>
<p>参数：</p>
<ul>
<li>beta：Softplus函数的beta值</li>
<li>threshold：阈值</li>
</ul>
<p>shape：</p>
<ul>
<li>输入：(N, *)，*表示任意维度组合</li>
<li>输出：(N, *)，与输入有相同的shape属性</li>
</ul>
<p>例子：</p>
<pre><code class="language-python">&gt;&gt;&gt; m = nn.Softplus()
&gt;&gt;&gt; input = autograd.Variable(torch.randn(2))
&gt;&gt;&gt; print(input)
&gt;&gt;&gt; print(m(input))
</code></pre>
<blockquote>
<p>class torch.nn.Softshrink(lambd=0.5)<a href="https://pytorch.org/docs/_modules/torch/nn/modules/activation.html#Softshrink"><font size=2>[source]</font></a></p>
</blockquote>
<p>对每个元素运用Softshrink函数，Softshrink函数定义如下：</p>
<pre><code>f(x) = x-lambda, if\ x &gt; lambda
f(x) = x+lambda, if\ x &lt; -lambda
f(x) = 0, otherwise
</code></pre>
<p>参数：</p>
<p>lambd：Softshrink函数的lambda值，默认为0.5</p>
<p>shape：</p>
<ul>
<li>输入：(N, *)，*表示任意维度组合</li>
<li>输出：(N, *)，与输入有相同的shape属性</li>
</ul>
<p>例子：</p>
<pre><code class="language-python">&gt;&gt;&gt; m = nn.Softshrink()
&gt;&gt;&gt; input = autograd.Variable(torch.randn(2))
&gt;&gt;&gt; print(input)
&gt;&gt;&gt; print(m(input))
</code></pre>
<blockquote>
<p>class torch.nn.Softsign <a href="https://pytorch.org/docs/_modules/torch/nn/modules/activation.html#Softsign"><font size=2>[source]</font></a></p>
</blockquote>
<p><span class="arithmatex">\(f(x) = x / (1 + |x|)\)</span></p>
<p>shape：</p>
<ul>
<li>输入：(N, *)，*表示任意维度组合</li>
<li>输出：(N, *)，与输入有相同的shape属性</li>
</ul>
<p>例子：</p>
<pre><code class="language-python">&gt;&gt;&gt; m = nn.Softsign()
&gt;&gt;&gt; input = autograd.Variable(torch.randn(2))
&gt;&gt;&gt; print(input)
&gt;&gt;&gt; print(m(input))
</code></pre>
<blockquote>
<p>class torch.nn.Softshrink(lambd=0.5)<a href="https://pytorch.org/docs/_modules/torch/nn/modules/activation.html#Softshrink"><font size=2>[source]</font></a></p>
</blockquote>
<p>对每个元素运用Tanhshrink函数，Tanhshrink函数定义如下：</p>
<div class="arithmatex">\[
Tanhshrink(x) = x - Tanh(x)
\]</div>
<p>shape：</p>
<ul>
<li>输入：(N, *)，*表示任意维度组合</li>
<li>输出：(N, *)，与输入有相同的shape属性</li>
</ul>
<p>例子：</p>
<pre><code class="language-python">&gt;&gt;&gt; m = nn.Tanhshrink()
&gt;&gt;&gt; input = autograd.Variable(torch.randn(2))
&gt;&gt;&gt; print(input)
&gt;&gt;&gt; print(m(input))
</code></pre>
<blockquote>
<p>class torch.nn.Softmin <a href="https://pytorch.org/docs/_modules/torch/nn/modules/activation.html#Softmin"><font size=2>[source]</font></a></p>
</blockquote>
<p>对n维输入张量运用Softmin函数，将张量的每个元素缩放到(0,1）区间且和为1。Softmin函数定义如下：</p>
<div class="arithmatex">\[f_i(x) = \frac{e^{(-x_i - shift)}} { \sum^j e^{(-x_j - shift)}},shift = max (x_i)\]</div>
<p>shape：</p>
<ul>
<li>输入：(N, L)</li>
<li>输出：(N, L)</li>
</ul>
<p>例子：</p>
<pre><code class="language-python">&gt;&gt;&gt; m = nn.Softmin()
&gt;&gt;&gt; input = autograd.Variable(torch.randn(2, 3))
&gt;&gt;&gt; print(input)
&gt;&gt;&gt; print(m(input))
</code></pre>
<hr />
<blockquote>
<p>class torch.nn.Softmax <a href="https://pytorch.org/docs/_modules/torch/nn/modules/activation.html#Softmax"><font size=2>[source]</font></a></p>
</blockquote>
<p>对n维输入张量运用Softmax函数，将张量的每个元素缩放到(0,1）区间且和为1。Softmax函数定义如下：</p>
<div class="arithmatex">\[f_i(x) = \frac{e^{(x_i - shift)}} { \sum^j e^{(x_j - shift)}},shift = max (x_i)\]</div>
<p>shape：</p>
<ul>
<li>输入：(N, L)</li>
<li>输出：(N, L)</li>
</ul>
<p>返回结果是一个与输入维度相同的张量，每个元素的取值范围在(0,1）区间。</p>
<p>例子：</p>
<pre><code class="language-python">&gt;&gt;&gt; m = nn.Softmax()
&gt;&gt;&gt; input = autograd.Variable(torch.randn(2, 3))
&gt;&gt;&gt; print(input)
&gt;&gt;&gt; print(m(input))
</code></pre>
<blockquote>
<p>class torch.nn.LogSoftmax <a href="https://pytorch.org/docs/_modules/torch/nn/modules/activation.html#LogSoftmax"><font size=2>[source]</font></a></p>
</blockquote>
<p>对n维输入张量运用LogSoftmax函数，LogSoftmax函数定义如下：</p>
<div class="arithmatex">\[f_i(x) = log \frac{e^{(x_i)}} {a}, a = \sum^j e^{(x_j)}\]</div>
<p>shape：</p>
<ul>
<li>输入：(N, L)</li>
<li>输出：(N, L)</li>
</ul>
<p>例子：</p>
<pre><code class="language-python">&gt;&gt;&gt; m = nn.LogSoftmax()
&gt;&gt;&gt; input = autograd.Variable(torch.randn(2, 3))
&gt;&gt;&gt; print(input)
&gt;&gt;&gt; print(m(input))
</code></pre>
<h2 id="normalization-layers-source">Normalization layers <a href="https://pytorch.org/docs/nn.html#normalization-layers"><font size=2>[source]</font></a></h2>
<h3 id="class-torchnnbatchnorm1dnum_features-eps1e-05-momentum01-affinetrue-source">class torch.nn.BatchNorm1d(num_features, eps=1e-05, momentum=0.1, affine=True) <a href="https://pytorch.org/docs/nn.html#torch.nn.BatchNorm1d"><font size=2>[source]</font></a></h3>
<p>对小批量(mini-batch)的2d或3d输入进行批标准化(Batch Normalization)操作</p>
<div class="arithmatex">\[ y = \frac{x - mean[x]}{ \sqrt{Var[x]} + \epsilon} * gamma + beta \]</div>
<p>在每一个小批量(mini-batch）数据中，计算输入各个维度的均值和标准差。gamma与beta是可学习的大小为C的参数向量(C为输入大小）</p>
<p>在训练时，该层计算每次输入的均值与方差，并进行移动平均。移动平均默认的动量值为0.1。</p>
<p>在验证时，训练求得的均值/方差将用于标准化验证数据。</p>
<p><strong>参数：</strong></p>
<ul>
<li><strong>num_features：</strong> 来自期望输入的特征数，该期望输入的大小为'batch_size x num_features [x width]'</li>
<li><strong>eps：</strong> 为保证数值稳定性(分母不能趋近或取0）,给分母加上的值。默认为1e-5。</li>
<li><strong>momentum：</strong> 动态均值和动态方差所使用的动量。默认为0.1。</li>
<li><strong>affine：</strong> 一个布尔值，当设为true，给该层添加可学习的仿射变换参数。</li>
</ul>
<p><strong>Shape：</strong>
- 输入：(N, C）或者(N, C, L)
- 输出：(N, C）或者(N，C，L）(输入输出相同）</p>
<p><strong>例子</strong></p>
<pre><code class="language-python">&gt;&gt;&gt; # With Learnable Parameters
&gt;&gt;&gt; m = nn.BatchNorm1d(100)
&gt;&gt;&gt; # Without Learnable Parameters
&gt;&gt;&gt; m = nn.BatchNorm1d(100, affine=False)
&gt;&gt;&gt; input = autograd.Variable(torch.randn(20, 100))
&gt;&gt;&gt; output = m(input)
</code></pre>
<hr />
<h3 id="class-torchnnbatchnorm2dnum_features-eps1e-05-momentum01-affinetruesource">class torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True)<a href="https://pytorch.org/docs/_modules/torch/nn/modules/batchnorm.html#BatchNorm2d"><font size=2>[source]</font></a></h3>
<p>对小批量(mini-batch)3d数据组成的4d输入进行批标准化(Batch Normalization)操作</p>
<div class="arithmatex">\[ y = \frac{x - mean[x]}{ \sqrt{Var[x]} + \epsilon} * gamma + beta \]</div>
<p>在每一个小批量(mini-batch）数据中，计算输入各个维度的均值和标准差。gamma与beta是可学习的大小为C的参数向量(C为输入大小）</p>
<p>在训练时，该层计算每次输入的均值与方差，并进行移动平均。移动平均默认的动量值为0.1。</p>
<p>在验证时，训练求得的均值/方差将用于标准化验证数据。</p>
<p><strong>参数：</strong></p>
<ul>
<li><strong>num_features：</strong> 来自期望输入的特征数，该期望输入的大小为'batch_size x num_features x height x width'</li>
<li><strong>eps：</strong> 为保证数值稳定性(分母不能趋近或取0）,给分母加上的值。默认为1e-5。</li>
<li><strong>momentum：</strong> 动态均值和动态方差所使用的动量。默认为0.1。</li>
<li><strong>affine：</strong> 一个布尔值，当设为true，给该层添加可学习的仿射变换参数。</li>
</ul>
<p><strong>Shape：</strong>
- 输入：(N, C，H, W)
- 输出：(N, C, H, W）(输入输出相同）</p>
<p><strong>例子</strong></p>
<pre><code class="language-python">&gt;&gt;&gt; # With Learnable Parameters
&gt;&gt;&gt; m = nn.BatchNorm2d(100)
&gt;&gt;&gt; # Without Learnable Parameters
&gt;&gt;&gt; m = nn.BatchNorm2d(100, affine=False)
&gt;&gt;&gt; input = autograd.Variable(torch.randn(20, 100, 35, 45))
&gt;&gt;&gt; output = m(input)
</code></pre>
<hr />
<h3 id="class-torchnnbatchnorm3dnum_features-eps1e-05-momentum01-affinetruesource">class torch.nn.BatchNorm3d(num_features, eps=1e-05, momentum=0.1, affine=True)<a href="https://pytorch.org/docs/nn.html#torch.nn.BatchNorm3d"><font size=2>[source]</font></a></h3>
<p>对小批量(mini-batch)4d数据组成的5d输入进行批标准化(Batch Normalization)操作</p>
<div class="arithmatex">\[ y = \frac{x - mean[x]}{ \sqrt{Var[x]} + \epsilon} * gamma + beta \]</div>
<p>在每一个小批量(mini-batch）数据中，计算输入各个维度的均值和标准差。gamma与beta是可学习的大小为C的参数向量(C为输入大小）</p>
<p>在训练时，该层计算每次输入的均值与方差，并进行移动平均。移动平均默认的动量值为0.1。</p>
<p>在验证时，训练求得的均值/方差将用于标准化验证数据。</p>
<p><strong>参数：</strong></p>
<ul>
<li><strong>num_features：</strong> 来自期望输入的特征数，该期望输入的大小为'batch_size x num_features depth x height x width'</li>
<li><strong>eps：</strong> 为保证数值稳定性(分母不能趋近或取0）,给分母加上的值。默认为1e-5。</li>
<li><strong>momentum：</strong> 动态均值和动态方差所使用的动量。默认为0.1。</li>
<li><strong>affine：</strong> 一个布尔值，当设为true，给该层添加可学习的仿射变换参数。</li>
</ul>
<p><strong>Shape：</strong>
- 输入：(N, C，D, H, W)
- 输出：(N, C, D, H, W）(输入输出相同）</p>
<p><strong>例子</strong></p>
<pre><code class="language-python">&gt;&gt;&gt; # With Learnable Parameters
&gt;&gt;&gt; m = nn.BatchNorm3d(100)
&gt;&gt;&gt; # Without Learnable Parameters
&gt;&gt;&gt; m = nn.BatchNorm3d(100, affine=False)
&gt;&gt;&gt; input = autograd.Variable(torch.randn(20, 100, 35, 45, 10))
&gt;&gt;&gt; output = m(input)
</code></pre>
<hr />
<h2 id="recurrent-layers">Recurrent layers</h2>
<h3 id="class-torchnnrnn-args-kwargssource">class torch.nn.RNN(<em> args, </em>* kwargs)[source]</h3>
<p>将一个多层的 <code>Elman RNN</code>，激活函数为<code>tanh</code>或者<code>ReLU</code>，用于输入序列。</p>
<p>对输入序列中每个元素，<code>RNN</code>每层的计算公式为
$$
h_t=tanh(w_{ih}<em> x_t+b_{ih}+w_{hh}</em> h_{t-1}+b_{hh})
$$
<span class="arithmatex">\(h_t\)</span>是时刻<span class="arithmatex">\(t\)</span>的隐状态。 <span class="arithmatex">\(x_t\)</span>是上一层时刻<span class="arithmatex">\(t\)</span>的隐状态，或者是第一层在时刻<span class="arithmatex">\(t\)</span>的输入。如果<code>nonlinearity='relu'</code>,那么将使用<code>relu</code>代替<code>tanh</code>作为激活函数。</p>
<p>参数说明:</p>
<ul>
<li>
<p>input_size – 输入<code>x</code>的特征数量。</p>
</li>
<li>
<p>hidden_size – 隐层的特征数量。</p>
</li>
<li>
<p>num_layers – RNN的层数。</p>
</li>
<li>
<p>nonlinearity – 指定非线性函数使用<code>tanh</code>还是<code>relu</code>。默认是<code>tanh</code>。</p>
</li>
<li>
<p>bias – 如果是<code>False</code>，那么RNN层就不会使用偏置权重 <span class="arithmatex">\(b_ih\)</span>和<span class="arithmatex">\(b_hh\)</span>,默认是<code>True</code></p>
</li>
<li>
<p>batch_first – 如果<code>True</code>的话，那么输入<code>Tensor</code>的shape应该是[batch_size, time_step, feature],输出也是这样。</p>
</li>
<li>
<p>dropout – 如果值非零，那么除了最后一层外，其它层的输出都会套上一个<code>dropout</code>层。</p>
</li>
<li>
<p>bidirectional – 如果<code>True</code>，将会变成一个双向<code>RNN</code>，默认为<code>False</code>。</p>
</li>
</ul>
<p><code>RNN</code>的输入：
<strong>(input, h_0)</strong>
- input (seq_len, batch, input_size): 保存输入序列特征的<code>tensor</code>。<code>input</code>可以是被填充的变长的序列。细节请看<code>torch.nn.utils.rnn.pack_padded_sequence()</code></p>
<ul>
<li>h_0 (num_layers * num_directions, batch, hidden_size): 保存着初始隐状态的<code>tensor</code></li>
</ul>
<p><code>RNN</code>的输出：
<strong>(output, h_n)</strong></p>
<ul>
<li>output (seq_len, batch, hidden_size * num_directions): 保存着<code>RNN</code>最后一层的输出特征。如果输入是被填充过的序列，那么输出也是被填充的序列。</li>
<li>h_n (num_layers * num_directions, batch, hidden_size): 保存着最后一个时刻隐状态。</li>
</ul>
<p><code>RNN</code>模型参数:</p>
<ul>
<li>
<p>weight_ih_l[k] – 第<code>k</code>层的 <code>input-hidden</code> 权重， 可学习，形状是<code>(input_size x hidden_size)</code>。</p>
</li>
<li>
<p>weight_hh_l[k] – 第<code>k</code>层的 <code>hidden-hidden</code> 权重， 可学习，形状是<code>(hidden_size x hidden_size)</code></p>
</li>
<li>
<p>bias_ih_l[k] – 第<code>k</code>层的 <code>input-hidden</code> 偏置， 可学习，形状是<code>(hidden_size)</code></p>
</li>
<li>
<p>bias_hh_l[k] – 第<code>k</code>层的 <code>hidden-hidden</code> 偏置， 可学习，形状是<code>(hidden_size)</code></p>
</li>
</ul>
<p>示例：</p>
<pre><code class="language-python">rnn = nn.RNN(10, 20, 2)
input = Variable(torch.randn(5, 3, 10))
h0 = Variable(torch.randn(2, 3, 20))
output, hn = rnn(input, h0)
</code></pre>
<h3 id="class-torchnnlstm-args-kwargssource">class torch.nn.LSTM(<em> args, </em>* kwargs)[source]</h3>
<p>将一个多层的 <code>(LSTM)</code> 应用到输入序列。</p>
<p>对输入序列的每个元素，<code>LSTM</code>的每层都会执行以下计算：
$$
\begin{aligned}
i_t &amp;= sigmoid(W_{ii}x_t+b_{ii}+W_{hi}h_{t-1}+b_{hi}) \
f_t &amp;= sigmoid(W_{if}x_t+b_{if}+W_{hf}h_{t-1}+b_{hf}) \
o_t &amp;= sigmoid(W_{io}x_t+b_{io}+W_{ho}h_{t-1}+b_{ho}) \
g_t &amp;= tanh(W_{ig}x_t+b_{ig}+W_{hg}h_{t-1}+b_{hg}) \
c_t &amp;= f_t<em>c_{t-1}+i_t</em>g_t \
h_t &amp;= o_t*tanh(c_t) \
\end{aligned}
$$
<span class="arithmatex">\(h_t\)</span>是时刻<span class="arithmatex">\(t\)</span>的隐状态,<span class="arithmatex">\(c_t\)</span>是时刻<span class="arithmatex">\(t\)</span>的细胞状态，<span class="arithmatex">\(x_t\)</span>是上一层的在时刻<span class="arithmatex">\(t\)</span>的隐状态或者是第一层在时刻<span class="arithmatex">\(t\)</span>的输入。<span class="arithmatex">\(i_t, f_t, g_t, o_t\)</span> 分别代表 输入门，遗忘门，细胞和输出门。</p>
<p>参数说明:</p>
<ul>
<li>
<p>input_size – 输入的特征维度</p>
</li>
<li>
<p>hidden_size – 隐状态的特征维度</p>
</li>
<li>
<p>num_layers – 层数(和时序展开要区分开）</p>
</li>
<li>
<p>bias – 如果为<code>False</code>，那么<code>LSTM</code>将不会使用<span class="arithmatex">\(b_{ih},b_{hh}\)</span>，默认为<code>True</code>。</p>
</li>
<li>
<p>batch_first – 如果为<code>True</code>，那么输入和输出<code>Tensor</code>的形状为<code>(batch, seq, feature)</code></p>
</li>
<li>
<p>dropout – 如果非零的话，将会在<code>RNN</code>的输出上加个<code>dropout</code>，最后一层除外。</p>
</li>
<li>
<p>bidirectional – 如果为<code>True</code>，将会变成一个双向<code>RNN</code>，默认为<code>False</code>。</p>
</li>
</ul>
<p><code>LSTM</code>输入:
input, (h_0, c_0)</p>
<ul>
<li>
<p>input (seq_len, batch, input_size): 包含输入序列特征的<code>Tensor</code>。也可以是<code>packed variable</code> ，详见 [pack_padded_sequence](#torch.nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=False[source])</p>
</li>
<li>
<p>h_0 (num_layers * num_directions, batch, hidden_size):保存着<code>batch</code>中每个元素的初始化隐状态的<code>Tensor</code></p>
</li>
<li>
<p>c_0 (num_layers * num_directions, batch, hidden_size): 保存着<code>batch</code>中每个元素的初始化细胞状态的<code>Tensor</code></p>
</li>
</ul>
<p><code>LSTM</code>输出
output, (h_n, c_n)</p>
<ul>
<li>
<p>output (seq_len, batch, hidden_size * num_directions): 保存<code>RNN</code>最后一层的输出的<code>Tensor</code>。 如果输入是<code>torch.nn.utils.rnn.PackedSequence</code>，那么输出也是<code>torch.nn.utils.rnn.PackedSequence</code>。</p>
</li>
<li>
<p>h_n (num_layers * num_directions, batch, hidden_size): <code>Tensor</code>，保存着<code>RNN</code>最后一个时间步的隐状态。</p>
</li>
<li>
<p>c_n (num_layers * num_directions, batch, hidden_size): <code>Tensor</code>，保存着<code>RNN</code>最后一个时间步的细胞状态。</p>
</li>
</ul>
<p><code>LSTM</code>模型参数:</p>
<ul>
<li>
<p>weight_ih_l[k] – 第<code>k</code>层可学习的<code>input-hidden</code>权重(<span class="arithmatex">\(W_{ii}|W_{if}|W_{ig}|W_{io}\)</span>)，形状为<code>(input_size x 4*hidden_size)</code></p>
</li>
<li>
<p>weight_hh_l[k] – 第<code>k</code>层可学习的<code>hidden-hidden</code>权重(<span class="arithmatex">\(W_{hi}|W_{hf}|W_{hg}|W_{ho}\)</span>)，形状为<code>(hidden_size x 4*hidden_size)</code>。</p>
</li>
<li>
<p>bias_ih_l[k] – 第<code>k</code>层可学习的<code>input-hidden</code>偏置(<span class="arithmatex">\(b_{ii}|b_{if}|b_{ig}|b_{io}\)</span>)，形状为<code>( 4*hidden_size)</code></p>
</li>
<li>
<p>bias_hh_l[k] – 第<code>k</code>层可学习的<code>hidden-hidden</code>偏置(<span class="arithmatex">\(b_{hi}|b_{hf}|b_{hg}|b_{ho}\)</span>)，形状为<code>( 4*hidden_size)</code>。
示例:</p>
</li>
</ul>
<pre><code class="language-python">lstm = nn.LSTM(10, 20, 2)
input = Variable(torch.randn(5, 3, 10))
h0 = Variable(torch.randn(2, 3, 20))
c0 = Variable(torch.randn(2, 3, 20))
output, hn = lstm(input, (h0, c0))
</code></pre>
<h3 id="class-torchnngru-args-kwargssource">class torch.nn.GRU(<em> args, </em>* kwargs)[source]</h3>
<p>将一个多层的<code>GRU</code>用于输入序列。</p>
<p>对输入序列中的每个元素，每层进行了一下计算：</p>
<p>$$
\begin{aligned}
r_t&amp;=sigmoid(W_{ir}x_t+b_{ir}+W_{hr}h_{(t-1)}+b_{hr}) \
i_t&amp;=sigmoid(W_{ii}x_t+b_{ii}+W_{hi}h_{(t-1)}+b_{hi}) \
n_t&amp;=tanh(W_{in}x_t+b_{in}+rt<em>(W_{hn}h_{(t-1)}+b_{hn})) \
h_t&amp;=(1-i_t)</em> nt+i_t*h(t-1)
\end{aligned}
$$
<span class="arithmatex">\(h_t\)</span>是是时间<span class="arithmatex">\(t\)</span>的上的隐状态，<span class="arithmatex">\(x_t\)</span>是前一层<span class="arithmatex">\(t\)</span>时刻的隐状态或者是第一层的<span class="arithmatex">\(t\)</span>时刻的输入，<span class="arithmatex">\(r_t, i_t, n_t\)</span>分别是重置门，输入门和新门。</p>
<p>参数说明：
- input_size – 期望的输入<span class="arithmatex">\(x\)</span>的特征值的维度
- hidden_size – 隐状态的维度
- num_layers – <code>RNN</code>的层数。
- bias – 如果为<code>False</code>，那么<code>RNN</code>层将不会使用<code>bias</code>，默认为<code>True</code>。
- batch_first – 如果为<code>True</code>的话，那么输入和输出的<code>tensor</code>的形状是<code>(batch, seq, feature)</code>。
- dropout –  如果非零的话，将会在<code>RNN</code>的输出上加个<code>dropout</code>，最后一层除外。
- bidirectional – 如果为<code>True</code>，将会变成一个双向<code>RNN</code>，默认为<code>False</code>。</p>
<p>输入：
input, h_0</p>
<ul>
<li>
<p>input (seq_len, batch, input_size):  包含输入序列特征的<code>Tensor</code>。也可以是<code>packed variable</code> ，详见 [pack_padded_sequence](#torch.nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=False[source])。</p>
</li>
<li>
<p>h_0 (num_layers * num_directions, batch, hidden_size):保存着<code>batch</code>中每个元素的初始化隐状态的<code>Tensor</code></p>
</li>
</ul>
<p>输出：
output, h_n</p>
<ul>
<li>
<p>output (seq_len, batch, hidden_size * num_directions): ten保存<code>RNN</code>最后一层的输出的<code>Tensor</code>。 如果输入是<code>torch.nn.utils.rnn.PackedSequence</code>，那么输出也是<code>torch.nn.utils.rnn.PackedSequence</code>。</p>
</li>
<li>
<p>h_n (num_layers * num_directions, batch, hidden_size): <code>Tensor</code>，保存着<code>RNN</code>最后一个时间步的隐状态。</p>
</li>
</ul>
<p>变量：</p>
<ul>
<li>
<p>weight_ih_l[k] – 第<code>k</code>层可学习的<code>input-hidden</code>权重(<span class="arithmatex">\(W_{ir}|W_{ii}|W_{in}\)</span>)，形状为<code>(input_size x 3*hidden_size)</code></p>
</li>
<li>
<p>weight_hh_l[k] – 第<code>k</code>层可学习的<code>hidden-hidden</code>权重(<span class="arithmatex">\(W_{hr}|W_{hi}|W_{hn}\)</span>)，形状为<code>(hidden_size x 3*hidden_size)</code>。</p>
</li>
<li>
<p>bias_ih_l[k] – 第<code>k</code>层可学习的<code>input-hidden</code>偏置(<span class="arithmatex">\(b_{ir}|b_{ii}|b_{in}\)</span>)，形状为<code>( 3*hidden_size)</code></p>
</li>
<li>
<p>bias_hh_l[k] – 第<code>k</code>层可学习的<code>hidden-hidden</code>偏置(<span class="arithmatex">\(b_{hr}|b_{hi}|b_{hn}\)</span>)，形状为<code>( 3*hidden_size)</code>。</p>
</li>
</ul>
<p>例子：</p>
<pre><code class="language-python"> rnn = nn.GRU(10, 20, 2)
 input = Variable(torch.randn(5, 3, 10))
 h0 = Variable(torch.randn(2, 3, 20))
 output, hn = rnn(input, h0)
</code></pre>
<h3 id="class-torchnnrnncellinput_size-hidden_size-biastrue-nonlinearitytanhsource">class torch.nn.RNNCell(input_size, hidden_size, bias=True, nonlinearity='tanh')[source]</h3>
<p>一个 <code>Elan RNN cell</code>，激活函数是<code>tanh</code>或<code>ReLU</code>，用于输入序列。
将一个多层的 <code>Elman RNNCell</code>，激活函数为<code>tanh</code>或者<code>ReLU</code>，用于输入序列。
$$
h'=tanh(w_{ih}<em> x+b_{ih}+w_{hh}</em> h+b_{hh})
$$
如果<code>nonlinearity=relu</code>，那么将会使用<code>ReLU</code>来代替<code>tanh</code>。</p>
<p>参数：</p>
<ul>
<li>
<p>input_size – 输入<span class="arithmatex">\(x\)</span>，特征的维度。</p>
</li>
<li>
<p>hidden_size – 隐状态特征的维度。</p>
</li>
<li>
<p>bias – 如果为<code>False</code>，<code>RNN cell</code>中将不会加入<code>bias</code>，默认为<code>True</code>。</p>
</li>
<li>
<p>nonlinearity – 用于选择非线性激活函数 [<code>tanh</code>|<code>relu</code>]. 默认值为： <code>tanh</code></p>
</li>
</ul>
<p>输入：
input, hidden</p>
<ul>
<li>
<p>input (batch, input_size): 包含输入特征的<code>tensor</code>。</p>
</li>
<li>
<p>hidden (batch, hidden_size): 保存着初始隐状态值的<code>tensor</code>。</p>
</li>
</ul>
<p>输出： h'</p>
<ul>
<li>h' (batch, hidden_size):下一个时刻的隐状态。</li>
</ul>
<p>变量：</p>
<ul>
<li>
<p>weight_ih –  <code>input-hidden</code> 权重， 可学习，形状是<code>(input_size x hidden_size)</code>。</p>
</li>
<li>
<p>weight_hh –  <code>hidden-hidden</code> 权重， 可学习，形状是<code>(hidden_size x hidden_size)</code></p>
</li>
<li>
<p>bias_ih –  <code>input-hidden</code> 偏置， 可学习，形状是<code>(hidden_size)</code></p>
</li>
<li>
<p>bias_hh –  <code>hidden-hidden</code> 偏置， 可学习，形状是<code>(hidden_size)</code></p>
</li>
</ul>
<p>例子：</p>
<pre><code class="language-python">rnn = nn.RNNCell(10, 20)
input = Variable(torch.randn(6, 3, 10))
hx = Variable(torch.randn(3, 20))
output = []
for i in range(6):
   hx = rnn(input[i], hx)
   output.append(hx)
</code></pre>
<h3 id="class-torchnnlstmcellinput_size-hidden_size-biastruesource">class torch.nn.LSTMCell(input_size, hidden_size, bias=True)[source]</h3>
<p><code>LSTM cell</code>。</p>
<div class="arithmatex">\[
\begin{aligned}
i &amp;= sigmoid(W_{ii}x+b_{ii}+W_{hi}h+b_{hi}) \\
f &amp;= sigmoid(W_{if}x+b_{if}+W_{hf}h+b_{hf}) \\
o &amp;= sigmoid(W_{io}x+b_{io}+W_{ho}h+b_{ho}) \\
g &amp;= tanh(W_{ig}x+b_{ig}+W_{hg}h+b_{hg}) \\
c' &amp;= f_t*c_{t-1}+i_t*g_t \\
h' &amp;= o_t*tanh(c')
\end{aligned}
\]</div>
<p>参数：</p>
<ul>
<li>input_size – 输入的特征维度。</li>
<li>hidden_size – 隐状态的维度。</li>
<li>bias – 如果为<code>False</code>，那么将不会使用<code>bias</code>。默认为<code>True</code>。</li>
</ul>
<p><code>LSTM</code>输入:
input, (h_0, c_0)</p>
<ul>
<li>
<p>input (seq_len, batch, input_size): 包含输入序列特征的<code>Tensor</code>。也可以是<code>packed variable</code> ，详见 [pack_padded_sequence](#torch.nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=False[source])</p>
</li>
<li>
<p>h_0 ( batch, hidden_size):保存着<code>batch</code>中每个元素的初始化隐状态的<code>Tensor</code></p>
</li>
<li>
<p>c_0 (batch, hidden_size): 保存着<code>batch</code>中每个元素的初始化细胞状态的<code>Tensor</code></p>
</li>
</ul>
<p>输出：
h_1, c_1</p>
<ul>
<li>h_1 (batch, hidden_size): 下一个时刻的隐状态。</li>
<li>c_1 (batch, hidden_size): 下一个时刻的细胞状态。</li>
</ul>
<p><code>LSTM</code>模型参数:</p>
<ul>
<li>
<p>weight_ih – <code>input-hidden</code>权重(<span class="arithmatex">\(W_{ii}|W_{if}|W_{ig}|W_{io}\)</span>)，形状为<code>(input_size x 4*hidden_size)</code></p>
</li>
<li>
<p>weight_hh – <code>hidden-hidden</code>权重(<span class="arithmatex">\(W_{hi}|W_{hf}|W_{hg}|W_{ho}\)</span>)，形状为<code>(hidden_size x 4*hidden_size)</code>。</p>
</li>
<li>
<p>bias_ih – <code>input-hidden</code>偏置(<span class="arithmatex">\(b_{ii}|b_{if}|b_{ig}|b_{io}\)</span>)，形状为<code>( 4*hidden_size)</code></p>
</li>
<li>
<p>bias_hh – <code>hidden-hidden</code>偏置(<span class="arithmatex">\(b_{hi}|b_{hf}|b_{hg}|b_{ho}\)</span>)，形状为<code>( 4*hidden_size)</code>。</p>
</li>
</ul>
<p>Examples:</p>
<pre><code class="language-python">rnn = nn.LSTMCell(10, 20)
input = Variable(torch.randn(6, 3, 10))
hx = Variable(torch.randn(3, 20))
cx = Variable(torch.randn(3, 20))
output = []
for i in range(6):
   hx, cx = rnn(input[i], (hx, cx))
   output.append(hx)
</code></pre>
<h3 id="class-torchnngrucellinput_size-hidden_size-biastruesource">class torch.nn.GRUCell(input_size, hidden_size, bias=True)[source]</h3>
<p>一个<code>GRU cell</code>。
$$
\begin{aligned}
r&amp;=sigmoid(W_{ir}x+b_{ir}+W_{hr}h+b_{hr})\
i&amp;=sigmoid(W_{ii}x+b_{ii}+W_{hi}h+b_{hi})\
n&amp;=tanh(W_{in}x+b_{in}+r<em>(W_{hn}h+b_{hn}))\
h'&amp;=(1-i)</em> n+i*h
\end{aligned}
$$</p>
<p>参数说明：
- input_size – 期望的输入<span class="arithmatex">\(x\)</span>的特征值的维度
- hidden_size – 隐状态的维度
- bias – 如果为<code>False</code>，那么<code>RNN</code>层将不会使用<code>bias</code>，默认为<code>True</code>。</p>
<p>输入：
input, h_0</p>
<ul>
<li>
<p>input (batch, input_size):  包含输入特征的<code>Tensor</code></p>
</li>
<li>
<p>h_0 (batch, hidden_size):保存着<code>batch</code>中每个元素的初始化隐状态的<code>Tensor</code></p>
</li>
</ul>
<p>输出：
h_1</p>
<ul>
<li>h_1 (batch, hidden_size): <code>Tensor</code>，保存着<code>RNN</code>下一个时刻的隐状态。</li>
</ul>
<p>变量：</p>
<ul>
<li>
<p>weight_ih – <code>input-hidden</code>权重(<span class="arithmatex">\(W_{ir}|W_{ii}|W_{in}\)</span>)，形状为<code>(input_size x 3*hidden_size)</code></p>
</li>
<li>
<p>weight_hh – <code>hidden-hidden</code>权重(<span class="arithmatex">\(W_{hr}|W_{hi}|W_{hn}\)</span>)，形状为<code>(hidden_size x 3*hidden_size)</code>。</p>
</li>
<li>
<p>bias_ih – <code>input-hidden</code>偏置(<span class="arithmatex">\(b_{ir}|b_{ii}|b_{in}\)</span>)，形状为<code>( 3*hidden_size)</code></p>
</li>
<li>
<p>bias_hh – <code>hidden-hidden</code>偏置(<span class="arithmatex">\(b_{hr}|b_{hi}|b_{hn}\)</span>)，形状为<code>( 3*hidden_size)</code>。</p>
</li>
</ul>
<p>例子：</p>
<pre><code class="language-python">rnn = nn.GRUCell(10, 20)
input = Variable(torch.randn(6, 3, 10))
hx = Variable(torch.randn(3, 20))
output = []
for i in range(6):
   hx = rnn(input[i], hx)
   output.append(hx)
</code></pre>
<h2 id="linear-layers">Linear layers</h2>
<pre><code class="language-python">class torch.nn.Linear(in_features, out_features, bias=True)
</code></pre>
<p>对输入数据做线性变换： <span class="arithmatex">\(<span class="arithmatex">\(y = Ax + b\)</span>\)</span> </p>
<p><strong>参数：</strong></p>
<ul>
<li><strong>in_features</strong> - 每个输入样本的大小</li>
<li><strong>out_features</strong> - 每个输出样本的大小</li>
<li><strong>bias</strong> - 若设置为False，这层不会学习偏置。默认值：True</li>
</ul>
<p><strong>形状：</strong></p>
<ul>
<li><strong>输入:</strong>  <span class="arithmatex">\(<span class="arithmatex">\((N, in\_features)\)</span>\)</span> </li>
<li><strong>输出：</strong>  <span class="arithmatex">\(<span class="arithmatex">\((N, out\_features)\)</span>\)</span> </li>
</ul>
<p><strong>变量：</strong></p>
<ul>
<li><strong>weight</strong> -形状为(out_features x in_features)的模块中可学习的权值</li>
<li><strong>bias</strong> -形状为(out_features)的模块中可学习的偏置</li>
</ul>
<p><strong>例子：</strong></p>
<pre><code class="language-python">&gt;&gt;&gt; m = nn.Linear(20, 30)
&gt;&gt;&gt; input = autograd.Variable(torch.randn(128, 20))
&gt;&gt;&gt; output = m(input)
&gt;&gt;&gt; print(output.size())
</code></pre>
<h2 id="dropout-layers">Dropout layers</h2>
<pre><code class="language-python">class torch.nn.Dropout(p=0.5, inplace=False)
</code></pre>
<p>随机将输入张量中部分元素设置为0。对于每次前向调用，被置0的元素都是随机的。</p>
<p><strong>参数：</strong></p>
<ul>
<li><strong>p</strong> - 将元素置0的概率。默认值：0.5</li>
<li><strong>in-place</strong> - 若设置为True，会在原地执行操作。默认值：False</li>
</ul>
<p><strong>形状：</strong></p>
<ul>
<li><strong>输入：</strong> 任意。输入可以为任意形状。</li>
<li><strong>输出：</strong> 相同。输出和输入形状相同。</li>
</ul>
<p><strong>例子：</strong></p>
<pre><code class="language-python">&gt;&gt;&gt; m = nn.Dropout(p=0.2)
&gt;&gt;&gt; input = autograd.Variable(torch.randn(20, 16))
&gt;&gt;&gt; output = m(input)
</code></pre>
<pre><code class="language-python">class torch.nn.Dropout2d(p=0.5, inplace=False)
</code></pre>
<p>随机将输入张量中整个通道设置为0。对于每次前向调用，被置0的通道都是随机的。</p>
<p><em>通常输入来自Conv2d模块。</em></p>
<p>像在论文<a href="https://arxiv.org/abs/1411.4280">Efficient Object Localization Using Convolutional Networks</a>，如果特征图中相邻像素是强相关的(在前几层卷积层很常见），那么iid dropout不会归一化激活，而只会降低学习率。</p>
<p>在这种情形，<code>nn.Dropout2d()</code>可以提高特征图之间的独立程度，所以应该使用它。</p>
<p><strong>参数：</strong></p>
<ul>
<li><strong>p</strong>(<em><a href="">float</a>, optional</em>) - 将元素置0的概率。</li>
<li><strong>in-place</strong>(<em><a href="">bool,</a> optional</em>) - 若设置为True，会在原地执行操作。</li>
</ul>
<p><strong>形状：</strong></p>
<ul>
<li><strong>输入：</strong>   <span class="arithmatex">\(<span class="arithmatex">\((N, C, H, W)\)</span>\)</span> </li>
<li><strong>输出：</strong>   <span class="arithmatex">\(<span class="arithmatex">\((N, C, H, W)\)</span>\)</span> (与输入形状相同）</li>
</ul>
<p><strong>例子：</strong></p>
<pre><code class="language-python">&gt;&gt;&gt; m = nn.Dropout2d(p=0.2)
&gt;&gt;&gt; input = autograd.Variable(torch.randn(20, 16, 32, 32))
&gt;&gt;&gt; output = m(input)
</code></pre>
<pre><code class="language-python">class torch.nn.Dropout3d(p=0.5, inplace=False)
</code></pre>
<p>随机将输入张量中整个通道设置为0。对于每次前向调用，被置0的通道都是随机的。</p>
<p><em>通常输入来自Conv3d模块。</em></p>
<p>像在论文<a href="https://arxiv.org/abs/1411.4280">Efficient Object Localization Using Convolutional Networks</a>，如果特征图中相邻像素是强相关的(在前几层卷积层很常见），那么iid dropout不会归一化激活，而只会降低学习率。</p>
<p>在这种情形，<code>nn.Dropout3d()</code>可以提高特征图之间的独立程度，所以应该使用它。</p>
<p><strong>参数：</strong></p>
<ul>
<li><strong>p</strong>(<em><a href="">float</a>, optional</em>) - 将元素置0的概率。</li>
<li><strong>in-place</strong>(<em><a href="">bool,</a> optional</em>) - 若设置为True，会在原地执行操作。</li>
</ul>
<p><strong>形状：</strong></p>
<ul>
<li><strong>输入：</strong>   <span class="arithmatex">\(<span class="arithmatex">\(N, C, D, H, W)\)</span>\)</span> </li>
<li><strong>输出：</strong>   <span class="arithmatex">\(<span class="arithmatex">\((N, C, D, H, W)\)</span>\)</span> (与输入形状相同）</li>
</ul>
<p><strong>例子：</strong></p>
<pre><code class="language-python">&gt;&gt;&gt; m = nn.Dropout3d(p=0.2)
&gt;&gt;&gt; input = autograd.Variable(torch.randn(20, 16, 4, 32, 32))
&gt;&gt;&gt; output = m(input)
</code></pre>
<h2 id="sparse-layers">Sparse layers</h2>
<pre><code class="language-python">class torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2, scale_grad_by_freq=False, sparse=False)
</code></pre>
<p>一个保存了固定字典和大小的简单查找表。</p>
<p>这个模块常用来保存词嵌入和用下标检索它们。模块的输入是一个下标的列表，输出是对应的词嵌入。</p>
<p><strong>参数：</strong></p>
<ul>
<li><strong>num_embeddings</strong> (<em><a href="">int</a></em>) - 嵌入字典的大小</li>
<li><strong>embedding_dim</strong> (<em><a href="">int</a></em>) - 每个嵌入向量的大小</li>
<li><strong>padding_idx </strong> (<em><a href="">int</a>, optional</em>) - 如果提供的话，输出遇到此下标时用零填充</li>
<li><strong>max_norm</strong> (<em><a href="">float</a>, optional</em>) - 如果提供的话，会重新归一化词嵌入，使它们的范数小于提供的值</li>
<li><strong>norm_type</strong> (<em><a href="">float</a>, optional</em>) - 对于max_norm选项计算p范数时的p</li>
<li><strong>scale_grad_by_freq</strong> (<em>boolean, optional</em>) - 如果提供的话，会根据字典中单词频率缩放梯度</li>
</ul>
<p><strong>变量：</strong></p>
<ul>
<li><strong>weight (<em><a href="https://pytorch.org/docs/tensors.html#torch.Tensor">Tensor</a></em>) </strong> -形状为(num_embeddings, embedding_dim)的模块中可学习的权值</li>
</ul>
<p><strong>形状：</strong></p>
<ul>
<li><strong>输入：</strong>  LongTensor <em>(N, W)</em>, N = mini-batch, W = 每个mini-batch中提取的下标数</li>
<li><strong>输出：</strong>  <em>(N, W, embedding_dim)</em></li>
</ul>
<p><strong>例子：</strong></p>
<pre><code class="language-python">&gt;&gt;&gt; # an Embedding module containing 10 tensors of size 3
&gt;&gt;&gt; embedding = nn.Embedding(10, 3)
&gt;&gt;&gt; # a batch of 2 samples of 4 indices each
&gt;&gt;&gt; input = Variable(torch.LongTensor([[1,2,4,5],[4,3,2,9]]))
&gt;&gt;&gt; embedding(input)

Variable containing:
(0 ,.,.) =
 -1.0822  1.2522  0.2434
  0.8393 -0.6062 -0.3348
  0.6597  0.0350  0.0837
  0.5521  0.9447  0.0498

(1 ,.,.) =
  0.6597  0.0350  0.0837
 -0.1527  0.0877  0.4260
  0.8393 -0.6062 -0.3348
 -0.8738 -0.9054  0.4281
[torch.FloatTensor of size 2x4x3]

&gt;&gt;&gt; # example with padding_idx
&gt;&gt;&gt; embedding = nn.Embedding(10, 3, padding_idx=0)
&gt;&gt;&gt; input = Variable(torch.LongTensor([[0,2,0,5]]))
&gt;&gt;&gt; embedding(input)

Variable containing:
(0 ,.,.) =
  0.0000  0.0000  0.0000
  0.3452  0.4937 -0.9361
  0.0000  0.0000  0.0000
  0.0706 -2.1962 -0.6276
[torch.FloatTensor of size 1x4x3]
</code></pre>
<h2 id="distance-functions">Distance functions</h2>
<pre><code class="language-python">class torch.nn.PairwiseDistance(p=2, eps=1e-06)
</code></pre>
<p>按批计算向量v1, v2之间的距离：</p>
<div class="arithmatex">\[\Vert x \Vert _p := \left( \sum\_{i=1}^n  \vert x_i \vert ^ p \right) ^ {1/p}\]</div>
<p><strong>参数：</strong></p>
<ul>
<li><strong>x</strong> (<em>Tensor</em>):  包含两个输入batch的张量</li>
<li><strong>p</strong> (real): 范数次数，默认值：2</li>
</ul>
<p><strong>形状：</strong></p>
<ul>
<li><strong>输入：</strong>   <span class="arithmatex">\(<span class="arithmatex">\((N, D)\)</span>\)</span> ，其中D=向量维数</li>
<li><strong>输出：</strong>   <span class="arithmatex">\(<span class="arithmatex">\((N, 1)\)</span>\)</span> </li>
</ul>
<pre><code class="language-python">&gt;&gt;&gt; pdist = nn.PairwiseDistance(2)
&gt;&gt;&gt; input1 = autograd.Variable(torch.randn(100, 128))
&gt;&gt;&gt; input2 = autograd.Variable(torch.randn(100, 128))
&gt;&gt;&gt; output = pdist(input1, input2)
</code></pre>
<h2 id="loss-functions">Loss functions</h2>
<p>基本用法：</p>
<pre><code class="language-python">criterion = LossCriterion() #构造函数有自己的参数
loss = criterion(x, y) #调用标准时也有参数
</code></pre>
<p>计算出来的结果已经对<code>mini-batch</code>取了平均。</p>
<h3 id="class-torchnnl1losssize_averagetruesource">class torch.nn.L1Loss(size_average=True)<a href="https://pytorch.org/docs/_modules/torch/nn/modules/loss.html#L1Loss"><font size=2>[source]</font></a></h3>
<p>创建一个衡量输入<code>x</code>(<code>模型预测输出</code>)和目标<code>y</code>之间差的绝对值的平均值的标准。
$$
loss(x,y)=1/n\sum|x_i-y_i|
$$</p>
<ul>
<li>
<p><code>x</code> 和 <code>y</code> 可以是任意形状，每个包含<code>n</code>个元素。</p>
</li>
<li>
<p>对<code>n</code>个元素对应的差值的绝对值求和，得出来的结果除以<code>n</code>。</p>
</li>
<li>
<p>如果在创建<code>L1Loss</code>实例的时候在构造函数中传入<code>size_average=False</code>，那么求出来的绝对值的和将不会除以<code>n</code></p>
</li>
</ul>
<h3 id="class-torchnnmselosssize_averagetruesource">class torch.nn.MSELoss(size_average=True)<a href="https://pytorch.org/docs/_modules/torch/nn/modules/loss.html#MSELoss"><font size=2>[source]</font></a></h3>
<p>创建一个衡量输入<code>x</code>(<code>模型预测输出</code>)和目标<code>y</code>之间均方误差标准。
$$
loss(x,y)=1/n\sum(x_i-y_i)^2
$$</p>
<ul>
<li>
<p><code>x</code> 和 <code>y</code> 可以是任意形状，每个包含<code>n</code>个元素。</p>
</li>
<li>
<p>对<code>n</code>个元素对应的差值的绝对值求和，得出来的结果除以<code>n</code>。</p>
</li>
<li>
<p>如果在创建<code>MSELoss</code>实例的时候在构造函数中传入<code>size_average=False</code>，那么求出来的平方和将不会除以<code>n</code></p>
</li>
</ul>
<h3 id="class-torchnncrossentropylossweightnone-size_averagetruesource">class torch.nn.CrossEntropyLoss(weight=None, size_average=True)<a href="https://pytorch.org/docs/_modules/torch/nn/modules/loss.html#CrossEntropyLoss"><font size=2>[source]</font></a></h3>
<p>此标准将<code>LogSoftMax</code>和<code>NLLLoss</code>集成到一个类中。</p>
<p>当训练一个多类分类器的时候，这个方法是十分有用的。</p>
<ul>
<li>weight(tensor): <code>1-D</code> tensor，<code>n</code>个元素，分别代表<code>n</code>类的权重，如果你的训练样本很不均衡的话，是非常有用的。默认值为None。</li>
</ul>
<p>调用时参数：</p>
<ul>
<li>
<p>input : 包含每个类的得分，<code>2-D</code> tensor,<code>shape</code>为 <code>batch*n</code></p>
</li>
<li>
<p>target: 大小为 <code>n</code> 的 <code>1—D</code> <code>tensor</code>，包含类别的索引(<code>0到 n-1</code>)。</p>
</li>
</ul>
<p>Loss可以表述为以下形式：
$$
\begin{aligned}
loss(x, class) &amp;= -\text{log}\frac{exp(x[class])}{\sum_j exp(x[j]))}\
               &amp;= -x[class] + log(\sum_j exp(x[j]))
\end{aligned}
$$
当<code>weight</code>参数被指定的时候，<code>loss</code>的计算公式变为：
$$
loss(x, class) = weights[class] * (-x[class] + log(\sum_j exp(x[j])))
$$
计算出的<code>loss</code>对<code>mini-batch</code>的大小取了平均。</p>
<p>形状(<code>shape</code>)：</p>
<ul>
<li>
<p>Input: (N,C) <code>C</code> 是类别的数量</p>
</li>
<li>
<p>Target: (N) <code>N</code>是<code>mini-batch</code>的大小，0 &lt;= targets[i] &lt;= C-1</p>
</li>
</ul>
<h3 id="class-torchnnnlllossweightnone-size_averagetruesource">class torch.nn.NLLLoss(weight=None, size_average=True)<a href="https://pytorch.org/docs/_modules/torch/nn/modules/loss.html#NLLLoss"><font size=2>[source]</font></a></h3>
<p>负的<code>log likelihood loss</code>损失。用于训练一个<code>n</code>类分类器。</p>
<p>如果提供的话，<code>weight</code>参数应该是一个<code>1-D</code>tensor，里面的值对应类别的权重。当你的训练集样本不均衡的话，使用这个参数是非常有用的。</p>
<p>输入是一个包含类别<code>log-probabilities</code>的<code>2-D</code> tensor，形状是<code>(mini-batch， n）</code></p>
<p>可以通过在最后一层加<code>LogSoftmax</code>来获得类别的<code>log-probabilities</code>。</p>
<p>如果您不想增加一个额外层的话，您可以使用<code>CrossEntropyLoss</code>。</p>
<p>此<code>loss</code>期望的<code>target</code>是类别的索引 (0 to N-1, where N = number of classes)</p>
<p>此<code>loss</code>可以被表示如下：
$$
loss(x, class) = -x[class]
$$
如果<code>weights</code>参数被指定的话，<code>loss</code>可以表示如下：
$$
loss(x, class) = -weights[class] * x[class]
$$
参数说明：</p>
<ul>
<li>
<p>weight (Tensor, optional) – 手动指定每个类别的权重。如果给定的话，必须是长度为<code>nclasses</code></p>
</li>
<li>
<p>size_average (bool, optional) – 默认情况下，会计算<code>mini-batch``loss</code>的平均值。然而，如果<code>size_average=False</code>那么将会把<code>mini-batch</code>中所有样本的<code>loss</code>累加起来。</p>
</li>
</ul>
<p>形状:</p>
<ul>
<li>
<p>Input: (N,C) , <code>C</code>是类别的个数</p>
</li>
<li>
<p>Target: (N) ， <code>target</code>中每个值的大小满足 <code>0 &lt;= targets[i] &lt;= C-1</code></p>
</li>
</ul>
<p>例子:</p>
<pre><code class="language-python"> m = nn.LogSoftmax()
 loss = nn.NLLLoss()
 # input is of size nBatch x nClasses = 3 x 5
 input = autograd.Variable(torch.randn(3, 5), requires_grad=True)
 # each element in target has to have 0 &lt;= value &lt; nclasses
 target = autograd.Variable(torch.LongTensor([1, 0, 4]))
 output = loss(m(input), target)
 output.backward()
</code></pre>
<h3 id="class-torchnnnllloss2dweightnone-size_averagetruesource">class torch.nn.NLLLoss2d(weight=None, size_average=True)<a href="https://pytorch.org/docs/_modules/torch/nn/modules/loss.html#NLLLoss2d"><font size=2>[source]</font></a></h3>
<p>对于图片的 <code>negative log likehood loss</code>。计算每个像素的 <code>NLL loss</code>。</p>
<p>参数说明：</p>
<ul>
<li>
<p>weight (Tensor, optional) – 用来作为每类的权重，如果提供的话，必须为<code>1-D</code>tensor，大小为<code>C</code>：类别的个数。</p>
</li>
<li>
<p>size_average – 默认情况下，会计算 <code>mini-batch</code> loss均值。如果设置为 <code>False</code> 的话，将会累加<code>mini-batch</code>中所有样本的<code>loss</code>值。默认值：<code>True</code>。</p>
</li>
</ul>
<p>形状：</p>
<ul>
<li>
<p>Input: (N,C,H,W)  <code>C</code> 类的数量</p>
</li>
<li>
<p>Target: (N,H,W) where each value is 0 &lt;= targets[i] &lt;= C-1</p>
</li>
</ul>
<p>例子：</p>
<pre><code class="language-python"> m = nn.Conv2d(16, 32, (3, 3)).float()
 loss = nn.NLLLoss2d()
 # input is of size nBatch x nClasses x height x width
 input = autograd.Variable(torch.randn(3, 16, 10, 10))
 # each element in target has to have 0 &lt;= value &lt; nclasses
 target = autograd.Variable(torch.LongTensor(3, 8, 8).random_(0, 4))
 output = loss(m(input), target)
 output.backward()
</code></pre>
<h3 id="class-torchnnkldivlossweightnone-size_averagetruesource">class torch.nn.KLDivLoss(weight=None, size_average=True)<a href="https://pytorch.org/docs/_modules/torch/nn/modules/loss.html#KLDivLoss"><font size=2>[source]</font></a></h3>
<p>计算 KL 散度损失。</p>
<p>KL散度常用来描述两个分布的距离，并在输出分布的空间上执行直接回归是有用的。</p>
<p>与<code>NLLLoss</code>一样，给定的输入应该是<code>log-probabilities</code>。然而。和<code>NLLLoss</code>不同的是，<code>input</code>不限于<code>2-D</code> tensor，因为此标准是基于<code>element</code>的。</p>
<p><code>target</code> 应该和 <code>input</code>的形状相同。</p>
<p>此loss可以表示为：
$$
loss(x,target)=\frac{1}{n}\sum_i(target_i*(log(target_i)-x_i))
$$
默认情况下，loss会基于<code>element</code>求平均。如果 <code>size_average=False</code> <code>loss</code> 会被累加起来。</p>
<h3 id="class-torchnnbcelossweightnone-size_averagetruesource">class torch.nn.BCELoss(weight=None, size_average=True)<a href="https://pytorch.org/docs/_modules/torch/nn/modules/loss.html#BCELoss"><font size=2>[source]</font></a></h3>
<p>计算 <code>target</code> 与 <code>output</code> 之间的二进制交叉熵。
$$
loss(o,t)=-\frac{1}{n}\sum_i(t[i]<em> log(o[i])+(1-t[i])</em> log(1-o[i]))
$$
如果<code>weight</code>被指定 ：
$$
loss(o,t)=-\frac{1}{n}\sum_iweights[i]<em> (t[i]</em> log(o[i])+(1-t[i])* log(1-o[i]))
$$</p>
<p>这个用于计算 <code>auto-encoder</code> 的 <code>reconstruction error</code>。注意 0&lt;=target[i]&lt;=1。</p>
<p>默认情况下，loss会基于<code>element</code>平均，如果<code>size_average=False</code>的话，<code>loss</code>会被累加。</p>
<h3 id="class-torchnnmarginrankinglossmargin0-size_averagetruesource">class torch.nn.MarginRankingLoss(margin=0, size_average=True)<a href="https://pytorch.org/docs/_modules/torch/nn/modules/loss.html#MarginRankingLoss"><font size=2>[source]</font></a></h3>
<p>创建一个标准，给定输入 <span class="arithmatex">\(x1\)</span>,<span class="arithmatex">\(x2\)</span>两个1-D mini-batch Tensor's，和一个<span class="arithmatex">\(y\)</span>(1-D mini-batch tensor) ,<span class="arithmatex">\(y\)</span>里面的值只能是-1或1。</p>
<p>如果 <code>y=1</code>，代表第一个输入的值应该大于第二个输入的值，如果<code>y=-1</code>的话，则相反。</p>
<p><code>mini-batch</code>中每个样本的loss的计算公式如下：</p>
<div class="arithmatex">\[loss(x, y) = max(0, -y * (x1 - x2) + margin)\]</div>
<p>如果<code>size_average=True</code>,那么求出的<code>loss</code>将会对<code>mini-batch</code>求平均，反之，求出的<code>loss</code>会累加。默认情况下，<code>size_average=True</code>。</p>
<h3 id="class-torchnnhingeembeddinglosssize_averagetruesource">class torch.nn.HingeEmbeddingLoss(size_average=True)<a href="https://pytorch.org/docs/_modules/torch/nn/modules/loss.html#HingeEmbeddingLoss"><font size=2>[source]</font></a></h3>
<p>给定一个输入 <span class="arithmatex">\(x\)</span>(2-D mini-batch tensor)和对应的 标签 <span class="arithmatex">\(y\)</span> (1-D tensor,1,-1)，此函数用来计算之间的损失值。这个<code>loss</code>通常用来测量两个输入是否相似，即：使用L1 成对距离。典型是用在学习非线性 <code>embedding</code>或者半监督学习中：</p>
<p>$$
loss(x,y)=\frac{1}{n}\sum_i
\begin{cases}
x_i, &amp;\text if~y_i==1 \
max(0, margin-x_i), &amp;if ~y_i==-1
\end{cases}
$$
<span class="arithmatex">\(x\)</span>和<span class="arithmatex">\(y\)</span>可以是任意形状，且都有<code>n</code>的元素，<code>loss</code>的求和操作作用在所有的元素上，然后除以<code>n</code>。如果您不想除以<code>n</code>的话，可以通过设置<code>size_average=False</code>。</p>
<p><code>margin</code>的默认值为1,可以通过构造函数来设置。</p>
<h3 id="class-torchnnmultilabelmarginlosssize_averagetruesource">class torch.nn.MultiLabelMarginLoss(size_average=True)<a href="https://pytorch.org/docs/_modules/torch/nn/modules/loss.html#MultiLabelMarginLoss"><font size=2>[source]</font></a></h3>
<p>计算多标签分类的 <code>hinge loss</code>(<code>margin-based loss</code>) ，计算<code>loss</code>时需要两个输入： input x(<code>2-D mini-batch Tensor</code>)，和 output y(<code>2-D tensor</code>表示mini-batch中样本类别的索引)。</p>
<p>$$
loss(x, y) = \frac{1}{x.size(0)}\sum_{i=0,j=0}^{I,J}(max(0, 1 - (x[y[j]] - x[i])))
$$
其中 <code>I=x.size(0),J=y.size(0)</code>。对于所有的 <code>i</code>和 <code>j</code>，满足 <span class="arithmatex">\(y[j]\neq0, i \neq y[j]\)</span></p>
<p><code>x</code> 和 <code>y</code> 必须具有同样的 <code>size</code>。</p>
<p>这个标准仅考虑了第一个非零 <code>y[j] targets</code>
此标准允许了，对于每个样本来说，可以有多个类别。</p>
<h3 id="class-torchnnsmoothl1losssize_averagetruesource">class torch.nn.SmoothL1Loss(size_average=True)<a href="https://pytorch.org/docs/_modules/torch/nn/modules/loss.html#SmoothL1Loss"><font size=2>[source]</font></a></h3>
<p>平滑版<code>L1 loss</code>。</p>
<p>loss的公式如下：
$$
loss(x, y) = \frac{1}{n}\sum_i
\begin{cases}
0.5*(x_i-y_i)^2, &amp; if~|x_i - y_i| &lt; 1\
|x_i - y_i| - 0.5,  &amp; otherwise  <br />
\end{cases}
$$
此loss对于异常点的敏感性不如<code>MSELoss</code>，而且，在某些情况下防止了梯度爆炸，(参照 <code>Fast R-CNN</code>)。这个<code>loss</code>有时也被称为 <code>Huber loss</code>。</p>
<p>x 和 y 可以是任何包含<code>n</code>个元素的tensor。默认情况下，求出来的<code>loss</code>会除以<code>n</code>，可以通过设置<code>size_average=True</code>使loss累加。</p>
<h3 id="class-torchnnsoftmarginlosssize_averagetruesource">class torch.nn.SoftMarginLoss(size_average=True)<a href="https://pytorch.org/docs/_modules/torch/nn/modules/loss.html#SoftMarginLoss"><font size=2>[source]</font></a></h3>
<p>创建一个标准，用来优化2分类的<code>logistic loss</code>。输入为 <code>x</code>(一个 2-D mini-batch Tensor）和 目标<code>y</code>(一个包含1或-1的Tensor）。
$$
loss(x, y) = \frac{1}{x.nelement()}\sum_i (log(1 + exp(-y[i]* x[i])))
$$
如果求出的<code>loss</code>不想被平均可以通过设置<code>size_average=False</code>。</p>
<h3 id="class-torchnnmultilabelsoftmarginlossweightnone-size_averagetruesource">class torch.nn.MultiLabelSoftMarginLoss(weight=None, size_average=True)<a href="https://pytorch.org/docs/_modules/torch/nn/modules/loss.html#MultiLabelSoftMarginLoss"><font size=2>[source]</font></a></h3>
<p>创建一个标准，基于输入x和目标y的 <code>max-entropy</code>，优化多标签 <code>one-versus-all</code> 的损失。<code>x</code>:2-D mini-batch Tensor;<code>y</code>:binary 2D Tensor。对每个mini-batch中的样本，对应的loss为：
$$
loss(x, y) = - \frac{1}{x.nElement()}\sum_{i=0}^I y[i]\text{log}\frac{exp(x[i])}{(1 + exp(x[i])}
                      + (1-y[i])\text{log}\frac{1}{1+exp(x[i])}
$$
其中 <code>I=x.nElement()-1</code>, <span class="arithmatex">\(y[i] \in \{0,1\}\)</span>，<code>y</code> 和 <code>x</code>必须要有同样<code>size</code>。</p>
<h3 id="class-torchnncosineembeddinglossmargin0-size_averagetruesource">class torch.nn.CosineEmbeddingLoss(margin=0, size_average=True)<a href="https://pytorch.org/docs/_modules/torch/nn/modules/loss.html#CosineEmbeddingLoss"><font size=2>[source]</font></a></h3>
<p>给定 输入 <code>Tensors</code>，<code>x1</code>, <code>x2</code> 和一个标签Tensor <code>y</code>(元素的值为1或-1)。此标准使用<code>cosine</code>距离测量两个输入是否相似，一般用来用来学习非线性<code>embedding</code>或者半监督学习。</p>
<p><code>margin</code>应该是-1到1之间的值，建议使用0到0.5。如果没有传入<code>margin</code>实参，默认值为0。</p>
<p>每个样本的loss是：
$$
loss(x, y) =
\begin{cases}
1 - cos(x1, x2),              &amp;if~y ==  1 \
max(0, cos(x1, x2) - margin), &amp;if~y == -1
\end{cases}
$$
如果<code>size_average=True</code> 求出的loss会对batch求均值，如果<code>size_average=False</code>的话，则会累加<code>loss</code>。默认情况<code>size_average=True</code>。</p>
<h3 id="class-torchnnmultimarginlossp1-margin1-weightnone-size_averagetruesource">class torch.nn.MultiMarginLoss(p=1, margin=1, weight=None, size_average=True)<a href="https://pytorch.org/docs/_modules/torch/nn/modules/loss.html#MultiMarginLoss"><font size=2>[source]</font></a></h3>
<p>用来计算multi-class classification的hinge loss(magin-based loss）。输入是 <code>x</code>(2D mini-batch Tensor), <code>y</code>(1D Tensor)包含类别的索引， <code>0 &lt;= y &lt;= x.size(1))</code>。</p>
<p>对每个mini-batch样本：
$$
loss(x, y) = \frac{1}{x.size(0)}\sum_{i=0}^I(max(0, margin - x[y] + x[i])^p)
$$
其中 <code>I=x.size(0)</code> <span class="arithmatex">\(i\neq y\)</span>。
可选择的，如果您不想所有的类拥有同样的权重的话，您可以通过在构造函数中传入<code>weights</code>参数来解决这个问题，<code>weights</code>是一个1D权重Tensor。</p>
<p>传入weights后，loss函数变为：
$$
loss(x, y) = \frac{1}{x.size(0)}\sum_imax(0, w[y] * (margin - x[y] - x[i]))^p
$$
默认情况下，求出的loss会对mini-batch取平均，可以通过设置<code>size_average=False</code>来取消取平均操作。</p>
<h2 id="vision-layers">Vision layers</h2>
<h3 id="class-torchnnpixelshuffleupscale_factorsource">class torch.nn.PixelShuffle(upscale_factor)<a href="https://pytorch.org/docs/_modules/torch/nn/modules/pixelshuffle.html#PixelShuffle"><font size=2>[source]</font></a></h3>
<p>将shape为<span class="arithmatex">\([N, C*r^2, H, W]\)</span>的<code>Tensor</code>重新排列为shape为<span class="arithmatex">\([N, C, H*r, W*r]\)</span>的Tensor。
当使用<code>stride=1/r</code> 的sub-pixel卷积的时候，这个方法是非常有用的。</p>
<p>请看paper<a href="https://arxiv.org/abs/1609.05158">Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network by Shi et. al (2016)</a> 获取详细信息。</p>
<p>参数说明：</p>
<ul>
<li>upscale_factor (int) – 增加空间分辨率的因子</li>
</ul>
<p>Shape:</p>
<ul>
<li>
<p>Input: <span class="arithmatex">\([N,C*upscale\_factor^2,H,W\)</span>]</p>
</li>
<li>
<p>Output: <span class="arithmatex">\([N,C,H*upscale\_factor,W*upscale\_factor]\)</span></p>
</li>
</ul>
<p>例子:</p>
<pre><code class="language-python">&gt;&gt;&gt; ps = nn.PixelShuffle(3)
&gt;&gt;&gt; input = autograd.Variable(torch.Tensor(1, 9, 4, 4))
&gt;&gt;&gt; output = ps(input)
&gt;&gt;&gt; print(output.size())
torch.Size([1, 1, 12, 12])
</code></pre>
<h3 id="class-torchnnupsamplingnearest2dsizenone-scale_factornonesource">class torch.nn.UpsamplingNearest2d(size=None, scale_factor=None)<a href="https://pytorch.org/docs/_modules/torch/nn/modules/upsampling.html#UpsamplingNearest2d"><font size=2>[source]</font></a></h3>
<p>对于多channel 输入 进行 <code>2-D</code> 最近邻上采样。</p>
<p>可以通过<code>size</code>或者<code>scale_factor</code>来指定上采样后的图片大小。</p>
<p>当给定<code>size</code>时，<code>size</code>的值将会是输出图片的大小。</p>
<p>参数：</p>
<ul>
<li>size (tuple, optional) – 一个包含两个整数的元组 (H_out, W_out)指定了输出的长宽</li>
<li>scale_factor (int, optional) – 长和宽的一个乘子</li>
</ul>
<p>形状：</p>
<ul>
<li>Input: (N,C,H_in,W_in)</li>
<li>Output: (N,C,H_out,W_out)  Hout=floor(H_in<em>scale_factor) Wout=floor(W_in</em>scale_factor)</li>
</ul>
<p>例子：</p>
<pre><code class="language-python">&gt;&gt;&gt; inp
Variable containing:
(0 ,0 ,.,.) =
  1  2
  3  4
[torch.FloatTensor of size 1x1x2x2]

&gt;&gt;&gt; m = nn.UpsamplingNearest2d(scale_factor=2)
&gt;&gt;&gt; m(inp)
Variable containing:
(0 ,0 ,.,.) =
  1  1  2  2
  1  1  2  2
  3  3  4  4
  3  3  4  4
[torch.FloatTensor of size 1x1x4x4]
</code></pre>
<h3 id="class-torchnnupsamplingbilinear2dsizenone-scale_factornonesource">class torch.nn.UpsamplingBilinear2d(size=None, scale_factor=None)<a href="https://pytorch.org/docs/_modules/torch/nn/modules/upsampling.html#UpsamplingBilinear2d"><font size=2>[source]</font></a></h3>
<p>对于多channel 输入 进行 <code>2-D</code> <code>bilinear</code> 上采样。</p>
<p>可以通过<code>size</code>或者<code>scale_factor</code>来指定上采样后的图片大小。</p>
<p>当给定<code>size</code>时，<code>size</code>的值将会是输出图片的大小。</p>
<p>参数：</p>
<ul>
<li>size (tuple, optional) – 一个包含两个整数的元组 (H_out, W_out)指定了输出的长宽</li>
<li>scale_factor (int, optional) – 长和宽的一个乘子</li>
</ul>
<p>形状：</p>
<ul>
<li>Input: (N,C,H_in,W_in)</li>
<li>Output: (N,C,H_out,W_out)  Hout=floor(H_in<em>scale_factor) Wout=floor(W_in</em>scale_factor)</li>
</ul>
<p>例子：</p>
<pre><code class="language-python">&gt;&gt;&gt; inp
Variable containing:
(0 ,0 ,.,.) =
  1  2
  3  4
[torch.FloatTensor of size 1x1x2x2]

&gt;&gt;&gt; m = nn.UpsamplingBilinear2d(scale_factor=2)
&gt;&gt;&gt; m(inp)
Variable containing:
(0 ,0 ,.,.) =
  1.0000  1.3333  1.6667  2.0000
  1.6667  2.0000  2.3333  2.6667
  2.3333  2.6667  3.0000  3.3333
  3.0000  3.3333  3.6667  4.0000
[torch.FloatTensor of size 1x1x4x4]
</code></pre>
<h2 id="multi-gpu-layers">Multi-GPU layers</h2>
<h3 id="class-torchnndataparallelmodule-device_idsnone-output_devicenone-dim0source">class torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0)<a href="https://pytorch.org/docs/_modules/torch/nn/parallel/data_parallel.html#DataParallel"><font size=2>[source]</font></a></h3>
<p>在模块级别上实现数据并行。</p>
<p>此容器通过将<code>mini-batch</code>划分到不同的设备上来实现给定<code>module</code>的并行。在<code>forward</code>过程中，<code>module</code>会在每个设备上都复制一遍，每个副本都会处理部分输入。在<code>backward</code>过程中，副本上的梯度会累加到原始<code>module</code>上。</p>
<p>batch的大小应该大于所使用的GPU的数量。还应当是GPU个数的整数倍，这样划分出来的每一块都会有相同的样本数量。</p>
<p>请看: <a href="">Use nn.DataParallel instead of multiprocessing</a></p>
<p>除了<code>Tensor</code>，任何位置参数和关键字参数都可以传到DataParallel中。所有的变量会通过指定的<code>dim</code>来划分(默认值为0）。原始类型将会被广播，但是所有的其它类型都会被浅复制。所以如果在模型的<code>forward</code>过程中写入的话，将会被损坏。</p>
<p>参数说明：</p>
<ul>
<li>module – 要被并行的module</li>
<li>device_ids – CUDA设备，默认为所有设备。</li>
<li>output_device – 输出设备(默认为device_ids[0]）</li>
</ul>
<p>例子：</p>
<pre><code class="language-python"> net = torch.nn.DataParallel(model, device_ids=[0, 1, 2])
 output = net(input_var)
</code></pre>
<h2 id="utilities">Utilities</h2>
<p>工具函数</p>
<h3 id="torchnnutilsclip_grad_normparameters-max_norm-norm_type2source">torch.nn.utils.clip_grad_norm(parameters, max_norm, norm_type=2)<a href="https://pytorch.org/docs/_modules/torch/nn/utils/clip_grad.html#clip_grad_norm"><font size=2>[source]</font></a></h3>
<p>Clips gradient norm of an iterable of parameters.</p>
<p>正则項的值由所有的梯度计算出来，就像他们连成一个向量一样。梯度被<code>in-place operation</code>修改。</p>
<p>参数说明:
- parameters (Iterable[Variable]) – 可迭代的<code>Variables</code>，它们的梯度即将被标准化。
- max_norm (float or int) – <code>clip</code>后，<code>gradients</code> p-norm 值
- norm_type (float or int) – 标准化的类型，p-norm. 可以是<code>inf</code> 代表 infinity norm.</p>
<p><a href="https://rorasa.wordpress.com/2012/05/13/l0-norm-l1-norm-l2-norm-l-infinity-norm/">关于norm</a></p>
<p>返回值:</p>
<p>所有参数的p-norm值。</p>
<h3 id="torchnnutilsrnnpackedsequence_cls-data-batch_sizessource">torch.nn.utils.rnn.PackedSequence(_cls, data, batch_sizes)<a href="https://pytorch.org/docs/_modules/torch/nn/utils/rnn.html#PackedSequence"><font size=2>[source]</font></a></h3>
<p>Holds the data and list of batch_sizes of a packed sequence.</p>
<p>All RNN modules accept packed sequences as inputs.
所有的<code>RNN</code>模块都接收这种被包裹后的序列作为它们的输入。</p>
<p><code>NOTE：</code>
这个类的实例不能手动创建。它们只能被 <code>pack_padded_sequence()</code> 实例化。</p>
<p>参数说明:</p>
<ul>
<li>
<p>data (Variable) – 包含打包后序列的<code>Variable</code>。</p>
</li>
<li>
<p>batch_sizes (list[int]) – 包含 <code>mini-batch</code> 中每个序列长度的列表。</p>
</li>
</ul>
<h3 id="torchnnutilsrnnpack_padded_sequenceinput-lengths-batch_firstfalsesource">torch.nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=False)<a href="https://pytorch.org/docs/_modules/torch/nn/utils/rnn.html#PackedSequence"><font size=2>[source]</font></a></h3>
<p>这里的<code>pack</code>，理解成压紧比较好。
将一个 填充过的变长序列 压紧。(填充时候，会有冗余，所以压紧一下）</p>
<p>输入的形状可以是(T<em>B</em>* )。<code>T</code>是最长序列长度，<code>B</code>是<code>batch size</code>，<code>*</code>代表任意维度(可以是0)。如果<code>batch_first=True</code>的话，那么相应的 <code>input size</code> 就是 <code>(B*T**)</code>。</p>
<p><code>Variable</code>中保存的序列，应该按序列长度的长短排序，长的在前，短的在后。即<code>input[:,0]</code>代表的是最长的序列，<code>input[:, B-1]</code>保存的是最短的序列。</p>
<p><code>NOTE：</code>
只要是维度大于等于2的<code>input</code>都可以作为这个函数的参数。你可以用它来打包<code>labels</code>，然后用<code>RNN</code>的输出和打包后的<code>labels</code>来计算<code>loss</code>。通过<code>PackedSequence</code>对象的<code>.data</code>属性可以获取 <code>Variable</code>。</p>
<p>参数说明:</p>
<ul>
<li>
<p>input (Variable) – 变长序列 被填充后的 batch</p>
</li>
<li>
<p>lengths (list[int]) – <code>Variable</code> 中 每个序列的长度。</p>
</li>
<li>
<p>batch_first (bool, optional) – 如果是<code>True</code>，input的形状应该是<code>B*T*size</code>。</p>
</li>
</ul>
<p>返回值:</p>
<p>一个<code>PackedSequence</code> 对象。</p>
<h3 id="torchnnutilsrnnpad_packed_sequencesequence-batch_firstfalsesource">torch.nn.utils.rnn.pad_packed_sequence(sequence, batch_first=False)<a href="https://pytorch.org/docs/_modules/torch/nn/utils/rnn.html#pack_padded_sequence"><font size=2>[source]</font></a></h3>
<p>填充<code>packed_sequence</code>。</p>
<p>上面提到的函数的功能是将一个填充后的变长序列压紧。 这个操作和pack_padded_sequence()是相反的。把压紧的序列再填充回来。</p>
<p>返回的Varaible的值的<code>size</code>是 <code>T*B**</code>, <code>T</code> 是最长序列的长度，<code>B</code> 是 batch_size,如果 <code>batch_first=True</code>,那么返回值是<code>B*T**</code>。</p>
<p>Batch中的元素将会以它们长度的逆序排列。</p>
<p>参数说明:</p>
<ul>
<li>
<p>sequence (PackedSequence) – 将要被填充的 batch</p>
</li>
<li>
<p>batch_first (bool, optional) – 如果为True，返回的数据的格式为 <code>B*T**</code>。</p>
</li>
</ul>
<p>返回值:
一个tuple，包含被填充后的序列，和batch中序列的长度列表。</p>
<p>例子：</p>
<pre><code class="language-python">import torch
import torch.nn as nn
from torch.autograd import Variable
from torch.nn import utils as nn_utils
batch_size = 2
max_length = 3
hidden_size = 2
n_layers =1

tensor_in = torch.FloatTensor([[1, 2, 3], [1, 0, 0]]).resize_(2,3,1)
tensor_in = Variable( tensor_in ) #[batch, seq, feature], [2, 3, 1]
seq_lengths = [3,1] # list of integers holding information about the batch size at each sequence step

# pack it
pack = nn_utils.rnn.pack_padded_sequence(tensor_in, seq_lengths, batch_first=True)

# initialize
rnn = nn.RNN(1, hidden_size, n_layers, batch_first=True)
h0 = Variable(torch.randn(n_layers, batch_size, hidden_size))

#forward
out, _ = rnn(pack, h0)

# unpack
unpacked = nn_utils.rnn.pad_packed_sequence(out)
print(unpacked)
</code></pre>
<p><a href="https://discuss.pytorch.org/t/how-can-i-compute-seq2seq-loss-using-mask/861">关于packed_sequence</a></p>
<hr/>
<div align="center">
  <p><a href="https://www.apachecn.org/" target="_blank"><font face="KaiTi" size="6" color="red">我们一直在努力</font></a><p>
  <p><a href="https://github.com/apachecn/pytorch-doc-zh" target="_blank">apachecn/pytorch-doc-zh</a></p>
  <p><a target="_blank" href="https://qm.qq.com/cgi-bin/qm/qr?k=5u_aAU-YlY3fH-m8meXTJzBEo2boQIUs&jump_from=webapi&authKey=CVZcReMt/vKdTXZBQ8ly+jWncXiSzzWOlrx5hybX5pSrKu6s0fvGX54+vHHlgYNt"><img border="0" src="https://pub.idqqimg.com/wpa/images/group.png" alt="【布客】中文翻译组" title="【布客】中文翻译组"></a></p>
  <p><span id="cnzz_stat_icon_1275211409"></span></p>
  <!-- <p><a href="https://get.brightdata.com/apachecn" target="_blank"><img src="/assets/images/partnerstack.gif" /></a><p> -->
  <div class="wwads-cn wwads-horizontal" data-id="206" style="max-width:680px"></div>
  <div style="text-align:center;margin:0 0 10.5px;">
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-3565452474788507" crossorigin="anonymous"></script>
    <!-- ApacheCNWide -->
    <ins class="adsbygoogle"
        style="display:inline-block;width:680px;height:90px"
        data-ad-client="ca-pub-3565452474788507"
        data-ad-slot="2543897000"></ins>
    <script>
        (adsbygoogle = window.adsbygoogle || []).push({});
    </script>
  </div>
</div>
<hr/>
<!-- 来必力City版安装代码 -->
<div id="lv-container" data-id="city" data-uid="MTAyMC81ODA2NC8zNDUyNw==">
  <script type="text/javascript">
  (function(d, s) {
      var j, e = d.getElementsByTagName(s)[0];

      if (typeof LivereTower === 'function') { return; }

      j = d.createElement(s);
      j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
      j.async = true;

      e.parentNode.insertBefore(j, e);
  })(document, 'script');
  </script>
<noscript> 为正常使用来必力评论功能请激活JavaScript</noscript>
</div>
<!-- City版安装代码已完成 -->






                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../Storage/" class="md-footer__link md-footer__link--prev" aria-label="Previous: torch.Storage" rel="prev">
            <div class="md-footer__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                torch.Storage
              </div>
            </div>
          </a>
        
        
          
          <a href="../functional/" class="md-footer__link md-footer__link--next" aria-label="Next: torch.nn.functional" rel="next">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                torch.nn.functional
              </div>
            </div>
            <div class="md-footer__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright" style="text-align: center; width: 100%;">
  
  
    <div>
      <div style="margin:0 0 10.5px;"><script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1275211409'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s5.cnzz.com/z_stat.php%3Fid%3D1275211409%26online%3D1%26show%3Dline' type='text/javascript'%3E%3C/script%3E"));</script></div>
      <p>Copyright © 2023 学习网站 <a href="http://beian.miit.gov.cn" target="_blank">京ICP备19016010号-1</a><br/>网站由 <a href="https://apachecn.org/cooperate/">@片刻小哥哥</a> 提供支持 | 联系QQ/微信: 529815144 请注明来意！</p>
    </div>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../../..", "features": ["content.code.copy", "content.action.edit", "content.action.view", "navigation.footer"], "search": "../../../assets/javascripts/workers/search.208ed371.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
  
      <script src="../../../assets/javascripts/bundle.b425cdc4.min.js"></script>
      
        
          <script src="../../../javascripts/mathjax.js"></script>
        
      
        
          <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        
      
        
          <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        
      
    
  <script src="../../../assets/javascripts/custom.a7283b5f.min.js"></script>

  </body>
</html>