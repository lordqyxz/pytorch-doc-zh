
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://pytorch.apachecn.org/1.4/75/">
      
      
        <link rel="prev" href="../74/">
      
      
        <link rel="next" href="../76/">
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.4.3, mkdocs-material-9.1.17">
    
    
      
        <title>torch.nn - 【布客】PyTorch 中文翻译</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.26e3688c.min.css">
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  <link rel="stylesheet" href="../../assets/stylesheets/custom.bea7efe8.min.css">
  <!-- google ads -->
  <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-3565452474788507" crossorigin="anonymous"></script>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-8DP4GX97XY"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-8DP4GX97XY');
  </script>
  <!-- google webmaster -->
  <meta name="google-site-verification" content="pyo9N70ZWyh8JB43bIu633mhxesJ1IcwWCZlM3jUfFo" />

  <!-- wwads-cn union -->
  <meta name="wwads-cn-verify" content="03c6b06952c750899bb03d998e631860" />
  <script type="text/javascript" charset="UTF-8" src="https://cdn.wwads.cn/js/makemoney.js" async></script>

  </head>
  
  
    <body dir="ltr">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#torchnn" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
        <aside class="md-banner">
          <div class="md-banner__inner md-grid md-typeset">
            
            
  => 组织无偿提供 中文版本（免费，秒级响应）
  <a target="_blank" href="https://chat.ibooker.org.cn/chat" style="color: red;">
    <span class="twemoji mastodon">
      <img src="https://data.apachecn.org/img/icon/ROBOT_TXT.svg" alt="ChatGPT - ailake.top">
    </span>
    <strong>ChatGPT - ailake.top</strong>
  </a> 一起来白嫖叭～！

          </div>
          
        </aside>
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="【布客】PyTorch 中文翻译" class="md-header__button md-logo" aria-label="【布客】PyTorch 中文翻译" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            【布客】PyTorch 中文翻译
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              torch.nn
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/apachecn/pytorch-doc-zh" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    apachecn/pytorch-doc-zh
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="【布客】PyTorch 中文翻译" class="md-nav__button md-logo" aria-label="【布客】PyTorch 中文翻译" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    【布客】PyTorch 中文翻译
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/apachecn/pytorch-doc-zh" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    apachecn/pytorch-doc-zh
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        PyTorch 中文文档 & 教程
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
      
      
      
        <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
          PyTorch 2.0 中文文档 & 教程
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 2.0 中文文档 & 教程
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1" >
      
      
      
        <label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="0">
          中文教程
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_1">
          <span class="md-nav__icon md-icon"></span>
          中文教程
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/README.md" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_2" >
      
      
      
        <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="0">
          中文文档
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_2">
          <span class="md-nav__icon md-icon"></span>
          中文文档
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/docs/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
      
      
      
        <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
          PyTorch 1.7 中文文档
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 1.7 中文文档
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" >
      
      
      
        <label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="0">
          学习 PyTorch
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_1">
          <span class="md-nav__icon md-icon"></span>
          学习 PyTorch
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_1" >
      
      
      
        <label class="md-nav__link" for="__nav_3_1_1" id="__nav_3_1_1_label" tabindex="0">
          PyTorch 深度学习：60 分钟的突击
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_1_1">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 深度学习：60 分钟的突击
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/02/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/03/" class="md-nav__link">
        张量
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/04/" class="md-nav__link">
        torch.autograd的简要介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/05/" class="md-nav__link">
        神经网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/06/" class="md-nav__link">
        训练分类器
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_2" >
      
      
      
        <label class="md-nav__link" for="__nav_3_1_2" id="__nav_3_1_2_label" tabindex="0">
          通过示例学习 PyTorch
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_1_2">
          <span class="md-nav__icon md-icon"></span>
          通过示例学习 PyTorch
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/07/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/08/" class="md-nav__link">
        热身：NumPy
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/09/" class="md-nav__link">
        PyTorch：张量
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/10/" class="md-nav__link">
        PyTorch：张量和 Autograd
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/11/" class="md-nav__link">
        PyTorch：定义新的 Autograd 函数
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/12/" class="md-nav__link">
        PyTorch：nn
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/13/" class="md-nav__link">
        PyTorch：optim
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/14/" class="md-nav__link">
        PyTorch：自定义nn模块
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/15/" class="md-nav__link">
        PyTorch：控制流 - 权重共享
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/16/" class="md-nav__link">
        torch.nn到底是什么？
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/17/" class="md-nav__link">
        使用 TensorBoard 可视化模型，数据和训练
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
      
      
      
        <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
          图片/视频
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_2">
          <span class="md-nav__icon md-icon"></span>
          图片/视频
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/19/" class="md-nav__link">
        torchvision对象检测微调教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/20/" class="md-nav__link">
        计算机视觉的迁移学习教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/21/" class="md-nav__link">
        对抗示例生成
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/22/" class="md-nav__link">
        DCGAN 教程
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_3" >
      
      
      
        <label class="md-nav__link" for="__nav_3_3" id="__nav_3_3_label" tabindex="0">
          音频
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_3">
          <span class="md-nav__icon md-icon"></span>
          音频
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/24/" class="md-nav__link">
        音频 I/O 和torchaudio的预处理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/25/" class="md-nav__link">
        使用torchaudio的语音命令识别
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4" id="__nav_3_4_label" tabindex="0">
          文本
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4">
          <span class="md-nav__icon md-icon"></span>
          文本
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/27/" class="md-nav__link">
        使用nn.Transformer和torchtext的序列到序列建模
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/28/" class="md-nav__link">
        从零开始的 NLP：使用字符级 RNN 分类名称
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/29/" class="md-nav__link">
        从零开始的 NLP：使用字符级 RNN 生成名称
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/30/" class="md-nav__link">
        从零开始的 NLP：使用序列到序列网络和注意力的翻译
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/31/" class="md-nav__link">
        使用torchtext的文本分类
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/32/" class="md-nav__link">
        torchtext语言翻译
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_5" >
      
      
      
        <label class="md-nav__link" for="__nav_3_5" id="__nav_3_5_label" tabindex="0">
          强化学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_5">
          <span class="md-nav__icon md-icon"></span>
          强化学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/34/" class="md-nav__link">
        强化学习（DQN）教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/35/" class="md-nav__link">
        训练玩马里奥的 RL 智能体
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_6" >
      
      
      
        <label class="md-nav__link" for="__nav_3_6" id="__nav_3_6_label" tabindex="0">
          在生产中部署 PyTorch 模型
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_6">
          <span class="md-nav__icon md-icon"></span>
          在生产中部署 PyTorch 模型
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/37/" class="md-nav__link">
        通过使用 Flask 的 REST API 在 Python 中部署 PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/38/" class="md-nav__link">
        TorchScript 简介
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/39/" class="md-nav__link">
        在 C-- 中加载 TorchScript 模型
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/40/" class="md-nav__link">
        将模型从 PyTorch 导出到 ONNX 并使用 ONNX 运行时运行它（可选）
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_7" >
      
      
      
        <label class="md-nav__link" for="__nav_3_7" id="__nav_3_7_label" tabindex="0">
          前端 API
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_7_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_7">
          <span class="md-nav__icon md-icon"></span>
          前端 API
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/42/" class="md-nav__link">
        PyTorch 中的命名张量简介（原型）
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/43/" class="md-nav__link">
        PyTorch 中通道在最后的内存格式（beta）
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/44/" class="md-nav__link">
        使用 PyTorch C-- 前端
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/45/" class="md-nav__link">
        自定义 C-- 和 CUDA 扩展
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/46/" class="md-nav__link">
        使用自定义 C-- 运算符扩展 TorchScript
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/47/" class="md-nav__link">
        使用自定义 C-- 类扩展 TorchScript
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/48/" class="md-nav__link">
        TorchScript 中的动态并行性
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/49/" class="md-nav__link">
        C-- 前端中的 Autograd
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/50/" class="md-nav__link">
        在 C-- 中注册调度运算符
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_8" >
      
      
      
        <label class="md-nav__link" for="__nav_3_8" id="__nav_3_8_label" tabindex="0">
          模型优化
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_8_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_8">
          <span class="md-nav__icon md-icon"></span>
          模型优化
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/52/" class="md-nav__link">
        分析您的 PyTorch 模块
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/53/" class="md-nav__link">
        使用 Ray Tune 的超参数调整
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/54/" class="md-nav__link">
        模型剪裁教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/55/" class="md-nav__link">
        LSTM 单词语言模型上的动态量化（beta）
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/56/" class="md-nav__link">
        BERT 上的动态量化（Beta）
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/57/" class="md-nav__link">
        PyTorch 中使用 Eager 模式的静态量化（beta）
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/58/" class="md-nav__link">
        计算机视觉的量化迁移学习教程（beta）
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_9" >
      
      
      
        <label class="md-nav__link" for="__nav_3_9" id="__nav_3_9_label" tabindex="0">
          并行和分布式训练
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_9_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_9">
          <span class="md-nav__icon md-icon"></span>
          并行和分布式训练
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/60/" class="md-nav__link">
        PyTorch 分布式概述
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/61/" class="md-nav__link">
        单机模型并行最佳实践
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/62/" class="md-nav__link">
        分布式数据并行入门
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/63/" class="md-nav__link">
        用 PyTorch 编写分布式应用
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/64/" class="md-nav__link">
        分布式 RPC 框架入门
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/65/" class="md-nav__link">
        使用分布式 RPC 框架实现参数服务器
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/66/" class="md-nav__link">
        使用 RPC 的分布式管道并行化
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/67/" class="md-nav__link">
        使用异步执行实现批量 RPC 处理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/68/" class="md-nav__link">
        将分布式DataParallel与分布式 RPC 框架相结合
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
      
      
      
        <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
          PyTorch 1.4 中文文档 & 教程
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 1.4 中文文档 & 教程
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1" >
      
      
      
        <label class="md-nav__link" for="__nav_4_1" id="__nav_4_1_label" tabindex="0">
          入门
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_1">
          <span class="md-nav__icon md-icon"></span>
          入门
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1_1" >
      
      
      
        <label class="md-nav__link" for="__nav_4_1_1" id="__nav_4_1_1_label" tabindex="0">
          使用 PyTorch 进行深度学习：60 分钟的闪电战
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_4_1_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_1_1">
          <span class="md-nav__icon md-icon"></span>
          使用 PyTorch 进行深度学习：60 分钟的闪电战
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../4/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../blitz/tensor_tutorial/" class="md-nav__link">
        什么是PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../blitz/autograd_tutorial/" class="md-nav__link">
        Autograd：自动求导
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../blitz/neural_networks_tutorial/" class="md-nav__link">
        神经网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../blitz/cifar10_tutorial/" class="md-nav__link">
        训练分类器
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../blitz/data_parallel_tutorial/" class="md-nav__link">
        可选：数据并行
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../5/" class="md-nav__link">
        编写自定义数据集，数据加载器和转换
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../6/" class="md-nav__link">
        使用 TensorBoard 可视化模型，数据和训练
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_2" >
      
      
      
        <label class="md-nav__link" for="__nav_4_2" id="__nav_4_2_label" tabindex="0">
          图片
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_2">
          <span class="md-nav__icon md-icon"></span>
          图片
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../8/" class="md-nav__link">
        TorchVision 对象检测微调教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../9/" class="md-nav__link">
        转移学习的计算机视觉教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../10/" class="md-nav__link">
        空间变压器网络教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../11/" class="md-nav__link">
        使用 PyTorch 进行神经传递
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../12/" class="md-nav__link">
        对抗示例生成
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../13/" class="md-nav__link">
        DCGAN 教程
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_3" >
      
      
      
        <label class="md-nav__link" for="__nav_4_3" id="__nav_4_3_label" tabindex="0">
          音频
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_3">
          <span class="md-nav__icon md-icon"></span>
          音频
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../15/" class="md-nav__link">
        torchaudio 教程
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_4" >
      
      
      
        <label class="md-nav__link" for="__nav_4_4" id="__nav_4_4_label" tabindex="0">
          文本
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_4">
          <span class="md-nav__icon md-icon"></span>
          文本
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../17/" class="md-nav__link">
        NLP From Scratch: 使用char-RNN对姓氏进行分类
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../18/" class="md-nav__link">
        NLP From Scratch: 生成名称与字符级RNN
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../19/" class="md-nav__link">
        NLP From Scratch: 基于注意力机制的 seq2seq 神经网络翻译
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../20/" class="md-nav__link">
        使用 TorchText 进行文本分类
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../21/" class="md-nav__link">
        使用 TorchText 进行语言翻译
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../22/" class="md-nav__link">
        使用 nn.Transformer 和 TorchText 进行序列到序列建模
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_5" >
      
      
      
        <label class="md-nav__link" for="__nav_4_5" id="__nav_4_5_label" tabindex="0">
          命名为 Tensor(实验性）
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_5">
          <span class="md-nav__icon md-icon"></span>
          命名为 Tensor(实验性）
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../24/" class="md-nav__link">
        (实验性)PyTorch 中的命名张量简介
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_6" >
      
      
      
        <label class="md-nav__link" for="__nav_4_6" id="__nav_4_6_label" tabindex="0">
          强化学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_6">
          <span class="md-nav__icon md-icon"></span>
          强化学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../26/" class="md-nav__link">
        强化学习(DQN)教程
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_7" >
      
      
      
        <label class="md-nav__link" for="__nav_4_7" id="__nav_4_7_label" tabindex="0">
          在生产中部署 PyTorch 模型
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_7_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_7">
          <span class="md-nav__icon md-icon"></span>
          在生产中部署 PyTorch 模型
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../28/" class="md-nav__link">
        通过带有 Flask 的 REST API 在 Python 中部署 PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../29/" class="md-nav__link">
        TorchScript 简介
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../30/" class="md-nav__link">
        在 C --中加载 TorchScript 模型
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../31/" class="md-nav__link">
        (可选）将模型从 PyTorch 导出到 ONNX 并使用 ONNX Runtime 运行
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_8" >
      
      
      
        <label class="md-nav__link" for="__nav_4_8" id="__nav_4_8_label" tabindex="0">
          并行和分布式训练
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_8_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_8">
          <span class="md-nav__icon md-icon"></span>
          并行和分布式训练
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../33/" class="md-nav__link">
        单机模型并行最佳实践
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../34/" class="md-nav__link">
        分布式数据并行入门
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../35/" class="md-nav__link">
        用 PyTorch 编写分布式应用程序
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../36/" class="md-nav__link">
        分布式 RPC 框架入门
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../37/" class="md-nav__link">
        (高级）带有 Amazon AWS 的 PyTorch 1.0 分布式训练师
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_9" >
      
      
      
        <label class="md-nav__link" for="__nav_4_9" id="__nav_4_9_label" tabindex="0">
          扩展 PyTorch
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_9_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_9">
          <span class="md-nav__icon md-icon"></span>
          扩展 PyTorch
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../39/" class="md-nav__link">
        使用自定义 C --运算符扩展 TorchScript
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../40/" class="md-nav__link">
        使用自定义 C --类扩展 TorchScript
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../41/" class="md-nav__link">
        使用 numpy 和 scipy 创建扩展
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../42/" class="md-nav__link">
        自定义 C --和 CUDA 扩展
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_10" >
      
      
      
        <label class="md-nav__link" for="__nav_4_10" id="__nav_4_10_label" tabindex="0">
          模型优化
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_10_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_10">
          <span class="md-nav__icon md-icon"></span>
          模型优化
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../44/" class="md-nav__link">
        LSTM Word 语言模型上的(实验）动态量化
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../45/" class="md-nav__link">
        (实验性）在 PyTorch 中使用 Eager 模式进行静态量化
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../46/" class="md-nav__link">
        (实验性）计算机视觉教程的量化转移学习
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../47/" class="md-nav__link">
        (实验）BERT 上的动态量化
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../48/" class="md-nav__link">
        修剪教程
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_11" >
      
      
      
        <label class="md-nav__link" for="__nav_4_11" id="__nav_4_11_label" tabindex="0">
          PyTorch 用其他语言
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_11_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_11">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 用其他语言
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../50/" class="md-nav__link">
        使用 PyTorch C --前端
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_12" >
      
      
      
        <label class="md-nav__link" for="__nav_4_12" id="__nav_4_12_label" tabindex="0">
          PyTorch 基础知识
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_12_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_12">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 基础知识
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../52/" class="md-nav__link">
        通过示例学习 PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../53/" class="md-nav__link">
        torch.nn 到底是什么？
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_13" >
      
      
      
        <label class="md-nav__link" for="__nav_4_13" id="__nav_4_13_label" tabindex="0">
          笔记
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_13_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_13">
          <span class="md-nav__icon md-icon"></span>
          笔记
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../56/" class="md-nav__link">
        自动求导机制
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../57/" class="md-nav__link">
        广播语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../58/" class="md-nav__link">
        CPU 线程和 TorchScript 推断
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../59/" class="md-nav__link">
        CUDA 语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../60/" class="md-nav__link">
        分布式 Autograd 设计
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../61/" class="md-nav__link">
        扩展 PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../62/" class="md-nav__link">
        经常问的问题
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../63/" class="md-nav__link">
        大规模部署的功能
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../64/" class="md-nav__link">
        并行处理最佳实践
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../65/" class="md-nav__link">
        重现性
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../66/" class="md-nav__link">
        远程参考协议
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../67/" class="md-nav__link">
        序列化语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../68/" class="md-nav__link">
        Windows 常见问题
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../69/" class="md-nav__link">
        XLA 设备上的 PyTorch
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_14" >
      
      
      
        <label class="md-nav__link" for="__nav_4_14" id="__nav_4_14_label" tabindex="0">
          语言绑定
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_14_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_14">
          <span class="md-nav__icon md-icon"></span>
          语言绑定
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../71/" class="md-nav__link">
        PyTorch C -- API
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../72/" class="md-nav__link">
        PyTorch Java API
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_15" checked>
      
      
      
        <label class="md-nav__link" for="__nav_4_15" id="__nav_4_15_label" tabindex="0">
          Python API
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_15_label" aria-expanded="true">
        <label class="md-nav__title" for="__nav_4_15">
          <span class="md-nav__icon md-icon"></span>
          Python API
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../74/" class="md-nav__link">
        torch
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          torch.nn
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        torch.nn
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    参数
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    货柜
  </a>
  
    <nav class="md-nav" aria-label="货柜">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    模组
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    顺序的
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    模块列表
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#moduledict" class="md-nav__link">
    ModuleDict
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    参数表
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#parameterdict" class="md-nav__link">
    ParameterDict
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    卷积层
  </a>
  
    <nav class="md-nav" aria-label="卷积层">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1d" class="md-nav__link">
    转换 1d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2d" class="md-nav__link">
    转换 2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3d" class="md-nav__link">
    转换 3d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convtranspose1d" class="md-nav__link">
    ConvTranspose1d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convtranspose2d" class="md-nav__link">
    ConvTranspose2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convtranspose3d" class="md-nav__link">
    ConvTranspose3d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    展开
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_9" class="md-nav__link">
    折
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_10" class="md-nav__link">
    汇聚层
  </a>
  
    <nav class="md-nav" aria-label="汇聚层">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#maxpool1d" class="md-nav__link">
    MaxPool1d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#maxpool2d" class="md-nav__link">
    MaxPool2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#maxpool3d" class="md-nav__link">
    MaxPool3d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#maxunpool1d" class="md-nav__link">
    MaxUnpool1d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#maxunpool2d" class="md-nav__link">
    MaxUnpool2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#maxunpool3d" class="md-nav__link">
    MaxUnpool3d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1d_1" class="md-nav__link">
    平均池 1d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2d_1" class="md-nav__link">
    平均池 2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3d_1" class="md-nav__link">
    平均池 3d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2d_2" class="md-nav__link">
    分数最大池 2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lppool1d" class="md-nav__link">
    LPPool1d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lppool2d" class="md-nav__link">
    LPPool2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adaptivemaxpool1d" class="md-nav__link">
    AdaptiveMaxPool1d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adaptivemaxpool2d" class="md-nav__link">
    AdaptiveMaxPool2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adaptivemaxpool3d" class="md-nav__link">
    AdaptiveMaxPool3d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adaptiveavgpool1d" class="md-nav__link">
    AdaptiveAvgPool1d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adaptiveavgpool2d" class="md-nav__link">
    AdaptiveAvgPool2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adaptiveavgpool3d" class="md-nav__link">
    AdaptiveAvgPool3d
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_11" class="md-nav__link">
    填充层
  </a>
  
    <nav class="md-nav" aria-label="填充层">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#reflectionpad1d" class="md-nav__link">
    ReflectionPad1d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reflectionpad2d" class="md-nav__link">
    ReflectionPad2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1d_2" class="md-nav__link">
    复制板 1d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2d_3" class="md-nav__link">
    复制板 2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3d_2" class="md-nav__link">
    复制板 3d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#zeropad2d" class="md-nav__link">
    ZeroPad2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#constantpad1d" class="md-nav__link">
    ConstantPad1d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#constantpad2d" class="md-nav__link">
    ConstantPad2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#constantpad3d" class="md-nav__link">
    ConstantPad3d
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_12" class="md-nav__link">
    非线性激活(加权和，非线性）
  </a>
  
    <nav class="md-nav" aria-label="非线性激活(加权和，非线性）">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#elu" class="md-nav__link">
    ELU
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_13" class="md-nav__link">
    硬收缩
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_14" class="md-nav__link">
    哈丹
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_15" class="md-nav__link">
    漏尿
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#logsigmoid" class="md-nav__link">
    LogSigmoid
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_16" class="md-nav__link">
    多头注意力
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_17" class="md-nav__link">
    预备
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#relu" class="md-nav__link">
    ReLU
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#relu6" class="md-nav__link">
    ReLU6
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rrelu" class="md-nav__link">
    RReLU
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#selu" class="md-nav__link">
    SELU
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_18" class="md-nav__link">
    中央图书馆
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_19" class="md-nav__link">
    格鲁
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_20" class="md-nav__link">
    乙状结肠
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_21" class="md-nav__link">
    软加
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_22" class="md-nav__link">
    软缩
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_23" class="md-nav__link">
    软签
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#h" class="md-nav__link">
    h
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tanhshrink" class="md-nav__link">
    Tanhshrink
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_24" class="md-nav__link">
    阈
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_25" class="md-nav__link">
    非线性激活(其他）
  </a>
  
    <nav class="md-nav" aria-label="非线性激活(其他）">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_26" class="md-nav__link">
    软敏
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_27" class="md-nav__link">
    软最大
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#softmax2d" class="md-nav__link">
    Softmax2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#logsoftmax" class="md-nav__link">
    LogSoftmax
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adaptivelogsoftmaxwithloss" class="md-nav__link">
    AdaptiveLogSoftmaxWithLoss
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_28" class="md-nav__link">
    归一化层
  </a>
  
    <nav class="md-nav" aria-label="归一化层">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#batchnorm1d" class="md-nav__link">
    BatchNorm1d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#batchnorm2d" class="md-nav__link">
    BatchNorm2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#batchnorm3d" class="md-nav__link">
    BatchNorm3d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_29" class="md-nav__link">
    集团规范
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#syncbatchnorm" class="md-nav__link">
    SyncBatchNorm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instancenorm1d" class="md-nav__link">
    InstanceNorm1d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instancenorm2d" class="md-nav__link">
    InstanceNorm2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instancenorm3d" class="md-nav__link">
    InstanceNorm3d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_30" class="md-nav__link">
    层范数
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#localresponsenorm" class="md-nav__link">
    LocalResponseNorm
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_31" class="md-nav__link">
    循环层
  </a>
  
    <nav class="md-nav" aria-label="循环层">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#rnn" class="md-nav__link">
    RNN 库
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rnn_1" class="md-nav__link">
    RNN
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lstm" class="md-nav__link">
    LSTM
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_32" class="md-nav__link">
    格鲁
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_33" class="md-nav__link">
    核细胞
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lstm_1" class="md-nav__link">
    LSTM 单元
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_34" class="md-nav__link">
    格鲁塞尔
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_35" class="md-nav__link">
    变压器层
  </a>
  
    <nav class="md-nav" aria-label="变压器层">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_36" class="md-nav__link">
    变压器
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_37" class="md-nav__link">
    变压器编码器
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_38" class="md-nav__link">
    变压器解码器
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformerencoderlayer" class="md-nav__link">
    TransformerEncoderLayer
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_39" class="md-nav__link">
    变压器解码器层
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_40" class="md-nav__link">
    线性层
  </a>
  
    <nav class="md-nav" aria-label="线性层">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_41" class="md-nav__link">
    身分识别
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_42" class="md-nav__link">
    线性的
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_43" class="md-nav__link">
    双线性
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_44" class="md-nav__link">
    辍学层
  </a>
  
    <nav class="md-nav" aria-label="辍学层">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_45" class="md-nav__link">
    退出
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dropout2d" class="md-nav__link">
    Dropout2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3d_3" class="md-nav__link">
    辍学 3d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#alphadropout" class="md-nav__link">
    AlphaDropout
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_46" class="md-nav__link">
    稀疏层
  </a>
  
    <nav class="md-nav" aria-label="稀疏层">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_47" class="md-nav__link">
    嵌入
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_48" class="md-nav__link">
    嵌入袋
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_49" class="md-nav__link">
    距离功能
  </a>
  
    <nav class="md-nav" aria-label="距离功能">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_50" class="md-nav__link">
    余弦相似度
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_51" class="md-nav__link">
    成对距离
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_52" class="md-nav__link">
    损失函数
  </a>
  
    <nav class="md-nav" aria-label="损失函数">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#l1" class="md-nav__link">
    L1 损失
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_53" class="md-nav__link">
    选配
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_54" class="md-nav__link">
    交叉熵损失
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ctcloss" class="md-nav__link">
    CTCLoss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_55" class="md-nav__link">
    亏损
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_56" class="md-nav__link">
    泊松
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_57" class="md-nav__link">
    损失
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bceloss" class="md-nav__link">
    BCELoss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bcewithlogitsloss" class="md-nav__link">
    BCEWithLogitsLoss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_58" class="md-nav__link">
    保证金排名损失
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_59" class="md-nav__link">
    嵌入损耗
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_60" class="md-nav__link">
    多标签保证金损失
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#l1_1" class="md-nav__link">
    平滑 L1 损失
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_61" class="md-nav__link">
    软保证金损失
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multilabelsoftmarginloss" class="md-nav__link">
    MultiLabelSoftMarginLoss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_62" class="md-nav__link">
    余弦嵌入损失
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_63" class="md-nav__link">
    多保证金亏损
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_64" class="md-nav__link">
    三重保证金亏损
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_65" class="md-nav__link">
    视觉层
  </a>
  
    <nav class="md-nav" aria-label="视觉层">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_66" class="md-nav__link">
    像素随机播放
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_67" class="md-nav__link">
    上采样
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#upsamplingnearest2d" class="md-nav__link">
    UpsamplingNearest2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2d_4" class="md-nav__link">
    上采样双线性 2d
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#dataparallel-gpu" class="md-nav__link">
    DataParallel 层(多 GPU，分布式）
  </a>
  
    <nav class="md-nav" aria-label="DataParallel 层(多 GPU，分布式）">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_68" class="md-nav__link">
    数据并行
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_69" class="md-nav__link">
    分布式数据并行
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_70" class="md-nav__link">
    实用工具
  </a>
  
    <nav class="md-nav" aria-label="实用工具">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#clip_grad_norm_" class="md-nav__link">
    clip_grad_norm_
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip_grad_value_" class="md-nav__link">
    clip_grad_value_
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#parameters_to_vector" class="md-nav__link">
    parameters_to_vector
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vector_to_parameters" class="md-nav__link">
    vector_to_parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#basepruningmethod" class="md-nav__link">
    BasePruningMethod
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_71" class="md-nav__link">
    修剪容器
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#identity" class="md-nav__link">
    Identity
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_72" class="md-nav__link">
    随机非结构化
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#l1_2" class="md-nav__link">
    L1 非结构化
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_73" class="md-nav__link">
    随机结构
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ln" class="md-nav__link">
    Ln 结构化
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#customfrommask" class="md-nav__link">
    CustomFromMask
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_74" class="md-nav__link">
    身份
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#random_unstructured" class="md-nav__link">
    random_unstructured
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#l1_unstructured" class="md-nav__link">
    l1_unstructured
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_75" class="md-nav__link">
    随机结构
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ln_" class="md-nav__link">
    ln_ 结构化
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#global_unstructured" class="md-nav__link">
    global_unstructured
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#custom_from_mask" class="md-nav__link">
    custom_from_mask
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_76" class="md-nav__link">
    去掉
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#is_pruned" class="md-nav__link">
    is_pruned
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#weight_norm" class="md-nav__link">
    weight_norm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#remove_weight_norm" class="md-nav__link">
    remove_weight_norm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#spectrum_norm" class="md-nav__link">
    Spectrum_norm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#remove_spectral_norm" class="md-nav__link">
    remove_spectral_norm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_77" class="md-nav__link">
    打包序列
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pack_padded_sequence" class="md-nav__link">
    pack_padded_sequence
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pad_packed_sequence" class="md-nav__link">
    pad_packed_sequence
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pad_sequence" class="md-nav__link">
    pad_sequence
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pack_sequence" class="md-nav__link">
    pack_sequence
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_78" class="md-nav__link">
    展平
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_79" class="md-nav__link">
    量化功能
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../76/" class="md-nav__link">
        torch功能
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../77/" class="md-nav__link">
        torch张量
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../78/" class="md-nav__link">
        张量属性
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../79/" class="md-nav__link">
        自动差分包-Torch.Autograd
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../80/" class="md-nav__link">
        torch.cuda
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../81/" class="md-nav__link">
        分布式通讯包-Torch.Distributed
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../82/" class="md-nav__link">
        概率分布-torch分布
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../83/" class="md-nav__link">
        torch.hub
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../84/" class="md-nav__link">
        torch脚本
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../85/" class="md-nav__link">
        torch.nn.init
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../86/" class="md-nav__link">
        torch.onnx
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../87/" class="md-nav__link">
        torch.optim
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../88/" class="md-nav__link">
        量化
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../89/" class="md-nav__link">
        分布式 RPC 框架
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../90/" class="md-nav__link">
        torch随机
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../91/" class="md-nav__link">
        torch稀疏
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../92/" class="md-nav__link">
        torch存储
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../93/" class="md-nav__link">
        torch.utils.bottleneck
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../94/" class="md-nav__link">
        torch.utils.checkpoint
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../95/" class="md-nav__link">
        torch.utils.cpp_extension
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../96/" class="md-nav__link">
        torch.utils.data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../97/" class="md-nav__link">
        torch.utils.dlpack
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../98/" class="md-nav__link">
        torch.utils.model_zoo
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../99/" class="md-nav__link">
        torch.utils.tensorboard
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../100/" class="md-nav__link">
        类型信息
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../101/" class="md-nav__link">
        命名张量
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../102/" class="md-nav__link">
        命名为 Tensors 操作员范围
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../103/" class="md-nav__link">
        糟糕！
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_16" >
      
      
      
        <label class="md-nav__link" for="__nav_4_16" id="__nav_4_16_label" tabindex="0">
          torchvision参考
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_16_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_16">
          <span class="md-nav__icon md-icon"></span>
          torchvision参考
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../105/" class="md-nav__link">
        torchvision
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_17" >
      
      
      
        <label class="md-nav__link" for="__nav_4_17" id="__nav_4_17_label" tabindex="0">
          音频参考
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_17_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_17">
          <span class="md-nav__icon md-icon"></span>
          音频参考
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../107/" class="md-nav__link">
        torchaudio
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_18" >
      
      
      
        <label class="md-nav__link" for="__nav_4_18" id="__nav_4_18_label" tabindex="0">
          torchtext参考
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_18_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_18">
          <span class="md-nav__icon md-icon"></span>
          torchtext参考
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../109/" class="md-nav__link">
        torchtext
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_19" >
      
      
      
        <label class="md-nav__link" for="__nav_4_19" id="__nav_4_19_label" tabindex="0">
          社区
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_19_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_19">
          <span class="md-nav__icon md-icon"></span>
          社区
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../111/" class="md-nav__link">
        PyTorch 贡献指南
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../112/" class="md-nav__link">
        PyTorch 治理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../113/" class="md-nav__link">
        PyTorch 治理| 感兴趣的人
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
      
      
      
        <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
          PyTorch 1.0 中文文档 & 教程
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 1.0 中文文档 & 教程
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/" class="md-nav__link">
        目录
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2" id="__nav_5_2_label" tabindex="0">
          中文教程
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2">
          <span class="md-nav__icon md-icon"></span>
          中文教程
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2_1" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2_1" id="__nav_5_2_1_label" tabindex="0">
          入门
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_2_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2_1">
          <span class="md-nav__icon md-icon"></span>
          入门
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2_1_1" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2_1_1" id="__nav_5_2_1_1_label" tabindex="0">
          PyTorch 深度学习: 60 分钟极速入门
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_5_2_1_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2_1_1">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 深度学习: 60 分钟极速入门
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/deep_learning_60min_blitz/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/blitz_tensor_tutorial/" class="md-nav__link">
        什么是 PyTorch？
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/blitz_autograd_tutorial/" class="md-nav__link">
        Autograd：自动求导
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/blitz_neural_networks_tutorial/" class="md-nav__link">
        神经网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/blitz_cifar10_tutorial/" class="md-nav__link">
        训练分类器
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/blitz_data_parallel_tutorial/" class="md-nav__link">
        可选：数据并行处理
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/data_loading_tutorial/" class="md-nav__link">
        数据加载和处理教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/pytorch_with_examples/" class="md-nav__link">
        用例子学习 PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/transfer_learning_tutorial/" class="md-nav__link">
        迁移学习教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/deploy_seq2seq_hybrid_frontend_tutorial/" class="md-nav__link">
        混合前端的 seq2seq 模型部署
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/saving_loading_models/" class="md-nav__link">
        Saving and Loading Models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/nn_tutorial/" class="md-nav__link">
        What is torch.nn really?
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2_2" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2_2" id="__nav_5_2_2_label" tabindex="0">
          图像
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_2_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2_2">
          <span class="md-nav__icon md-icon"></span>
          图像
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/finetuning_torchvision_models_tutorial/" class="md-nav__link">
        Torchvision 模型微调
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/spatial_transformer_tutorial/" class="md-nav__link">
        空间变换器网络教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/neural_style_tutorial/" class="md-nav__link">
        使用 PyTorch 进行图像风格转换
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/fgsm_tutorial/" class="md-nav__link">
        对抗性示例生成
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/super_resolution_with_caffe2/" class="md-nav__link">
        使用 ONNX 将模型从 PyTorch 传输到 Caffe2 和移动端
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2_3" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2_3" id="__nav_5_2_3_label" tabindex="0">
          文本
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_2_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2_3">
          <span class="md-nav__icon md-icon"></span>
          文本
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/chatbot_tutorial/" class="md-nav__link">
        聊天机器人教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/char_rnn_generation_tutorial/" class="md-nav__link">
        使用字符级别特征的 RNN 网络生成姓氏
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/char_rnn_classification_tutorial/" class="md-nav__link">
        使用字符级别特征的 RNN 网络进行姓氏分类
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2_3_4" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2_3_4" id="__nav_5_2_3_4_label" tabindex="0">
          Deep Learning for NLP with Pytorch
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_5_2_3_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2_3_4">
          <span class="md-nav__icon md-icon"></span>
          Deep Learning for NLP with Pytorch
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/deep_learning_nlp_tutorial/" class="md-nav__link">
        在深度学习和 NLP 中使用 Pytorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/nlp_pytorch_tutorial/" class="md-nav__link">
        PyTorch 介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/nlp_deep_learning_tutorial/" class="md-nav__link">
        使用 PyTorch 进行深度学习
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/nlp_word_embeddings_tutorial/" class="md-nav__link">
        Word Embeddings: Encoding Lexical Semantics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/nlp_sequence_models_tutorial/" class="md-nav__link">
        序列模型和 LSTM 网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/nlp_advanced_tutorial/" class="md-nav__link">
        Advanced: Making Dynamic Decisions and the Bi-LSTM CRF
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/seq2seq_translation_tutorial/" class="md-nav__link">
        基于注意力机制的 seq2seq 神经网络翻译
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2_4" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2_4" id="__nav_5_2_4_label" tabindex="0">
          生成
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_2_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2_4">
          <span class="md-nav__icon md-icon"></span>
          生成
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/dcgan_faces_tutorial/" class="md-nav__link">
        DCGAN Tutorial
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2_5" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2_5" id="__nav_5_2_5_label" tabindex="0">
          强化学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_2_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2_5">
          <span class="md-nav__icon md-icon"></span>
          强化学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/reinforcement_q_learning/" class="md-nav__link">
        Reinforcement Learning (DQN) Tutorial
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2_6" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2_6" id="__nav_5_2_6_label" tabindex="0">
          扩展 PyTorch
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_2_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2_6">
          <span class="md-nav__icon md-icon"></span>
          扩展 PyTorch
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/numpy_extensions_tutorial/" class="md-nav__link">
        用 numpy 和 scipy 创建扩展
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/cpp_extension/" class="md-nav__link">
        Custom C-- and CUDA Extensions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torch_script_custom_ops/" class="md-nav__link">
        Extending TorchScript with Custom C-- Operators
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2_7" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2_7" id="__nav_5_2_7_label" tabindex="0">
          生产性使用
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_2_7_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2_7">
          <span class="md-nav__icon md-icon"></span>
          生产性使用
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/dist_tuto/" class="md-nav__link">
        Writing Distributed Applications with PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/aws_distributed_training_tutorial/" class="md-nav__link">
        使用 Amazon AWS 进行分布式训练
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/ONNXLive/" class="md-nav__link">
        ONNX 现场演示教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/cpp_export/" class="md-nav__link">
        在 C-- 中加载 PYTORCH 模型
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2_8" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2_8" id="__nav_5_2_8_label" tabindex="0">
          其它语言中的 PyTorch
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_2_8_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2_8">
          <span class="md-nav__icon md-icon"></span>
          其它语言中的 PyTorch
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/cpp_frontend/" class="md-nav__link">
        使用 PyTorch C-- 前端
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3" >
      
      
      
        <label class="md-nav__link" for="__nav_5_3" id="__nav_5_3_label" tabindex="0">
          中文文档
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_3">
          <span class="md-nav__icon md-icon"></span>
          中文文档
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3_1" >
      
      
      
        <label class="md-nav__link" for="__nav_5_3_1" id="__nav_5_3_1_label" tabindex="0">
          注解
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_3_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_3_1">
          <span class="md-nav__icon md-icon"></span>
          注解
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/notes_autograd/" class="md-nav__link">
        自动求导机制
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/notes_broadcasting/" class="md-nav__link">
        广播语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/notes_cuda/" class="md-nav__link">
        CUDA 语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/notes_extending/" class="md-nav__link">
        Extending PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/notes_faq/" class="md-nav__link">
        Frequently Asked Questions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/notes_multiprocessing/" class="md-nav__link">
        Multiprocessing best practices
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/notes_randomness/" class="md-nav__link">
        Reproducibility
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/notes_serialization/" class="md-nav__link">
        Serialization semantics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/notes_windows/" class="md-nav__link">
        Windows FAQ
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3_2" >
      
      
      
        <label class="md-nav__link" for="__nav_5_3_2" id="__nav_5_3_2_label" tabindex="0">
          包参考
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_3_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_3_2">
          <span class="md-nav__icon md-icon"></span>
          包参考
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3_2_1" >
      
      
      
        <label class="md-nav__link" for="__nav_5_3_2_1" id="__nav_5_3_2_1_label" tabindex="0">
          torch
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_5_3_2_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_3_2_1">
          <span class="md-nav__icon md-icon"></span>
          torch
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torch/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torch_tensors/" class="md-nav__link">
        Tensors
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torch_random_sampling/" class="md-nav__link">
        Random sampling
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torch_serialization_parallelism_utilities/" class="md-nav__link">
        Serialization, Parallelism, Utilities
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3_2_1_5" >
      
      
      
        <label class="md-nav__link" for="__nav_5_3_2_1_5" id="__nav_5_3_2_1_5_label" tabindex="0">
          Math operations
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="5" aria-labelledby="__nav_5_3_2_1_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_3_2_1_5">
          <span class="md-nav__icon md-icon"></span>
          Math operations
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torch_math_operations_pointwise_ops/" class="md-nav__link">
        Pointwise Ops
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torch_math_operations_reduction_ops/" class="md-nav__link">
        Reduction Ops
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torch_math_operations_comparison_ops/" class="md-nav__link">
        Comparison Ops
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torch_math_operations_spectral_ops/" class="md-nav__link">
        Spectral Ops
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torch_math_operations_other_ops/" class="md-nav__link">
        Other Operations
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torch_math_operations_blas_lapack_ops/" class="md-nav__link">
        BLAS and LAPACK Operations
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/tensors/" class="md-nav__link">
        torch.Tensor
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/tensor_attributes/" class="md-nav__link">
        Tensor Attributes
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/type_info/" class="md-nav__link">
        数据类型信息
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/sparse/" class="md-nav__link">
        torch.sparse
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/cuda/" class="md-nav__link">
        torch.cuda
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/storage/" class="md-nav__link">
        torch.Storage
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/nn/" class="md-nav__link">
        torch.nn
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/nn_functional/" class="md-nav__link">
        torch.nn.functional
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/nn_init/" class="md-nav__link">
        torch.nn.init
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/optim/" class="md-nav__link">
        torch.optim
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/autograd/" class="md-nav__link">
        Automatic differentiation package - torch.autograd
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/distributed/" class="md-nav__link">
        Distributed communication package - torch.distributed
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/distributions/" class="md-nav__link">
        Probability distributions - torch.distributions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/jit/" class="md-nav__link">
        Torch Script
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/multiprocessing/" class="md-nav__link">
        多进程包 - torch.multiprocessing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/bottleneck/" class="md-nav__link">
        torch.utils.bottleneck
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/checkpoint/" class="md-nav__link">
        torch.utils.checkpoint
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/docs_cpp_extension/" class="md-nav__link">
        torch.utils.cpp_extension
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/data/" class="md-nav__link">
        torch.utils.data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/dlpack/" class="md-nav__link">
        torch.utils.dlpack
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/hub/" class="md-nav__link">
        torch.hub
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/model_zoo/" class="md-nav__link">
        torch.utils.model_zoo
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/onnx/" class="md-nav__link">
        torch.onnx
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/distributed_deprecated/" class="md-nav__link">
        Distributed communication package (deprecated) - torch.distributed.deprecated
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3_3" >
      
      
      
        <label class="md-nav__link" for="__nav_5_3_3" id="__nav_5_3_3_label" tabindex="0">
          torchvision 参考
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_3_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_3_3">
          <span class="md-nav__icon md-icon"></span>
          torchvision 参考
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/docs_torchvision_ref/" class="md-nav__link">
        目录
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torchvision_datasets/" class="md-nav__link">
        torchvision.datasets
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torchvision_models/" class="md-nav__link">
        torchvision.models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torchvision_transforms/" class="md-nav__link">
        torchvision.transforms
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torchvision_utils/" class="md-nav__link">
        torchvision.utils
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
      
      
      
        <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
          PyTorch 0.4 中文文档
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 0.4 中文文档
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_1" >
      
      
      
        <label class="md-nav__link" for="__nav_6_1" id="__nav_6_1_label" tabindex="0">
          笔记
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_6_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_6_1">
          <span class="md-nav__icon md-icon"></span>
          笔记
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/1/" class="md-nav__link">
        自动求导机制
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/2/" class="md-nav__link">
        广播语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/3/" class="md-nav__link">
        CUDA 语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/4/" class="md-nav__link">
        扩展 PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/5/" class="md-nav__link">
        常见问题
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/6/" class="md-nav__link">
        多进程最佳实践
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/7/" class="md-nav__link">
        序列化语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/8/" class="md-nav__link">
        Windows 常见问题
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_2" >
      
      
      
        <label class="md-nav__link" for="__nav_6_2" id="__nav_6_2_label" tabindex="0">
          包参考
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_6_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_6_2">
          <span class="md-nav__icon md-icon"></span>
          包参考
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/10/" class="md-nav__link">
        Torch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/11/" class="md-nav__link">
        torch.Tensor
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/12/" class="md-nav__link">
        Tensor Attributes
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/13/" class="md-nav__link">
        torch.sparse
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/14/" class="md-nav__link">
        torch.cuda
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/15/" class="md-nav__link">
        torch.Storage
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/16/" class="md-nav__link">
        torch.nn
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/17/" class="md-nav__link">
        torch.nn.functional
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/18/" class="md-nav__link">
        自动差异化包 - torch.autograd
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/19/" class="md-nav__link">
        torch.optim
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/20/" class="md-nav__link">
        torch.nn.init
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/21/" class="md-nav__link">
        torch.distributions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/22/" class="md-nav__link">
        Multiprocessing 包 - torch.multiprocessing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/23/" class="md-nav__link">
        分布式通讯包 - torch.distributed
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/24/" class="md-nav__link">
        torch.utils.bottleneck
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/25/" class="md-nav__link">
        torch.utils.checkpoint
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/26/" class="md-nav__link">
        torch.utils.cpp_extension
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/27/" class="md-nav__link">
        torch.utils.data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/28/" class="md-nav__link">
        torch.utils.ffi
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/29/" class="md-nav__link">
        torch.utils.model_zoo
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/30/" class="md-nav__link">
        torch.onnx
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/31/" class="md-nav__link">
        遗留包 - torch.legacy
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_3" >
      
      
      
        <label class="md-nav__link" for="__nav_6_3" id="__nav_6_3_label" tabindex="0">
          torchvision 参考
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_6_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_6_3">
          <span class="md-nav__icon md-icon"></span>
          torchvision 参考
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/33/" class="md-nav__link">
        torchvision
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/34/" class="md-nav__link">
        torchvision.datasets
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/35/" class="md-nav__link">
        torchvision.models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/36/" class="md-nav__link">
        torchvision.transform
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/37/" class="md-nav__link">
        torchvision.utils
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
      
      
      
        <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
          PyTorch 0.3 中文文档 & 教程
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 0.3 中文文档 & 教程
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/" class="md-nav__link">
        目录
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_2" >
      
      
      
        <label class="md-nav__link" for="__nav_7_2" id="__nav_7_2_label" tabindex="0">
          中文教程
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_7_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_2">
          <span class="md-nav__icon md-icon"></span>
          中文教程
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_2_1" >
      
      
      
        <label class="md-nav__link" for="__nav_7_2_1" id="__nav_7_2_1_label" tabindex="0">
          初学者教程
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_7_2_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_2_1">
          <span class="md-nav__icon md-icon"></span>
          初学者教程
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_2_1_1" >
      
      
      
        <label class="md-nav__link" for="__nav_7_2_1_1" id="__nav_7_2_1_1_label" tabindex="0">
          PyTorch 深度学习: 60 分钟极速入门教程
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_7_2_1_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_2_1_1">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 深度学习: 60 分钟极速入门教程
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/deep_learning_60min_blitz/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/blitz_tensor_tutorial/" class="md-nav__link">
        PyTorch 是什么？
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/blitz_autograd_tutorial/" class="md-nav__link">
        自动求导: 自动微分
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/blitz_neural_networks_tutorial/" class="md-nav__link">
        神经网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/blitz_cifar10_tutorial/" class="md-nav__link">
        训练一个分类器
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/blitz_data_parallel_tutorial/" class="md-nav__link">
        可选: 数据并行
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_2_1_2" >
      
      
      
        <label class="md-nav__link" for="__nav_7_2_1_2" id="__nav_7_2_1_2_label" tabindex="0">
          PyTorch for former Torch users
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_7_2_1_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_2_1_2">
          <span class="md-nav__icon md-icon"></span>
          PyTorch for former Torch users
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/former_torchies_tutorial/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/former_torchies_tensor_tutorial/" class="md-nav__link">
        Tensors
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/former_torchies_autograd_tutorial/" class="md-nav__link">
        Autograd (自动求导)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/former_torchies_nn_tutorial/" class="md-nav__link">
        nn package
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/former_torchies_parallelism_tutorial/" class="md-nav__link">
        Multi-GPU examples
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_2_1_3" >
      
      
      
        <label class="md-nav__link" for="__nav_7_2_1_3" id="__nav_7_2_1_3_label" tabindex="0">
          跟着例子学习 PyTorch
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_7_2_1_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_2_1_3">
          <span class="md-nav__icon md-icon"></span>
          跟着例子学习 PyTorch
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/pytorch_with_examples/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/pytorch_with_examples_warm-up-numpy/" class="md-nav__link">
        Warm-up: numpy
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/pytorch_with_examples_pytorch-tensors/" class="md-nav__link">
        PyTorch: Tensors
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/pytorch_with_examples_pytorch-variables-and-autograd/" class="md-nav__link">
        PyTorch: 变量和autograd
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/pytorch_with_examples_pytorch-defining-new-autograd-functions/" class="md-nav__link">
        PyTorch: 定义新的autograd函数
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/pytorch_with_examples_tensorflow-static-graphs/" class="md-nav__link">
        TensorFlow: 静态图
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/pytorch_with_examples_pytorch-nn/" class="md-nav__link">
        PyTorch: nn包
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/pytorch_with_examples_pytorch-optim/" class="md-nav__link">
        PyTorch: optim包
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/pytorch_with_examples_pytorch-custom-nn-modules/" class="md-nav__link">
        PyTorch: 定制化nn模块
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/pytorch_with_examples_pytorch-control-flow-weight-sharing/" class="md-nav__link">
        PyTorch: 动态控制流程 - 权重共享
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/transfer_learning_tutorial/" class="md-nav__link">
        迁移学习教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/data_loading_tutorial/" class="md-nav__link">
        数据加载和处理教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_2_1_6" >
      
      
      
        <label class="md-nav__link" for="__nav_7_2_1_6" id="__nav_7_2_1_6_label" tabindex="0">
          针对NLP的Pytorch深度学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_7_2_1_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_2_1_6">
          <span class="md-nav__icon md-icon"></span>
          针对NLP的Pytorch深度学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/deep_learning_nlp_tutorial/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/nlp_pytorch_tutorial/" class="md-nav__link">
        PyTorch介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/nlp_deep_learning_tutorial/" class="md-nav__link">
        PyTorch深度学习
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/nlp_word_embeddings_tutorial/" class="md-nav__link">
        词汇嵌入:编码词汇语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/nlp_sequence_models_tutorial/" class="md-nav__link">
        序列模型和 LSTM 网络(长短记忆网络）
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/nlp_advanced_tutorial/" class="md-nav__link">
        高级教程: 作出动态决策和 Bi-LSTM CRF
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_2_2" >
      
      
      
        <label class="md-nav__link" for="__nav_7_2_2" id="__nav_7_2_2_label" tabindex="0">
          中级教程
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_7_2_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_2_2">
          <span class="md-nav__icon md-icon"></span>
          中级教程
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/char_rnn_classification_tutorial/" class="md-nav__link">
        用字符级RNN分类名称
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/char_rnn_generation_tutorial/" class="md-nav__link">
        基与字符级RNN(Char-RNN）的人名生成
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/seq2seq_translation_tutorial/" class="md-nav__link">
        用基于注意力机制的seq2seq神经网络进行翻译
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/reinforcement_q_learning/" class="md-nav__link">
        强化学习(DQN）教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/dist_tuto/" class="md-nav__link">
        Writing Distributed Applications with PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/spatial_transformer_tutorial/" class="md-nav__link">
        空间转换网络 (Spatial Transformer Networks) 教程
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_2_3" >
      
      
      
        <label class="md-nav__link" for="__nav_7_2_3" id="__nav_7_2_3_label" tabindex="0">
          高级教程
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_7_2_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_2_3">
          <span class="md-nav__icon md-icon"></span>
          高级教程
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/neural_style_tutorial/" class="md-nav__link">
        用 PyTorch 做 神经转换 (Neural Transfer)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/numpy_extensions_tutorial/" class="md-nav__link">
        使用 numpy 和 scipy 创建扩展
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/super_resolution_with_caffe2/" class="md-nav__link">
        使用 ONNX 将模型从 PyTorch 迁移到 Caffe2 和 Mobile
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/c_extension/" class="md-nav__link">
        为 pytorch 自定义 C 扩展
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_3" >
      
      
      
        <label class="md-nav__link" for="__nav_7_3" id="__nav_7_3_label" tabindex="0">
          中文文档
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_7_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_3">
          <span class="md-nav__icon md-icon"></span>
          中文文档
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_3_1" >
      
      
      
        <label class="md-nav__link" for="__nav_7_3_1" id="__nav_7_3_1_label" tabindex="0">
          介绍
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_7_3_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_3_1">
          <span class="md-nav__icon md-icon"></span>
          介绍
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/notes_autograd/" class="md-nav__link">
        自动求导机制
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/notes_broadcasting/" class="md-nav__link">
        广播语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/notes_cuda/" class="md-nav__link">
        CUDA 语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/notes_extending/" class="md-nav__link">
        扩展 PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/notes_multiprocessing/" class="md-nav__link">
        多进程的最佳实践
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/notes_serialization/" class="md-nav__link">
        序列化语义
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_3_2" >
      
      
      
        <label class="md-nav__link" for="__nav_7_3_2" id="__nav_7_3_2_label" tabindex="0">
          Package 参考
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_7_3_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_3_2">
          <span class="md-nav__icon md-icon"></span>
          Package 参考
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/torch/" class="md-nav__link">
        torch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/tensors/" class="md-nav__link">
        torch.Tensor
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/sparse/" class="md-nav__link">
        torch.sparse
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/storage/" class="md-nav__link">
        torch.Storage
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/nn/" class="md-nav__link">
        torch.nn
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/optim/" class="md-nav__link">
        torch.optim
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/autograd/" class="md-nav__link">
        Automatic differentiation package - torch.autograd
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/distributions/" class="md-nav__link">
        Probability distributions - torch.distributions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/multiprocessing/" class="md-nav__link">
        Multiprocessing package - torch.multiprocessing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/distributed/" class="md-nav__link">
        Distributed communication package - torch.distributed
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/legacy/" class="md-nav__link">
        Legacy package - torch.legacy
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/cuda/" class="md-nav__link">
        torch.cuda
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/ffi/" class="md-nav__link">
        torch.utils.ffi
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/data/" class="md-nav__link">
        torch.utils.data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/model_zoo/" class="md-nav__link">
        torch.utils.model_zoo
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/onnx/" class="md-nav__link">
        torch.onnx
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_3_3" >
      
      
      
        <label class="md-nav__link" for="__nav_7_3_3" id="__nav_7_3_3_label" tabindex="0">
          torchvision 参考
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_7_3_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_3_3">
          <span class="md-nav__icon md-icon"></span>
          torchvision 参考
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/torchvision/" class="md-nav__link">
        torchvision
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/datasets/" class="md-nav__link">
        torchvision.datasets
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/models/" class="md-nav__link">
        torchvision.models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/transforms/" class="md-nav__link">
        torchvision.transforms
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/utils/" class="md-nav__link">
        torchvision.utils
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8" >
      
      
      
        <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0">
          PyTorch 0.2 中文文档
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_8">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 0.2 中文文档
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8_2" >
      
      
      
        <label class="md-nav__link" for="__nav_8_2" id="__nav_8_2_label" tabindex="0">
          说明
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_8_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_8_2">
          <span class="md-nav__icon md-icon"></span>
          说明
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/notes/autograd/" class="md-nav__link">
        自动求导机制
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/notes/cuda/" class="md-nav__link">
        CUDA语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/notes/extending/" class="md-nav__link">
        扩展PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/notes/multiprocessing/" class="md-nav__link">
        多进程最佳实践
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/notes/serialization/" class="md-nav__link">
        序列化语义
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8_3" >
      
      
      
        <label class="md-nav__link" for="__nav_8_3" id="__nav_8_3_label" tabindex="0">
          PACKAGE参考
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_8_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_8_3">
          <span class="md-nav__icon md-icon"></span>
          PACKAGE参考
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/package_references/torch/" class="md-nav__link">
        torch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/package_references/Tensor/" class="md-nav__link">
        torch.Tensor
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/package_references/Storage/" class="md-nav__link">
        torch.Storage
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/package_references/torch-nn/" class="md-nav__link">
        torch.nn
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/package_references/functional/" class="md-nav__link">
        torch.nn.functional
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/package_references/torch-autograd/" class="md-nav__link">
        torch.autograd
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/package_references/torch-optim/" class="md-nav__link">
        torch.optim
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/package_references/nn_init/" class="md-nav__link">
        torch.nn.init
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/package_references/torch-multiprocessing/" class="md-nav__link">
        torch.multiprocessing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/package_references/legacy/" class="md-nav__link">
        torch.legacy
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/package_references/torch-cuda/" class="md-nav__link">
        torch.cuda
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/package_references/ffi/" class="md-nav__link">
        torch.utils.ffi
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/package_references/data/" class="md-nav__link">
        torch.utils.data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/package_references/model_zoo/" class="md-nav__link">
        torch.utils.model_zoo
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8_4" >
      
      
      
        <label class="md-nav__link" for="__nav_8_4" id="__nav_8_4_label" tabindex="0">
          TORCHVISION参考
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_8_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_8_4">
          <span class="md-nav__icon md-icon"></span>
          TORCHVISION参考
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/torchvision/torchvision/" class="md-nav__link">
        torchvision
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/torchvision/torchvision-datasets/" class="md-nav__link">
        torchvision.datasets
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/torchvision/torchvision-models/" class="md-nav__link">
        torchvision.models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/torchvision/torchvision-transform/" class="md-nav__link">
        torchvision.transforms
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/torchvision/torchvision-utils/" class="md-nav__link">
        torchvision.utils
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/acknowledgement/" class="md-nav__link">
        致谢
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../contrib/" class="md-nav__link">
        贡献者
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="https://www.apachecn.org/about/" class="md-nav__link">
        关于我们
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="https://docs.apachecn.org" class="md-nav__link">
        中文资源合集
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    参数
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    货柜
  </a>
  
    <nav class="md-nav" aria-label="货柜">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    模组
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    顺序的
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    模块列表
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#moduledict" class="md-nav__link">
    ModuleDict
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    参数表
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#parameterdict" class="md-nav__link">
    ParameterDict
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    卷积层
  </a>
  
    <nav class="md-nav" aria-label="卷积层">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1d" class="md-nav__link">
    转换 1d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2d" class="md-nav__link">
    转换 2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3d" class="md-nav__link">
    转换 3d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convtranspose1d" class="md-nav__link">
    ConvTranspose1d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convtranspose2d" class="md-nav__link">
    ConvTranspose2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convtranspose3d" class="md-nav__link">
    ConvTranspose3d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    展开
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_9" class="md-nav__link">
    折
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_10" class="md-nav__link">
    汇聚层
  </a>
  
    <nav class="md-nav" aria-label="汇聚层">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#maxpool1d" class="md-nav__link">
    MaxPool1d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#maxpool2d" class="md-nav__link">
    MaxPool2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#maxpool3d" class="md-nav__link">
    MaxPool3d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#maxunpool1d" class="md-nav__link">
    MaxUnpool1d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#maxunpool2d" class="md-nav__link">
    MaxUnpool2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#maxunpool3d" class="md-nav__link">
    MaxUnpool3d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1d_1" class="md-nav__link">
    平均池 1d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2d_1" class="md-nav__link">
    平均池 2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3d_1" class="md-nav__link">
    平均池 3d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2d_2" class="md-nav__link">
    分数最大池 2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lppool1d" class="md-nav__link">
    LPPool1d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lppool2d" class="md-nav__link">
    LPPool2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adaptivemaxpool1d" class="md-nav__link">
    AdaptiveMaxPool1d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adaptivemaxpool2d" class="md-nav__link">
    AdaptiveMaxPool2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adaptivemaxpool3d" class="md-nav__link">
    AdaptiveMaxPool3d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adaptiveavgpool1d" class="md-nav__link">
    AdaptiveAvgPool1d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adaptiveavgpool2d" class="md-nav__link">
    AdaptiveAvgPool2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adaptiveavgpool3d" class="md-nav__link">
    AdaptiveAvgPool3d
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_11" class="md-nav__link">
    填充层
  </a>
  
    <nav class="md-nav" aria-label="填充层">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#reflectionpad1d" class="md-nav__link">
    ReflectionPad1d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reflectionpad2d" class="md-nav__link">
    ReflectionPad2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1d_2" class="md-nav__link">
    复制板 1d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2d_3" class="md-nav__link">
    复制板 2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3d_2" class="md-nav__link">
    复制板 3d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#zeropad2d" class="md-nav__link">
    ZeroPad2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#constantpad1d" class="md-nav__link">
    ConstantPad1d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#constantpad2d" class="md-nav__link">
    ConstantPad2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#constantpad3d" class="md-nav__link">
    ConstantPad3d
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_12" class="md-nav__link">
    非线性激活(加权和，非线性）
  </a>
  
    <nav class="md-nav" aria-label="非线性激活(加权和，非线性）">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#elu" class="md-nav__link">
    ELU
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_13" class="md-nav__link">
    硬收缩
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_14" class="md-nav__link">
    哈丹
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_15" class="md-nav__link">
    漏尿
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#logsigmoid" class="md-nav__link">
    LogSigmoid
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_16" class="md-nav__link">
    多头注意力
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_17" class="md-nav__link">
    预备
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#relu" class="md-nav__link">
    ReLU
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#relu6" class="md-nav__link">
    ReLU6
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rrelu" class="md-nav__link">
    RReLU
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#selu" class="md-nav__link">
    SELU
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_18" class="md-nav__link">
    中央图书馆
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_19" class="md-nav__link">
    格鲁
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_20" class="md-nav__link">
    乙状结肠
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_21" class="md-nav__link">
    软加
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_22" class="md-nav__link">
    软缩
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_23" class="md-nav__link">
    软签
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#h" class="md-nav__link">
    h
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tanhshrink" class="md-nav__link">
    Tanhshrink
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_24" class="md-nav__link">
    阈
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_25" class="md-nav__link">
    非线性激活(其他）
  </a>
  
    <nav class="md-nav" aria-label="非线性激活(其他）">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_26" class="md-nav__link">
    软敏
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_27" class="md-nav__link">
    软最大
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#softmax2d" class="md-nav__link">
    Softmax2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#logsoftmax" class="md-nav__link">
    LogSoftmax
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adaptivelogsoftmaxwithloss" class="md-nav__link">
    AdaptiveLogSoftmaxWithLoss
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_28" class="md-nav__link">
    归一化层
  </a>
  
    <nav class="md-nav" aria-label="归一化层">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#batchnorm1d" class="md-nav__link">
    BatchNorm1d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#batchnorm2d" class="md-nav__link">
    BatchNorm2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#batchnorm3d" class="md-nav__link">
    BatchNorm3d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_29" class="md-nav__link">
    集团规范
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#syncbatchnorm" class="md-nav__link">
    SyncBatchNorm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instancenorm1d" class="md-nav__link">
    InstanceNorm1d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instancenorm2d" class="md-nav__link">
    InstanceNorm2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instancenorm3d" class="md-nav__link">
    InstanceNorm3d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_30" class="md-nav__link">
    层范数
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#localresponsenorm" class="md-nav__link">
    LocalResponseNorm
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_31" class="md-nav__link">
    循环层
  </a>
  
    <nav class="md-nav" aria-label="循环层">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#rnn" class="md-nav__link">
    RNN 库
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rnn_1" class="md-nav__link">
    RNN
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lstm" class="md-nav__link">
    LSTM
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_32" class="md-nav__link">
    格鲁
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_33" class="md-nav__link">
    核细胞
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lstm_1" class="md-nav__link">
    LSTM 单元
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_34" class="md-nav__link">
    格鲁塞尔
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_35" class="md-nav__link">
    变压器层
  </a>
  
    <nav class="md-nav" aria-label="变压器层">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_36" class="md-nav__link">
    变压器
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_37" class="md-nav__link">
    变压器编码器
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_38" class="md-nav__link">
    变压器解码器
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformerencoderlayer" class="md-nav__link">
    TransformerEncoderLayer
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_39" class="md-nav__link">
    变压器解码器层
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_40" class="md-nav__link">
    线性层
  </a>
  
    <nav class="md-nav" aria-label="线性层">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_41" class="md-nav__link">
    身分识别
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_42" class="md-nav__link">
    线性的
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_43" class="md-nav__link">
    双线性
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_44" class="md-nav__link">
    辍学层
  </a>
  
    <nav class="md-nav" aria-label="辍学层">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_45" class="md-nav__link">
    退出
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dropout2d" class="md-nav__link">
    Dropout2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3d_3" class="md-nav__link">
    辍学 3d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#alphadropout" class="md-nav__link">
    AlphaDropout
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_46" class="md-nav__link">
    稀疏层
  </a>
  
    <nav class="md-nav" aria-label="稀疏层">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_47" class="md-nav__link">
    嵌入
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_48" class="md-nav__link">
    嵌入袋
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_49" class="md-nav__link">
    距离功能
  </a>
  
    <nav class="md-nav" aria-label="距离功能">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_50" class="md-nav__link">
    余弦相似度
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_51" class="md-nav__link">
    成对距离
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_52" class="md-nav__link">
    损失函数
  </a>
  
    <nav class="md-nav" aria-label="损失函数">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#l1" class="md-nav__link">
    L1 损失
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_53" class="md-nav__link">
    选配
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_54" class="md-nav__link">
    交叉熵损失
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ctcloss" class="md-nav__link">
    CTCLoss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_55" class="md-nav__link">
    亏损
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_56" class="md-nav__link">
    泊松
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_57" class="md-nav__link">
    损失
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bceloss" class="md-nav__link">
    BCELoss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bcewithlogitsloss" class="md-nav__link">
    BCEWithLogitsLoss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_58" class="md-nav__link">
    保证金排名损失
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_59" class="md-nav__link">
    嵌入损耗
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_60" class="md-nav__link">
    多标签保证金损失
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#l1_1" class="md-nav__link">
    平滑 L1 损失
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_61" class="md-nav__link">
    软保证金损失
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multilabelsoftmarginloss" class="md-nav__link">
    MultiLabelSoftMarginLoss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_62" class="md-nav__link">
    余弦嵌入损失
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_63" class="md-nav__link">
    多保证金亏损
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_64" class="md-nav__link">
    三重保证金亏损
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_65" class="md-nav__link">
    视觉层
  </a>
  
    <nav class="md-nav" aria-label="视觉层">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_66" class="md-nav__link">
    像素随机播放
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_67" class="md-nav__link">
    上采样
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#upsamplingnearest2d" class="md-nav__link">
    UpsamplingNearest2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2d_4" class="md-nav__link">
    上采样双线性 2d
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#dataparallel-gpu" class="md-nav__link">
    DataParallel 层(多 GPU，分布式）
  </a>
  
    <nav class="md-nav" aria-label="DataParallel 层(多 GPU，分布式）">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_68" class="md-nav__link">
    数据并行
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_69" class="md-nav__link">
    分布式数据并行
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_70" class="md-nav__link">
    实用工具
  </a>
  
    <nav class="md-nav" aria-label="实用工具">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#clip_grad_norm_" class="md-nav__link">
    clip_grad_norm_
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip_grad_value_" class="md-nav__link">
    clip_grad_value_
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#parameters_to_vector" class="md-nav__link">
    parameters_to_vector
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vector_to_parameters" class="md-nav__link">
    vector_to_parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#basepruningmethod" class="md-nav__link">
    BasePruningMethod
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_71" class="md-nav__link">
    修剪容器
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#identity" class="md-nav__link">
    Identity
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_72" class="md-nav__link">
    随机非结构化
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#l1_2" class="md-nav__link">
    L1 非结构化
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_73" class="md-nav__link">
    随机结构
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ln" class="md-nav__link">
    Ln 结构化
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#customfrommask" class="md-nav__link">
    CustomFromMask
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_74" class="md-nav__link">
    身份
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#random_unstructured" class="md-nav__link">
    random_unstructured
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#l1_unstructured" class="md-nav__link">
    l1_unstructured
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_75" class="md-nav__link">
    随机结构
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ln_" class="md-nav__link">
    ln_ 结构化
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#global_unstructured" class="md-nav__link">
    global_unstructured
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#custom_from_mask" class="md-nav__link">
    custom_from_mask
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_76" class="md-nav__link">
    去掉
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#is_pruned" class="md-nav__link">
    is_pruned
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#weight_norm" class="md-nav__link">
    weight_norm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#remove_weight_norm" class="md-nav__link">
    remove_weight_norm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#spectrum_norm" class="md-nav__link">
    Spectrum_norm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#remove_spectral_norm" class="md-nav__link">
    remove_spectral_norm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_77" class="md-nav__link">
    打包序列
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pack_padded_sequence" class="md-nav__link">
    pack_padded_sequence
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pad_packed_sequence" class="md-nav__link">
    pad_packed_sequence
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pad_sequence" class="md-nav__link">
    pad_sequence
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pack_sequence" class="md-nav__link">
    pack_sequence
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_78" class="md-nav__link">
    展平
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_79" class="md-nav__link">
    量化功能
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
    <a href="https://github.com/apachecn/pytorch-doc-zh/edit/master/docs/1.4/75.md" title="Edit this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4v-2m10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1 2.1 2.1Z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/apachecn/pytorch-doc-zh/raw/master/docs/1.4/75.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.15 8.15 0 0 1-1.23-2Z"/></svg>
    </a>
  


<div class="wwads-cn wwads-horizontal" data-id="206" style="max-width:680px"></div>
<h1 id="torchnn">torch.nn</h1>
<blockquote>
<p>原文： <a href="https://pytorch.org/docs/stable/nn.html">https://pytorch.org/docs/stable/nn.html</a></p>
</blockquote>
<h2 id="_1">参数</h2>
<hr />
<pre><code>class torch.nn.Parameter¶
</code></pre>
<p>一种被视为模块参数的 Tensor。</p>
<p>参数是 <a href="tensors.html#torch.Tensor" title="torch.Tensor"><code>Tensor</code></a> 子类，当与 <a href="#torch.nn.Module" title="torch.nn.Module"><code>Module</code></a> 一起使用时，具有非常特殊的属性-将它们分配为模块属性时，它们会自动添加到其列表中 参数，并会出现，例如 在 <a href="#torch.nn.Module.parameters" title="torch.nn.Module.parameters"><code>parameters()</code></a> 迭代器中。 分配张量不会产生这种效果。 这是因为可能要在模型中缓存一些临时状态，例如 RNN 的最后一个隐藏状态。 如果不存在 <a href="#torch.nn.Parameter" title="torch.nn.Parameter"><code>Parameter</code></a> 这样的类，这些临时人员也将被注册。</p>
<p>参数</p>
<ul>
<li>
<p><strong>数据</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–参数张量。</p>
</li>
<li>
<p><strong>require_grad</strong> (<em>布尔</em> <em>，</em> <em>可选</em>）–如果参数需要渐变。 有关更多详细信息，请参见<a href="notes/autograd.html#excluding-subgraphs">从后向</a>中排除子图。 默认值： &lt;cite&gt;True&lt;/cite&gt;</p>
</li>
</ul>
<h2 id="_2">货柜</h2>
<h3 id="_3">模组</h3>
<hr />
<pre><code>class torch.nn.Module¶
</code></pre>
<p>所有神经网络模块的基类。</p>
<p>您的模型也应该继承此类。</p>
<p>模块也可以包含其他模块，从而可以将它们嵌套在树形结构中。 您可以将子模块分配为常规属性：</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>以这种方式分配的子模块将被注册，并且当您调用 <a href="#torch.nn.Module.to" title="torch.nn.Module.to"><code>to()</code></a> 等时，也会转换其参数。</p>
<hr />
<pre><code>add_module(name, module)¶
</code></pre>
<p>将子模块添加到当前模块。</p>
<p>可以使用给定名称将模块作为属性访问。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>名称</strong>(<em>字符串</em>）–子模块的名称。 可以使用给定名称从该模块访问子模块</p>
</li>
<li>
<p><strong>模块</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><em>模块</em></a>)–要添加到该模块的子模块。</p>
</li>
</ul>
<hr />
<pre><code>apply(fn)¶
</code></pre>
<p>将<code>fn</code>递归应用于每个子模块(由<code>.children()</code>返回）以及自身。 典型的用法包括初始化模型的参数(另请参见 <a href="nn.init.html#nn-init-doc">torch.nn.init</a>)。</p>
<p>Parameters</p>
<p><strong>fn</strong>  (<a href="#torch.nn.Module" title="torch.nn.Module"><code>Module</code></a> -&gt;无）–应用于每个子模块的功能</p>
<p>退货</p>
<p>自</p>
<p>返回类型</p>
<p><a href="#torch.nn.Module" title="torch.nn.Module">模块</a></p>
<p>例：</p>
<pre><code>&gt;&gt;&gt; def init_weights(m):
&gt;&gt;&gt;     print(m)
&gt;&gt;&gt;     if type(m) == nn.Linear:
&gt;&gt;&gt;         m.weight.data.fill_(1.0)
&gt;&gt;&gt;         print(m.weight)
&gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))
&gt;&gt;&gt; net.apply(init_weights)
Linear(in_features=2, out_features=2, bias=True)
Parameter containing:
tensor([[ 1.,  1.],
        [ 1.,  1.]])
Linear(in_features=2, out_features=2, bias=True)
Parameter containing:
tensor([[ 1.,  1.],
        [ 1.,  1.]])
Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
)
Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
)

</code></pre>
<hr />
<pre><code>buffers(recurse=True)¶
</code></pre>
<p>返回模块缓冲区上的迭代器。</p>
<p>Parameters</p>
<p><strong>递归</strong> (<em>bool</em> )–如果为 True，则产生此模块和所有子模块的缓冲区。 否则，仅产生作为该模块直接成员的缓冲区。</p>
<pre><code>Yields
</code></pre>
<p><em>torch张紧器</em> –模块缓冲区</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; for buf in model.buffers():
&gt;&gt;&gt;     print(type(buf.data), buf.size())
&lt;class 'torch.FloatTensor'&gt; (20L,)
&lt;class 'torch.FloatTensor'&gt; (20L, 1L, 5L, 5L)

</code></pre>
<hr />
<pre><code>children()¶
</code></pre>
<p>返回直接子代模块上的迭代器。</p>
<pre><code>Yields
</code></pre>
<p><em>模块</em> –子模块</p>
<hr />
<pre><code>cpu()¶
</code></pre>
<p>将所有模型参数和缓冲区移至 CPU。</p>
<p>Returns</p>
<p>self</p>
<p>Return type</p>
<p><a href="#torch.nn.Module" title="torch.nn.Module">Module</a></p>
<hr />
<pre><code>cuda(device=None)¶
</code></pre>
<p>将所有模型参数和缓冲区移至 GPU。</p>
<p>这也使相关的参数并缓冲不同的对象。 因此，在构建优化程序之前，如果模块在优化过程中可以在 GPU 上运行，则应调用它。</p>
<p>Parameters</p>
<p><strong>设备</strong> (<em>python：int</em> <em>，</em> <em>可选</em>）–如果指定，则所有参数都将复制到该设备</p>
<p>Returns</p>
<p>self</p>
<p>Return type</p>
<p><a href="#torch.nn.Module" title="torch.nn.Module">Module</a></p>
<hr />
<pre><code>double()¶
</code></pre>
<p>将所有浮点参数和缓冲区强制转换为<code>double</code>数据类型。</p>
<p>Returns</p>
<p>self</p>
<p>Return type</p>
<p><a href="#torch.nn.Module" title="torch.nn.Module">Module</a></p>
<pre><code>dump_patches = False¶
</code></pre>
<p>这为 <a href="#torch.nn.Module.load_state_dict" title="torch.nn.Module.load_state_dict"><code>load_state_dict()</code></a> 提供了更好的 BC 支持。 在 <a href="#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code>state_dict()</code></a> 中，版本号将保存为返回状态 dict 的属性 &lt;cite&gt;_metadata&lt;/cite&gt; 中，因此会被腌制。 &lt;cite&gt;_metadata&lt;/cite&gt; 是字典，其键遵循状态 dict 的命名约定。 有关如何在加载中使用此信息的信息，请参见<code>_load_from_state_dict</code>。</p>
<p>如果从模块添加/删除了新的参数/缓冲区，则该数字将增加，并且模块的 &lt;cite&gt;_load_from_state_dict&lt;/cite&gt; 方法可以比较版本号，并且如果状态 dict 来自更改之前，则可以进行适当的更改。</p>
<hr />
<pre><code>eval()¶
</code></pre>
<p>将模块设置为评估模式。</p>
<p>这仅对某些模块有影响。 请参阅特定模块的文档，以了解其在训练/评估模式下的行为的详细信息(如果受到影响），例如 <a href="#torch.nn.Dropout" title="torch.nn.Dropout"><code>Dropout</code></a> ，<code>BatchNorm</code>等</p>
<p>这等效于 <a href="#torch.nn.Module.train" title="torch.nn.Module.train"><code>self.train(False)</code></a> 。</p>
<p>Returns</p>
<p>self</p>
<p>Return type</p>
<p><a href="#torch.nn.Module" title="torch.nn.Module">Module</a></p>
<hr />
<pre><code>extra_repr()¶
</code></pre>
<p>设置模块的额外表示形式</p>
<p>要打印自定义的额外信息，您应该在自己的模块中重新实现此方法。 单行和多行字符串都是可以接受的。</p>
<hr />
<pre><code>float()¶
</code></pre>
<p>将所有浮点参数和缓冲区强制转换为 float 数据类型。</p>
<p>Returns</p>
<p>self</p>
<p>Return type</p>
<p><a href="#torch.nn.Module" title="torch.nn.Module">Module</a></p>
<hr />
<pre><code>forward(*input)¶
</code></pre>
<p>定义每次调用时执行的计算。</p>
<p>应该被所有子类覆盖。</p>
<p>注意</p>
<p>尽管需要在此函数中定义向前传递的配方，但此后应调用 <a href="#torch.nn.Module" title="torch.nn.Module"><code>Module</code></a> 实例，而不是此实例，因为前者负责运行已注册的钩子，而后者则静默地忽略它们。</p>
<hr />
<pre><code>half()¶
</code></pre>
<p>将所有浮点参数和缓冲区强制转换为<code>half</code>数据类型。</p>
<p>Returns</p>
<p>self</p>
<p>Return type</p>
<p><a href="#torch.nn.Module" title="torch.nn.Module">Module</a></p>
<hr />
<pre><code>load_state_dict(state_dict, strict=True)¶
</code></pre>
<p>将参数和缓冲区从 <a href="#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code>state_dict</code></a> 复制到此模块及其子代中。 如果<code>strict</code>为<code>True</code>，则 <a href="#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code>state_dict</code></a> 的键必须与该模块的 <a href="#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code>state_dict()</code></a> 功能返回的键完全匹配。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>state_dict</strong>  (<em>dict</em> )–包含参数和持久缓冲区的 dict。</p>
</li>
<li>
<p><strong>严格</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–是否严格要求 <a href="#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code>state_dict</code></a> 中的键与 此模块的 <a href="#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code>state_dict()</code></a> 功能返回的键。 默认值：<code>True</code></p>
</li>
</ul>
<p>Returns</p>
<ul>
<li>
<p><strong>missing_keys</strong> 是包含缺失键的 str 列表</p>
</li>
<li>
<p><strong>意外的密钥</strong>是包含意外密钥的 str 列表</p>
</li>
</ul>
<p>Return type</p>
<p>具有<code>missing_keys</code>和<code>unexpected_keys</code>字段的<code>NamedTuple</code></p>
<hr />
<pre><code>modules()¶
</code></pre>
<p>返回网络中所有模块的迭代器。</p>
<pre><code>Yields
</code></pre>
<p><em>模块</em> –网络中的模块</p>
<p>Note</p>
<p>重复的模块仅返回一次。 在以下示例中，<code>l</code>将仅返回一次。</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; l = nn.Linear(2, 2)
&gt;&gt;&gt; net = nn.Sequential(l, l)
&gt;&gt;&gt; for idx, m in enumerate(net.modules()):
        print(idx, '-&gt;', m)

0 -&gt; Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
)
1 -&gt; Linear(in_features=2, out_features=2, bias=True)

</code></pre>
<hr />
<pre><code>named_buffers(prefix='', recurse=True)¶
</code></pre>
<p>返回模块缓冲区上的迭代器，同时产生缓冲区的名称和缓冲区本身。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>前缀</strong> (<em>str</em> )–前缀为所有缓冲区名称的前缀。</p>
</li>
<li>
<p><strong>recurse</strong> (<em>bool</em>) – if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.</p>
</li>
</ul>
<pre><code>Yields
</code></pre>
<p><em>(字符串，torch张量）</em> –包含名称和缓冲区的元组</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; for name, buf in self.named_buffers():
&gt;&gt;&gt;    if name in ['running_var']:
&gt;&gt;&gt;        print(buf.size())

</code></pre>
<hr />
<pre><code>named_children()¶
</code></pre>
<p>返回直接子模块的迭代器，同时产生模块名称和模块本身。</p>
<pre><code>Yields
</code></pre>
<p><em>(字符串，模块）</em> –包含名称和子模块的元组</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; for name, module in model.named_children():
&gt;&gt;&gt;     if name in ['conv4', 'conv5']:
&gt;&gt;&gt;         print(module)

</code></pre>
<hr />
<pre><code>named_modules(memo=None, prefix='')¶
</code></pre>
<p>在网络中的所有模块上返回一个迭代器，同时产生模块的名称和模块本身。</p>
<pre><code>Yields
</code></pre>
<p><em>(字符串，模块）</em> –名称和模块的元组</p>
<p>Note</p>
<p>Duplicate modules are returned only once. In the following example, <code>l</code> will be returned only once.</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; l = nn.Linear(2, 2)
&gt;&gt;&gt; net = nn.Sequential(l, l)
&gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):
        print(idx, '-&gt;', m)

0 -&gt; ('', Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
))
1 -&gt; ('0', Linear(in_features=2, out_features=2, bias=True))

</code></pre>
<hr />
<pre><code>named_parameters(prefix='', recurse=True)¶
</code></pre>
<p>返回模块参数上的迭代器，同时产生参数名称和参数本身。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>前缀</strong> (<em>str</em> )–前缀所有参数名称。</p>
</li>
<li>
<p><strong>递归</strong> (<em>bool</em> )–如果为 True，则产生该模块和所有子模块的参数。 否则，仅产生作为该模块直接成员的参数。</p>
</li>
</ul>
<pre><code>Yields
</code></pre>
<p><em>(字符串，参数）</em> –包含名称和参数的元组</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; for name, param in self.named_parameters():
&gt;&gt;&gt;    if name in ['bias']:
&gt;&gt;&gt;        print(param.size())

</code></pre>
<hr />
<pre><code>parameters(recurse=True)¶
</code></pre>
<p>返回模块参数上的迭代器。</p>
<p>通常将其传递给优化器。</p>
<p>Parameters</p>
<p><strong>recurse</strong> (<em>bool</em>) – if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.</p>
<pre><code>Yields
</code></pre>
<p><em>参数</em> –模块参数</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; for param in model.parameters():
&gt;&gt;&gt;     print(type(param.data), param.size())
&lt;class 'torch.FloatTensor'&gt; (20L,)
&lt;class 'torch.FloatTensor'&gt; (20L, 1L, 5L, 5L)

</code></pre>
<hr />
<pre><code>register_backward_hook(hook)¶
</code></pre>
<p>在模块上注册向后挂钩。</p>
<p>每当计算相对于模块输入的梯度时，都会调用该挂钩。 挂钩应具有以下签名：</p>
<pre><code>hook(module, grad_input, grad_output) -&gt; Tensor or None

</code></pre>
<p>如果模块具有多个输入或输出，则<code>grad_input</code>和<code>grad_output</code>可能是元组。 挂钩不应该修改其参数，但可以选择相对于输入返回新的梯度，该梯度将在后续计算中代替<code>grad_input</code>使用。</p>
<p>Returns</p>
<p>可以通过调用<code>handle.remove()</code>来删除添加的钩子的句柄</p>
<p>Return type</p>
<p><code>torch.utils.hooks.RemovableHandle</code></p>
<p>警告</p>
<p>对于执行许多操作的复杂 <a href="#torch.nn.Module" title="torch.nn.Module"><code>Module</code></a> ，当前实现不具有所呈现的行为。 在某些故障情况下，<code>grad_input</code>和<code>grad_output</code>将仅包含输入和输出的子集的梯度​​。 对于此类 <a href="#torch.nn.Module" title="torch.nn.Module"><code>Module</code></a> ，应在特定的输入或输出上直接使用 <a href="autograd.html#torch.Tensor.register_hook" title="torch.Tensor.register_hook"><code>torch.Tensor.register_hook()</code></a> 以获取所需的梯度。</p>
<hr />
<pre><code>register_buffer(name, tensor)¶
</code></pre>
<p>将持久性缓冲区添加到模块。</p>
<p>这通常用于注册不应被视为模型参数的缓冲区。 例如，BatchNorm 的<code>running_mean</code>不是参数，而是持久状态的一部分。</p>
<p>可以使用给定名称将缓冲区作为属性进行访问。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>名称</strong>(<em>字符串</em>）–缓冲区的名称。 可以使用给定名称从此模块访问缓冲区</p>
</li>
<li>
<p><strong>张量</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要注册的缓冲区。</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; self.register_buffer('running_mean', torch.zeros(num_features))

</code></pre>
<hr />
<pre><code>register_forward_hook(hook)¶
</code></pre>
<p>在模块上注册一个前向挂钩。</p>
<p>每当 <a href="#torch.nn.Module.forward" title="torch.nn.Module.forward"><code>forward()</code></a> 计算输出后，该挂钩都会被调用。 它应具有以下签名：</p>
<pre><code>hook(module, input, output) -&gt; None or modified output

</code></pre>
<p>挂钩可以修改输出。 它可以就地修改输入，但是不会对正向产生影响，因为在调用 <a href="#torch.nn.Module.forward" title="torch.nn.Module.forward"><code>forward()</code></a> 之后会调用它。</p>
<p>Returns</p>
<p>a handle that can be used to remove the added hook by calling <code>handle.remove()</code></p>
<p>Return type</p>
<p><code>torch.utils.hooks.RemovableHandle</code></p>
<hr />
<pre><code>register_forward_pre_hook(hook)¶
</code></pre>
<p>在模块上注册前向预钩。</p>
<p>每次调用 <a href="#torch.nn.Module.forward" title="torch.nn.Module.forward"><code>forward()</code></a> 之前，都会调用该挂钩。 它应具有以下签名：</p>
<pre><code>hook(module, input) -&gt; None or modified input

</code></pre>
<p>挂钩可以修改输入。 用户可以在挂钩中返回一个元组或一个修改后的值。 如果返回单个值，则将值包装到一个元组中(除非该值已经是一个元组）。</p>
<p>Returns</p>
<p>a handle that can be used to remove the added hook by calling <code>handle.remove()</code></p>
<p>Return type</p>
<p><code>torch.utils.hooks.RemovableHandle</code></p>
<hr />
<pre><code>register_parameter(name, param)¶
</code></pre>
<p>向模块添加参数。</p>
<p>可以使用给定名称将参数作为属性访问。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>名称</strong>(<em>字符串</em>）–参数的名称。 可以使用给定名称从此模块访问参数</p>
</li>
<li>
<p><strong>参数</strong> (<a href="#torch.nn.Parameter" title="torch.nn.Parameter"><em>参数</em></a>)–要添加到模块的参数。</p>
</li>
</ul>
<hr />
<pre><code>requires_grad_(requires_grad=True)¶
</code></pre>
<p>更改 autograd 是否应记录此模块中参数的操作。</p>
<p>此方法就地设置参数的<code>requires_grad</code>属性。</p>
<p>此方法有助于冻结模块的一部分以分别微调或训练模型的各个部分(例如 GAN 训练）。</p>
<p>Parameters</p>
<p><strong>require_grad</strong>  (<em>bool</em> )– autograd 是否应记录此模块中参数的操作。 默认值：<code>True</code>。</p>
<p>Returns</p>
<p>self</p>
<p>Return type</p>
<p><a href="#torch.nn.Module" title="torch.nn.Module">Module</a></p>
<hr />
<pre><code>state_dict(destination=None, prefix='', keep_vars=False)¶
</code></pre>
<p>返回包含模块整个状态的字典。</p>
<p>包括参数和持久缓冲区(例如运行平均值）。 键是相应的参数和缓冲区名称。</p>
<p>Returns</p>
<p>包含模块整体状态的字典</p>
<p>Return type</p>
<p>字典</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; module.state_dict().keys()
['bias', 'weight']

</code></pre>
<hr />
<pre><code>to(*args, **kwargs)¶
</code></pre>
<p>移动和/或强制转换参数和缓冲区。</p>
<p>这可以称为</p>
<hr />
<pre><code>to(device=None, dtype=None, non_blocking=False)
</code></pre>
<hr />
<pre><code>to(dtype, non_blocking=False)
</code></pre>
<hr />
<pre><code>to(tensor, non_blocking=False)
</code></pre>
<p>它的签名类似于 <a href="tensors.html#torch.Tensor.to" title="torch.Tensor.to"><code>torch.Tensor.to()</code></a> ，但仅接受所需的浮点<code>dtype</code>。 此外，此方法只会将浮点参数和缓冲区强制转换为<code>dtype</code>(如果给定）。 如果已给定，则积分参数和缓冲区将被移动<code>device</code>，但 dtypes 不变。 设置<code>non_blocking</code>时，如果可能，它将尝试相对于主机进行异步转换/移动，例如，将具有固定内存的 CPU 张量移动到 CUDA 设备。</p>
<p>请参见下面的示例。</p>
<p>Note</p>
<p>此方法就地修改模块。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>设备</strong>(<code>torch.device</code>）–该模块中参数和缓冲区的所需设备</p>
</li>
<li>
<p><strong>dtype</strong> (<code>torch.dtype</code>）–此模块中浮点参数和缓冲区的所需浮点类型</p>
</li>
<li>
<p><strong>张量</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>torch张量</em></a>)–张量，其 dtype 和 device 是此模块中所有参数和缓冲区的所需 dtype 和 device</p>
</li>
</ul>
<p>Returns</p>
<p>self</p>
<p>Return type</p>
<p><a href="#torch.nn.Module" title="torch.nn.Module">Module</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; linear = nn.Linear(2, 2)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1913, -0.3420],
        [-0.5113, -0.2325]])
&gt;&gt;&gt; linear.to(torch.double)
Linear(in_features=2, out_features=2, bias=True)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1913, -0.3420],
        [-0.5113, -0.2325]], dtype=torch.float64)
&gt;&gt;&gt; gpu1 = torch.device(&quot;cuda:1&quot;)
&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)
Linear(in_features=2, out_features=2, bias=True)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1914, -0.3420],
        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')
&gt;&gt;&gt; cpu = torch.device(&quot;cpu&quot;)
&gt;&gt;&gt; linear.to(cpu)
Linear(in_features=2, out_features=2, bias=True)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1914, -0.3420],
        [-0.5112, -0.2324]], dtype=torch.float16)

</code></pre>
<hr />
<pre><code>train(mode=True)¶
</code></pre>
<p>将模块设置为训练模式。</p>
<p>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. <a href="#torch.nn.Dropout" title="torch.nn.Dropout"><code>Dropout</code></a>, <code>BatchNorm</code>, etc.</p>
<p>Parameters</p>
<p><strong>模式</strong> (<em>bool</em> )–是设置训练模式(<code>True</code>）还是评估模式(<code>False</code>）。 默认值：<code>True</code>。</p>
<p>Returns</p>
<p>self</p>
<p>Return type</p>
<p><a href="#torch.nn.Module" title="torch.nn.Module">Module</a></p>
<hr />
<pre><code>type(dst_type)¶
</code></pre>
<p>将所有参数和缓冲区强制转换为<code>dst_type</code>。</p>
<p>Parameters</p>
<p><strong>dst_type</strong>  (<em>python：type</em> <em>或</em> <em>字符串</em>）–所需类型</p>
<p>Returns</p>
<p>self</p>
<p>Return type</p>
<p><a href="#torch.nn.Module" title="torch.nn.Module">Module</a></p>
<hr />
<pre><code>zero_grad()¶
</code></pre>
<p>将所有模型参数的梯度设置为零。</p>
<h3 id="_4">顺序的</h3>
<hr />
<pre><code>class torch.nn.Sequential(*args)¶
</code></pre>
<p>顺序容器。 模块将按照在构造函数中传递的顺序添加到模块中。 或者，也可以传递模块的有序字典。</p>
<p>为了更容易理解，这是一个小示例：</p>
<pre><code># Example of using Sequential
model = nn.Sequential(
          nn.Conv2d(1,20,5),
          nn.ReLU(),
          nn.Conv2d(20,64,5),
          nn.ReLU()
        )

# Example of using Sequential with OrderedDict
model = nn.Sequential(OrderedDict([
          ('conv1', nn.Conv2d(1,20,5)),
          ('relu1', nn.ReLU()),
          ('conv2', nn.Conv2d(20,64,5)),
          ('relu2', nn.ReLU())
        ]))

</code></pre>
<h3 id="_5">模块列表</h3>
<hr />
<pre><code>class torch.nn.ModuleList(modules=None)¶
</code></pre>
<p>将子模块保存在列表中。</p>
<p><a href="#torch.nn.ModuleList" title="torch.nn.ModuleList"><code>ModuleList</code></a> 可以像常规 Python 列表一样被索引，但是其中包含的模块已正确注册，并且对所有 <a href="#torch.nn.Module" title="torch.nn.Module"><code>Module</code></a> 方法都是可见的。</p>
<p>Parameters</p>
<p><strong>模块</strong>(<em>可迭代</em> <em>，</em> <em>可选</em>）–可迭代的模块</p>
<p>Example:</p>
<pre><code>class MyModule(nn.Module):
    def __init__(self):
        super(MyModule, self).__init__()
        self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])

    def forward(self, x):
        # ModuleList can act as an iterable, or be indexed using ints
        for i, l in enumerate(self.linears):
            x = self.linears[i // 2](x) + l(x)
        return x

</code></pre>
<hr />
<pre><code>append(module)¶
</code></pre>
<p>将给定模块附加到列表的末尾。</p>
<p>Parameters</p>
<p><strong>模块</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>)–要附加的模块</p>
<hr />
<pre><code>extend(modules)¶
</code></pre>
<p>将可迭代的 Python 模块附加到列表的末尾。</p>
<p>Parameters</p>
<p><strong>模块</strong>(<em>可迭代</em>）–可迭代的模块</p>
<hr />
<pre><code>insert(index, module)¶
</code></pre>
<p>在列表中给定索引之前插入给定模块。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>索引</strong> (<em>python：int</em> )–要插入的索引。</p>
</li>
<li>
<p><strong>模块</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>)–要插入的模块</p>
</li>
</ul>
<h3 id="moduledict">ModuleDict</h3>
<hr />
<pre><code>class torch.nn.ModuleDict(modules=None)¶
</code></pre>
<p>将子模块保存在字典中。</p>
<p><a href="#torch.nn.ModuleDict" title="torch.nn.ModuleDict"><code>ModuleDict</code></a> 可以像常规的 Python 字典一样被索引，但是其中包含的模块已正确注册，并且对所有 <a href="#torch.nn.Module" title="torch.nn.Module"><code>Module</code></a> 方法都是可见的。</p>
<p><a href="#torch.nn.ModuleDict" title="torch.nn.ModuleDict"><code>ModuleDict</code></a> 是<strong>有序</strong>字典，</p>
<ul>
<li>
<p>插入顺序，以及</p>
</li>
<li>
<p>在 <a href="#torch.nn.ModuleDict.update" title="torch.nn.ModuleDict.update"><code>update()</code></a> 中，<code>OrderedDict</code>或另一个 <a href="#torch.nn.ModuleDict" title="torch.nn.ModuleDict"><code>ModuleDict</code></a> 的合并顺序 (<a href="#torch.nn.ModuleDict.update" title="torch.nn.ModuleDict.update"><code>update()</code></a> 的顺序）。</p>
</li>
</ul>
<p>请注意， <a href="#torch.nn.ModuleDict.update" title="torch.nn.ModuleDict.update"><code>update()</code></a> 和其他无序映射类型(例如 Python 的普通<code>dict</code>）不会保留合并映射的顺序。</p>
<p>Parameters</p>
<p><strong>模块</strong>(<em>可迭代</em> <em>，</em> <em>可选</em>）–(字符串：模块）的映射(字典）或键值对的可迭代 类型(字符串，模块）</p>
<p>Example:</p>
<pre><code>class MyModule(nn.Module):
    def __init__(self):
        super(MyModule, self).__init__()
        self.choices = nn.ModuleDict({
                'conv': nn.Conv2d(10, 10, 3),
                'pool': nn.MaxPool2d(3)
        })
        self.activations = nn.ModuleDict([
                ['lrelu', nn.LeakyReLU()],
                ['prelu', nn.PReLU()]
        ])

    def forward(self, x, choice, act):
        x = self.choices[choice](x)
        x = self.activations[act](x)
        return x

</code></pre>
<hr />
<pre><code>clear()¶
</code></pre>
<p>从 ModuleDict 中删除所有项目。</p>
<hr />
<pre><code>items()¶
</code></pre>
<p>返回一个可迭代的 ModuleDict 键/值对。</p>
<hr />
<pre><code>keys()¶
</code></pre>
<p>返回一个可迭代的 ModuleDict 键。</p>
<hr />
<pre><code>pop(key)¶
</code></pre>
<p>从 ModuleDict 中删除密钥并返回其模块。</p>
<p>Parameters</p>
<p><strong>键</strong>(<em>字符串</em>）–从 ModuleDict 弹出的键</p>
<hr />
<pre><code>update(modules)¶
</code></pre>
<p>使用来自映射或可迭代，覆盖现有键的键值对更新 <a href="#torch.nn.ModuleDict" title="torch.nn.ModuleDict"><code>ModuleDict</code></a> 。</p>
<p>Note</p>
<p>如果<code>modules</code>是<code>OrderedDict</code>， <a href="#torch.nn.ModuleDict" title="torch.nn.ModuleDict"><code>ModuleDict</code></a> 或键值对的可迭代项，则将保留其中的新元素顺序。</p>
<p>Parameters</p>
<p><strong>模块</strong>(<em>可迭代</em>）–从字符串到 <a href="#torch.nn.Module" title="torch.nn.Module"><code>Module</code></a> 的映射(字典），或键值对类型的可迭代(字符串， [] <code>Module</code>](#torch.nn.Module "torch.nn.Module"))</p>
<hr />
<pre><code>values()¶
</code></pre>
<p>返回一个 ModuleDict 值的可迭代值。</p>
<h3 id="_6">参数表</h3>
<hr />
<pre><code>class torch.nn.ParameterList(parameters=None)¶
</code></pre>
<p>将参数保存在列表中。</p>
<p><a href="#torch.nn.ParameterList" title="torch.nn.ParameterList"><code>ParameterList</code></a> 可以像常规 Python 列表一样被索引，但是其中包含的参数已正确注册，并且将由所有 <a href="#torch.nn.Module" title="torch.nn.Module"><code>Module</code></a> 方法可见。</p>
<p>Parameters</p>
<p><strong>参数</strong>(<em>可迭代的</em> <em>，</em> <em>可选</em>）–可迭代的 <a href="#torch.nn.Parameter" title="torch.nn.Parameter"><code>Parameter</code></a></p>
<p>Example:</p>
<pre><code>class MyModule(nn.Module):
    def __init__(self):
        super(MyModule, self).__init__()
        self.params = nn.ParameterList([nn.Parameter(torch.randn(10, 10)) for i in range(10)])

    def forward(self, x):
        # ParameterList can act as an iterable, or be indexed using ints
        for i, p in enumerate(self.params):
            x = self.params[i // 2].mm(x) + p.mm(x)
        return x

</code></pre>
<hr />
<pre><code>append(parameter)¶
</code></pre>
<p>在列表的末尾附加一个给定的参数。</p>
<p>Parameters</p>
<p><strong>参数</strong> (<a href="#torch.nn.Parameter" title="torch.nn.Parameter"><em>nn.Parameter</em></a>)–要附加的参数</p>
<hr />
<pre><code>extend(parameters)¶
</code></pre>
<p>将可迭代的 Python 参数附加到列表的末尾。</p>
<p>Parameters</p>
<p><strong>参数</strong>(<em>可迭代</em>）–可迭代的参数</p>
<h3 id="parameterdict">ParameterDict</h3>
<hr />
<pre><code>class torch.nn.ParameterDict(parameters=None)¶
</code></pre>
<p>将参数保存在字典中。</p>
<p>可以像常规 Python 词典一样对 ParameterDict 进行索引，但是它包含的参数已正确注册，并且对所有 Module 方法都可见。</p>
<p><a href="#torch.nn.ParameterDict" title="torch.nn.ParameterDict"><code>ParameterDict</code></a> 是<strong>有序</strong>字典，</p>
<ul>
<li>
<p>the order of insertion, and</p>
</li>
<li>
<p>在 <a href="#torch.nn.ParameterDict.update" title="torch.nn.ParameterDict.update"><code>update()</code></a> 中，<code>OrderedDict</code>或另一个 <a href="#torch.nn.ParameterDict" title="torch.nn.ParameterDict"><code>ParameterDict</code></a> 的合并顺序 (<a href="#torch.nn.ParameterDict.update" title="torch.nn.ParameterDict.update"><code>update()</code></a> 的顺序）。</p>
</li>
</ul>
<p>请注意， <a href="#torch.nn.ParameterDict.update" title="torch.nn.ParameterDict.update"><code>update()</code></a> 和其他无序映射类型(例如 Python 的普通<code>dict</code>）不会保留合并映射的顺序。</p>
<p>Parameters</p>
<p><strong>参数</strong>(<em>可迭代的</em> <em>，</em> <em>可选</em>）–(字符串： <a href="#torch.nn.Parameter" title="torch.nn.Parameter"><code>Parameter</code></a>)的映射(字典） 或类型(字符串 <a href="#torch.nn.Parameter" title="torch.nn.Parameter"><code>Parameter</code></a>)的键值对的可迭代</p>
<p>Example:</p>
<pre><code>class MyModule(nn.Module):
    def __init__(self):
        super(MyModule, self).__init__()
        self.params = nn.ParameterDict({
                'left': nn.Parameter(torch.randn(5, 10)),
                'right': nn.Parameter(torch.randn(5, 10))
        })

    def forward(self, x, choice):
        x = self.params[choice].mm(x)
        return x

</code></pre>
<hr />
<pre><code>clear()¶
</code></pre>
<p>从 ParameterDict 中删除所有项目。</p>
<hr />
<pre><code>items()¶
</code></pre>
<p>返回一个 ParameterDict 键/值对的可迭代对象。</p>
<hr />
<pre><code>keys()¶
</code></pre>
<p>返回一个可迭代的 ParameterDict 键。</p>
<hr />
<pre><code>pop(key)¶
</code></pre>
<p>从 ParameterDict 中删除键并返回其参数。</p>
<p>Parameters</p>
<p><strong>键</strong>(<em>字符串</em>）–从 ParameterDict 弹出的键</p>
<hr />
<pre><code>update(parameters)¶
</code></pre>
<p>使用来自映射或可迭代，覆盖现有键的键值对更新 <a href="#torch.nn.ParameterDict" title="torch.nn.ParameterDict"><code>ParameterDict</code></a> 。</p>
<p>Note</p>
<p>如果<code>parameters</code>是<code>OrderedDict</code>， <a href="#torch.nn.ParameterDict" title="torch.nn.ParameterDict"><code>ParameterDict</code></a> 或键值对的可迭代项，则将保留其中的新元素顺序。</p>
<p>Parameters</p>
<p><strong>参数</strong>(<em>可迭代的</em>）–从字符串到 <a href="#torch.nn.Parameter" title="torch.nn.Parameter"><code>Parameter</code></a> 的映射(字典），或键值对类型的可迭代(字符串， [] <code>Parameter</code>](#torch.nn.Parameter "torch.nn.Parameter"))</p>
<hr />
<pre><code>values()¶
</code></pre>
<p>返回 ParameterDict 值的可迭代值。</p>
<h2 id="_7">卷积层</h2>
<h3 id="1d">转换 1d</h3>
<hr />
<pre><code>class torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')¶
</code></pre>
<p>在由多个输入平面组成的输入信号上应用一维卷积。</p>
<p>在最简单的情况下，具有输入大小<img alt="" src="../img/389888c578290cbc3e72be83c4f68529.jpg" />和输出<img alt="" src="../img/d0fd37cc84a2718f11fca2f5fe5f1f21.jpg" />的图层的输出值可以精确地描述为：</p>
<p><img alt="" src="../img/3788482e43b1506ce76e35e7462bd087.jpg" /></p>
<p>其中<img alt="" src="../img/63cc18482b16797d73da2c5f29670973.jpg" />是有效的<a href="https://en.wikipedia.org/wiki/Cross-correlation">互相关</a>运算符，<img alt="" src="../img/7ea0d6aa290d37c8bc57957206bafe80.jpg" />是批处理大小，<img alt="" src="../img/efc1fa1ed22c59909c50ea4639e0e861.jpg" />表示通道数，<img alt="" src="../img/0abd5691618d0a16f37deae1aacd79cf.jpg" />是信号序列的长度。</p>
<ul>
<li>
<p><code>stride</code>控制互相关的步幅，单个数字或一个元素的元组。</p>
</li>
<li>
<p>对于<code>padding</code>点数，<code>padding</code>控制两侧的隐式零填充量。</p>
</li>
<li>
<p><code>dilation</code>控制内核点之间的间距； 也称为àtrous 算法。 很难描述，但是此<a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">链接</a>很好地展示了<code>dilation</code>的功能。</p>
</li>
<li>
<p><code>groups</code>控制输入和输出之间的连接。 <code>in_channels</code>和<code>out_channels</code>必须都可以被<code>groups</code>整除。 例如，</p>
<p>&gt; *   在组= 1 时，所有输入都卷积为所有输出。
&gt;   <br />
&gt;   <br />
&gt; *   在 groups = 2 时，该操作等效于并排设置两个 conv 层，每个 conv 层看到一半的输入通道，并产生一半的输出通道，并且随后都将它们级联。
&gt;   <br />
&gt;   <br />
&gt; *   在 groups = <code>in_channels</code>时，每个输入通道都与自己的大小为<img alt="" src="../img/94c69313dd3c68c719fccb21f85643de.jpg" />的一组滤波器卷积。</p>
</li>
</ul>
<p>Note</p>
<p>根据内核的大小，输入的(最后）几列可能会丢失，因为它是有效的<a href="https://en.wikipedia.org/wiki/Cross-correlation">互相关</a>，而不是完整的<a href="https://en.wikipedia.org/wiki/Cross-correlation">互相关</a> 。 由用户决定是否添加适当的填充。</p>
<p>Note</p>
<p>当&lt;cite&gt;组== in_channels&lt;/cite&gt; 和 &lt;cite&gt;out_channels == K * in_channels&lt;/cite&gt; 时，其中 &lt;cite&gt;K&lt;/cite&gt; 是一个正整数，此操作在文献中也被称为深度卷积。</p>
<p>换句话说，对于大小为<img alt="" src="../img/fb40f9b5ce1b3633fccac2cb2a121b4b.jpg" />的输入，可以通过参数<img alt="" src="../img/eda26261480f3d87c4683f7ff1f85264.jpg" />构造具有深度乘数 &lt;cite&gt;K&lt;/cite&gt; 的深度卷积。</p>
<p>Note</p>
<p>在某些情况下，将 CUDA 后端与 CuDNN 一起使用时，该运算符可能会选择不确定的算法来提高性能。 如果不希望这样做，则可以通过设置<code>torch.backends.cudnn.deterministic = True</code>来使操作具有确定性(可能会降低性能）。 请参阅关于<a href="notes/randomness.html">可再现性</a>的注意事项作为背景。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>in_channels</strong>  (<em>python：int</em> )–输入图像中的通道数</p>
</li>
<li>
<p><strong>out_channels</strong>  (<em>python：int</em> )–卷积产生的通道数</p>
</li>
<li>
<p><strong>kernel_size</strong>  (<em>python：int</em> <em>或</em> <em>元组</em>）–卷积内核的大小</p>
</li>
<li>
<p><strong>步幅</strong> (<em>python：int</em> <em>或</em> <em>元组</em> <em>，</em> <em>可选</em>）–步幅 卷积。 默认值：1</p>
</li>
<li>
<p><strong>填充</strong> (<em>python：int</em> <em>或</em> <em>元组</em> <em>，</em> <em>可选</em>）–零填充 添加到输入的两侧。 默认值：0</p>
</li>
<li>
<p><strong>padding_mode</strong> (<em>字符串</em> <em>，</em> <em>可选</em>）– &lt;cite&gt;零&lt;/cite&gt;</p>
</li>
<li>
<p><strong>扩展</strong> (<em>python：int</em> <em>或</em> <em>元组</em> <em>，</em> <em>可选</em>）–内核之间的间隔 元素。 默认值：1</p>
</li>
<li>
<p><strong>组</strong> (<em>python：int</em> <em>，</em> <em>可选</em>）–从输入通道到输出通道的阻塞连接数。 默认值：1</p>
</li>
<li>
<p><strong>偏置</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–如果<code>True</code>，则向输出添加可学习的偏置。 默认值：<code>True</code></p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>输入：<img alt="" src="../img/fb40f9b5ce1b3633fccac2cb2a121b4b.jpg" /></p>
</li>
<li>
<p>输出：<img alt="" src="../img/ef45efa67341c1d4c314021b6c367754.jpg" />其中</p>
<p><img alt="" src="../img/15b8fe9e6414cba859044bec3b81b75c.jpg" /></p>
</li>
</ul>
<pre><code>Variables
</code></pre>
<ul>
<li>
<p><strong>〜Conv1d.weight</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–形状为<img alt="" src="../img/4516b71ba91f060b0c265a4c4727e52c.jpg" />的模块的可学习重量。 这些权重的值来自<img alt="" src="../img/b0cc88ecd067ab9281ee698299aa4b54.jpg" />，其中<img alt="" src="../img/f5cd795ba873a76b7f6726bee81124fb.jpg" /></p>
</li>
<li>
<p><strong>〜Conv1d.bias</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–形状模块的可学习偏差(out_channels）。 如果<code>bias</code>为<code>True</code>，则这些权重的值将从<img alt="" src="../img/b0cc88ecd067ab9281ee698299aa4b54.jpg" />采样，其中<img alt="" src="../img/f5cd795ba873a76b7f6726bee81124fb.jpg" /></p>
</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; m = nn.Conv1d(16, 33, 3, stride=2)
&gt;&gt;&gt; input = torch.randn(20, 16, 50)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="2d">转换 2d</h3>
<hr />
<pre><code>class torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')¶
</code></pre>
<p>对由多个输入平面组成的输入信号应用 2D 卷积。</p>
<p>在最简单的情况下，具有输入大小<img alt="" src="../img/ebc1a6232b58539fa5ea972a081d02e7.jpg" />和输出<img alt="" src="../img/fd668284243ffd327a45292741759228.jpg" />的图层的输出值可以精确地描述为：</p>
<p><img alt="" src="../img/1e0dec80310b7dc3d8be31d3a71b4f06.jpg" /></p>
<p>其中<img alt="" src="../img/63cc18482b16797d73da2c5f29670973.jpg" />是有效的 2D <a href="https://en.wikipedia.org/wiki/Cross-correlation">互相关</a>运算符，<img alt="" src="../img/7ea0d6aa290d37c8bc57957206bafe80.jpg" />是批处理大小，<img alt="" src="../img/efc1fa1ed22c59909c50ea4639e0e861.jpg" />表示通道数，<img alt="" src="../img/46b26232cd5303a361b8d40cf8829d0c.jpg" />是输入平面的高度(以像素为单位），并且[ <img alt="" src="../img/c19cb9c60d3941e4a875ce492c5c913c.jpg" />是以像素为单位的宽度。</p>
<ul>
<li>
<p><code>stride</code>控制互相关的步幅，单个数字或元组。</p>
</li>
<li>
<p>对于每个维度的<code>padding</code>点数，<code>padding</code>控制两侧的隐式零填充量。</p>
</li>
<li>
<p><code>dilation</code> controls the spacing between the kernel points; also known as the à trous algorithm. It is harder to describe, but this <a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code>dilation</code> does.</p>
</li>
<li>
<p><code>groups</code> controls the connections between inputs and outputs. <code>in_channels</code> and <code>out_channels</code> must both be divisible by <code>groups</code>. For example,</p>
<p>&gt; *   At groups=1, all inputs are convolved to all outputs.
&gt;   <br />
&gt;   <br />
&gt; *   At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels, and producing half the output channels, and both subsequently concatenated.
&gt;   <br />
&gt;   <br />
&gt; *   在 groups = <code>in_channels</code>时，每个输入通道都与自己的一组过滤器卷积，大小为<img alt="" src="../img/94c69313dd3c68c719fccb21f85643de.jpg" />。</p>
</li>
</ul>
<p>参数<code>kernel_size</code>，<code>stride</code>，<code>padding</code>和<code>dilation</code>可以是：</p>
<blockquote>
<ul>
<li>
<p>单个<code>int</code> –在这种情况下，高度和宽度尺寸将使用相同的值</p>
</li>
<li>
<p>两个整数的<code>tuple</code> –在这种情况下，第一个 &lt;cite&gt;int&lt;/cite&gt; 用于高度尺寸，第二个 &lt;cite&gt;int&lt;/cite&gt; 用于宽度尺寸</p>
</li>
</ul>
</blockquote>
<p>Note</p>
<p>根据内核的大小，输入的(最后）几列可能会丢失，因为它是有效的<a href="https://en.wikipedia.org/wiki/Cross-correlation">互相关</a>，而不是完整的<a href="https://en.wikipedia.org/wiki/Cross-correlation">互相关</a> 。 由用户决定是否添加适当的填充。</p>
<p>Note</p>
<p>When &lt;cite&gt;groups == in_channels&lt;/cite&gt; and &lt;cite&gt;out_channels == K * in_channels&lt;/cite&gt;, where &lt;cite&gt;K&lt;/cite&gt; is a positive integer, this operation is also termed in literature as depthwise convolution.</p>
<p>换句话说，对于大小为<img alt="" src="../img/cd6a6bb778ff0a799fbc705e51246950.jpg" />的输入，可以通过参数<img alt="" src="../img/0c81fe1921429c04830a517d614a4956.jpg" />构造具有深度乘数 &lt;cite&gt;K&lt;/cite&gt; 的深度卷积。</p>
<p>Note</p>
<p>In some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting <code>torch.backends.cudnn.deterministic = True</code>. Please see the notes on <a href="notes/randomness.html">Reproducibility</a> for background.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>in_channels</strong> (<em>python:int</em>) – Number of channels in the input image</p>
</li>
<li>
<p><strong>out_channels</strong> (<em>python:int</em>) – Number of channels produced by the convolution</p>
</li>
<li>
<p><strong>kernel_size</strong> (<em>python:int</em> <em>or</em> <em>tuple</em>) – Size of the convolving kernel</p>
</li>
<li>
<p><strong>stride</strong> (<em>python:int</em> <em>or</em> <em>tuple__,</em> <em>optional</em>) – Stride of the convolution. Default: 1</p>
</li>
<li>
<p><strong>填充</strong> (<em>python：int</em> <em>或</em> <em>元组</em> <em>，</em> <em>可选</em>）–零填充 添加到输入的两侧。 默认值：0</p>
</li>
<li>
<p><strong>padding_mode</strong> (<em>string__,</em> <em>optional</em>) – &lt;cite&gt;zeros&lt;/cite&gt;</p>
</li>
<li>
<p><strong>扩展</strong> (<em>python：int</em> <em>或</em> <em>元组</em> <em>，</em> <em>可选</em>）–内核之间的间隔 元素。 默认值：1</p>
</li>
<li>
<p><strong>组</strong> (<em>python：int</em> <em>，</em> <em>可选</em>）–从输入通道到输出通道的阻塞连接数。 默认值：1</p>
</li>
<li>
<p><strong>bias</strong> (<em>bool__,</em> <em>optional</em>) – If <code>True</code>, adds a learnable bias to the output. Default: <code>True</code></p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>输入：<img alt="" src="../img/cd6a6bb778ff0a799fbc705e51246950.jpg" /></p>
</li>
<li>
<p>输出：<img alt="" src="../img/ae35297993da53bc04b13f0135cb72f3.jpg" />其中</p>
<p><img alt="" src="../img/ad9caf2b2c584048eef1ebec367973dd.jpg" /></p>
<p><img alt="" src="../img/b7e72ab3fe122f4b4b2b93b8d856898a.jpg" /></p>
</li>
</ul>
<pre><code>Variables
</code></pre>
<ul>
<li>
<p><strong>〜Conv2d.weight</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–形状为<img alt="" src="../img/0db82968e798a3a1f8734ea48e18cbe2.jpg" /> <img alt="" src="../img/3abbb9da024bf33461bd1469399422f3.jpg" />的模块的可学习重量。 这些权重的值取自<img alt="" src="../img/b0cc88ecd067ab9281ee698299aa4b54.jpg" />，其中<img alt="" src="../img/642525a3baeba4eac7289fe9389f6607.jpg" /></p>
</li>
<li>
<p><strong>〜Conv2d.bias</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–形状模块的可学习偏差(out_channels）。 如果<code>bias</code>为<code>True</code>，则这些权重的值将从<img alt="" src="../img/b0cc88ecd067ab9281ee698299aa4b54.jpg" />采样，其中<img alt="" src="../img/642525a3baeba4eac7289fe9389f6607.jpg" /></p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; # With square kernels and equal stride
&gt;&gt;&gt; m = nn.Conv2d(16, 33, 3, stride=2)
&gt;&gt;&gt; # non-square kernels and unequal stride and with padding
&gt;&gt;&gt; m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))
&gt;&gt;&gt; # non-square kernels and unequal stride and with padding and dilation
&gt;&gt;&gt; m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))
&gt;&gt;&gt; input = torch.randn(20, 16, 50, 100)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="3d">转换 3d</h3>
<hr />
<pre><code>class torch.nn.Conv3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')¶
</code></pre>
<p>在由多个输入平面组成的输入信号上应用 3D 卷积。</p>
<p>在最简单的情况下，具有输入大小<img alt="" src="../img/e47b8bcebf74ecbe39c8d20f77453c72.jpg" />和输出<img alt="" src="../img/334df99d96c3b01e0f1cfc33f255b17c.jpg" />的图层的输出值可以精确地描述为：</p>
<p><img alt="" src="../img/5721b4bcf9cf5ef46c358b85783e6a5d.jpg" /></p>
<p>其中<img alt="" src="../img/63cc18482b16797d73da2c5f29670973.jpg" />是有效的 3D <a href="https://en.wikipedia.org/wiki/Cross-correlation">互相关</a>运算符</p>
<ul>
<li>
<p><code>stride</code>控制互相关的步幅。</p>
</li>
<li>
<p><code>padding</code> controls the amount of implicit zero-paddings on both sides for <code>padding</code> number of points for each dimension.</p>
</li>
<li>
<p><code>dilation</code>控制内核点之间的间距； 也称为àtrous 算法。 很难描述，但是此<a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">链接</a>很好地展示了<code>dilation</code>的功能。</p>
</li>
<li>
<p><code>groups</code> controls the connections between inputs and outputs. <code>in_channels</code> and <code>out_channels</code> must both be divisible by <code>groups</code>. For example,</p>
<p>&gt; *   At groups=1, all inputs are convolved to all outputs.
&gt;   <br />
&gt;   <br />
&gt; *   At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels, and producing half the output channels, and both subsequently concatenated.
&gt;   <br />
&gt;   <br />
&gt; *   在 groups = <code>in_channels</code>时，每个输入通道都与自己的大小为<img alt="" src="../img/94c69313dd3c68c719fccb21f85643de.jpg" />的一组滤波器卷积。</p>
</li>
</ul>
<p>The parameters <code>kernel_size</code>, <code>stride</code>, <code>padding</code>, <code>dilation</code> can either be:</p>
<blockquote>
<ul>
<li>
<p>单个<code>int</code> –在这种情况下，深度，高度和宽度尺寸使用相同的值</p>
</li>
<li>
<p>三个整数的<code>tuple</code> –在这种情况下，第一个 &lt;cite&gt;int&lt;/cite&gt; 用于深度尺寸，第二个 &lt;cite&gt;int&lt;/cite&gt; 用于高度尺寸，第三个 &lt;cite&gt;int&lt;/cite&gt; 为宽度尺寸</p>
</li>
</ul>
</blockquote>
<p>Note</p>
<p>Depending of the size of your kernel, several (of the last) columns of the input might be lost, because it is a valid <a href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>, and not a full <a href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>. It is up to the user to add proper padding.</p>
<p>Note</p>
<p>When &lt;cite&gt;groups == in_channels&lt;/cite&gt; and &lt;cite&gt;out_channels == K * in_channels&lt;/cite&gt;, where &lt;cite&gt;K&lt;/cite&gt; is a positive integer, this operation is also termed in literature as depthwise convolution.</p>
<p>换句话说，对于大小为<img alt="" src="../img/ea9924fefcff6c2a6746a1886427a412.jpg" />的输入，可以通过参数<img alt="" src="../img/0c81fe1921429c04830a517d614a4956.jpg" />构造具有深度乘数 &lt;cite&gt;K&lt;/cite&gt; 的深度卷积。</p>
<p>Note</p>
<p>In some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting <code>torch.backends.cudnn.deterministic = True</code>. Please see the notes on <a href="notes/randomness.html">Reproducibility</a> for background.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>in_channels</strong> (<em>python:int</em>) – Number of channels in the input image</p>
</li>
<li>
<p><strong>out_channels</strong> (<em>python:int</em>) – Number of channels produced by the convolution</p>
</li>
<li>
<p><strong>kernel_size</strong> (<em>python:int</em> <em>or</em> <em>tuple</em>) – Size of the convolving kernel</p>
</li>
<li>
<p><strong>stride</strong> (<em>python:int</em> <em>or</em> <em>tuple__,</em> <em>optional</em>) – Stride of the convolution. Default: 1</p>
</li>
<li>
<p><strong>填充</strong> (<em>python：int</em> <em>或</em> <em>元组</em> <em>，</em> <em>可选</em>）–零填充 添加到输入的所有三个方面。 默认值：0</p>
</li>
<li>
<p><strong>padding_mode</strong> (<em>string__,</em> <em>optional</em>) – &lt;cite&gt;zeros&lt;/cite&gt;</p>
</li>
<li>
<p><strong>dilation</strong> (<em>python:int</em> <em>or</em> <em>tuple__,</em> <em>optional</em>) – Spacing between kernel elements. Default: 1</p>
</li>
<li>
<p><strong>groups</strong> (<em>python:int__,</em> <em>optional</em>) – Number of blocked connections from input channels to output channels. Default: 1</p>
</li>
<li>
<p><strong>bias</strong> (<em>bool__,</em> <em>optional</em>) – If <code>True</code>, adds a learnable bias to the output. Default: <code>True</code></p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>输入：<img alt="" src="../img/ea9924fefcff6c2a6746a1886427a412.jpg" /></p>
</li>
<li>
<p>输出：<img alt="" src="../img/334df99d96c3b01e0f1cfc33f255b17c.jpg" />其中</p>
<p><img alt="" src="../img/c272f9e32547bdc4f775186188e9f190.jpg" /></p>
<p><img alt="" src="../img/877160f781cd24e128b9c65eb319d0a1.jpg" /></p>
<p><img alt="" src="../img/ee64f0e0f467a32ccac09323cbf52fc8.jpg" /></p>
</li>
</ul>
<pre><code>Variables
</code></pre>
<ul>
<li>
<p><strong>〜Conv3d.weight</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–形状为<img alt="" src="../img/0db82968e798a3a1f8734ea48e18cbe2.jpg" /> <img alt="" src="../img/becdafe4af052684f83273e724322e6e.jpg" />的模块的可学习重量。 这些权重的值取自<img alt="" src="../img/b0cc88ecd067ab9281ee698299aa4b54.jpg" />，其中<img alt="" src="../img/322da54cd498fff4f970d91c19f1eead.jpg" /></p>
</li>
<li>
<p><strong>〜Conv3d.bias</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–形状模块的可学习偏差(out_channels）。 如果<code>bias</code>为<code>True</code>，则这些权重的值将从<img alt="" src="../img/b0cc88ecd067ab9281ee698299aa4b54.jpg" />采样，其中<img alt="" src="../img/322da54cd498fff4f970d91c19f1eead.jpg" /></p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; # With square kernels and equal stride
&gt;&gt;&gt; m = nn.Conv3d(16, 33, 3, stride=2)
&gt;&gt;&gt; # non-square kernels and unequal stride and with padding
&gt;&gt;&gt; m = nn.Conv3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(4, 2, 0))
&gt;&gt;&gt; input = torch.randn(20, 16, 10, 50, 100)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="convtranspose1d">ConvTranspose1d</h3>
<hr />
<pre><code>class torch.nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros')¶
</code></pre>
<p>在由多个输入平面组成的输入图像上应用一维转置的卷积运算符。</p>
<p>该模块可以看作是 Conv1d 相对于其输入的梯度。 它也被称为分数步法卷积或反卷积(尽管它不是实际的反卷积运算）。</p>
<ul>
<li>
<p><code>stride</code> controls the stride for the cross-correlation.</p>
</li>
<li>
<p>对于<code>dilation * (kernel_size - 1) - padding</code>点数，<code>padding</code>控制两侧的隐式零填充量。 有关详细信息，请参见下面的注释。</p>
</li>
<li>
<p><code>output_padding</code>控制添加到输出形状一侧的附加尺寸。 有关详细信息，请参见下面的注释。</p>
</li>
<li>
<p><code>dilation</code> controls the spacing between the kernel points; also known as the à trous algorithm. It is harder to describe, but this <a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code>dilation</code> does.</p>
</li>
<li>
<p><code>groups</code> controls the connections between inputs and outputs. <code>in_channels</code> and <code>out_channels</code> must both be divisible by <code>groups</code>. For example,</p>
<p>&gt; *   At groups=1, all inputs are convolved to all outputs.
&gt;   <br />
&gt;   <br />
&gt; *   At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels, and producing half the output channels, and both subsequently concatenated.
&gt;   <br />
&gt;   <br />
&gt; *   在 groups = <code>in_channels</code>时，每个输入通道都与自己的一组过滤器(大小为<img alt="" src="../img/94c69313dd3c68c719fccb21f85643de.jpg" />）卷积。</p>
</li>
</ul>
<p>Note</p>
<p>Depending of the size of your kernel, several (of the last) columns of the input might be lost, because it is a valid <a href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>, and not a full <a href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>. It is up to the user to add proper padding.</p>
<p>Note</p>
<p><code>padding</code>参数有效地将<code>dilation * (kernel_size - 1) - padding</code>的零填充量添加到两种输入大小。 进行设置时，以相同的参数初始化 <a href="#torch.nn.Conv1d" title="torch.nn.Conv1d"><code>Conv1d</code></a> 和 <a href="#torch.nn.ConvTranspose1d" title="torch.nn.ConvTranspose1d"><code>ConvTranspose1d</code></a> 时，它们在输入和输出形状方面彼此相反。 但是，当<code>stride &amp;gt; 1</code>， <a href="#torch.nn.Conv1d" title="torch.nn.Conv1d"><code>Conv1d</code></a> 将多个输入形状映射到相同的输出形状时。 提供<code>output_padding</code>可通过有效地增加一侧的计算输出形状来解决这种歧义。 请注意，<code>output_padding</code>仅用于查找输出形状，而实际上并未向输出添加零填充。</p>
<p>Note</p>
<p>In some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting <code>torch.backends.cudnn.deterministic = True</code>. Please see the notes on <a href="notes/randomness.html">Reproducibility</a> for background.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>in_channels</strong> (<em>python:int</em>) – Number of channels in the input image</p>
</li>
<li>
<p><strong>out_channels</strong> (<em>python:int</em>) – Number of channels produced by the convolution</p>
</li>
<li>
<p><strong>kernel_size</strong> (<em>python:int</em> <em>or</em> <em>tuple</em>) – Size of the convolving kernel</p>
</li>
<li>
<p><strong>stride</strong> (<em>python:int</em> <em>or</em> <em>tuple__,</em> <em>optional</em>) – Stride of the convolution. Default: 1</p>
</li>
<li>
<p><strong>填充</strong> (<em>python：int</em> <em>或</em> <em>元组</em> <em>，</em> <em>可选</em>）– <code>dilation * (kernel_size - 1) - padding</code> 零填充将添加到输入的两侧。 默认值：0</p>
</li>
<li>
<p><strong>output_padding</strong>  (<em>python：int</em> <em>或</em> <em>元组</em> <em>，</em> <em>可选</em>）–已添加其他大小 到输出形状的一侧。 默认值：0</p>
</li>
<li>
<p><strong>groups</strong> (<em>python:int__,</em> <em>optional</em>) – Number of blocked connections from input channels to output channels. Default: 1</p>
</li>
<li>
<p><strong>bias</strong> (<em>bool__,</em> <em>optional</em>) – If <code>True</code>, adds a learnable bias to the output. Default: <code>True</code></p>
</li>
<li>
<p><strong>dilation</strong> (<em>python:int</em> <em>or</em> <em>tuple__,</em> <em>optional</em>) – Spacing between kernel elements. Default: 1</p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/fb40f9b5ce1b3633fccac2cb2a121b4b.jpg" /></p>
</li>
<li>
<p>Output: <img alt="" src="../img/ef45efa67341c1d4c314021b6c367754.jpg" /> where</p>
<p><img alt="" src="../img/bf2b3272da42edc265e084f4a31b17f6.jpg" /></p>
</li>
</ul>
<pre><code>Variables
</code></pre>
<ul>
<li>
<p><strong>〜ConvTranspose1d.weight</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–形状为<img alt="" src="../img/e751884a182984d938dfe52e94e3e41a.jpg" /> <img alt="" src="../img/fca63821e6a1d537dfb9cd5a1aa4bc5a.jpg" />的模块的可学习重量。 这些权重的值取自<img alt="" src="../img/b0cc88ecd067ab9281ee698299aa4b54.jpg" />，其中<img alt="" src="../img/f5cd795ba873a76b7f6726bee81124fb.jpg" /></p>
</li>
<li>
<p><strong>〜ConvTranspose1d.bias</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–形状模块的可学习偏差(out_channels）。 如果<code>bias</code>为<code>True</code>，则这些权重的值将从<img alt="" src="../img/b0cc88ecd067ab9281ee698299aa4b54.jpg" />采样，其中<img alt="" src="../img/f5cd795ba873a76b7f6726bee81124fb.jpg" /></p>
</li>
</ul>
<h3 id="convtranspose2d">ConvTranspose2d</h3>
<hr />
<pre><code>class torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros')¶
</code></pre>
<p>在由多个输入平面组成的输入图像上应用二维转置卷积运算符。</p>
<p>该模块可以看作是 Conv2d 相对于其输入的梯度。 它也被称为分数步法卷积或反卷积(尽管它不是实际的反卷积运算）。</p>
<ul>
<li>
<p><code>stride</code> controls the stride for the cross-correlation.</p>
</li>
<li>
<p><code>padding</code> controls the amount of implicit zero-paddings on both sides for <code>dilation * (kernel_size - 1) - padding</code> number of points. See note below for details.</p>
</li>
<li>
<p><code>output_padding</code> controls the additional size added to one side of the output shape. See note below for details.</p>
</li>
<li>
<p><code>dilation</code> controls the spacing between the kernel points; also known as the à trous algorithm. It is harder to describe, but this <a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code>dilation</code> does.</p>
</li>
<li>
<p><code>groups</code> controls the connections between inputs and outputs. <code>in_channels</code> and <code>out_channels</code> must both be divisible by <code>groups</code>. For example,</p>
<p>&gt; *   At groups=1, all inputs are convolved to all outputs.
&gt;   <br />
&gt;   <br />
&gt; *   At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels, and producing half the output channels, and both subsequently concatenated.
&gt;   <br />
&gt;   <br />
&gt; *   At groups= <code>in_channels</code>, each input channel is convolved with its own set of filters (of size <img alt="" src="../img/94c69313dd3c68c719fccb21f85643de.jpg" />).</p>
</li>
</ul>
<p>参数<code>kernel_size</code>，<code>stride</code>，<code>padding</code>和<code>output_padding</code>可以是：</p>
<blockquote>
<ul>
<li>
<p>单个<code>int</code> –在这种情况下，高度和宽度尺寸将使用相同的值</p>
</li>
<li>
<p>a <code>tuple</code> of two ints – in which case, the first &lt;cite&gt;int&lt;/cite&gt; is used for the height dimension, and the second &lt;cite&gt;int&lt;/cite&gt; for the width dimension</p>
</li>
</ul>
</blockquote>
<p>Note</p>
<p>Depending of the size of your kernel, several (of the last) columns of the input might be lost, because it is a valid <a href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>, and not a full <a href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>. It is up to the user to add proper padding.</p>
<p>Note</p>
<p><code>padding</code>参数有效地将<code>dilation * (kernel_size - 1) - padding</code>的零填充量添加到两种输入大小。 进行设置时，以相同的参数初始化 <a href="#torch.nn.Conv2d" title="torch.nn.Conv2d"><code>Conv2d</code></a> 和 <a href="#torch.nn.ConvTranspose2d" title="torch.nn.ConvTranspose2d"><code>ConvTranspose2d</code></a> 时，它们在输入和输出形状方面彼此相反。 但是，当<code>stride &amp;gt; 1</code>， <a href="#torch.nn.Conv2d" title="torch.nn.Conv2d"><code>Conv2d</code></a> 将多个输入形状映射到相同的输出形状时。 提供<code>output_padding</code>可通过有效地增加一侧的计算输出形状来解决这种歧义。 请注意，<code>output_padding</code>仅用于查找输出形状，而实际上并未向输出添加零填充。</p>
<p>Note</p>
<p>In some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting <code>torch.backends.cudnn.deterministic = True</code>. Please see the notes on <a href="notes/randomness.html">Reproducibility</a> for background.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>in_channels</strong> (<em>python:int</em>) – Number of channels in the input image</p>
</li>
<li>
<p><strong>out_channels</strong> (<em>python:int</em>) – Number of channels produced by the convolution</p>
</li>
<li>
<p><strong>kernel_size</strong> (<em>python:int</em> <em>or</em> <em>tuple</em>) – Size of the convolving kernel</p>
</li>
<li>
<p><strong>stride</strong> (<em>python:int</em> <em>or</em> <em>tuple__,</em> <em>optional</em>) – Stride of the convolution. Default: 1</p>
</li>
<li>
<p><strong>填充</strong> (<em>python：int</em> <em>或</em> <em>元组</em> <em>，</em> <em>可选</em>）– <code>dilation * (kernel_size - 1) - padding</code> 零填充将添加到输入中每个维度的两侧。 默认值：0</p>
</li>
<li>
<p><strong>output_padding</strong>  (<em>python：int</em> <em>或</em> <em>元组</em> <em>，</em> <em>可选</em>）–已添加其他大小 输出形状中每个尺寸的一侧。 默认值：0</p>
</li>
<li>
<p><strong>groups</strong> (<em>python:int__,</em> <em>optional</em>) – Number of blocked connections from input channels to output channels. Default: 1</p>
</li>
<li>
<p><strong>bias</strong> (<em>bool__,</em> <em>optional</em>) – If <code>True</code>, adds a learnable bias to the output. Default: <code>True</code></p>
</li>
<li>
<p><strong>dilation</strong> (<em>python:int</em> <em>or</em> <em>tuple__,</em> <em>optional</em>) – Spacing between kernel elements. Default: 1</p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/cd6a6bb778ff0a799fbc705e51246950.jpg" /></p>
</li>
<li>
<p>Output: <img alt="" src="../img/ae35297993da53bc04b13f0135cb72f3.jpg" /> where</p>
</li>
</ul>
<p><img alt="" src="../img/897f76358473ed304adbb3cc222ad245.jpg" /></p>
<p><img alt="" src="../img/ef7a0c708fd99d67f651091040fb24fc.jpg" /></p>
<pre><code>Variables
</code></pre>
<ul>
<li>
<p><strong>〜ConvTranspose2d.weight</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–形状为<img alt="" src="../img/e751884a182984d938dfe52e94e3e41a.jpg" /> <img alt="" src="../img/3abbb9da024bf33461bd1469399422f3.jpg" />的模块的可学习重量。 这些权重的值取自<img alt="" src="../img/b0cc88ecd067ab9281ee698299aa4b54.jpg" />，其中<img alt="" src="../img/642525a3baeba4eac7289fe9389f6607.jpg" /></p>
</li>
<li>
<p><strong>〜ConvTranspose2d.bias</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–形状模块的可学习偏差(out_channels）如果<code>bias</code>为<code>True</code>，则值 这些权重来自<img alt="" src="../img/b0cc88ecd067ab9281ee698299aa4b54.jpg" />，其中<img alt="" src="../img/642525a3baeba4eac7289fe9389f6607.jpg" /></p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; # With square kernels and equal stride
&gt;&gt;&gt; m = nn.ConvTranspose2d(16, 33, 3, stride=2)
&gt;&gt;&gt; # non-square kernels and unequal stride and with padding
&gt;&gt;&gt; m = nn.ConvTranspose2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))
&gt;&gt;&gt; input = torch.randn(20, 16, 50, 100)
&gt;&gt;&gt; output = m(input)
&gt;&gt;&gt; # exact output size can be also specified as an argument
&gt;&gt;&gt; input = torch.randn(1, 16, 12, 12)
&gt;&gt;&gt; downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1)
&gt;&gt;&gt; upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1)
&gt;&gt;&gt; h = downsample(input)
&gt;&gt;&gt; h.size()
torch.Size([1, 16, 6, 6])
&gt;&gt;&gt; output = upsample(h, output_size=input.size())
&gt;&gt;&gt; output.size()
torch.Size([1, 16, 12, 12])

</code></pre>
<h3 id="convtranspose3d">ConvTranspose3d</h3>
<hr />
<pre><code>class torch.nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros')¶
</code></pre>
<p>在由多个输入平面组成的输入图像上应用 3D 转置卷积运算符。 转置的卷积运算符将每个输入值逐个元素地乘以一个可学习的内核，并对所有输入特征平面的输出求和。</p>
<p>该模块可以看作是 Conv3d 相对于其输入的梯度。 它也被称为分数步法卷积或反卷积(尽管它不是实际的反卷积运算）。</p>
<ul>
<li>
<p><code>stride</code> controls the stride for the cross-correlation.</p>
</li>
<li>
<p><code>padding</code> controls the amount of implicit zero-paddings on both sides for <code>dilation * (kernel_size - 1) - padding</code> number of points. See note below for details.</p>
</li>
<li>
<p><code>output_padding</code> controls the additional size added to one side of the output shape. See note below for details.</p>
</li>
<li>
<p><code>dilation</code> controls the spacing between the kernel points; also known as the à trous algorithm. It is harder to describe, but this <a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code>dilation</code> does.</p>
</li>
<li>
<p><code>groups</code> controls the connections between inputs and outputs. <code>in_channels</code> and <code>out_channels</code> must both be divisible by <code>groups</code>. For example,</p>
<p>&gt; *   At groups=1, all inputs are convolved to all outputs.
&gt;   <br />
&gt;   <br />
&gt; *   At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels, and producing half the output channels, and both subsequently concatenated.
&gt;   <br />
&gt;   <br />
&gt; *   At groups= <code>in_channels</code>, each input channel is convolved with its own set of filters (of size <img alt="" src="../img/94c69313dd3c68c719fccb21f85643de.jpg" />).</p>
</li>
</ul>
<p>The parameters <code>kernel_size</code>, <code>stride</code>, <code>padding</code>, <code>output_padding</code> can either be:</p>
<blockquote>
<ul>
<li>
<p>单个<code>int</code> –在这种情况下，深度，高度和宽度尺寸使用相同的值</p>
</li>
<li>
<p>a <code>tuple</code> of three ints – in which case, the first &lt;cite&gt;int&lt;/cite&gt; is used for the depth dimension, the second &lt;cite&gt;int&lt;/cite&gt; for the height dimension and the third &lt;cite&gt;int&lt;/cite&gt; for the width dimension</p>
</li>
</ul>
</blockquote>
<p>Note</p>
<p>Depending of the size of your kernel, several (of the last) columns of the input might be lost, because it is a valid <a href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>, and not a full <a href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>. It is up to the user to add proper padding.</p>
<p>Note</p>
<p><code>padding</code>参数有效地将<code>dilation * (kernel_size - 1) - padding</code>的零填充量添加到两种输入大小。 进行设置时，以相同的参数初始化 <a href="#torch.nn.Conv3d" title="torch.nn.Conv3d"><code>Conv3d</code></a> 和 <a href="#torch.nn.ConvTranspose3d" title="torch.nn.ConvTranspose3d"><code>ConvTranspose3d</code></a> 时，它们在输入和输出形状方面彼此相反。 但是，当<code>stride &amp;gt; 1</code>， <a href="#torch.nn.Conv3d" title="torch.nn.Conv3d"><code>Conv3d</code></a> 将多个输入形状映射到相同的输出形状时。 提供<code>output_padding</code>可通过有效地增加一侧的计算输出形状来解决这种歧义。 请注意，<code>output_padding</code>仅用于查找输出形状，而实际上并未向输出添加零填充。</p>
<p>Note</p>
<p>In some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting <code>torch.backends.cudnn.deterministic = True</code>. Please see the notes on <a href="notes/randomness.html">Reproducibility</a> for background.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>in_channels</strong> (<em>python:int</em>) – Number of channels in the input image</p>
</li>
<li>
<p><strong>out_channels</strong> (<em>python:int</em>) – Number of channels produced by the convolution</p>
</li>
<li>
<p><strong>kernel_size</strong> (<em>python:int</em> <em>or</em> <em>tuple</em>) – Size of the convolving kernel</p>
</li>
<li>
<p><strong>stride</strong> (<em>python:int</em> <em>or</em> <em>tuple__,</em> <em>optional</em>) – Stride of the convolution. Default: 1</p>
</li>
<li>
<p><strong>padding</strong> (<em>python:int</em> <em>or</em> <em>tuple__,</em> <em>optional</em>) – <code>dilation * (kernel_size - 1) - padding</code> zero-padding will be added to both sides of each dimension in the input. Default: 0</p>
</li>
<li>
<p><strong>output_padding</strong> (<em>python:int</em> <em>or</em> <em>tuple__,</em> <em>optional</em>) – Additional size added to one side of each dimension in the output shape. Default: 0</p>
</li>
<li>
<p><strong>groups</strong> (<em>python:int__,</em> <em>optional</em>) – Number of blocked connections from input channels to output channels. Default: 1</p>
</li>
<li>
<p><strong>bias</strong> (<em>bool__,</em> <em>optional</em>) – If <code>True</code>, adds a learnable bias to the output. Default: <code>True</code></p>
</li>
<li>
<p><strong>dilation</strong> (<em>python:int</em> <em>or</em> <em>tuple__,</em> <em>optional</em>) – Spacing between kernel elements. Default: 1</p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/ea9924fefcff6c2a6746a1886427a412.jpg" /></p>
</li>
<li>
<p>Output: <img alt="" src="../img/334df99d96c3b01e0f1cfc33f255b17c.jpg" /> where</p>
</li>
</ul>
<p><img alt="" src="../img/bd5f37f592d6e5711d965c30b6b75550.jpg" /></p>
<p><img alt="" src="../img/196611264eaf650ed8890e33d2c003c1.jpg" /></p>
<p><img alt="" src="../img/29974f33c417d33ea20e858ca0591145.jpg" /></p>
<pre><code>Variables
</code></pre>
<ul>
<li>
<p><strong>〜ConvTranspose3d.weight</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–形状为<img alt="" src="../img/e751884a182984d938dfe52e94e3e41a.jpg" /> <img alt="" src="../img/becdafe4af052684f83273e724322e6e.jpg" />的模块的可学习重量。 这些权重的值取自<img alt="" src="../img/b0cc88ecd067ab9281ee698299aa4b54.jpg" />，其中<img alt="" src="../img/322da54cd498fff4f970d91c19f1eead.jpg" /></p>
</li>
<li>
<p><strong>〜ConvTranspose3d.bias</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–形状模块的可学习偏差(out_channels）如果<code>bias</code>为<code>True</code>，则值 这些权重来自<img alt="" src="../img/b0cc88ecd067ab9281ee698299aa4b54.jpg" />，其中<img alt="" src="../img/322da54cd498fff4f970d91c19f1eead.jpg" /></p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; # With square kernels and equal stride
&gt;&gt;&gt; m = nn.ConvTranspose3d(16, 33, 3, stride=2)
&gt;&gt;&gt; # non-square kernels and unequal stride and with padding
&gt;&gt;&gt; m = nn.ConvTranspose3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(0, 4, 2))
&gt;&gt;&gt; input = torch.randn(20, 16, 10, 50, 100)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="_8">展开</h3>
<hr />
<pre><code>class torch.nn.Unfold(kernel_size, dilation=1, padding=0, stride=1)¶
</code></pre>
<p>从批处理输入张量中提取滑动局部块。</p>
<p>考虑形状为<img alt="" src="../img/877a6a1f684db60343e3c9847af621cd.jpg" />的成批<code>input</code>张量，其中<img alt="" src="../img/7ea0d6aa290d37c8bc57957206bafe80.jpg" />为批尺寸，<img alt="" src="../img/efc1fa1ed22c59909c50ea4639e0e861.jpg" />为通道尺寸，<img alt="" src="../img/dcef98688866c0d5a21137cf53bf228d.jpg" />代表任意空间尺寸。 此操作将<code>input</code>的空间尺寸内每个<code>kernel_size</code>大小的滑动块压平为形状为<img alt="" src="../img/1d973867a23d1f40b00464d31b7c8ad0.jpg" />的 3-D <code>output</code>张量的列(即最后一个尺寸），其中<img alt="" src="../img/f6ec742d91044014e35f248f04908933.jpg" />为总数 每个块内的值数量(一个块具有<img alt="" src="../img/aaf792bdc837dc9f304786716fbaf375.jpg" />个空间位置，每个位置包含<img alt="" src="../img/efc1fa1ed22c59909c50ea4639e0e861.jpg" />通道矢量），<img alt="" src="../img/0abd5691618d0a16f37deae1aacd79cf.jpg" />是此类块的总数：</p>
<p><img alt="" src="../img/b01a7cb9ed8863632b341c05b01e6134.jpg" /></p>
<p>其中<img alt="" src="../img/cfad63d010fbb974286a8107ef1f436b.jpg" />由<code>input</code>(以上<img alt="" src="../img/dcef98688866c0d5a21137cf53bf228d.jpg" />）的空间尺寸形成，而<img alt="" src="../img/95d07e221a9b945feb93422a44ac3d42.jpg" />在所有空间尺寸上。</p>
<p>因此，在最后一个维度(列维度）上索引<code>output</code>将给出特定块内的所有值。</p>
<p><code>padding</code>，<code>stride</code>和<code>dilation</code>自变量指定如何检索滑块。</p>
<ul>
<li>
<p><code>stride</code>控制滑块的步幅。</p>
</li>
<li>
<p>在重塑之前，<code>padding</code>控制每个维的<code>padding</code>个点的两侧的隐式零填充量。</p>
</li>
<li>
<p><code>dilation</code> controls the spacing between the kernel points; also known as the à trous algorithm. It is harder to describe, but this <a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code>dilation</code> does.</p>
</li>
</ul>
<p>Parameters</p>
<ul>
<li>
<p><strong>kernel_size</strong>  (<em>python：int</em> <em>或</em> <em>元组</em>）–滑块的大小</p>
</li>
<li>
<p><strong>跨度</strong> (<em>python：int</em> <em>或</em> <em>元组</em> <em>，</em> <em>可选</em>）–跨度 输入空间维度中的滑块。 默认值：1</p>
</li>
<li>
<p><strong>填充</strong> (<em>python：int</em> <em>或</em> <em>元组</em> <em>，</em> <em>可选</em>）–隐式零填充 将被添加到输入的两侧。 默认值：0</p>
</li>
<li>
<p><strong>扩张</strong> (<em>python：int</em> <em>或</em> <em>元组</em> <em>，</em> <em>可选</em>）–一个参数 控制邻域内元素的步幅。 默认值：1</p>
</li>
<li>
<p>如果<code>kernel_size</code>，<code>dilation</code>，<code>padding</code>或<code>stride</code>是长度为 1 的 int 或元组，则它们的值将在所有空间维度上复制。</p>
</li>
<li>
<p>对于两个输入空间维度，此操作有时称为<code>im2col</code>。</p>
</li>
</ul>
<p>Note</p>
<p><a href="#torch.nn.Fold" title="torch.nn.Fold"><code>Fold</code></a> 通过对来自所有包含块的所有值求和来计算所得大张量中的每个组合值。 <a href="#torch.nn.Unfold" title="torch.nn.Unfold"><code>Unfold</code></a> 通过复制大张量来提取局部块中的值。 因此，如果这些块重叠，则它们不是彼此相反。</p>
<p>通常，折叠和展开操作如下相关。 考虑使用相同参数创建的 <a href="#torch.nn.Fold" title="torch.nn.Fold"><code>Fold</code></a> 和 <a href="#torch.nn.Unfold" title="torch.nn.Unfold"><code>Unfold</code></a> 实例：</p>
<pre><code>&gt;&gt;&gt; fold_params = dict(kernel_size=..., dilation=..., padding=..., stride=...)
&gt;&gt;&gt; fold = nn.Fold(output_size=..., **fold_params)
&gt;&gt;&gt; unfold = nn.Unfold(**fold_params)

</code></pre>
<p>然后，对于任何(受支持的）<code>input</code>张量，以下等式成立：</p>
<pre><code>fold(unfold(input)) == divisor * input

</code></pre>
<p>其中<code>divisor</code>是仅取决于<code>input</code>的形状和 dtype 的张量：</p>
<pre><code>&gt;&gt;&gt; input_ones = torch.ones(input.shape, dtype=input.dtype)
&gt;&gt;&gt; divisor = fold(unfold(input_ones))

</code></pre>
<p>当<code>divisor</code>张量不包含零元素时，则<code>fold</code>和<code>unfold</code>运算互为逆(最大除数）。</p>
<p>Warning</p>
<p>当前，仅支持 4D 输入张量(像图像一样的批状张量）。</p>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>输入：<img alt="" src="../img/877a6a1f684db60343e3c9847af621cd.jpg" /></p>
</li>
<li>
<p>输出：<img alt="" src="../img/1d973867a23d1f40b00464d31b7c8ad0.jpg" />如上所述</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; unfold = nn.Unfold(kernel_size=(2, 3))
&gt;&gt;&gt; input = torch.randn(2, 5, 3, 4)
&gt;&gt;&gt; output = unfold(input)
&gt;&gt;&gt; # each patch contains 30 values (2x3=6 vectors, each of 5 channels)
&gt;&gt;&gt; # 4 blocks (2x3 kernels) in total in the 3x4 input
&gt;&gt;&gt; output.size()
torch.Size([2, 30, 4])

&gt;&gt;&gt; # Convolution is equivalent with Unfold + Matrix Multiplication + Fold (or view to output shape)
&gt;&gt;&gt; inp = torch.randn(1, 3, 10, 12)
&gt;&gt;&gt; w = torch.randn(2, 3, 4, 5)
&gt;&gt;&gt; inp_unf = torch.nn.functional.unfold(inp, (4, 5))
&gt;&gt;&gt; out_unf = inp_unf.transpose(1, 2).matmul(w.view(w.size(0), -1).t()).transpose(1, 2)
&gt;&gt;&gt; out = torch.nn.functional.fold(out_unf, (7, 8), (1, 1))
&gt;&gt;&gt; # or equivalently (and avoiding a copy),
&gt;&gt;&gt; # out = out_unf.view(1, 2, 7, 8)
&gt;&gt;&gt; (torch.nn.functional.conv2d(inp, w) - out).abs().max()
tensor(1.9073e-06)

</code></pre>
<h3 id="_9">折</h3>
<hr />
<pre><code>class torch.nn.Fold(output_size, kernel_size, dilation=1, padding=0, stride=1)¶
</code></pre>
<p>将一系列滑动局部块组合成一个大型的张量。</p>
<p>考虑一个包含<img alt="" src="../img/107898a042c04e970116ec9bca99beee.jpg" />形状的滑动局部块(例如图像块）的批处理<code>input</code>张量，其中<img alt="" src="../img/7ea0d6aa290d37c8bc57957206bafe80.jpg" />是批处理尺寸，<img alt="" src="../img/f6ec742d91044014e35f248f04908933.jpg" />是一个块内的值数(一个块具有<img alt="" src="../img/aaf792bdc837dc9f304786716fbaf375.jpg" /> 每个包含<img alt="" src="../img/efc1fa1ed22c59909c50ea4639e0e861.jpg" />通道向量的空间位置），<img alt="" src="../img/0abd5691618d0a16f37deae1aacd79cf.jpg" />是块的总数。 (这与 <a href="#torch.nn.Unfold" title="torch.nn.Unfold"><code>Unfold</code></a> 的输出形状完全相同。）此操作通过求和重叠值，将这些局部块组合为形状为<img alt="" src="../img/810862d519ad29a81c5af001a20a2c30.jpg" />的大<code>output</code>张量。 与 <a href="#torch.nn.Unfold" title="torch.nn.Unfold"><code>Unfold</code></a> 类似，参数必须满足</p>
<p><img alt="" src="../img/26e958ae0c8101dd13cbdcd7a99611ea.jpg" /></p>
<p>其中<img alt="" src="../img/95d07e221a9b945feb93422a44ac3d42.jpg" />覆盖所有空间尺寸。</p>
<ul>
<li><code>output_size</code>描述了滑动局部块的大包含张量的空间形状。 当多个输入形状例如使用<code>stride &amp;gt; 0</code>映射到相同数量的滑块时，解决歧义很有用。</li>
</ul>
<p>The <code>padding</code>, <code>stride</code> and <code>dilation</code> arguments specify how the sliding blocks are retrieved.</p>
<ul>
<li>
<p><code>stride</code> controls the stride for the sliding blocks.</p>
</li>
<li>
<p><code>padding</code> controls the amount of implicit zero-paddings on both sides for <code>padding</code> number of points for each dimension before reshaping.</p>
</li>
<li>
<p><code>dilation</code> controls the spacing between the kernel points; also known as the à trous algorithm. It is harder to describe, but this <a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code>dilation</code> does.</p>
</li>
</ul>
<p>Parameters</p>
<ul>
<li>
<p><strong>output_size</strong>  (<em>python：int</em> <em>或</em> <em>元组</em>）–输出的空间尺寸形状(即<code>output.sizes()[2:]</code>）</p>
</li>
<li>
<p><strong>kernel_size</strong> (<em>python:int</em> <em>or</em> <em>tuple</em>) – the size of the sliding blocks</p>
</li>
<li>
<p><strong>跨度</strong> (<em>python：int</em> <em>或</em> <em>元组</em>）–滑动块在输入空间维度上的跨度。 默认值：1</p>
</li>
<li>
<p><strong>padding</strong> (<em>python:int</em> <em>or</em> <em>tuple__,</em> <em>optional</em>) – implicit zero padding to be added on both sides of input. Default: 0</p>
</li>
<li>
<p><strong>dilation</strong> (<em>python:int</em> <em>or</em> <em>tuple__,</em> <em>optional</em>) – a parameter that controls the stride of elements within the neighborhood. Default: 1</p>
</li>
<li>
<p>如果<code>output_size</code>，<code>kernel_size</code>，<code>dilation</code>，<code>padding</code>或<code>stride</code>是长度为 1 的整数或元组，则它们的值将在所有空间维度上复制。</p>
</li>
<li>
<p>对于两个输出空间维度，此操作有时称为<code>col2im</code>。</p>
</li>
</ul>
<p>Note</p>
<p><a href="#torch.nn.Fold" title="torch.nn.Fold"><code>Fold</code></a> calculates each combined value in the resulting large tensor by summing all values from all containing blocks. <a href="#torch.nn.Unfold" title="torch.nn.Unfold"><code>Unfold</code></a> extracts the values in the local blocks by copying from the large tensor. So, if the blocks overlap, they are not inverses of each other.</p>
<p>In general, folding and unfolding operations are related as follows. Consider <a href="#torch.nn.Fold" title="torch.nn.Fold"><code>Fold</code></a> and <a href="#torch.nn.Unfold" title="torch.nn.Unfold"><code>Unfold</code></a> instances created with the same parameters:</p>
<pre><code>&gt;&gt;&gt; fold_params = dict(kernel_size=..., dilation=..., padding=..., stride=...)
&gt;&gt;&gt; fold = nn.Fold(output_size=..., **fold_params)
&gt;&gt;&gt; unfold = nn.Unfold(**fold_params)

</code></pre>
<p>Then for any (supported) <code>input</code> tensor the following equality holds:</p>
<pre><code>fold(unfold(input)) == divisor * input

</code></pre>
<p>where <code>divisor</code> is a tensor that depends only on the shape and dtype of the <code>input</code>:</p>
<pre><code>&gt;&gt;&gt; input_ones = torch.ones(input.shape, dtype=input.dtype)
&gt;&gt;&gt; divisor = fold(unfold(input_ones))

</code></pre>
<p>When the <code>divisor</code> tensor contains no zero elements, then <code>fold</code> and <code>unfold</code> operations are inverses of each other (upto constant divisor).</p>
<p>Warning</p>
<p>当前，仅支持 4D 输出张量(像图像一样的批状张量）。</p>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>输入：<img alt="" src="../img/1d973867a23d1f40b00464d31b7c8ad0.jpg" /></p>
</li>
<li>
<p>输出：<img alt="" src="../img/810862d519ad29a81c5af001a20a2c30.jpg" />如上所述</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; fold = nn.Fold(output_size=(4, 5), kernel_size=(2, 2))
&gt;&gt;&gt; input = torch.randn(1, 3 * 2 * 2, 12)
&gt;&gt;&gt; output = fold(input)
&gt;&gt;&gt; output.size()
torch.Size([1, 3, 4, 5])

</code></pre>
<h2 id="_10">汇聚层</h2>
<h3 id="maxpool1d">MaxPool1d</h3>
<hr />
<pre><code>class torch.nn.MaxPool1d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)¶
</code></pre>
<p>在由多个输入平面组成的输入信号上应用一维最大池化。</p>
<p>在最简单的情况下，具有输入大小<img alt="" src="../img/c8627f9c1b3afa3c758e61cb22621e8b.jpg" />和输出<img alt="" src="../img/400fa9ec0c9d670a091fe6b6dd2544eb.jpg" />的图层的输出值可以精确地描述为：</p>
<p><img alt="" src="../img/43820191c54c184b8f7f6d0103a1136b.jpg" /></p>
<p>如果<code>padding</code>不为零，则对于<code>padding</code>点的数量，输入将在两侧隐式填充零。 <code>dilation</code>控制内核点之间的间距。 很难描述，但是此<a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">链接</a>很好地展示了<code>dilation</code>的功能。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>kernel_size</strong> –取最大值的窗口大小</p>
</li>
<li>
<p><strong>步幅</strong> –窗口的步幅。 默认值为<code>kernel_size</code></p>
</li>
<li>
<p><strong>填充</strong> –在两侧都添加隐式零填充</p>
</li>
<li>
<p><strong>膨胀</strong> –控制窗口中元素步幅的参数</p>
</li>
<li>
<p><strong>return_indices</strong> –如果<code>True</code>，将返回最大索引以及输出。 以后对 <a href="#torch.nn.MaxUnpool1d" title="torch.nn.MaxUnpool1d"><code>torch.nn.MaxUnpool1d</code></a> 有用</p>
</li>
<li>
<p><strong>ceil_mode</strong> –为 True 时，将使用 &lt;cite&gt;ceil&lt;/cite&gt; 而不是 &lt;cite&gt;floor&lt;/cite&gt; 计算输出形状</p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>输入：<img alt="" src="../img/ebdb89b2925fa29c5ff32408e3e76e52.jpg" /></p>
</li>
<li>
<p>输出：<img alt="" src="../img/400fa9ec0c9d670a091fe6b6dd2544eb.jpg" />，其中</p>
<p><img alt="" src="../img/da7161298c362ece060692c4793903c6.jpg" /></p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; # pool of size=3, stride=2
&gt;&gt;&gt; m = nn.MaxPool1d(3, stride=2)
&gt;&gt;&gt; input = torch.randn(20, 16, 50)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="maxpool2d">MaxPool2d</h3>
<hr />
<pre><code>class torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)¶
</code></pre>
<p>在由多个输入平面组成的输入信号上应用 2D 最大合并。</p>
<p>在最简单的情况下，具有输入大小<img alt="" src="../img/a52adc3e96ebecd26270170745455e66.jpg" />，输出<img alt="" src="../img/bf0bebeaa2a77220349c5240a79ad798.jpg" />和<code>kernel_size</code> <img alt="" src="../img/4daf59a768ce95d4d33035b02dcca57e.jpg" />的图层的输出值可以精确地描述为：</p>
<p><img alt="" src="../img/fe27b5410627b9e2edd15032aa9f2750.jpg" /></p>
<p>If <code>padding</code> is non-zero, then the input is implicitly zero-padded on both sides for <code>padding</code> number of points. <code>dilation</code> controls the spacing between the kernel points. It is harder to describe, but this <a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code>dilation</code> does.</p>
<p>The parameters <code>kernel_size</code>, <code>stride</code>, <code>padding</code>, <code>dilation</code> can either be:</p>
<blockquote>
<ul>
<li>
<p>a single <code>int</code> – in which case the same value is used for the height and width dimension</p>
</li>
<li>
<p>a <code>tuple</code> of two ints – in which case, the first &lt;cite&gt;int&lt;/cite&gt; is used for the height dimension, and the second &lt;cite&gt;int&lt;/cite&gt; for the width dimension</p>
</li>
</ul>
</blockquote>
<p>Parameters</p>
<ul>
<li>
<p><strong>kernel_size</strong> – the size of the window to take a max over</p>
</li>
<li>
<p><strong>stride</strong> – the stride of the window. Default value is <code>kernel_size</code></p>
</li>
<li>
<p><strong>padding</strong> – implicit zero padding to be added on both sides</p>
</li>
<li>
<p><strong>dilation</strong> – a parameter that controls the stride of elements in the window</p>
</li>
<li>
<p><strong>return_indices</strong> –如果<code>True</code>，将返回最大索引以及输出。 以后对 <a href="#torch.nn.MaxUnpool2d" title="torch.nn.MaxUnpool2d"><code>torch.nn.MaxUnpool2d</code></a> 有用</p>
</li>
<li>
<p><strong>ceil_mode</strong> – when True, will use &lt;cite&gt;ceil&lt;/cite&gt; instead of &lt;cite&gt;floor&lt;/cite&gt; to compute the output shape</p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>输入：<img alt="" src="../img/55ac2d313b1b37ea5379f0f94f8d6982.jpg" /></p>
</li>
<li>
<p>输出：<img alt="" src="../img/bf0bebeaa2a77220349c5240a79ad798.jpg" />，其中</p>
<p><img alt="" src="../img/5467fc09a54438b55d98368253b3433e.jpg" /></p>
<p><img alt="" src="../img/6f9c97c4acf4929e3d0bf35a4d9aaf28.jpg" /></p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; # pool of square window of size=3, stride=2
&gt;&gt;&gt; m = nn.MaxPool2d(3, stride=2)
&gt;&gt;&gt; # pool of non-square window
&gt;&gt;&gt; m = nn.MaxPool2d((3, 2), stride=(2, 1))
&gt;&gt;&gt; input = torch.randn(20, 16, 50, 32)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="maxpool3d">MaxPool3d</h3>
<hr />
<pre><code>class torch.nn.MaxPool3d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)¶
</code></pre>
<p>在由多个输入平面组成的输入信号上应用 3D 最大池化。</p>
<p>在最简单的情况下，具有输入大小<img alt="" src="../img/2bbc80f07f22b3961bbc382bd5a5e8f3.jpg" />，输出<img alt="" src="../img/56dd521f825f0c201df32048c532fd13.jpg" />和<code>kernel_size</code> <img alt="" src="../img/03748ca7d89d81c13263964fc390c160.jpg" />的图层的输出值可以精确地描述为：</p>
<p><img alt="" src="../img/82094692f87a4b5a642c7e32bf9a53b5.jpg" /></p>
<p>If <code>padding</code> is non-zero, then the input is implicitly zero-padded on both sides for <code>padding</code> number of points. <code>dilation</code> controls the spacing between the kernel points. It is harder to describe, but this <a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code>dilation</code> does.</p>
<p>The parameters <code>kernel_size</code>, <code>stride</code>, <code>padding</code>, <code>dilation</code> can either be:</p>
<blockquote>
<ul>
<li>
<p>a single <code>int</code> – in which case the same value is used for the depth, height and width dimension</p>
</li>
<li>
<p>a <code>tuple</code> of three ints – in which case, the first &lt;cite&gt;int&lt;/cite&gt; is used for the depth dimension, the second &lt;cite&gt;int&lt;/cite&gt; for the height dimension and the third &lt;cite&gt;int&lt;/cite&gt; for the width dimension</p>
</li>
</ul>
</blockquote>
<p>Parameters</p>
<ul>
<li>
<p><strong>kernel_size</strong> – the size of the window to take a max over</p>
</li>
<li>
<p><strong>stride</strong> – the stride of the window. Default value is <code>kernel_size</code></p>
</li>
<li>
<p><strong>填充</strong> –在所有三个面上都添加隐式零填充</p>
</li>
<li>
<p><strong>dilation</strong> – a parameter that controls the stride of elements in the window</p>
</li>
<li>
<p><strong>return_indices</strong> –如果<code>True</code>，将返回最大索引以及输出。 以后对 <a href="#torch.nn.MaxUnpool3d" title="torch.nn.MaxUnpool3d"><code>torch.nn.MaxUnpool3d</code></a> 有用</p>
</li>
<li>
<p><strong>ceil_mode</strong> – when True, will use &lt;cite&gt;ceil&lt;/cite&gt; instead of &lt;cite&gt;floor&lt;/cite&gt; to compute the output shape</p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>输入：<img alt="" src="../img/fc79fbdaec72bad3be239985d50f2f7c.jpg" /></p>
</li>
<li>
<p>输出：<img alt="" src="../img/56dd521f825f0c201df32048c532fd13.jpg" />，其中</p>
<p><img alt="" src="../img/4bfb9d67411de2b82fb127463e475a45.jpg" /></p>
<p><img alt="" src="../img/fa83c9c5e1d7b5183d972fc80a348628.jpg" /></p>
<p><img alt="" src="../img/5018c0ee3425370121539a09abad3baf.jpg" /></p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; # pool of square window of size=3, stride=2
&gt;&gt;&gt; m = nn.MaxPool3d(3, stride=2)
&gt;&gt;&gt; # pool of non-square window
&gt;&gt;&gt; m = nn.MaxPool3d((3, 2, 2), stride=(2, 1, 2))
&gt;&gt;&gt; input = torch.randn(20, 16, 50,44, 31)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="maxunpool1d">MaxUnpool1d</h3>
<hr />
<pre><code>class torch.nn.MaxUnpool1d(kernel_size, stride=None, padding=0)¶
</code></pre>
<p>计算 <a href="#torch.nn.MaxPool1d" title="torch.nn.MaxPool1d"><code>MaxPool1d</code></a> 的局部逆。</p>
<p><a href="#torch.nn.MaxPool1d" title="torch.nn.MaxPool1d"><code>MaxPool1d</code></a> 不能完全反转，因为会丢失非最大值。</p>
<p><a href="#torch.nn.MaxUnpool1d" title="torch.nn.MaxUnpool1d"><code>MaxUnpool1d</code></a> 接收包括最大值索引在内的 <a href="#torch.nn.MaxPool1d" title="torch.nn.MaxPool1d"><code>MaxPool1d</code></a> 的输出作为输入，并计算一个部分逆，其中所有非最大值都设置为零。</p>
<p>Note</p>
<p><a href="#torch.nn.MaxPool1d" title="torch.nn.MaxPool1d"><code>MaxPool1d</code></a> 可以将多个输入大小映射到相同的输出大小。 因此，反转过程可能会变得模棱两可。 为了解决这个问题，您可以在前进调用中提供所需的输出大小作为附加参数<code>output_size</code>。 请参阅下面的输入和示例。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>kernel_size</strong>  (<em>python：int</em> <em>或</em> <em>元组</em>）–最大池窗口的大小。</p>
</li>
<li>
<p><strong>跨度</strong> (<em>python：int</em> <em>或</em> <em>元组</em>）–最大合并窗口的跨度。 默认设置为<code>kernel_size</code>。</p>
</li>
<li>
<p><strong>填充</strong> (<em>python：int</em> <em>或</em> <em>元组</em>）–已添加到输入中的填充</p>
</li>
</ul>
<pre><code>Inputs:
</code></pre>
<ul>
<li>
<p>&lt;cite&gt;输入&lt;/cite&gt;：输入张量反转</p>
</li>
<li>
<p>&lt;cite&gt;指标&lt;/cite&gt;： <a href="#torch.nn.MaxPool1d" title="torch.nn.MaxPool1d"><code>MaxPool1d</code></a> 给出的指标</p>
</li>
<li>
<p>&lt;cite&gt;output_size&lt;/cite&gt; (可选）：目标输出大小</p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>输入：<img alt="" src="../img/49d327ebf6dab513b5f0d319c95dbd6c.jpg" /></p>
</li>
<li>
<p>输出：<img alt="" src="../img/a8bea90b6c2a79a8509fd05974e8e2d2.jpg" />，其中</p>
<p><img alt="" src="../img/e990b8ef5054ba238a99f99477aad9d2.jpg" /></p>
<p>或由呼叫运营商中的<code>output_size</code>给定</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; pool = nn.MaxPool1d(2, stride=2, return_indices=True)
&gt;&gt;&gt; unpool = nn.MaxUnpool1d(2, stride=2)
&gt;&gt;&gt; input = torch.tensor([[[1., 2, 3, 4, 5, 6, 7, 8]]])
&gt;&gt;&gt; output, indices = pool(input)
&gt;&gt;&gt; unpool(output, indices)
tensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.]]])

&gt;&gt;&gt; # Example showcasing the use of output_size
&gt;&gt;&gt; input = torch.tensor([[[1., 2, 3, 4, 5, 6, 7, 8, 9]]])
&gt;&gt;&gt; output, indices = pool(input)
&gt;&gt;&gt; unpool(output, indices, output_size=input.size())
tensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.,  0.]]])

&gt;&gt;&gt; unpool(output, indices)
tensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.]]])

</code></pre>
<h3 id="maxunpool2d">MaxUnpool2d</h3>
<hr />
<pre><code>class torch.nn.MaxUnpool2d(kernel_size, stride=None, padding=0)¶
</code></pre>
<p>计算 <a href="#torch.nn.MaxPool2d" title="torch.nn.MaxPool2d"><code>MaxPool2d</code></a> 的局部逆。</p>
<p><a href="#torch.nn.MaxPool2d" title="torch.nn.MaxPool2d"><code>MaxPool2d</code></a> 不能完全反转，因为会丢失非最大值。</p>
<p><a href="#torch.nn.MaxUnpool2d" title="torch.nn.MaxUnpool2d"><code>MaxUnpool2d</code></a> 接收包括最大值索引在内的 <a href="#torch.nn.MaxPool2d" title="torch.nn.MaxPool2d"><code>MaxPool2d</code></a> 的输出作为输入，并计算一个部分逆，其中所有非最大值都设置为零。</p>
<p>Note</p>
<p><a href="#torch.nn.MaxPool2d" title="torch.nn.MaxPool2d"><code>MaxPool2d</code></a> 可以将多个输入大小映射到相同的输出大小。 因此，反转过程可能会变得模棱两可。 为了解决这个问题，您可以在前进调用中提供所需的输出大小作为附加参数<code>output_size</code>。 请参阅下面的输入和示例。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>kernel_size</strong> (<em>python:int</em> <em>or</em> <em>tuple</em>) – Size of the max pooling window.</p>
</li>
<li>
<p><strong>stride</strong> (<em>python:int</em> <em>or</em> <em>tuple</em>) – Stride of the max pooling window. It is set to <code>kernel_size</code> by default.</p>
</li>
<li>
<p><strong>padding</strong> (<em>python:int</em> <em>or</em> <em>tuple</em>) – Padding that was added to the input</p>
</li>
</ul>
<pre><code>Inputs:
</code></pre>
<ul>
<li>
<p>&lt;cite&gt;input&lt;/cite&gt;: the input Tensor to invert</p>
</li>
<li>
<p>&lt;cite&gt;指标&lt;/cite&gt;： <a href="#torch.nn.MaxPool2d" title="torch.nn.MaxPool2d"><code>MaxPool2d</code></a> 给出的指标</p>
</li>
<li>
<p>&lt;cite&gt;output_size&lt;/cite&gt; (optional): the targeted output size</p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/55ac2d313b1b37ea5379f0f94f8d6982.jpg" /></p>
</li>
<li>
<p>Output: <img alt="" src="../img/bf0bebeaa2a77220349c5240a79ad798.jpg" />, where</p>
<p><img alt="" src="../img/3982de51962e82b1d018d8b07e92e94c.jpg" /></p>
<p><img alt="" src="../img/fc143ba2ea13cc056efd5f81c4b24f0d.jpg" /></p>
<p>or as given by <code>output_size</code> in the call operator</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; pool = nn.MaxPool2d(2, stride=2, return_indices=True)
&gt;&gt;&gt; unpool = nn.MaxUnpool2d(2, stride=2)
&gt;&gt;&gt; input = torch.tensor([[[[ 1.,  2,  3,  4],
                            [ 5,  6,  7,  8],
                            [ 9, 10, 11, 12],
                            [13, 14, 15, 16]]]])
&gt;&gt;&gt; output, indices = pool(input)
&gt;&gt;&gt; unpool(output, indices)
tensor([[[[  0.,   0.,   0.,   0.],
          [  0.,   6.,   0.,   8.],
          [  0.,   0.,   0.,   0.],
          [  0.,  14.,   0.,  16.]]]])

&gt;&gt;&gt; # specify a different output size than input size
&gt;&gt;&gt; unpool(output, indices, output_size=torch.Size([1, 1, 5, 5]))
tensor([[[[  0.,   0.,   0.,   0.,   0.],
          [  6.,   0.,   8.,   0.,   0.],
          [  0.,   0.,   0.,  14.,   0.],
          [ 16.,   0.,   0.,   0.,   0.],
          [  0.,   0.,   0.,   0.,   0.]]]])

</code></pre>
<h3 id="maxunpool3d">MaxUnpool3d</h3>
<hr />
<pre><code>class torch.nn.MaxUnpool3d(kernel_size, stride=None, padding=0)¶
</code></pre>
<p>计算 <a href="#torch.nn.MaxPool3d" title="torch.nn.MaxPool3d"><code>MaxPool3d</code></a> 的局部逆。</p>
<p><a href="#torch.nn.MaxPool3d" title="torch.nn.MaxPool3d"><code>MaxPool3d</code></a> 不能完全反转，因为会丢失非最大值。 <a href="#torch.nn.MaxUnpool3d" title="torch.nn.MaxUnpool3d"><code>MaxUnpool3d</code></a> 将包含最大值索引的 <a href="#torch.nn.MaxPool3d" title="torch.nn.MaxPool3d"><code>MaxPool3d</code></a> 的输出作为输入，并计算将所有非最大值均设置为零的部分逆。</p>
<p>Note</p>
<p><a href="#torch.nn.MaxPool3d" title="torch.nn.MaxPool3d"><code>MaxPool3d</code></a> 可以将多个输入大小映射到相同的输出大小。 因此，反转过程可能会变得模棱两可。 为了解决这个问题，您可以在前进调用中提供所需的输出大小作为附加参数<code>output_size</code>。 请参阅下面的输入部分。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>kernel_size</strong> (<em>python:int</em> <em>or</em> <em>tuple</em>) – Size of the max pooling window.</p>
</li>
<li>
<p><strong>stride</strong> (<em>python:int</em> <em>or</em> <em>tuple</em>) – Stride of the max pooling window. It is set to <code>kernel_size</code> by default.</p>
</li>
<li>
<p><strong>padding</strong> (<em>python:int</em> <em>or</em> <em>tuple</em>) – Padding that was added to the input</p>
</li>
</ul>
<pre><code>Inputs:
</code></pre>
<ul>
<li>
<p>&lt;cite&gt;input&lt;/cite&gt;: the input Tensor to invert</p>
</li>
<li>
<p>&lt;cite&gt;指标&lt;/cite&gt;： <a href="#torch.nn.MaxPool3d" title="torch.nn.MaxPool3d"><code>MaxPool3d</code></a> 给出的指标</p>
</li>
<li>
<p>&lt;cite&gt;output_size&lt;/cite&gt; (optional): the targeted output size</p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/fc79fbdaec72bad3be239985d50f2f7c.jpg" /></p>
</li>
<li>
<p>Output: <img alt="" src="../img/56dd521f825f0c201df32048c532fd13.jpg" />, where</p>
<p><img alt="" src="../img/71361469116a054aede423e0bd148bf4.jpg" /></p>
<p><img alt="" src="../img/feb5897251223dcad0693cf2a1da8af3.jpg" /></p>
<p><img alt="" src="../img/59d370ddffd32c0865a8e17e31bd82e2.jpg" /></p>
<p>or as given by <code>output_size</code> in the call operator</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; # pool of square window of size=3, stride=2
&gt;&gt;&gt; pool = nn.MaxPool3d(3, stride=2, return_indices=True)
&gt;&gt;&gt; unpool = nn.MaxUnpool3d(3, stride=2)
&gt;&gt;&gt; output, indices = pool(torch.randn(20, 16, 51, 33, 15))
&gt;&gt;&gt; unpooled_output = unpool(output, indices)
&gt;&gt;&gt; unpooled_output.size()
torch.Size([20, 16, 51, 33, 15])

</code></pre>
<h3 id="1d_1">平均池 1d</h3>
<hr />
<pre><code>class torch.nn.AvgPool1d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True)¶
</code></pre>
<p>在由多个输入平面组成的输入信号上应用一维平均池。</p>
<p>在最简单的情况下，具有输入大小<img alt="" src="../img/c8627f9c1b3afa3c758e61cb22621e8b.jpg" />，输出<img alt="" src="../img/400fa9ec0c9d670a091fe6b6dd2544eb.jpg" />和<code>kernel_size</code> <img alt="" src="../img/678502bee29f9b13e7115b18864a5822.jpg" />的图层的输出值可以精确地描述为：</p>
<p><img alt="" src="../img/9713cf9cafe0511ec6fae3dd535d8c6d.jpg" /></p>
<p>如果<code>padding</code>不为零，则对于<code>padding</code>点的数量，输入将在两侧隐式填充零。</p>
<p>参数<code>kernel_size</code>，<code>stride</code>和<code>padding</code>可以分别是<code>int</code>或一个元素元组。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>kernel_size</strong> –窗口的大小</p>
</li>
<li>
<p><strong>stride</strong> – the stride of the window. Default value is <code>kernel_size</code></p>
</li>
<li>
<p><strong>padding</strong> – implicit zero padding to be added on both sides</p>
</li>
<li>
<p><strong>ceil_mode</strong> – when True, will use &lt;cite&gt;ceil&lt;/cite&gt; instead of &lt;cite&gt;floor&lt;/cite&gt; to compute the output shape</p>
</li>
<li>
<p><strong>count_include_pad</strong> –为 True 时，将在平均计算中包括零填充</p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/ebdb89b2925fa29c5ff32408e3e76e52.jpg" /></p>
</li>
<li>
<p>Output: <img alt="" src="../img/400fa9ec0c9d670a091fe6b6dd2544eb.jpg" />, where</p>
<p><img alt="" src="../img/4cb501bf4833a8694de3d527ab0001e3.jpg" /></p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; # pool with window of size=3, stride=2
&gt;&gt;&gt; m = nn.AvgPool1d(3, stride=2)
&gt;&gt;&gt; m(torch.tensor([[[1.,2,3,4,5,6,7]]]))
tensor([[[ 2.,  4.,  6.]]])

</code></pre>
<h3 id="2d_1">平均池 2d</h3>
<hr />
<pre><code>class torch.nn.AvgPool2d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None)¶
</code></pre>
<p>在由多个输入平面组成的输入信号上应用 2D 平均池。</p>
<p>In the simplest case, the output value of the layer with input size <img alt="" src="../img/a52adc3e96ebecd26270170745455e66.jpg" />, output <img alt="" src="../img/bf0bebeaa2a77220349c5240a79ad798.jpg" /> and <code>kernel_size</code> <img alt="" src="../img/4daf59a768ce95d4d33035b02dcca57e.jpg" /> can be precisely described as:</p>
<p><img alt="" src="../img/1fa02e1ba2135277ac264025f33c0558.jpg" /></p>
<p>If <code>padding</code> is non-zero, then the input is implicitly zero-padded on both sides for <code>padding</code> number of points.</p>
<p>参数<code>kernel_size</code>，<code>stride</code>和<code>padding</code>可以是：</p>
<blockquote>
<ul>
<li>
<p>a single <code>int</code> – in which case the same value is used for the height and width dimension</p>
</li>
<li>
<p>a <code>tuple</code> of two ints – in which case, the first &lt;cite&gt;int&lt;/cite&gt; is used for the height dimension, and the second &lt;cite&gt;int&lt;/cite&gt; for the width dimension</p>
</li>
</ul>
</blockquote>
<p>Parameters</p>
<ul>
<li>
<p><strong>kernel_size</strong> – the size of the window</p>
</li>
<li>
<p><strong>stride</strong> – the stride of the window. Default value is <code>kernel_size</code></p>
</li>
<li>
<p><strong>padding</strong> – implicit zero padding to be added on both sides</p>
</li>
<li>
<p><strong>ceil_mode</strong> – when True, will use &lt;cite&gt;ceil&lt;/cite&gt; instead of &lt;cite&gt;floor&lt;/cite&gt; to compute the output shape</p>
</li>
<li>
<p><strong>count_include_pad</strong> – when True, will include the zero-padding in the averaging calculation</p>
</li>
<li>
<p><strong>divisor_override</strong> -如果指定，它将用作除数，否则 attr： &lt;cite&gt;kernel_size&lt;/cite&gt;</p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/55ac2d313b1b37ea5379f0f94f8d6982.jpg" /></p>
</li>
<li>
<p>Output: <img alt="" src="../img/bf0bebeaa2a77220349c5240a79ad798.jpg" />, where</p>
<p><img alt="" src="../img/7b639669791c7345f93992a5c7622d4c.jpg" /></p>
<p><img alt="" src="../img/2a993094b2d682a6a6e07f7b49c2816e.jpg" /></p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; # pool of square window of size=3, stride=2
&gt;&gt;&gt; m = nn.AvgPool2d(3, stride=2)
&gt;&gt;&gt; # pool of non-square window
&gt;&gt;&gt; m = nn.AvgPool2d((3, 2), stride=(2, 1))
&gt;&gt;&gt; input = torch.randn(20, 16, 50, 32)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="3d_1">平均池 3d</h3>
<hr />
<pre><code>class torch.nn.AvgPool3d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None)¶
</code></pre>
<p>在由多个输入平面组成的输入信号上应用 3D 平均池。</p>
<p>In the simplest case, the output value of the layer with input size <img alt="" src="../img/2bbc80f07f22b3961bbc382bd5a5e8f3.jpg" />, output <img alt="" src="../img/56dd521f825f0c201df32048c532fd13.jpg" /> and <code>kernel_size</code> <img alt="" src="../img/03748ca7d89d81c13263964fc390c160.jpg" /> can be precisely described as:</p>
<p><img alt="" src="../img/2dcd89093555c2087d67b4c3ec935878.jpg" /></p>
<p>如果<code>padding</code>不为零，则对于<code>padding</code>点的数量，输入将在所有三个侧面隐式填充零。</p>
<p>参数<code>kernel_size</code>和<code>stride</code>可以是：</p>
<blockquote>
<ul>
<li>
<p>a single <code>int</code> – in which case the same value is used for the depth, height and width dimension</p>
</li>
<li>
<p>a <code>tuple</code> of three ints – in which case, the first &lt;cite&gt;int&lt;/cite&gt; is used for the depth dimension, the second &lt;cite&gt;int&lt;/cite&gt; for the height dimension and the third &lt;cite&gt;int&lt;/cite&gt; for the width dimension</p>
</li>
</ul>
</blockquote>
<p>Parameters</p>
<ul>
<li>
<p><strong>kernel_size</strong> – the size of the window</p>
</li>
<li>
<p><strong>stride</strong> – the stride of the window. Default value is <code>kernel_size</code></p>
</li>
<li>
<p><strong>padding</strong> – implicit zero padding to be added on all three sides</p>
</li>
<li>
<p><strong>ceil_mode</strong> – when True, will use &lt;cite&gt;ceil&lt;/cite&gt; instead of &lt;cite&gt;floor&lt;/cite&gt; to compute the output shape</p>
</li>
<li>
<p><strong>count_include_pad</strong> – when True, will include the zero-padding in the averaging calculation</p>
</li>
<li>
<p><strong>divisor_override</strong> – if specified, it will be used as divisor, otherwise attr:&lt;cite&gt;kernel_size&lt;/cite&gt; will be used</p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/fc79fbdaec72bad3be239985d50f2f7c.jpg" /></p>
</li>
<li>
<p>Output: <img alt="" src="../img/56dd521f825f0c201df32048c532fd13.jpg" />, where</p>
<p><img alt="" src="../img/64aa8807d91f0ba9ebdf03386e66fbb6.jpg" /></p>
<p><img alt="" src="../img/ed56c5151e995f1bda1d069ce6f54ed4.jpg" /></p>
<p><img alt="" src="../img/9ae086e4cb30bb1a948442f031e29a5e.jpg" /></p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; # pool of square window of size=3, stride=2
&gt;&gt;&gt; m = nn.AvgPool3d(3, stride=2)
&gt;&gt;&gt; # pool of non-square window
&gt;&gt;&gt; m = nn.AvgPool3d((3, 2, 2), stride=(2, 1, 2))
&gt;&gt;&gt; input = torch.randn(20, 16, 50,44, 31)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="2d_2">分数最大池 2d</h3>
<hr />
<pre><code>class torch.nn.FractionalMaxPool2d(kernel_size, output_size=None, output_ratio=None, return_indices=False, _random_samples=None)¶
</code></pre>
<p>在由多个输入平面组成的输入信号上应用 2D 分数最大池化。</p>
<p>Ben Graham 的论文 <a href="http://arxiv.org/abs/1412.6071">Fractional MaxPooling</a> 中详细描述了分数最大池化</p>
<p>在<img alt="" src="../img/713170155d933e7975460d6d6a614bbf.jpg" />区域中通过由目标输出大小确定的随机步长应用最大合并操作。 输出要素的数量等于输入平面的数量。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>kernel_size</strong> –接管最大值的窗口大小。 可以是单个数字 k(对于 k x k 的平方核）或元组&lt;cite&gt;(kh，kw）&lt;/cite&gt;</p>
</li>
<li>
<p><strong>output_size</strong> – &lt;cite&gt;oH x oW&lt;/cite&gt; 形式的图像的目标输出尺寸。 可以是一个元组&lt;cite&gt;(oH，oW）&lt;/cite&gt;，也可以是一个正方形图像 &lt;cite&gt;oH x oH&lt;/cite&gt; 的一个数字 oH</p>
</li>
<li>
<p><strong>output_ratio</strong> –如果希望输出大小与输入大小的比率，可以指定此选项。 这必须是范围为(0，1）的数字或元组</p>
</li>
<li>
<p><strong>return_indices</strong> -如果<code>True</code>，则将返回索引以及输出。 有助于传递给<code>nn.MaxUnpool2d()</code>。 默认值：<code>False</code></p>
</li>
</ul>
<p>例子</p>
<pre><code>&gt;&gt;&gt; # pool of square window of size=3, and target output size 13x12
&gt;&gt;&gt; m = nn.FractionalMaxPool2d(3, output_size=(13, 12))
&gt;&gt;&gt; # pool of square window and target output size being half of input image size
&gt;&gt;&gt; m = nn.FractionalMaxPool2d(3, output_ratio=(0.5, 0.5))
&gt;&gt;&gt; input = torch.randn(20, 16, 50, 32)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="lppool1d">LPPool1d</h3>
<hr />
<pre><code>class torch.nn.LPPool1d(norm_type, kernel_size, stride=None, ceil_mode=False)¶
</code></pre>
<p>在由多个输入平面组成的输入信号上应用一维功率平均池。</p>
<p>在每个窗口上，计算的函数为：</p>
<p><img alt="" src="../img/bffa4a1011ac942b757a49165ed89ed9.jpg" /></p>
<ul>
<li>
<p>在 p = <img alt="" src="../img/0292eef69e9017d41e615561beb5d81e.jpg" />时，获得最大池化</p>
</li>
<li>
<p>在 p = 1 时，总和池(与平均池成正比）</p>
</li>
</ul>
<p>Note</p>
<p>如果 &lt;cite&gt;p&lt;/cite&gt; 的幂的和为零，则此函数的梯度不确定。 在这种情况下，此实现会将梯度设置为零。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>kernel_size</strong> –单个整数，窗口的大小</p>
</li>
<li>
<p><strong>跨度</strong> –一个 int，即窗口的跨度。 默认值为<code>kernel_size</code></p>
</li>
<li>
<p><strong>ceil_mode</strong> – when True, will use &lt;cite&gt;ceil&lt;/cite&gt; instead of &lt;cite&gt;floor&lt;/cite&gt; to compute the output shape</p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/ebdb89b2925fa29c5ff32408e3e76e52.jpg" /></p>
</li>
<li>
<p>Output: <img alt="" src="../img/400fa9ec0c9d670a091fe6b6dd2544eb.jpg" />, where</p>
<p><img alt="" src="../img/29e02e0c9771a0a7f4f31205aaea405b.jpg" /></p>
</li>
</ul>
<pre><code>Examples::
</code></pre>
<pre><code>&gt;&gt;&gt; # power-2 pool of window of length 3, with stride 2.
&gt;&gt;&gt; m = nn.LPPool1d(2, 3, stride=2)
&gt;&gt;&gt; input = torch.randn(20, 16, 50)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="lppool2d">LPPool2d</h3>
<hr />
<pre><code>class torch.nn.LPPool2d(norm_type, kernel_size, stride=None, ceil_mode=False)¶
</code></pre>
<p>在由多个输入平面组成的输入信号上应用 2D 功率平均池。</p>
<p>On each window, the function computed is:</p>
<p><img alt="" src="../img/bffa4a1011ac942b757a49165ed89ed9.jpg" /></p>
<ul>
<li>
<p>At p = <img alt="" src="../img/0292eef69e9017d41e615561beb5d81e.jpg" />, one gets Max Pooling</p>
</li>
<li>
<p>在 p = 1 时，将获得“汇总池”(与平均池成比例）</p>
</li>
</ul>
<p>The parameters <code>kernel_size</code>, <code>stride</code> can either be:</p>
<blockquote>
<ul>
<li>
<p>a single <code>int</code> – in which case the same value is used for the height and width dimension</p>
</li>
<li>
<p>a <code>tuple</code> of two ints – in which case, the first &lt;cite&gt;int&lt;/cite&gt; is used for the height dimension, and the second &lt;cite&gt;int&lt;/cite&gt; for the width dimension</p>
</li>
</ul>
</blockquote>
<p>Note</p>
<p>If the sum to the power of &lt;cite&gt;p&lt;/cite&gt; is zero, the gradient of this function is not defined. This implementation will set the gradient to zero in this case.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>kernel_size</strong> – the size of the window</p>
</li>
<li>
<p><strong>stride</strong> – the stride of the window. Default value is <code>kernel_size</code></p>
</li>
<li>
<p><strong>ceil_mode</strong> – when True, will use &lt;cite&gt;ceil&lt;/cite&gt; instead of &lt;cite&gt;floor&lt;/cite&gt; to compute the output shape</p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/55ac2d313b1b37ea5379f0f94f8d6982.jpg" /></p>
</li>
<li>
<p>Output: <img alt="" src="../img/bf0bebeaa2a77220349c5240a79ad798.jpg" />, where</p>
<p><img alt="" src="../img/2aedec930d35781c0a1b689065d3d0d6.jpg" /></p>
<p><img alt="" src="../img/13d301de9efa4636e4b148de67a96b9e.jpg" /></p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; # power-2 pool of square window of size=3, stride=2
&gt;&gt;&gt; m = nn.LPPool2d(2, 3, stride=2)
&gt;&gt;&gt; # pool of non-square window of power 1.2
&gt;&gt;&gt; m = nn.LPPool2d(1.2, (3, 2), stride=(2, 1))
&gt;&gt;&gt; input = torch.randn(20, 16, 50, 32)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="adaptivemaxpool1d">AdaptiveMaxPool1d</h3>
<hr />
<pre><code>class torch.nn.AdaptiveMaxPool1d(output_size, return_indices=False)¶
</code></pre>
<p>在由多个输入平面组成的输入信号上应用一维自适应最大池化。</p>
<p>对于任何输入大小，输出大小均为 H。 输出要素的数量等于输入平面的数量。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>output_size</strong> –目标输出大小 H</p>
</li>
<li>
<p><strong>return_indices</strong> -如果<code>True</code>，则将返回索引以及输出。 传递给 nn.MaxUnpool1d 很有用。 默认值：<code>False</code></p>
</li>
</ul>
<p>Examples</p>
<pre><code>&gt;&gt;&gt; # target output size of 5
&gt;&gt;&gt; m = nn.AdaptiveMaxPool1d(5)
&gt;&gt;&gt; input = torch.randn(1, 64, 8)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="adaptivemaxpool2d">AdaptiveMaxPool2d</h3>
<hr />
<pre><code>class torch.nn.AdaptiveMaxPool2d(output_size, return_indices=False)¶
</code></pre>
<p>在由多个输入平面组成的输入信号上应用 2D 自适应最大池化。</p>
<p>对于任何输入大小，输出大小均为 H xW。 输出要素的数量等于输入平面的数量。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>output_size</strong> – H x W 形式的图像的目标输出大小。可以是元组(H，W），也可以是正方形图像 H x H 的单个 H。H 和 W 可以是 <code>int</code>或<code>None</code>表示大小与输入的大小相同。</p>
</li>
<li>
<p><strong>return_indices</strong> -如果<code>True</code>，则将返回索引以及输出。 传递给 nn.MaxUnpool2d 很有用。 默认值：<code>False</code></p>
</li>
</ul>
<p>Examples</p>
<pre><code>&gt;&gt;&gt; # target output size of 5x7
&gt;&gt;&gt; m = nn.AdaptiveMaxPool2d((5,7))
&gt;&gt;&gt; input = torch.randn(1, 64, 8, 9)
&gt;&gt;&gt; output = m(input)
&gt;&gt;&gt; # target output size of 7x7 (square)
&gt;&gt;&gt; m = nn.AdaptiveMaxPool2d(7)
&gt;&gt;&gt; input = torch.randn(1, 64, 10, 9)
&gt;&gt;&gt; output = m(input)
&gt;&gt;&gt; # target output size of 10x7
&gt;&gt;&gt; m = nn.AdaptiveMaxPool2d((None, 7))
&gt;&gt;&gt; input = torch.randn(1, 64, 10, 9)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="adaptivemaxpool3d">AdaptiveMaxPool3d</h3>
<hr />
<pre><code>class torch.nn.AdaptiveMaxPool3d(output_size, return_indices=False)¶
</code></pre>
<p>在由多个输入平面组成的输入信号上应用 3D 自适应最大池化。</p>
<p>对于任何输入大小，输出大小均为 D xH xW。 输出要素的数量等于输入平面的数量。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>output_size</strong> – D x H x W 形式的图像的目标输出尺寸。可以是一个元组(D，H，W），也可以是一个多维数据集 D x D x D 的单个 D。D， H 和 W 可以是<code>int</code>或<code>None</code>，这意味着大小将与输入的大小相同。</p>
</li>
<li>
<p><strong>return_indices</strong> -如果<code>True</code>，则将返回索引以及输出。 传递给 nn.MaxUnpool3d 很有用。 默认值：<code>False</code></p>
</li>
</ul>
<p>Examples</p>
<pre><code>&gt;&gt;&gt; # target output size of 5x7x9
&gt;&gt;&gt; m = nn.AdaptiveMaxPool3d((5,7,9))
&gt;&gt;&gt; input = torch.randn(1, 64, 8, 9, 10)
&gt;&gt;&gt; output = m(input)
&gt;&gt;&gt; # target output size of 7x7x7 (cube)
&gt;&gt;&gt; m = nn.AdaptiveMaxPool3d(7)
&gt;&gt;&gt; input = torch.randn(1, 64, 10, 9, 8)
&gt;&gt;&gt; output = m(input)
&gt;&gt;&gt; # target output size of 7x9x8
&gt;&gt;&gt; m = nn.AdaptiveMaxPool3d((7, None, None))
&gt;&gt;&gt; input = torch.randn(1, 64, 10, 9, 8)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="adaptiveavgpool1d">AdaptiveAvgPool1d</h3>
<hr />
<pre><code>class torch.nn.AdaptiveAvgPool1d(output_size)¶
</code></pre>
<p>在由多个输入平面组成的输入信号上应用一维自适应平均池。</p>
<p>The output size is H, for any input size. The number of output features is equal to the number of input planes.</p>
<p>Parameters</p>
<p><strong>output_size</strong> – the target output size H</p>
<p>Examples</p>
<pre><code>&gt;&gt;&gt; # target output size of 5
&gt;&gt;&gt; m = nn.AdaptiveAvgPool1d(5)
&gt;&gt;&gt; input = torch.randn(1, 64, 8)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="adaptiveavgpool2d">AdaptiveAvgPool2d</h3>
<hr />
<pre><code>class torch.nn.AdaptiveAvgPool2d(output_size)¶
</code></pre>
<p>在由多个输入平面组成的输入信号上应用 2D 自适应平均池。</p>
<p>The output is of size H x W, for any input size. The number of output features is equal to the number of input planes.</p>
<p>Parameters</p>
<p><strong>output_size</strong> – the target output size of the image of the form H x W. Can be a tuple (H, W) or a single H for a square image H x H. H and W can be either a <code>int</code>, or <code>None</code> which means the size will be the same as that of the input.</p>
<p>Examples</p>
<pre><code>&gt;&gt;&gt; # target output size of 5x7
&gt;&gt;&gt; m = nn.AdaptiveAvgPool2d((5,7))
&gt;&gt;&gt; input = torch.randn(1, 64, 8, 9)
&gt;&gt;&gt; output = m(input)
&gt;&gt;&gt; # target output size of 7x7 (square)
&gt;&gt;&gt; m = nn.AdaptiveAvgPool2d(7)
&gt;&gt;&gt; input = torch.randn(1, 64, 10, 9)
&gt;&gt;&gt; output = m(input)
&gt;&gt;&gt; # target output size of 10x7
&gt;&gt;&gt; m = nn.AdaptiveMaxPool2d((None, 7))
&gt;&gt;&gt; input = torch.randn(1, 64, 10, 9)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="adaptiveavgpool3d">AdaptiveAvgPool3d</h3>
<hr />
<pre><code>class torch.nn.AdaptiveAvgPool3d(output_size)¶
</code></pre>
<p>在由多个输入平面组成的输入信号上应用 3D 自适应平均池。</p>
<p>The output is of size D x H x W, for any input size. The number of output features is equal to the number of input planes.</p>
<p>Parameters</p>
<p><strong>output_size</strong> – D x H x W 形式的目标输出大小。可以是元组(D，H，W），也可以是多维数据集 D xD x D 的单个数字 D。D，H 和 W 可以是<code>int</code>或<code>None</code>，这意味着大小将与输入的大小相同。</p>
<p>Examples</p>
<pre><code>&gt;&gt;&gt; # target output size of 5x7x9
&gt;&gt;&gt; m = nn.AdaptiveAvgPool3d((5,7,9))
&gt;&gt;&gt; input = torch.randn(1, 64, 8, 9, 10)
&gt;&gt;&gt; output = m(input)
&gt;&gt;&gt; # target output size of 7x7x7 (cube)
&gt;&gt;&gt; m = nn.AdaptiveAvgPool3d(7)
&gt;&gt;&gt; input = torch.randn(1, 64, 10, 9, 8)
&gt;&gt;&gt; output = m(input)
&gt;&gt;&gt; # target output size of 7x9x8
&gt;&gt;&gt; m = nn.AdaptiveMaxPool3d((7, None, None))
&gt;&gt;&gt; input = torch.randn(1, 64, 10, 9, 8)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h2 id="_11">填充层</h2>
<h3 id="reflectionpad1d">ReflectionPad1d</h3>
<hr />
<pre><code>class torch.nn.ReflectionPad1d(padding)¶
</code></pre>
<p>使用输入边界的反射来填充输入张量。</p>
<p>对于 &lt;cite&gt;N&lt;/cite&gt; 维填充，请使用 <a href="nn.functional.html#torch.nn.functional.pad" title="torch.nn.functional.pad"><code>torch.nn.functional.pad()</code></a> 。</p>
<p>Parameters</p>
<p><strong>填充</strong> (<em>python：int</em> <em>，</em> <em>元组</em>）–填充的大小。 如果为 &lt;cite&gt;int&lt;/cite&gt; ，则在所有边界中使用相同的填充。 如果 2- &lt;cite&gt;元组&lt;/cite&gt;，则使用(<img alt="" src="../img/f4a9e75c6c83f2d041e9b7dbe42ca14a.jpg" />，<img alt="" src="../img/4fda6c4bdc8f5480493fb645b30005c0.jpg" />）</p>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>输入：<img alt="" src="../img/23913305314c2d86c6063bd9944df485.jpg" /></p>
</li>
<li>
<p>输出：<img alt="" src="../img/6540eac5cf9523bdf619bd561683d713.jpg" />其中</p>
<p><img alt="" src="../img/a69aa095b550ba751700aebe53044183.jpg" /></p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.ReflectionPad1d(2)
&gt;&gt;&gt; input = torch.arange(8, dtype=torch.float).reshape(1, 2, 4)
&gt;&gt;&gt; input
tensor([[[0., 1., 2., 3.],
         [4., 5., 6., 7.]]])
&gt;&gt;&gt; m(input)
tensor([[[2., 1., 0., 1., 2., 3., 2., 1.],
         [6., 5., 4., 5., 6., 7., 6., 5.]]])
&gt;&gt;&gt; # using different paddings for different sides
&gt;&gt;&gt; m = nn.ReflectionPad1d((3, 1))
&gt;&gt;&gt; m(input)
tensor([[[3., 2., 1., 0., 1., 2., 3., 2.],
         [7., 6., 5., 4., 5., 6., 7., 6.]]])

</code></pre>
<h3 id="reflectionpad2d">ReflectionPad2d</h3>
<hr />
<pre><code>class torch.nn.ReflectionPad2d(padding)¶
</code></pre>
<p>Pads the input tensor using the reflection of the input boundary.</p>
<p>For &lt;cite&gt;N&lt;/cite&gt;-dimensional padding, use <a href="nn.functional.html#torch.nn.functional.pad" title="torch.nn.functional.pad"><code>torch.nn.functional.pad()</code></a>.</p>
<p>Parameters</p>
<p><strong>填充</strong> (<em>python：int</em> <em>，</em> <em>元组</em>）–填充的大小。 如果为 &lt;cite&gt;int&lt;/cite&gt; ，则在所有边界中使用相同的填充。 如果是 4- &lt;cite&gt;元组&lt;/cite&gt;，则使用(<img alt="" src="../img/f4a9e75c6c83f2d041e9b7dbe42ca14a.jpg" />，<img alt="" src="../img/4fda6c4bdc8f5480493fb645b30005c0.jpg" />，<img alt="" src="../img/bf9f8dd668bdde9f3c23a90d3a57930f.jpg" />和<img alt="" src="../img/cfec41af85db79a5706503afae97a671.jpg" />）</p>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/55ac2d313b1b37ea5379f0f94f8d6982.jpg" /></p>
</li>
<li>
<p>输出：<img alt="" src="../img/bf0bebeaa2a77220349c5240a79ad798.jpg" />其中</p>
<p><img alt="" src="../img/192d4317c04bfa41672172fe8bea3dcc.jpg" /></p>
<p><img alt="" src="../img/a69aa095b550ba751700aebe53044183.jpg" /></p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.ReflectionPad2d(2)
&gt;&gt;&gt; input = torch.arange(9, dtype=torch.float).reshape(1, 1, 3, 3)
&gt;&gt;&gt; input
tensor([[[[0., 1., 2.],
          [3., 4., 5.],
          [6., 7., 8.]]]])
&gt;&gt;&gt; m(input)
tensor([[[[8., 7., 6., 7., 8., 7., 6.],
          [5., 4., 3., 4., 5., 4., 3.],
          [2., 1., 0., 1., 2., 1., 0.],
          [5., 4., 3., 4., 5., 4., 3.],
          [8., 7., 6., 7., 8., 7., 6.],
          [5., 4., 3., 4., 5., 4., 3.],
          [2., 1., 0., 1., 2., 1., 0.]]]])
&gt;&gt;&gt; # using different paddings for different sides
&gt;&gt;&gt; m = nn.ReflectionPad2d((1, 1, 2, 0))
&gt;&gt;&gt; m(input)
tensor([[[[7., 6., 7., 8., 7.],
          [4., 3., 4., 5., 4.],
          [1., 0., 1., 2., 1.],
          [4., 3., 4., 5., 4.],
          [7., 6., 7., 8., 7.]]]])

</code></pre>
<h3 id="1d_2">复制板 1d</h3>
<hr />
<pre><code>class torch.nn.ReplicationPad1d(padding)¶
</code></pre>
<p>使用输入边界的复制来填充输入张量。</p>
<p>For &lt;cite&gt;N&lt;/cite&gt;-dimensional padding, use <a href="nn.functional.html#torch.nn.functional.pad" title="torch.nn.functional.pad"><code>torch.nn.functional.pad()</code></a>.</p>
<p>Parameters</p>
<p><strong>padding</strong> (<em>python:int__,</em> <em>tuple</em>) – the size of the padding. If is &lt;cite&gt;int&lt;/cite&gt;, uses the same padding in all boundaries. If a 2-&lt;cite&gt;tuple&lt;/cite&gt;, uses (<img alt="" src="../img/f4a9e75c6c83f2d041e9b7dbe42ca14a.jpg" />, <img alt="" src="../img/4fda6c4bdc8f5480493fb645b30005c0.jpg" />)</p>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/23913305314c2d86c6063bd9944df485.jpg" /></p>
</li>
<li>
<p>Output: <img alt="" src="../img/6540eac5cf9523bdf619bd561683d713.jpg" /> where</p>
<p><img alt="" src="../img/a69aa095b550ba751700aebe53044183.jpg" /></p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.ReplicationPad1d(2)
&gt;&gt;&gt; input = torch.arange(8, dtype=torch.float).reshape(1, 2, 4)
&gt;&gt;&gt; input
tensor([[[0., 1., 2., 3.],
         [4., 5., 6., 7.]]])
&gt;&gt;&gt; m(input)
tensor([[[0., 0., 0., 1., 2., 3., 3., 3.],
         [4., 4., 4., 5., 6., 7., 7., 7.]]])
&gt;&gt;&gt; # using different paddings for different sides
&gt;&gt;&gt; m = nn.ReplicationPad1d((3, 1))
&gt;&gt;&gt; m(input)
tensor([[[0., 0., 0., 0., 1., 2., 3., 3.],
         [4., 4., 4., 4., 5., 6., 7., 7.]]])

</code></pre>
<h3 id="2d_3">复制板 2d</h3>
<hr />
<pre><code>class torch.nn.ReplicationPad2d(padding)¶
</code></pre>
<p>Pads the input tensor using replication of the input boundary.</p>
<p>For &lt;cite&gt;N&lt;/cite&gt;-dimensional padding, use <a href="nn.functional.html#torch.nn.functional.pad" title="torch.nn.functional.pad"><code>torch.nn.functional.pad()</code></a>.</p>
<p>Parameters</p>
<p><strong>padding</strong> (<em>python:int__,</em> <em>tuple</em>) – the size of the padding. If is &lt;cite&gt;int&lt;/cite&gt;, uses the same padding in all boundaries. If a 4-&lt;cite&gt;tuple&lt;/cite&gt;, uses (<img alt="" src="../img/f4a9e75c6c83f2d041e9b7dbe42ca14a.jpg" />, <img alt="" src="../img/4fda6c4bdc8f5480493fb645b30005c0.jpg" />, <img alt="" src="../img/bf9f8dd668bdde9f3c23a90d3a57930f.jpg" />, <img alt="" src="../img/cfec41af85db79a5706503afae97a671.jpg" />)</p>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/55ac2d313b1b37ea5379f0f94f8d6982.jpg" /></p>
</li>
<li>
<p>Output: <img alt="" src="../img/bf0bebeaa2a77220349c5240a79ad798.jpg" /> where</p>
<p><img alt="" src="../img/192d4317c04bfa41672172fe8bea3dcc.jpg" /></p>
<p><img alt="" src="../img/a69aa095b550ba751700aebe53044183.jpg" /></p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.ReplicationPad2d(2)
&gt;&gt;&gt; input = torch.arange(9, dtype=torch.float).reshape(1, 1, 3, 3)
&gt;&gt;&gt; input
tensor([[[[0., 1., 2.],
          [3., 4., 5.],
          [6., 7., 8.]]]])
&gt;&gt;&gt; m(input)
tensor([[[[0., 0., 0., 1., 2., 2., 2.],
          [0., 0., 0., 1., 2., 2., 2.],
          [0., 0., 0., 1., 2., 2., 2.],
          [3., 3., 3., 4., 5., 5., 5.],
          [6., 6., 6., 7., 8., 8., 8.],
          [6., 6., 6., 7., 8., 8., 8.],
          [6., 6., 6., 7., 8., 8., 8.]]]])
&gt;&gt;&gt; # using different paddings for different sides
&gt;&gt;&gt; m = nn.ReplicationPad2d((1, 1, 2, 0))
&gt;&gt;&gt; m(input)
tensor([[[[0., 0., 1., 2., 2.],
          [0., 0., 1., 2., 2.],
          [0., 0., 1., 2., 2.],
          [3., 3., 4., 5., 5.],
          [6., 6., 7., 8., 8.]]]])

</code></pre>
<h3 id="3d_2">复制板 3d</h3>
<hr />
<pre><code>class torch.nn.ReplicationPad3d(padding)¶
</code></pre>
<p>Pads the input tensor using replication of the input boundary.</p>
<p>For &lt;cite&gt;N&lt;/cite&gt;-dimensional padding, use <a href="nn.functional.html#torch.nn.functional.pad" title="torch.nn.functional.pad"><code>torch.nn.functional.pad()</code></a>.</p>
<p>Parameters</p>
<p><strong>填充</strong> (<em>python：int</em> <em>，</em> <em>元组</em>）–填充的大小。 如果为 &lt;cite&gt;int&lt;/cite&gt; ，则在所有边界中使用相同的填充。 如果是 6-&lt;cite&gt;元组&lt;/cite&gt;，则使用(<img alt="" src="../img/f4a9e75c6c83f2d041e9b7dbe42ca14a.jpg" />，<img alt="" src="../img/4fda6c4bdc8f5480493fb645b30005c0.jpg" />，<img alt="" src="../img/bf9f8dd668bdde9f3c23a90d3a57930f.jpg" />，<img alt="" src="../img/cfec41af85db79a5706503afae97a671.jpg" />，<img alt="" src="../img/63c78e70029d32b9d2b08c55fa632d97.jpg" />，<img alt="" src="../img/dae815c89460555554170bf0ca9ddf0a.jpg" />）</p>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/fc79fbdaec72bad3be239985d50f2f7c.jpg" /></p>
</li>
<li>
<p>输出：<img alt="" src="../img/56dd521f825f0c201df32048c532fd13.jpg" />其中</p>
<p><img alt="" src="../img/76aabf68fba62ce0f2007d97ed8c6a22.jpg" /></p>
<p><img alt="" src="../img/192d4317c04bfa41672172fe8bea3dcc.jpg" /></p>
<p><img alt="" src="../img/a69aa095b550ba751700aebe53044183.jpg" /></p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.ReplicationPad3d(3)
&gt;&gt;&gt; input = torch.randn(16, 3, 8, 320, 480)
&gt;&gt;&gt; output = m(input)
&gt;&gt;&gt; # using different paddings for different sides
&gt;&gt;&gt; m = nn.ReplicationPad3d((3, 3, 6, 6, 1, 1))
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="zeropad2d">ZeroPad2d</h3>
<hr />
<pre><code>class torch.nn.ZeroPad2d(padding)¶
</code></pre>
<p>用零填充输入张量边界。</p>
<p>For &lt;cite&gt;N&lt;/cite&gt;-dimensional padding, use <a href="nn.functional.html#torch.nn.functional.pad" title="torch.nn.functional.pad"><code>torch.nn.functional.pad()</code></a>.</p>
<p>Parameters</p>
<p><strong>padding</strong> (<em>python:int__,</em> <em>tuple</em>) – the size of the padding. If is &lt;cite&gt;int&lt;/cite&gt;, uses the same padding in all boundaries. If a 4-&lt;cite&gt;tuple&lt;/cite&gt;, uses (<img alt="" src="../img/f4a9e75c6c83f2d041e9b7dbe42ca14a.jpg" />, <img alt="" src="../img/4fda6c4bdc8f5480493fb645b30005c0.jpg" />, <img alt="" src="../img/bf9f8dd668bdde9f3c23a90d3a57930f.jpg" />, <img alt="" src="../img/cfec41af85db79a5706503afae97a671.jpg" />)</p>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/55ac2d313b1b37ea5379f0f94f8d6982.jpg" /></p>
</li>
<li>
<p>Output: <img alt="" src="../img/bf0bebeaa2a77220349c5240a79ad798.jpg" /> where</p>
<p><img alt="" src="../img/192d4317c04bfa41672172fe8bea3dcc.jpg" /></p>
<p><img alt="" src="../img/a69aa095b550ba751700aebe53044183.jpg" /></p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.ZeroPad2d(2)
&gt;&gt;&gt; input = torch.randn(1, 1, 3, 3)
&gt;&gt;&gt; input
tensor([[[[-0.1678, -0.4418,  1.9466],
          [ 0.9604, -0.4219, -0.5241],
          [-0.9162, -0.5436, -0.6446]]]])
&gt;&gt;&gt; m(input)
tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000, -0.1678, -0.4418,  1.9466,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.9604, -0.4219, -0.5241,  0.0000,  0.0000],
          [ 0.0000,  0.0000, -0.9162, -0.5436, -0.6446,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]])
&gt;&gt;&gt; # using different paddings for different sides
&gt;&gt;&gt; m = nn.ZeroPad2d((1, 1, 2, 0))
&gt;&gt;&gt; m(input)
tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
          [ 0.0000, -0.1678, -0.4418,  1.9466,  0.0000],
          [ 0.0000,  0.9604, -0.4219, -0.5241,  0.0000],
          [ 0.0000, -0.9162, -0.5436, -0.6446,  0.0000]]]])

</code></pre>
<h3 id="constantpad1d">ConstantPad1d</h3>
<hr />
<pre><code>class torch.nn.ConstantPad1d(padding, value)¶
</code></pre>
<p>用恒定值填充输入张量边界。</p>
<p>For &lt;cite&gt;N&lt;/cite&gt;-dimensional padding, use <a href="nn.functional.html#torch.nn.functional.pad" title="torch.nn.functional.pad"><code>torch.nn.functional.pad()</code></a>.</p>
<p>Parameters</p>
<p><strong>填充</strong> (<em>python：int</em> <em>，</em> <em>元组</em>）–填充的大小。 如果为 &lt;cite&gt;int&lt;/cite&gt; ，则在两个边界中使用相同的填充。 如果 2- &lt;cite&gt;元组&lt;/cite&gt;，则使用(<img alt="" src="../img/f4a9e75c6c83f2d041e9b7dbe42ca14a.jpg" />，<img alt="" src="../img/4fda6c4bdc8f5480493fb645b30005c0.jpg" />）</p>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/23913305314c2d86c6063bd9944df485.jpg" /></p>
</li>
<li>
<p>Output: <img alt="" src="../img/6540eac5cf9523bdf619bd561683d713.jpg" /> where</p>
<p><img alt="" src="../img/a69aa095b550ba751700aebe53044183.jpg" /></p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.ConstantPad1d(2, 3.5)
&gt;&gt;&gt; input = torch.randn(1, 2, 4)
&gt;&gt;&gt; input
tensor([[[-1.0491, -0.7152, -0.0749,  0.8530],
         [-1.3287,  1.8966,  0.1466, -0.2771]]])
&gt;&gt;&gt; m(input)
tensor([[[ 3.5000,  3.5000, -1.0491, -0.7152, -0.0749,  0.8530,  3.5000,
           3.5000],
         [ 3.5000,  3.5000, -1.3287,  1.8966,  0.1466, -0.2771,  3.5000,
           3.5000]]])
&gt;&gt;&gt; m = nn.ConstantPad1d(2, 3.5)
&gt;&gt;&gt; input = torch.randn(1, 2, 3)
&gt;&gt;&gt; input
tensor([[[ 1.6616,  1.4523, -1.1255],
         [-3.6372,  0.1182, -1.8652]]])
&gt;&gt;&gt; m(input)
tensor([[[ 3.5000,  3.5000,  1.6616,  1.4523, -1.1255,  3.5000,  3.5000],
         [ 3.5000,  3.5000, -3.6372,  0.1182, -1.8652,  3.5000,  3.5000]]])
&gt;&gt;&gt; # using different paddings for different sides
&gt;&gt;&gt; m = nn.ConstantPad1d((3, 1), 3.5)
&gt;&gt;&gt; m(input)
tensor([[[ 3.5000,  3.5000,  3.5000,  1.6616,  1.4523, -1.1255,  3.5000],
         [ 3.5000,  3.5000,  3.5000, -3.6372,  0.1182, -1.8652,  3.5000]]])

</code></pre>
<h3 id="constantpad2d">ConstantPad2d</h3>
<hr />
<pre><code>class torch.nn.ConstantPad2d(padding, value)¶
</code></pre>
<p>Pads the input tensor boundaries with a constant value.</p>
<p>For &lt;cite&gt;N&lt;/cite&gt;-dimensional padding, use <a href="nn.functional.html#torch.nn.functional.pad" title="torch.nn.functional.pad"><code>torch.nn.functional.pad()</code></a>.</p>
<p>Parameters</p>
<p><strong>padding</strong> (<em>python:int__,</em> <em>tuple</em>) – the size of the padding. If is &lt;cite&gt;int&lt;/cite&gt;, uses the same padding in all boundaries. If a 4-&lt;cite&gt;tuple&lt;/cite&gt;, uses (<img alt="" src="../img/f4a9e75c6c83f2d041e9b7dbe42ca14a.jpg" />, <img alt="" src="../img/4fda6c4bdc8f5480493fb645b30005c0.jpg" />, <img alt="" src="../img/bf9f8dd668bdde9f3c23a90d3a57930f.jpg" />, <img alt="" src="../img/cfec41af85db79a5706503afae97a671.jpg" />)</p>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/55ac2d313b1b37ea5379f0f94f8d6982.jpg" /></p>
</li>
<li>
<p>Output: <img alt="" src="../img/bf0bebeaa2a77220349c5240a79ad798.jpg" /> where</p>
<p><img alt="" src="../img/192d4317c04bfa41672172fe8bea3dcc.jpg" /></p>
<p><img alt="" src="../img/a69aa095b550ba751700aebe53044183.jpg" /></p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.ConstantPad2d(2, 3.5)
&gt;&gt;&gt; input = torch.randn(1, 2, 2)
&gt;&gt;&gt; input
tensor([[[ 1.6585,  0.4320],
         [-0.8701, -0.4649]]])
&gt;&gt;&gt; m(input)
tensor([[[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],
         [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],
         [ 3.5000,  3.5000,  1.6585,  0.4320,  3.5000,  3.5000],
         [ 3.5000,  3.5000, -0.8701, -0.4649,  3.5000,  3.5000],
         [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],
         [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000]]])
&gt;&gt;&gt; # using different paddings for different sides
&gt;&gt;&gt; m = nn.ConstantPad2d((3, 0, 2, 1), 3.5)
&gt;&gt;&gt; m(input)
tensor([[[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000],
         [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000],
         [ 3.5000,  3.5000,  3.5000,  1.6585,  0.4320],
         [ 3.5000,  3.5000,  3.5000, -0.8701, -0.4649],
         [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000]]])

</code></pre>
<h3 id="constantpad3d">ConstantPad3d</h3>
<hr />
<pre><code>class torch.nn.ConstantPad3d(padding, value)¶
</code></pre>
<p>Pads the input tensor boundaries with a constant value.</p>
<p>For &lt;cite&gt;N&lt;/cite&gt;-dimensional padding, use <a href="nn.functional.html#torch.nn.functional.pad" title="torch.nn.functional.pad"><code>torch.nn.functional.pad()</code></a>.</p>
<p>Parameters</p>
<p><strong>padding</strong> (<em>python:int__,</em> <em>tuple</em>) – the size of the padding. If is &lt;cite&gt;int&lt;/cite&gt;, uses the same padding in all boundaries. If a 6-&lt;cite&gt;tuple&lt;/cite&gt;, uses (<img alt="" src="../img/f4a9e75c6c83f2d041e9b7dbe42ca14a.jpg" />, <img alt="" src="../img/4fda6c4bdc8f5480493fb645b30005c0.jpg" />, <img alt="" src="../img/bf9f8dd668bdde9f3c23a90d3a57930f.jpg" />, <img alt="" src="../img/cfec41af85db79a5706503afae97a671.jpg" />, <img alt="" src="../img/63c78e70029d32b9d2b08c55fa632d97.jpg" />, <img alt="" src="../img/dae815c89460555554170bf0ca9ddf0a.jpg" />)</p>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/fc79fbdaec72bad3be239985d50f2f7c.jpg" /></p>
</li>
<li>
<p>Output: <img alt="" src="../img/56dd521f825f0c201df32048c532fd13.jpg" /> where</p>
<p><img alt="" src="../img/76aabf68fba62ce0f2007d97ed8c6a22.jpg" /></p>
<p><img alt="" src="../img/192d4317c04bfa41672172fe8bea3dcc.jpg" /></p>
<p><img alt="" src="../img/a69aa095b550ba751700aebe53044183.jpg" /></p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.ConstantPad3d(3, 3.5)
&gt;&gt;&gt; input = torch.randn(16, 3, 10, 20, 30)
&gt;&gt;&gt; output = m(input)
&gt;&gt;&gt; # using different paddings for different sides
&gt;&gt;&gt; m = nn.ConstantPad3d((3, 3, 6, 6, 0, 1), 3.5)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h2 id="_12">非线性激活(加权和，非线性）</h2>
<h3 id="elu">ELU</h3>
<hr />
<pre><code>class torch.nn.ELU(alpha=1.0, inplace=False)¶
</code></pre>
<p>应用逐元素函数：</p>
<p><img alt="" src="../img/c5e7c03fee8917faedb7fcf2fef1a593.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>alpha</strong> – ELU 公式的<img alt="" src="../img/5b9866fb35b01c553ed3e738e3972ae9.jpg" />值。 默认值：1.0</p>
</li>
<li>
<p><strong>就地</strong> –可以选择就地进行操作。 默认值：<code>False</code></p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>输入：<img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" />其中 &lt;cite&gt;*&lt;/cite&gt; 表示任意数量的附加尺寸</p>
</li>
<li>
<p>输出：<img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" />，形状与输入相同</p>
</li>
</ul>
<p><img alt="_images/ELU.png" src="../img/44ab96bef64fc433ab24751d5c30526e.jpg" /></p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.ELU()
&gt;&gt;&gt; input = torch.randn(2)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="_13">硬收缩</h3>
<hr />
<pre><code>class torch.nn.Hardshrink(lambd=0.5)¶
</code></pre>
<p>逐个应用硬收缩功能：</p>
<p><img alt="" src="../img/97f440ec36daa2093e7e7aca910b0819.jpg" /></p>
<p>Parameters</p>
<p><strong>lambd</strong> – Hardshrink 配方的<img alt="" src="../img/ace98fffbe21761d9b8a0b4aff240ff7.jpg" />值。 默认值：0.5</p>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" /> where &lt;cite&gt;*&lt;/cite&gt; means, any number of additional dimensions</p>
</li>
<li>
<p>Output: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" />, same shape as the input</p>
</li>
</ul>
<p><img alt="_images/Hardshrink.png" src="../img/955b766439e8a312baa729ee3b76c621.jpg" /></p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.Hardshrink()
&gt;&gt;&gt; input = torch.randn(2)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="_14">哈丹</h3>
<hr />
<pre><code>class torch.nn.Hardtanh(min_val=-1.0, max_val=1.0, inplace=False, min_value=None, max_value=None)¶
</code></pre>
<p>逐个应用 HardTanh 函数</p>
<p>HardTanh 定义为：</p>
<p><img alt="" src="../img/1496ab2871db25542e024baf4718d64b.jpg" /></p>
<p>线性区域<img alt="" src="../img/40aaae477f686a7f99e525e03e575c65.jpg" />的范围可以使用<code>min_val</code>和<code>max_val</code>进行调整。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>min_val</strong> –线性区域范围的最小值。 默认值：-1</p>
</li>
<li>
<p><strong>max_val</strong> –线性区域范围的最大值。 默认值：1</p>
</li>
<li>
<p><strong>inplace</strong> – can optionally do the operation in-place. Default: <code>False</code></p>
</li>
</ul>
<p>不推荐使用关键字参数<code>min_value</code>和<code>max_value</code>，而推荐使用<code>min_val</code>和<code>max_val</code>。</p>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" /> where &lt;cite&gt;*&lt;/cite&gt; means, any number of additional dimensions</p>
</li>
<li>
<p>Output: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" />, same shape as the input</p>
</li>
</ul>
<p><img alt="_images/Hardtanh.png" src="../img/79662b90ae58a9359051e5032482bb4c.jpg" /></p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.Hardtanh(-2, 2)
&gt;&gt;&gt; input = torch.randn(2)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="_15">漏尿</h3>
<hr />
<pre><code>class torch.nn.LeakyReLU(negative_slope=0.01, inplace=False)¶
</code></pre>
<p>Applies the element-wise function:</p>
<p><img alt="" src="../img/1455d3fb2de3b83766fb59cff140f6ec.jpg" /></p>
<p>要么</p>
<p><img alt="" src="../img/1a65bee2740264a1fc72dab804d625d3.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>negative_slope</strong> –控制负斜率的角度。 默认值：1e-2</p>
</li>
<li>
<p><strong>inplace</strong> – can optionally do the operation in-place. Default: <code>False</code></p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" /> where &lt;cite&gt;*&lt;/cite&gt; means, any number of additional dimensions</p>
</li>
<li>
<p>Output: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" />, same shape as the input</p>
</li>
</ul>
<p><img alt="_images/LeakyReLU.png" src="../img/54a37bc88308e141c9078705de017c83.jpg" /></p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.LeakyReLU(0.1)
&gt;&gt;&gt; input = torch.randn(2)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="logsigmoid">LogSigmoid</h3>
<hr />
<pre><code>class torch.nn.LogSigmoid¶
</code></pre>
<p>Applies the element-wise function:</p>
<p><img alt="" src="../img/35ec278c9221eb391ddb68f4da554f19.jpg" /></p>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" /> where &lt;cite&gt;*&lt;/cite&gt; means, any number of additional dimensions</p>
</li>
<li>
<p>Output: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" />, same shape as the input</p>
</li>
</ul>
<p><img alt="_images/LogSigmoid.png" src="../img/7d5aeb9eec680aa700989609a7a7b7e1.jpg" /></p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.LogSigmoid()
&gt;&gt;&gt; input = torch.randn(2)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="_16">多头注意力</h3>
<hr />
<pre><code>class torch.nn.MultiheadAttention(embed_dim, num_heads, dropout=0.0, bias=True, add_bias_kv=False, add_zero_attn=False, kdim=None, vdim=None)¶
</code></pre>
<p>允许模型共同关注来自不同表示子空间的信息。 请参阅参考：注意就是您所需要的</p>
<p><img alt="" src="../img/499ed10e7f6255ef2a2300ad36d10775.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>embed_dim</strong> -模型的总尺寸。</p>
</li>
<li>
<p><strong>num_heads</strong> –平行注意头。</p>
</li>
<li>
<p><strong>dropout</strong> – attn_output_weights 上的 Dropout 层。 默认值：0.0</p>
</li>
<li>
<p><strong>偏置</strong> –将偏置添加为模块参数。 默认值：True。</p>
</li>
<li>
<p><strong>add_bias_kv</strong> –将偏差添加到键和值序列的 dim = 0。</p>
</li>
<li>
<p><strong>add_zero_attn</strong> –将新一批零添加到调暗值为 1 的键和值序列。</p>
</li>
<li>
<p><strong>kdim</strong> -密钥中的功能总数。 默认值：无。</p>
</li>
<li>
<p><strong>vdim</strong> -密钥中的功能总数。 默认值：无。</p>
</li>
<li>
<p><strong>注意</strong> –如果 kdim 和 vdim 为 None，则将它们设置为 embed_dim，以便</p>
</li>
<li>
<p><strong>键和值具有相同数量的功能。</strong> (<em>查询</em> <em>，</em>）–</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)
&gt;&gt;&gt; attn_output, attn_output_weights = multihead_attn(query, key, value)

</code></pre>
<hr />
<pre><code>forward(query, key, value, key_padding_mask=None, need_weights=True, attn_mask=None)¶
</code></pre>
<p>Parameters</p>
<ul>
<li>
<p><strong>键，值</strong>(<em>查询</em> <em>，</em>）–将查询和一组键值对映射到输出。 有关更多详细信息，请参见“注意就是全部”。</p>
</li>
<li>
<p><strong>key_padding_mask</strong> –如果提供，则将忽略按键中指定的填充元素。 这是一个二进制掩码。 当值为 True 时，注意层上的相应值将用-inf 填充。</p>
</li>
<li>
<p><strong>need_weights</strong> -输出 attn_output_weights。</p>
</li>
<li>
<p><strong>attn_mask</strong> –防止注意某些位置的遮罩。 这是一个附加蒙版(即这些值将添加到关注层）。</p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>输入：</p>
</li>
<li>
<p>查询：<img alt="" src="../img/529ccb8235be5ce651604b73a85dde69.jpg" />其中 L 是目标序列长度，N 是批处理大小，E 是嵌入维数。</p>
</li>
<li>
<p>密钥：<img alt="" src="../img/5dc2d865ca031350cad0cde77d835371.jpg" />，其中 S 是源序列长度，N 是批处理大小，E 是嵌入维数。</p>
</li>
<li>
<p>值：<img alt="" src="../img/5dc2d865ca031350cad0cde77d835371.jpg" />其中 S 是源序列长度，N 是批处理大小，E 是嵌入维数。</p>
</li>
<li>
<p>key_padding_mask：<img alt="" src="../img/8fd771cabec0d2f2ff6ac3992568c469.jpg" />，ByteTensor，其中 N 是批处理大小，S 是源序列长度。</p>
</li>
<li>
<p>attn_mask：<img alt="" src="../img/f0e09889f424c9b3b8faca32118289c1.jpg" />其中 L 是目标序列长度，S 是源序列长度。</p>
</li>
<li>
<p>输出：</p>
</li>
<li>
<p>attn_output：<img alt="" src="../img/529ccb8235be5ce651604b73a85dde69.jpg" />其中 L 是目标序列长度，N 是批处理大小，E 是嵌入维数。</p>
</li>
<li>
<p>attn_output_weights：<img alt="" src="../img/5752622b6a92ce86bbe3b9d2969a9d66.jpg" />其中 N 是批处理大小，L 是目标序列长度，S 是源序列长度。</p>
</li>
</ul>
<h3 id="_17">预备</h3>
<hr />
<pre><code>class torch.nn.PReLU(num_parameters=1, init=0.25)¶
</code></pre>
<p>Applies the element-wise function:</p>
<p><img alt="" src="../img/5135325fd5b0905bf88d37741bdc5da4.jpg" /></p>
<p>or</p>
<p><img alt="" src="../img/025c3a9fa9a97792e8e5a4c8012b8825.jpg" /></p>
<p>此处<img alt="" src="../img/48ac074c6a0b5bcede0089a5f966a70c.jpg" />是可学习的参数。 当不带参数调用时， &lt;cite&gt;nn.PReLU(）&lt;/cite&gt;在所有输入通道上使用单个参数<img alt="" src="../img/48ac074c6a0b5bcede0089a5f966a70c.jpg" />。 如果使用 &lt;cite&gt;nn.PReLU(nChannels）&lt;/cite&gt;进行调用，则每个输入通道将使用单独的<img alt="" src="../img/48ac074c6a0b5bcede0089a5f966a70c.jpg" />。</p>
<p>Note</p>
<p>学习<img alt="" src="../img/48ac074c6a0b5bcede0089a5f966a70c.jpg" />以获得良好性能时，不应使用重量衰减。</p>
<p>Note</p>
<p>通道暗淡是输入的第二暗淡。 当输入的亮度为&lt; 2 时，则不存在通道的亮度，并且通道数= 1。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>num_parameters</strong>  (<em>python：int</em> )–要学习的<img alt="" src="../img/48ac074c6a0b5bcede0089a5f966a70c.jpg" />的数量。 尽管将 int 作为输入，但是只有两个值是合法的：1，即输入的通道数。 默认值：1</p>
</li>
<li>
<p><strong>初始</strong> (<em>python：float</em> )– <img alt="" src="../img/48ac074c6a0b5bcede0089a5f966a70c.jpg" />的初始值。 默认值：0.25</p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" /> where &lt;cite&gt;*&lt;/cite&gt; means, any number of additional dimensions</p>
</li>
<li>
<p>Output: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" />, same shape as the input</p>
</li>
</ul>
<pre><code>Variables
</code></pre>
<p><strong>〜PReLU.weight</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–可学习的形状权重(<code>num_parameters</code>）。</p>
<p><img alt="_images/PReLU.png" src="../img/4e58e5d3566e1be273a835a64712eaaf.jpg" /></p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.PReLU()
&gt;&gt;&gt; input = torch.randn(2)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="relu">ReLU</h3>
<hr />
<pre><code>class torch.nn.ReLU(inplace=False)¶
</code></pre>
<p>将整流的线性单位函数按元素应用：</p>
<p><img alt="" src="../img/479d7406d924fedf21aa7a57172c69a3.jpg" /></p>
<p>Parameters</p>
<p><strong>inplace</strong> – can optionally do the operation in-place. Default: <code>False</code></p>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" /> where &lt;cite&gt;*&lt;/cite&gt; means, any number of additional dimensions</p>
</li>
<li>
<p>Output: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" />, same shape as the input</p>
</li>
</ul>
<p><img alt="_images/ReLU.png" src="../img/6310010257e2eb7e6a37ce9f0d7fc4ed.jpg" /></p>
<p>Examples:</p>
<pre><code>  &gt;&gt;&gt; m = nn.ReLU()
  &gt;&gt;&gt; input = torch.randn(2)
  &gt;&gt;&gt; output = m(input)

An implementation of CReLU - https://arxiv.org/abs/1603.05201

  &gt;&gt;&gt; m = nn.ReLU()
  &gt;&gt;&gt; input = torch.randn(2).unsqueeze(0)
  &gt;&gt;&gt; output = torch.cat((m(input),m(-input)))

</code></pre>
<h3 id="relu6">ReLU6</h3>
<hr />
<pre><code>class torch.nn.ReLU6(inplace=False)¶
</code></pre>
<p>Applies the element-wise function:</p>
<p><img alt="" src="../img/791d1d9f00f4b34f864836e5cefa49a4.jpg" /></p>
<p>Parameters</p>
<p><strong>inplace</strong> – can optionally do the operation in-place. Default: <code>False</code></p>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" /> where &lt;cite&gt;*&lt;/cite&gt; means, any number of additional dimensions</p>
</li>
<li>
<p>Output: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" />, same shape as the input</p>
</li>
</ul>
<p><img alt="_images/ReLU6.png" src="../img/12c6704779de4a524de8bf42f5796287.jpg" /></p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.ReLU6()
&gt;&gt;&gt; input = torch.randn(2)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="rrelu">RReLU</h3>
<hr />
<pre><code>class torch.nn.RReLU(lower=0.125, upper=0.3333333333333333, inplace=False)¶
</code></pre>
<p>如本文所述，按元素应用随机泄漏的整流衬套单元功能：</p>
<p><a href="https://arxiv.org/abs/1505.00853">卷积网络</a>中修正激活的经验评估。</p>
<p>该函数定义为：</p>
<p><img alt="" src="../img/3c7ed25ca4cafb4784b7bcbc43321fb3.jpg" /></p>
<p>其中<img alt="" src="../img/48ac074c6a0b5bcede0089a5f966a70c.jpg" />是从均匀分布<img alt="" src="../img/33de3619232320be3d5a0ef586a503bb.jpg" />中随机抽样的。</p>
<blockquote>
<p>参见： <a href="https://arxiv.org/pdf/1505.00853.pdf">https://arxiv.org/pdf/1505.00853.pdf</a></p>
</blockquote>
<p>Parameters</p>
<ul>
<li>
<p><strong>下</strong>-均匀分布的下限。 默认值：<img alt="" src="../img/7fdbd374a4af4cd940f7107ed36f1831.jpg" /></p>
</li>
<li>
<p><strong>上限</strong> –均匀分布的上限。 默认值：<img alt="" src="../img/f15b0cd379070f938858329a150dc2c1.jpg" /></p>
</li>
<li>
<p><strong>inplace</strong> – can optionally do the operation in-place. Default: <code>False</code></p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" /> where &lt;cite&gt;*&lt;/cite&gt; means, any number of additional dimensions</p>
</li>
<li>
<p>Output: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" />, same shape as the input</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.RReLU(0.1, 0.3)
&gt;&gt;&gt; input = torch.randn(2)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="selu">SELU</h3>
<hr />
<pre><code>class torch.nn.SELU(inplace=False)¶
</code></pre>
<p>按元素应用，例如：</p>
<p><img alt="" src="../img/91588fdcae8d4d29dd9ef21216071e54.jpg" /></p>
<p><img alt="" src="../img/76c85d438f026f3cf39b2f3e52103def.jpg" />和<img alt="" src="../img/51f95b16eb87d1a6b48c4869ab574f6f.jpg" />。</p>
<p>更多细节可以在论文<a href="https://arxiv.org/abs/1706.02515">自归一化神经网络</a>中找到。</p>
<p>Parameters</p>
<p><strong>原位</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–可以选择就地进行操作。 默认值：<code>False</code></p>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" /> where &lt;cite&gt;*&lt;/cite&gt; means, any number of additional dimensions</p>
</li>
<li>
<p>Output: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" />, same shape as the input</p>
</li>
</ul>
<p><img alt="_images/SELU.png" src="../img/4f63dbc1f0a463ef9b06d78891cb8ed5.jpg" /></p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.SELU()
&gt;&gt;&gt; input = torch.randn(2)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="_18">中央图书馆</h3>
<hr />
<pre><code>class torch.nn.CELU(alpha=1.0, inplace=False)¶
</code></pre>
<p>Applies the element-wise function:</p>
<p><img alt="" src="../img/13b33520ebfb250b602a998c0d61e98a.jpg" /></p>
<p>可以在论文<a href="https://arxiv.org/abs/1704.07483">连续微分指数线性单位</a>中找到更多详细信息。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>alpha</strong> – CELU 配方的<img alt="" src="../img/5b9866fb35b01c553ed3e738e3972ae9.jpg" />值。 默认值：1.0</p>
</li>
<li>
<p><strong>inplace</strong> – can optionally do the operation in-place. Default: <code>False</code></p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" /> where &lt;cite&gt;*&lt;/cite&gt; means, any number of additional dimensions</p>
</li>
<li>
<p>Output: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" />, same shape as the input</p>
</li>
</ul>
<p><img alt="_images/CELU.png" src="../img/540d61b63f5e733ecfdb9e24f282b23a.jpg" /></p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.CELU()
&gt;&gt;&gt; input = torch.randn(2)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="_19">格鲁</h3>
<hr />
<pre><code>class torch.nn.GELU¶
</code></pre>
<p>应用高斯误差线性单位函数：</p>
<p><img alt="" src="../img/0bb43555b9ea300611320dc14864a6ec.jpg" /></p>
<p>其中<img alt="" src="../img/f4dfad219d70d8ecfc8583097735d64b.jpg" />是高斯分布的累积分布函数。</p>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" /> where &lt;cite&gt;*&lt;/cite&gt; means, any number of additional dimensions</p>
</li>
<li>
<p>Output: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" />, same shape as the input</p>
</li>
</ul>
<p><img alt="_images/GELU.png" src="../img/54df9c84472b090d06682197dc413624.jpg" /></p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.GELU()
&gt;&gt;&gt; input = torch.randn(2)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="_20">乙状结肠</h3>
<hr />
<pre><code>class torch.nn.Sigmoid¶
</code></pre>
<p>Applies the element-wise function:</p>
<p><img alt="" src="../img/1d9f97ae665ccb45e178f2242fca850b.jpg" /></p>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" /> where &lt;cite&gt;*&lt;/cite&gt; means, any number of additional dimensions</p>
</li>
<li>
<p>Output: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" />, same shape as the input</p>
</li>
</ul>
<p><img alt="_images/Sigmoid.png" src="../img/d792043841736dd9735e88dc99553129.jpg" /></p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.Sigmoid()
&gt;&gt;&gt; input = torch.randn(2)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="_21">软加</h3>
<hr />
<pre><code>class torch.nn.Softplus(beta=1, threshold=20)¶
</code></pre>
<p>Applies the element-wise function:</p>
<p><img alt="" src="../img/0cdc02b5fd2f1ce168240ec6b26d8818.jpg" /></p>
<p>SoftPlus 是 ReLU 函数的平滑近似，可用于将机器的输出约束为始终为正。</p>
<p>为了获得数值稳定性，对于超过一定值的输入，实现将恢复为线性函数。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>beta</strong> – Softplus 制剂的<img alt="" src="../img/53a496ec7d546e2af9595a7055dd6a7e.jpg" />值。 默认值：1</p>
</li>
<li>
<p><strong>阈值</strong> –高于此阈值的值恢复为线性函数。 默认值：20</p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" /> where &lt;cite&gt;*&lt;/cite&gt; means, any number of additional dimensions</p>
</li>
<li>
<p>Output: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" />, same shape as the input</p>
</li>
</ul>
<p><img alt="_images/Softplus.png" src="../img/eeaae7ea7b1fdbe29ec22c49189a71a3.jpg" /></p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.Softplus()
&gt;&gt;&gt; input = torch.randn(2)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="_22">软缩</h3>
<hr />
<pre><code>class torch.nn.Softshrink(lambd=0.5)¶
</code></pre>
<p>逐个应用软收缩功能：</p>
<p><img alt="" src="../img/3ad8b87218010f71d5c2aa1c57d9664a.jpg" /></p>
<p>Parameters</p>
<p><strong>lambd</strong> –软收缩配方的<img alt="" src="../img/ace98fffbe21761d9b8a0b4aff240ff7.jpg" />值。 默认值：0.5</p>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" /> where &lt;cite&gt;*&lt;/cite&gt; means, any number of additional dimensions</p>
</li>
<li>
<p>Output: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" />, same shape as the input</p>
</li>
</ul>
<p><img alt="_images/Softshrink.png" src="../img/de8ec7b51032ad8a91d16f6c239d7495.jpg" /></p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.Softshrink()
&gt;&gt;&gt; input = torch.randn(2)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="_23">软签</h3>
<hr />
<pre><code>class torch.nn.Softsign¶
</code></pre>
<p>Applies the element-wise function:</p>
<p><img alt="" src="../img/f9488363d2896ddc39fa6bc32c76774e.jpg" /></p>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" /> where &lt;cite&gt;*&lt;/cite&gt; means, any number of additional dimensions</p>
</li>
<li>
<p>Output: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" />, same shape as the input</p>
</li>
</ul>
<p><img alt="_images/Softsign.png" src="../img/5e25d0eab65ec475300b5782bb557c58.jpg" /></p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.Softsign()
&gt;&gt;&gt; input = torch.randn(2)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="h">h</h3>
<hr />
<pre><code>class torch.nn.Tanh¶
</code></pre>
<p>Applies the element-wise function:</p>
<p><img alt="" src="../img/04d80416f9caea1021bba208ab72c20d.jpg" /></p>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" /> where &lt;cite&gt;*&lt;/cite&gt; means, any number of additional dimensions</p>
</li>
<li>
<p>Output: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" />, same shape as the input</p>
</li>
</ul>
<p><img alt="_images/Tanh.png" src="../img/e63af8d38904626d01a0dcd358bad9b3.jpg" /></p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.Tanh()
&gt;&gt;&gt; input = torch.randn(2)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="tanhshrink">Tanhshrink</h3>
<hr />
<pre><code>class torch.nn.Tanhshrink¶
</code></pre>
<p>Applies the element-wise function:</p>
<p><img alt="" src="../img/d528e71d69503f997fe4c30a8c54be58.jpg" /></p>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" /> where &lt;cite&gt;*&lt;/cite&gt; means, any number of additional dimensions</p>
</li>
<li>
<p>Output: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" />, same shape as the input</p>
</li>
</ul>
<p><img alt="_images/Tanhshrink.png" src="../img/b99c5a67856b080f52ffe8518f08aea0.jpg" /></p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.Tanhshrink()
&gt;&gt;&gt; input = torch.randn(2)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="_24">阈</h3>
<hr />
<pre><code>class torch.nn.Threshold(threshold, value, inplace=False)¶
</code></pre>
<p>设置输入张量的每个元素的阈值。</p>
<p>阈值定义为：</p>
<p><img alt="" src="../img/a020eb5af6bf1ffadb40693fd689d2ea.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>阈值</strong> –达到阈值的值</p>
</li>
<li>
<p><strong>值</strong> –要替换为的值</p>
</li>
<li>
<p><strong>inplace</strong> – can optionally do the operation in-place. Default: <code>False</code></p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" /> where &lt;cite&gt;*&lt;/cite&gt; means, any number of additional dimensions</p>
</li>
<li>
<p>Output: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" />, same shape as the input</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.Threshold(0.1, 20)
&gt;&gt;&gt; input = torch.randn(2)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h2 id="_25">非线性激活(其他）</h2>
<h3 id="_26">软敏</h3>
<hr />
<pre><code>class torch.nn.Softmin(dim=None)¶
</code></pre>
<p>将 Softmin 函数应用于缩放后的 n 维输入张量，以便 n 维输出张量的元素在 &lt;cite&gt;[0，1]&lt;/cite&gt; 范围内，总和为 1。</p>
<p>Softmin 定义为：</p>
<p><img alt="" src="../img/7afef0400cd1c4dc21023db7faf49f70.jpg" /></p>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>输入：<img alt="" src="../img/39f05b69c0ea6b9b6b228f948944c4f6.jpg" />其中 &lt;cite&gt;*&lt;/cite&gt; 表示任意数量的附加尺寸</p>
</li>
<li>
<p>输出：<img alt="" src="../img/39f05b69c0ea6b9b6b228f948944c4f6.jpg" />，形状与输入相同</p>
</li>
</ul>
<p>Parameters</p>
<p><strong>dim</strong>  (<em>python：int</em> )–将计算 Softmin 的维度(因此，沿着 dim 的每个切片的总和为 1）。</p>
<p>Returns</p>
<p>与输入具有相同尺寸和形状的张量，其值在[0，1]范围内</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.Softmin()
&gt;&gt;&gt; input = torch.randn(2, 3)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="_27">软最大</h3>
<hr />
<pre><code>class torch.nn.Softmax(dim=None)¶
</code></pre>
<p>将 Softmax 函数应用于缩放后的 n 维输入 Tensor，以使 n 维输出 Tensor 的元素在[0,1]范围内，总和为 1。</p>
<p>Softmax 定义为：</p>
<p><img alt="" src="../img/95a0318d187cfbba10eb2fbff14fab34.jpg" /></p>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/39f05b69c0ea6b9b6b228f948944c4f6.jpg" /> where &lt;cite&gt;*&lt;/cite&gt; means, any number of additional dimensions</p>
</li>
<li>
<p>Output: <img alt="" src="../img/39f05b69c0ea6b9b6b228f948944c4f6.jpg" />, same shape as the input</p>
</li>
</ul>
<p>Returns</p>
<p>与输入具有相同尺寸和形状的张量，其值在[0，1]范围内</p>
<p>Parameters</p>
<p><strong>dim</strong>  (<em>python：int</em> )–将计算 Softmax 的维度(因此，沿着 dim 的每个切片的总和为 1）。</p>
<p>Note</p>
<p>该模块无法直接与 NLLLoss 配合使用，后者希望 Log 是在 Softmax 及其自身之间进行计算的。 请改用 &lt;cite&gt;LogSoftmax&lt;/cite&gt; (速度更快，并且具有更好的数值属性）。</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.Softmax(dim=1)
&gt;&gt;&gt; input = torch.randn(2, 3)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="softmax2d">Softmax2d</h3>
<hr />
<pre><code>class torch.nn.Softmax2d¶
</code></pre>
<p>将 SoftMax 应用于要素上的每个空间位置。</p>
<p>当给定<code>Channels x Height x Width</code>的图像时，它将 &lt;cite&gt;Softmax&lt;/cite&gt; 应用于每个位置<img alt="" src="../img/d2a003cefb989716f16059bf68177a10.jpg" /></p>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>输入：<img alt="" src="../img/a52adc3e96ebecd26270170745455e66.jpg" /></p>
</li>
<li>
<p>输出：<img alt="" src="../img/a52adc3e96ebecd26270170745455e66.jpg" />(形状与输入相同）</p>
</li>
</ul>
<p>Returns</p>
<p>a Tensor of the same dimension and shape as the input with values in the range [0, 1]</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.Softmax2d()
&gt;&gt;&gt; # you softmax over the 2nd dimension
&gt;&gt;&gt; input = torch.randn(2, 3, 12, 13)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="logsoftmax">LogSoftmax</h3>
<hr />
<pre><code>class torch.nn.LogSoftmax(dim=None)¶
</code></pre>
<p>将<img alt="" src="../img/9ff2442dae5e6512bbf6dc1497a9d539.jpg" />功能应用于 n 维输入张量。 LogSoftmax 公式可以简化为：</p>
<p><img alt="" src="../img/cc8e8c4f9c15cf26a9b94479dd98d29f.jpg" /></p>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/39f05b69c0ea6b9b6b228f948944c4f6.jpg" /> where &lt;cite&gt;*&lt;/cite&gt; means, any number of additional dimensions</p>
</li>
<li>
<p>Output: <img alt="" src="../img/39f05b69c0ea6b9b6b228f948944c4f6.jpg" />, same shape as the input</p>
</li>
</ul>
<p>Parameters</p>
<p><strong>暗淡的</strong> (<em>python：int</em> )–用来计算 LogSoftmax 的尺寸。</p>
<p>Returns</p>
<p>与输入具有相同尺寸和形状的张量，其值在[-inf，0）范围内</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.LogSoftmax()
&gt;&gt;&gt; input = torch.randn(2, 3)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="adaptivelogsoftmaxwithloss">AdaptiveLogSoftmaxWithLoss</h3>
<hr />
<pre><code>class torch.nn.AdaptiveLogSoftmaxWithLoss(in_features, n_classes, cutoffs, div_value=4.0, head_bias=False)¶
</code></pre>
<p>如 Edouard Grave，Armand Joulin，MoustaphaCissé，David Grangier 和 HervéJégou 在<a href="https://arxiv.org/abs/1609.04309">中针对 GPU</a> 所述的高效 softmax 逼近。</p>
<p>自适应 softmax 是用于训练具有大输出空间的模型的近似策略。 当标签分布高度不平衡时，例如在自然语言建模中，单词频率分布大致遵循 <a href="https://en.wikipedia.org/wiki/Zipf%27s_law">Zipf 定律</a>时，此方法最为有效。</p>
<p>自适应 softmax 根据标签的频率将标签划分为几个簇。 这些集群每个可能包含不同数量的目标。 此外，包含较少标签的群集将较低维的嵌入分配给这些标签，从而加快了计算速度。 对于每个小批量，仅评估至少存在一个目标的集群。</p>
<p>这个想法是，频繁访问的集群(如第一个集群，包含最频繁的标签），也应该便宜计算-也就是说，包含少量分配的标签。</p>
<p>我们强烈建议您查看原始文件以了解更多详细信息。</p>
<ul>
<li>
<p><code>cutoffs</code>应该是按升序排序的有序整数序列。 它控制集群的数量以及将目标划分为集群。 例如，设置<code>cutoffs = [10, 100, 1000]</code>意味着第一个 &lt;cite&gt;10 个&lt;/cite&gt;目标将分配给自适应 softmax 的“头部”，目标 &lt;cite&gt;11、12，…，100 个&lt;/cite&gt;将分配给第一个目标 集群，目标 &lt;cite&gt;101、102，…，1000&lt;/cite&gt; 将分配给第二个集群，而目标 &lt;cite&gt;1001、1002，…，n_classes-1&lt;/cite&gt; 将分配给最后一个，第三个 簇。</p>
</li>
<li>
<p><code>div_value</code>用于计算每个附加聚类的大小，以<img alt="" src="../img/52df40a157498a14092c7cf42153b0f6.jpg" />的形式给出，其中<img alt="" src="../img/aa857537c49ebedfbb3ca077dded967b.jpg" />是聚类索引(具有较少索引的聚类具有较大索引，而聚类从<img alt="" src="../img/21c5bc8d40ee5ab2e18d64aaba8359c7.jpg" />开始）。</p>
</li>
<li>
<p><code>head_bias</code>如果设置为 True，则会向自适应 softmax 的“头部”添加一个偏差项。 有关详细信息，请参见纸张。 在官方实现中设置为 False。</p>
</li>
</ul>
<p>Warning</p>
<p>传递给该模块的标签应根据其频率进行分类。 这意味着最频繁的标签应由索引 &lt;cite&gt;0&lt;/cite&gt; 表示，最不频繁的标签应由索引 &lt;cite&gt;n_classes-1&lt;/cite&gt; 表示。</p>
<p>Note</p>
<p>该模块返回带有<code>output</code>和<code>loss</code>字段的<code>NamedTuple</code>。 有关详细信息，请参见其他文档。</p>
<p>Note</p>
<p>要计算所有类别的对数概率，可以使用<code>log_prob</code>方法。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>in_features</strong>  (<em>python：int</em> )–输入张量中的特征数</p>
</li>
<li>
<p><strong>n_classes</strong>  (<em>python：int</em> )–数据集中的类数</p>
</li>
<li>
<p><strong>临界值</strong>(<em>序列</em>）–用于将目标分配给其存储桶的临界值</p>
</li>
<li>
<p><strong>div_value</strong>  (<em>python：float</em> <em>，</em> <em>可选</em>）–用作计算集群大小的指数的值。 默认值：4.0</p>
</li>
<li>
<p><strong>head_bias</strong>  (<em>bool</em> <em>，</em> <em>可选</em>）–如果<code>True</code>，则向自适应 softmax 的“ head”添加一个偏差项。 默认值：<code>False</code></p>
</li>
</ul>
<p>Returns</p>
<ul>
<li>
<p><strong>输出</strong>是大小为<code>N</code>的张量，其中包含每个示例的计算目标对数概率</p>
</li>
<li>
<p><strong>损失</strong>是表示计算出的负对数似然损失的标量</p>
</li>
</ul>
<p>Return type</p>
<p>具有<code>output</code>和<code>loss</code>字段的<code>NamedTuple</code></p>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>输入：<img alt="" src="../img/dd3351bba6ec6a127562961e9f650ec8.jpg" /></p>
</li>
<li>
<p>目标：<img alt="" src="../img/a01009d68498c3f4597b15bab746c24d.jpg" />其中每个值都满足<img alt="" src="../img/543f8e6c63c63301cc0be24c75e793a9.jpg" /></p>
</li>
<li>
<p>输出 1：<img alt="" src="../img/a01009d68498c3f4597b15bab746c24d.jpg" /></p>
</li>
<li>
<p>输出 2：<code>Scalar</code></p>
</li>
</ul>
<hr />
<pre><code>log_prob(input)¶
</code></pre>
<p>计算所有<img alt="" src="../img/a11e93305dfae60e93e17c782260fd8a.jpg" />的日志概率</p>
<p>Parameters</p>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–小批量示例</p>
<p>Returns</p>
<p><img alt="" src="../img/d4dcd3627d629fee6bba9fd29e7c72cb.jpg" />范围内每个类别<img alt="" src="../img/009c69db54af1829f05c23f52be15e24.jpg" />的对数概率，其中<img alt="" src="../img/a11e93305dfae60e93e17c782260fd8a.jpg" />是传递给<code>AdaptiveLogSoftmaxWithLoss</code>构造函数的参数。</p>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>输入：<img alt="" src="../img/dd3351bba6ec6a127562961e9f650ec8.jpg" /></p>
</li>
<li>
<p>输出：<img alt="" src="../img/8596ce063ef9fe317fc5b2dfa43e62b9.jpg" /></p>
</li>
</ul>
<hr />
<pre><code>predict(input)¶
</code></pre>
<p>这等效于 &lt;cite&gt;self.log_pob(input）.argmax(dim = 1）&lt;/cite&gt;，但在某些情况下效率更高。</p>
<p>Parameters</p>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – a minibatch of examples</p>
<p>Returns</p>
<p>每个示例中概率最高的类别</p>
<p>Return type</p>
<p>输出(<a href="tensors.html#torch.Tensor" title="torch.Tensor">张量</a>）</p>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/dd3351bba6ec6a127562961e9f650ec8.jpg" /></p>
</li>
<li>
<p>输出：<img alt="" src="../img/a01009d68498c3f4597b15bab746c24d.jpg" /></p>
</li>
</ul>
<h2 id="_28">归一化层</h2>
<h3 id="batchnorm1d">BatchNorm1d</h3>
<hr />
<pre><code>class torch.nn.BatchNorm1d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)¶
</code></pre>
<p>如论文<a href="https://arxiv.org/abs/1502.03167">中所述，对 2D 或 3D 输入(具有可选附加通道尺寸的 1D 输入的微型批处理）应用批归一化：通过减少内部协变量偏移</a>加速深度网络训练。</p>
<p><img alt="" src="../img/0461ac444409992379e18e4db4ad957d.jpg" /></p>
<p>均值和标准偏差是在微型批次上按维度计算的，并且<img alt="" src="../img/1f8b1c3a15bdc67a9d0642538fee2580.jpg" />和<img alt="" src="../img/53a496ec7d546e2af9595a7055dd6a7e.jpg" />是大小为 &lt;cite&gt;C&lt;/cite&gt; 的可学习参数矢量(其中 &lt;cite&gt;C&lt;/cite&gt; 是输入大小 )。 默认情况下，<img alt="" src="../img/1f8b1c3a15bdc67a9d0642538fee2580.jpg" />的元素设置为 1，<img alt="" src="../img/53a496ec7d546e2af9595a7055dd6a7e.jpg" />的元素设置为 0。</p>
<p>同样默认情况下，在训练过程中，该层会继续对其计算的均值和方差进行估算，然后将其用于评估期间的标准化。 运行估计保持默认值<code>momentum</code> 0.1。</p>
<p>如果<code>track_running_stats</code>设置为<code>False</code>，则此层将不保持运行估计，而是在评估期间也使用批处理统计信息。</p>
<p>Note</p>
<p><code>momentum</code>参数不同于优化程序类中使用的参数以及传统的动量概念。 在数学上，此处用于运行统计信息的更新规则为<img alt="" src="../img/2c303aa11b9e538beb8ce738974ca899.jpg" />，其中<img alt="" src="../img/1533dcca53d64ca456ac5ad11b03a2cf.jpg" />是估计的统计信息，<img alt="" src="../img/9c89ef5a4fdc4fecb9f6768daaabd7cc.jpg" />是新的观测值。</p>
<p>由于批量归一化是在 &lt;cite&gt;C&lt;/cite&gt; 维度上完成的，因此要计算&lt;cite&gt;(N，L）&lt;/cite&gt;切片的统计信息，因此通常将其称为“时间批量归一化”。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>num_features</strong> – <img alt="" src="../img/efc1fa1ed22c59909c50ea4639e0e861.jpg" />来自大小为<img alt="" src="../img/c8627f9c1b3afa3c758e61cb22621e8b.jpg" />的预期输入，或<img alt="" src="../img/0abd5691618d0a16f37deae1aacd79cf.jpg" />来自大小为<img alt="" src="../img/31fe1df19acb2378e09eca30ff008201.jpg" />的输入</p>
</li>
<li>
<p><strong>eps</strong> –分母增加的值，以保证数值稳定性。 默认值：1e-5</p>
</li>
<li>
<p><strong>动量</strong> –用于 running_mean 和 running_var 计算的值。 可以设置为<code>None</code>以获得累积移动平均线(即简单平均线）。 默认值：0.1</p>
</li>
<li>
<p><strong>仿射</strong> –一个布尔值，当设置为<code>True</code>时，此模块具有可学习的仿射参数。 默认值：<code>True</code></p>
</li>
<li>
<p><strong>track_running_stats</strong> –一个布尔值，设置为<code>True</code>时，此模块跟踪运行平均值和方差；设置为<code>False</code>时，此模块不跟踪此类统计信息，并且始终使用批处理统计信息 训练和评估模式。 默认值：<code>True</code></p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>输入：<img alt="" src="../img/e2f1df6fc3cf4eb0f5bee6a89bb2f37b.jpg" />或<img alt="" src="../img/c8627f9c1b3afa3c758e61cb22621e8b.jpg" /></p>
</li>
<li>
<p>输出：<img alt="" src="../img/e2f1df6fc3cf4eb0f5bee6a89bb2f37b.jpg" />或<img alt="" src="../img/c8627f9c1b3afa3c758e61cb22621e8b.jpg" />(形状与输入相同）</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; # With Learnable Parameters
&gt;&gt;&gt; m = nn.BatchNorm1d(100)
&gt;&gt;&gt; # Without Learnable Parameters
&gt;&gt;&gt; m = nn.BatchNorm1d(100, affine=False)
&gt;&gt;&gt; input = torch.randn(20, 100)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="batchnorm2d">BatchNorm2d</h3>
<hr />
<pre><code>class torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)¶
</code></pre>
<p>如论文<a href="https://arxiv.org/abs/1502.03167">中所述，对 4D 输入(具有附加通道尺寸的 2D 输入的微型批处理）应用批归一化：通过减少内部协变量偏移</a>来加速深度网络训练。</p>
<p><img alt="" src="../img/b4eedd00eca90b97b69e6b2958bf0356.jpg" /></p>
<p>The mean and standard-deviation are calculated per-dimension over the mini-batches and <img alt="" src="../img/1f8b1c3a15bdc67a9d0642538fee2580.jpg" /> and <img alt="" src="../img/53a496ec7d546e2af9595a7055dd6a7e.jpg" /> are learnable parameter vectors of size &lt;cite&gt;C&lt;/cite&gt; (where &lt;cite&gt;C&lt;/cite&gt; is the input size). By default, the elements of <img alt="" src="../img/1f8b1c3a15bdc67a9d0642538fee2580.jpg" /> are set to 1 and the elements of <img alt="" src="../img/53a496ec7d546e2af9595a7055dd6a7e.jpg" /> are set to 0.</p>
<p>Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default <code>momentum</code> of 0.1.</p>
<p>If <code>track_running_stats</code> is set to <code>False</code>, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well.</p>
<p>Note</p>
<p>This <code>momentum</code> argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is <img alt="" src="../img/2c303aa11b9e538beb8ce738974ca899.jpg" />, where <img alt="" src="../img/1533dcca53d64ca456ac5ad11b03a2cf.jpg" /> is the estimated statistic and <img alt="" src="../img/9c89ef5a4fdc4fecb9f6768daaabd7cc.jpg" /> is the new observed value.</p>
<p>由于批量归一化是在 &lt;cite&gt;C&lt;/cite&gt; 维度上完成的，因此要计算&lt;cite&gt;(N，H，W）&lt;/cite&gt;切片的统计信息，因此通常将其称为“空间批量归一化”。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>num_features</strong> – <img alt="" src="../img/efc1fa1ed22c59909c50ea4639e0e861.jpg" />来自大小为<img alt="" src="../img/a52adc3e96ebecd26270170745455e66.jpg" />的预期输入</p>
</li>
<li>
<p><strong>eps</strong> – a value added to the denominator for numerical stability. Default: 1e-5</p>
</li>
<li>
<p><strong>momentum</strong> – the value used for the running_mean and running_var computation. Can be set to <code>None</code> for cumulative moving average (i.e. simple average). Default: 0.1</p>
</li>
<li>
<p><strong>affine</strong> – a boolean value that when set to <code>True</code>, this module has learnable affine parameters. Default: <code>True</code></p>
</li>
<li>
<p><strong>track_running_stats</strong> – a boolean value that when set to <code>True</code>, this module tracks the running mean and variance, and when set to <code>False</code>, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: <code>True</code></p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/a52adc3e96ebecd26270170745455e66.jpg" /></p>
</li>
<li>
<p>Output: <img alt="" src="../img/a52adc3e96ebecd26270170745455e66.jpg" /> (same shape as input)</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; # With Learnable Parameters
&gt;&gt;&gt; m = nn.BatchNorm2d(100)
&gt;&gt;&gt; # Without Learnable Parameters
&gt;&gt;&gt; m = nn.BatchNorm2d(100, affine=False)
&gt;&gt;&gt; input = torch.randn(20, 100, 35, 45)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="batchnorm3d">BatchNorm3d</h3>
<hr />
<pre><code>class torch.nn.BatchNorm3d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)¶
</code></pre>
<p>如论文<a href="https://arxiv.org/abs/1502.03167">中所述，对 5D 输入(具有附加通道尺寸的 3D 输入的微型批处理）应用批归一化：通过减少内部协变量偏移</a>加速深度网络训练。</p>
<p><img alt="" src="../img/b4eedd00eca90b97b69e6b2958bf0356.jpg" /></p>
<p>The mean and standard-deviation are calculated per-dimension over the mini-batches and <img alt="" src="../img/1f8b1c3a15bdc67a9d0642538fee2580.jpg" /> and <img alt="" src="../img/53a496ec7d546e2af9595a7055dd6a7e.jpg" /> are learnable parameter vectors of size &lt;cite&gt;C&lt;/cite&gt; (where &lt;cite&gt;C&lt;/cite&gt; is the input size). By default, the elements of <img alt="" src="../img/1f8b1c3a15bdc67a9d0642538fee2580.jpg" /> are set to 1 and the elements of <img alt="" src="../img/53a496ec7d546e2af9595a7055dd6a7e.jpg" /> are set to 0.</p>
<p>Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default <code>momentum</code> of 0.1.</p>
<p>If <code>track_running_stats</code> is set to <code>False</code>, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well.</p>
<p>Note</p>
<p>This <code>momentum</code> argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is <img alt="" src="../img/2c303aa11b9e538beb8ce738974ca899.jpg" />, where <img alt="" src="../img/1533dcca53d64ca456ac5ad11b03a2cf.jpg" /> is the estimated statistic and <img alt="" src="../img/9c89ef5a4fdc4fecb9f6768daaabd7cc.jpg" /> is the new observed value.</p>
<p>由于批量归一化是在 &lt;cite&gt;C&lt;/cite&gt; 维度上完成的，因此要计算&lt;cite&gt;(N，D，H，W）&lt;/cite&gt;切片的统计信息，因此通常将这种体积批量归一化或时空称为术语 批处理规范化。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>num_features</strong> – <img alt="" src="../img/efc1fa1ed22c59909c50ea4639e0e861.jpg" />来自大小为<img alt="" src="../img/2bbc80f07f22b3961bbc382bd5a5e8f3.jpg" />的预期输入</p>
</li>
<li>
<p><strong>eps</strong> – a value added to the denominator for numerical stability. Default: 1e-5</p>
</li>
<li>
<p><strong>momentum</strong> – the value used for the running_mean and running_var computation. Can be set to <code>None</code> for cumulative moving average (i.e. simple average). Default: 0.1</p>
</li>
<li>
<p><strong>affine</strong> – a boolean value that when set to <code>True</code>, this module has learnable affine parameters. Default: <code>True</code></p>
</li>
<li>
<p><strong>track_running_stats</strong> – a boolean value that when set to <code>True</code>, this module tracks the running mean and variance, and when set to <code>False</code>, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: <code>True</code></p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>输入：<img alt="" src="../img/2bbc80f07f22b3961bbc382bd5a5e8f3.jpg" /></p>
</li>
<li>
<p>输出：<img alt="" src="../img/2bbc80f07f22b3961bbc382bd5a5e8f3.jpg" />(形状与输入相同）</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; # With Learnable Parameters
&gt;&gt;&gt; m = nn.BatchNorm3d(100)
&gt;&gt;&gt; # Without Learnable Parameters
&gt;&gt;&gt; m = nn.BatchNorm3d(100, affine=False)
&gt;&gt;&gt; input = torch.randn(20, 100, 35, 45, 10)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="_29">集团规范</h3>
<hr />
<pre><code>class torch.nn.GroupNorm(num_groups, num_channels, eps=1e-05, affine=True)¶
</code></pre>
<p>如论文<a href="https://arxiv.org/abs/1803.08494">组归一化</a>中所述，将组归一化应用于微型输入。</p>
<p><img alt="" src="../img/69cf729fafaba56b72f793a28ce26c03.jpg" /></p>
<p>输入通道分为<code>num_groups</code>组，每个组包含<code>num_channels / num_groups</code>通道。 均值和标准差在每个组中分别计算。 如果<code>affine</code>为<code>True</code>，则<img alt="" src="../img/1f8b1c3a15bdc67a9d0642538fee2580.jpg" />和<img alt="" src="../img/53a496ec7d546e2af9595a7055dd6a7e.jpg" />是大小为<code>num_channels</code>的可学习的每通道仿射变换参数矢量。</p>
<p>该层使用在训练和评估模式下从输入数据中计算出的统计信息。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>num_groups</strong>  (<em>python：int</em> )–将通道分隔为的组数</p>
</li>
<li>
<p><strong>num_channels</strong>  (<em>python：int</em> )–输入中预期的通道数</p>
</li>
<li>
<p><strong>eps</strong> –分母增加的值，以保证数值稳定性。 默认值：1e-5</p>
</li>
<li>
<p><strong>仿射</strong> –一个布尔值，当设置为<code>True</code>时，此模块具有可学习的每通道仿射参数，分别初始化为 1(用于权重）和零(用于偏差）。 默认值：<code>True</code>。</p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>输入：<img alt="" src="../img/877a6a1f684db60343e3c9847af621cd.jpg" />，其中<img alt="" src="../img/e0bbc54aba19d567c90f43905db2b6d9.jpg" /></p>
</li>
<li>
<p>输出：<img alt="" src="../img/877a6a1f684db60343e3c9847af621cd.jpg" />(形状与输入相同）</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; input = torch.randn(20, 6, 10, 10)
&gt;&gt;&gt; # Separate 6 channels into 3 groups
&gt;&gt;&gt; m = nn.GroupNorm(3, 6)
&gt;&gt;&gt; # Separate 6 channels into 6 groups (equivalent with InstanceNorm)
&gt;&gt;&gt; m = nn.GroupNorm(6, 6)
&gt;&gt;&gt; # Put all 6 channels into a single group (equivalent with LayerNorm)
&gt;&gt;&gt; m = nn.GroupNorm(1, 6)
&gt;&gt;&gt; # Activating the module
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="syncbatchnorm">SyncBatchNorm</h3>
<hr />
<pre><code>class torch.nn.SyncBatchNorm(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, process_group=None)¶
</code></pre>
<p>如论文<a href="https://arxiv.org/abs/1502.03167">中所述，对 N 维输入(具有附加通道维的[N-2] D 输入的小批量）进行批量归一化批量归一化：通过减少内部协变量偏移</a>加速深度网络训练 。</p>
<p><img alt="" src="../img/b4eedd00eca90b97b69e6b2958bf0356.jpg" /></p>
<p>均值和标准偏差是在同一过程组的所有微型批次中按维度计算的。 <img alt="" src="../img/1f8b1c3a15bdc67a9d0642538fee2580.jpg" />和<img alt="" src="../img/53a496ec7d546e2af9595a7055dd6a7e.jpg" />是大小为 &lt;cite&gt;C&lt;/cite&gt; (其中 &lt;cite&gt;C&lt;/cite&gt; 是输入大小）的可学习参数向量。 默认情况下，从<img alt="" src="../img/b11c87ef1ebfb151e4d12923865dd6a5.jpg" />采样<img alt="" src="../img/1f8b1c3a15bdc67a9d0642538fee2580.jpg" />的元素，并将<img alt="" src="../img/53a496ec7d546e2af9595a7055dd6a7e.jpg" />的元素设置为 0。</p>
<p>Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default <code>momentum</code> of 0.1.</p>
<p>If <code>track_running_stats</code> is set to <code>False</code>, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well.</p>
<p>Note</p>
<p><code>momentum</code>参数不同于优化程序类中使用的参数以及传统的动量概念。 在数学上，此处用于运行统计信息的更新规则为<img alt="" src="../img/b8b7f43cac4585f2aa1b335a69407a47.jpg" />，其中<img alt="" src="../img/1533dcca53d64ca456ac5ad11b03a2cf.jpg" />是估计的统计信息，<img alt="" src="../img/9c89ef5a4fdc4fecb9f6768daaabd7cc.jpg" />是新的观测值。</p>
<p>由于批量归一化是在 &lt;cite&gt;C&lt;/cite&gt; 维度上完成的，因此要计算&lt;cite&gt;(N，+）&lt;/cite&gt;切片的统计信息，因此通常将其称为体积批量归一化或时空批量归一化。</p>
<p>当前，SyncBatchNorm 仅支持每个进程具有单个 GPU 的 DistributedDataParallel。 在使用 DDP 包装网络之前，使用 torch.nn.SyncBatchNorm.convert_sync_batchnorm(）将 BatchNorm 层转换为 SyncBatchNorm。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>num_features</strong> – <img alt="" src="../img/efc1fa1ed22c59909c50ea4639e0e861.jpg" />来自大小为<img alt="" src="../img/ef6c8fc7d9870f420b625ab9c2aa991c.jpg" />的预期输入</p>
</li>
<li>
<p><strong>eps</strong> – a value added to the denominator for numerical stability. Default: 1e-5</p>
</li>
<li>
<p><strong>momentum</strong> – the value used for the running_mean and running_var computation. Can be set to <code>None</code> for cumulative moving average (i.e. simple average). Default: 0.1</p>
</li>
<li>
<p><strong>affine</strong> – a boolean value that when set to <code>True</code>, this module has learnable affine parameters. Default: <code>True</code></p>
</li>
<li>
<p><strong>track_running_stats</strong> – a boolean value that when set to <code>True</code>, this module tracks the running mean and variance, and when set to <code>False</code>, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: <code>True</code></p>
</li>
<li>
<p><strong>process_group</strong> –统计信息的同步分别在每个进程组内发生。 默认行为是在整个世界范围内同步</p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>输入：<img alt="" src="../img/ef6c8fc7d9870f420b625ab9c2aa991c.jpg" /></p>
</li>
<li>
<p>输出：<img alt="" src="../img/ef6c8fc7d9870f420b625ab9c2aa991c.jpg" />(形状与输入相同）</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; # With Learnable Parameters
&gt;&gt;&gt; m = nn.SyncBatchNorm(100)
&gt;&gt;&gt; # creating process group (optional)
&gt;&gt;&gt; # process_ids is a list of int identifying rank ids.
&gt;&gt;&gt; process_group = torch.distributed.new_group(process_ids)
&gt;&gt;&gt; # Without Learnable Parameters
&gt;&gt;&gt; m = nn.BatchNorm3d(100, affine=False, process_group=process_group)
&gt;&gt;&gt; input = torch.randn(20, 100, 35, 45, 10)
&gt;&gt;&gt; output = m(input)

&gt;&gt;&gt; # network is nn.BatchNorm layer
&gt;&gt;&gt; sync_bn_network = nn.SyncBatchNorm.convert_sync_batchnorm(network, process_group)
&gt;&gt;&gt; # only single gpu per process is currently supported
&gt;&gt;&gt; ddp_sync_bn_network = torch.nn.parallel.DistributedDataParallel(
&gt;&gt;&gt;                         sync_bn_network,
&gt;&gt;&gt;                         device_ids=[args.local_rank],
&gt;&gt;&gt;                         output_device=args.local_rank)

</code></pre>
<hr />
<pre><code>classmethod convert_sync_batchnorm(module, process_group=None)¶
</code></pre>
<p>辅助函数将模型中的 &lt;cite&gt;torch.nn.BatchNormND&lt;/cite&gt; 层转换为 &lt;cite&gt;torch.nn.SyncBatchNorm&lt;/cite&gt; 层。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>模块</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>)–包含模块</p>
</li>
<li>
<p><strong>process_group</strong> (<em>可选</em>）–进程组到范围的同步，</p>
</li>
</ul>
<p>默认是整个世界</p>
<p>Returns</p>
<p>具有转换后的 &lt;cite&gt;torch.nn.SyncBatchNorm&lt;/cite&gt; 层的原始模块</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; # Network with nn.BatchNorm layer
&gt;&gt;&gt; module = torch.nn.Sequential(
&gt;&gt;&gt;            torch.nn.Linear(20, 100),
&gt;&gt;&gt;            torch.nn.BatchNorm1d(100)
&gt;&gt;&gt;          ).cuda()
&gt;&gt;&gt; # creating process group (optional)
&gt;&gt;&gt; # process_ids is a list of int identifying rank ids.
&gt;&gt;&gt; process_group = torch.distributed.new_group(process_ids)
&gt;&gt;&gt; sync_bn_module = convert_sync_batchnorm(module, process_group)

</code></pre>
<h3 id="instancenorm1d">InstanceNorm1d</h3>
<hr />
<pre><code>class torch.nn.InstanceNorm1d(num_features, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)¶
</code></pre>
<p>如论文<a href="https://arxiv.org/abs/1607.08022">中所述，将实例标准化应用于 3D 输入(具有可选附加通道尺寸的 1D 输入的微型批处理）实例标准化：快速样式化</a>的缺失成分。</p>
<p><img alt="" src="../img/b4eedd00eca90b97b69e6b2958bf0356.jpg" /></p>
<p>微型批处理中每个对象的维数均值和标准差分别计算。 如果<code>affine</code>为<code>True</code>，则<img alt="" src="../img/1f8b1c3a15bdc67a9d0642538fee2580.jpg" />和<img alt="" src="../img/53a496ec7d546e2af9595a7055dd6a7e.jpg" />是大小为 &lt;cite&gt;C&lt;/cite&gt; (其中 &lt;cite&gt;C&lt;/cite&gt; 为输入大小）的可学习参数矢量。</p>
<p>默认情况下，该层使用在训练和评估模式下从输入数据计算出的实例统计信息。</p>
<p>如果<code>track_running_stats</code>设置为<code>True</code>，则在训练过程中，此层将继续对其计算的均值和方差进行估算，然后将其用于评估期间的标准化。 运行估计保持默认值<code>momentum</code> 0.1。</p>
<p>Note</p>
<p>This <code>momentum</code> argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is <img alt="" src="../img/b8b7f43cac4585f2aa1b335a69407a47.jpg" />, where <img alt="" src="../img/1533dcca53d64ca456ac5ad11b03a2cf.jpg" /> is the estimated statistic and <img alt="" src="../img/9c89ef5a4fdc4fecb9f6768daaabd7cc.jpg" /> is the new observed value.</p>
<p>Note</p>
<p><a href="#torch.nn.InstanceNorm1d" title="torch.nn.InstanceNorm1d"><code>InstanceNorm1d</code></a> 和 <a href="#torch.nn.LayerNorm" title="torch.nn.LayerNorm"><code>LayerNorm</code></a> 非常相似，但有一些细微的差异。 <a href="#torch.nn.InstanceNorm1d" title="torch.nn.InstanceNorm1d"><code>InstanceNorm1d</code></a> 应用于多维数据序列之类的通道数据的每个通道，但是 <a href="#torch.nn.LayerNorm" title="torch.nn.LayerNorm"><code>LayerNorm</code></a> 通常应用于整个样本，并且通常用于 NLP 任务。 另外， <a href="#torch.nn.LayerNorm" title="torch.nn.LayerNorm"><code>LayerNorm</code></a> 应用逐元素仿射变换，而 <a href="#torch.nn.InstanceNorm1d" title="torch.nn.InstanceNorm1d"><code>InstanceNorm1d</code></a> 通常不应用仿射变换。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>num_features</strong> – <img alt="" src="../img/efc1fa1ed22c59909c50ea4639e0e861.jpg" /> from an expected input of size <img alt="" src="../img/c8627f9c1b3afa3c758e61cb22621e8b.jpg" /> or <img alt="" src="../img/0abd5691618d0a16f37deae1aacd79cf.jpg" /> from input of size <img alt="" src="../img/31fe1df19acb2378e09eca30ff008201.jpg" /></p>
</li>
<li>
<p><strong>eps</strong> – a value added to the denominator for numerical stability. Default: 1e-5</p>
</li>
<li>
<p><strong>动量</strong> –用于 running_mean 和 running_var 计算的值。 默认值：0.1</p>
</li>
<li>
<p><strong>仿射</strong> –一个布尔值，当设置为<code>True</code>时，此模块具有可学习的仿射参数，其初始化方式与批量标准化相同。 默认值：<code>False</code>。</p>
</li>
<li>
<p><strong>track_running_stats</strong> –一个布尔值，设置为<code>True</code>时，此模块跟踪运行平均值和方差；设置为<code>False</code>时，此模块不跟踪此类统计信息，并且始终使用批处理统计信息 训练和评估模式。 默认值：<code>False</code></p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>输入：<img alt="" src="../img/c8627f9c1b3afa3c758e61cb22621e8b.jpg" /></p>
</li>
<li>
<p>输出：<img alt="" src="../img/c8627f9c1b3afa3c758e61cb22621e8b.jpg" />(形状与输入相同）</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; # Without Learnable Parameters
&gt;&gt;&gt; m = nn.InstanceNorm1d(100)
&gt;&gt;&gt; # With Learnable Parameters
&gt;&gt;&gt; m = nn.InstanceNorm1d(100, affine=True)
&gt;&gt;&gt; input = torch.randn(20, 100, 40)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="instancenorm2d">InstanceNorm2d</h3>
<hr />
<pre><code>class torch.nn.InstanceNorm2d(num_features, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)¶
</code></pre>
<p>如论文<a href="https://arxiv.org/abs/1607.08022">中所述，将实例标准化应用于 4D 输入(具有附加通道尺寸的 2D 输入的小批量）实例标准化：快速样式化</a>的缺失成分。</p>
<p><img alt="" src="../img/b4eedd00eca90b97b69e6b2958bf0356.jpg" /></p>
<p>The mean and standard-deviation are calculated per-dimension separately for each object in a mini-batch. <img alt="" src="../img/1f8b1c3a15bdc67a9d0642538fee2580.jpg" /> and <img alt="" src="../img/53a496ec7d546e2af9595a7055dd6a7e.jpg" /> are learnable parameter vectors of size &lt;cite&gt;C&lt;/cite&gt; (where &lt;cite&gt;C&lt;/cite&gt; is the input size) if <code>affine</code> is <code>True</code>.</p>
<p>By default, this layer uses instance statistics computed from input data in both training and evaluation modes.</p>
<p>If <code>track_running_stats</code> is set to <code>True</code>, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default <code>momentum</code> of 0.1.</p>
<p>Note</p>
<p>This <code>momentum</code> argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is <img alt="" src="../img/b8b7f43cac4585f2aa1b335a69407a47.jpg" />, where <img alt="" src="../img/1533dcca53d64ca456ac5ad11b03a2cf.jpg" /> is the estimated statistic and <img alt="" src="../img/9c89ef5a4fdc4fecb9f6768daaabd7cc.jpg" /> is the new observed value.</p>
<p>Note</p>
<p><a href="#torch.nn.InstanceNorm2d" title="torch.nn.InstanceNorm2d"><code>InstanceNorm2d</code></a> 和 <a href="#torch.nn.LayerNorm" title="torch.nn.LayerNorm"><code>LayerNorm</code></a> 非常相似，但有一些细微的差异。 <a href="#torch.nn.InstanceNorm2d" title="torch.nn.InstanceNorm2d"><code>InstanceNorm2d</code></a> 适用于 RGB 图像之类的通道数据的每个通道，但是 <a href="#torch.nn.LayerNorm" title="torch.nn.LayerNorm"><code>LayerNorm</code></a> 通常适用于整个样本，并且通常用于 NLP 任务。 另外， <a href="#torch.nn.LayerNorm" title="torch.nn.LayerNorm"><code>LayerNorm</code></a> 应用逐元素仿射变换，而 <a href="#torch.nn.InstanceNorm2d" title="torch.nn.InstanceNorm2d"><code>InstanceNorm2d</code></a> 通常不应用仿射变换。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>num_features</strong> – <img alt="" src="../img/efc1fa1ed22c59909c50ea4639e0e861.jpg" /> from an expected input of size <img alt="" src="../img/a52adc3e96ebecd26270170745455e66.jpg" /></p>
</li>
<li>
<p><strong>eps</strong> – a value added to the denominator for numerical stability. Default: 1e-5</p>
</li>
<li>
<p><strong>momentum</strong> – the value used for the running_mean and running_var computation. Default: 0.1</p>
</li>
<li>
<p><strong>affine</strong> – a boolean value that when set to <code>True</code>, this module has learnable affine parameters, initialized the same way as done for batch normalization. Default: <code>False</code>.</p>
</li>
<li>
<p><strong>track_running_stats</strong> – a boolean value that when set to <code>True</code>, this module tracks the running mean and variance, and when set to <code>False</code>, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: <code>False</code></p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/a52adc3e96ebecd26270170745455e66.jpg" /></p>
</li>
<li>
<p>Output: <img alt="" src="../img/a52adc3e96ebecd26270170745455e66.jpg" /> (same shape as input)</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; # Without Learnable Parameters
&gt;&gt;&gt; m = nn.InstanceNorm2d(100)
&gt;&gt;&gt; # With Learnable Parameters
&gt;&gt;&gt; m = nn.InstanceNorm2d(100, affine=True)
&gt;&gt;&gt; input = torch.randn(20, 100, 35, 45)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="instancenorm3d">InstanceNorm3d</h3>
<hr />
<pre><code>class torch.nn.InstanceNorm3d(num_features, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)¶
</code></pre>
<p>如论文<a href="https://arxiv.org/abs/1607.08022">中所述，将实例标准化应用于 5D 输入(具有附加通道尺寸的 3D 输入的微型批处理）实例标准化：快速样式化</a>缺少的成分。</p>
<p><img alt="" src="../img/b4eedd00eca90b97b69e6b2958bf0356.jpg" /></p>
<p>微型批处理中每个对象的维数均值和标准差分别计算。 如果<code>affine</code>为<code>True</code>，则<img alt="" src="../img/1f8b1c3a15bdc67a9d0642538fee2580.jpg" />和<img alt="" src="../img/53a496ec7d546e2af9595a7055dd6a7e.jpg" />是大小为 C 的可学习参数矢量(其中 C 为输入大小）。</p>
<p>By default, this layer uses instance statistics computed from input data in both training and evaluation modes.</p>
<p>If <code>track_running_stats</code> is set to <code>True</code>, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default <code>momentum</code> of 0.1.</p>
<p>Note</p>
<p>This <code>momentum</code> argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is <img alt="" src="../img/b8b7f43cac4585f2aa1b335a69407a47.jpg" />, where <img alt="" src="../img/1533dcca53d64ca456ac5ad11b03a2cf.jpg" /> is the estimated statistic and <img alt="" src="../img/9c89ef5a4fdc4fecb9f6768daaabd7cc.jpg" /> is the new observed value.</p>
<p>Note</p>
<p><a href="#torch.nn.InstanceNorm3d" title="torch.nn.InstanceNorm3d"><code>InstanceNorm3d</code></a> 和 <a href="#torch.nn.LayerNorm" title="torch.nn.LayerNorm"><code>LayerNorm</code></a> 非常相似，但有一些细微的差异。 <a href="#torch.nn.InstanceNorm3d" title="torch.nn.InstanceNorm3d"><code>InstanceNorm3d</code></a> 适用于通道数据的每个通道，例如具有 RGB 颜色的 3D 模型，但 <a href="#torch.nn.LayerNorm" title="torch.nn.LayerNorm"><code>LayerNorm</code></a> 通常适用于整个样本，并且经常用于 NLP 任务。 另外， <a href="#torch.nn.LayerNorm" title="torch.nn.LayerNorm"><code>LayerNorm</code></a> 应用逐元素仿射变换，而 <a href="#torch.nn.InstanceNorm3d" title="torch.nn.InstanceNorm3d"><code>InstanceNorm3d</code></a> 通常不应用仿射变换。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>num_features</strong> – <img alt="" src="../img/efc1fa1ed22c59909c50ea4639e0e861.jpg" /> from an expected input of size <img alt="" src="../img/2bbc80f07f22b3961bbc382bd5a5e8f3.jpg" /></p>
</li>
<li>
<p><strong>eps</strong> – a value added to the denominator for numerical stability. Default: 1e-5</p>
</li>
<li>
<p><strong>momentum</strong> – the value used for the running_mean and running_var computation. Default: 0.1</p>
</li>
<li>
<p><strong>affine</strong> – a boolean value that when set to <code>True</code>, this module has learnable affine parameters, initialized the same way as done for batch normalization. Default: <code>False</code>.</p>
</li>
<li>
<p><strong>track_running_stats</strong> – a boolean value that when set to <code>True</code>, this module tracks the running mean and variance, and when set to <code>False</code>, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: <code>False</code></p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/2bbc80f07f22b3961bbc382bd5a5e8f3.jpg" /></p>
</li>
<li>
<p>Output: <img alt="" src="../img/2bbc80f07f22b3961bbc382bd5a5e8f3.jpg" /> (same shape as input)</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; # Without Learnable Parameters
&gt;&gt;&gt; m = nn.InstanceNorm3d(100)
&gt;&gt;&gt; # With Learnable Parameters
&gt;&gt;&gt; m = nn.InstanceNorm3d(100, affine=True)
&gt;&gt;&gt; input = torch.randn(20, 100, 35, 45, 10)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="_30">层范数</h3>
<hr />
<pre><code>class torch.nn.LayerNorm(normalized_shape, eps=1e-05, elementwise_affine=True)¶
</code></pre>
<p>如论文<a href="https://arxiv.org/abs/1607.06450">层规范化</a>中所述，在小批量输入上应用层规范化。</p>
<p><img alt="" src="../img/69cf729fafaba56b72f793a28ce26c03.jpg" /></p>
<p>平均值和标准偏差是在最后一定数量的尺寸上分别计算的，这些尺寸必须具有<code>normalized_shape</code>指定的形状。 如果<code>elementwise_affine</code>为<code>True</code>，则<img alt="" src="../img/1f8b1c3a15bdc67a9d0642538fee2580.jpg" />和<img alt="" src="../img/53a496ec7d546e2af9595a7055dd6a7e.jpg" />是<code>normalized_shape</code>的可学习仿射变换参数。</p>
<p>Note</p>
<p>与批量归一化和实例归一化不同，后者使用<code>affine</code>选项对每个通道/平面应用标量缩放和偏置，而层归一化则使用<code>elementwise_affine</code>对每个元素缩放和偏置。</p>
<p>This layer uses statistics computed from input data in both training and evaluation modes.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>normalized_shape</strong>  (<em>python：int</em> <em>或</em> <em>列表</em> <em>或</em> <em>torch尺寸</em>）–</p>
<p>输入尺寸的预期输入</p>
<p><img alt="" src="../img/9d0f6388e6381112ed5bfa3b80ce64d5.jpg" /></p>
<p>如果使用单个整数，则将其视为一个单例列表，并且此模块将在最后一个预期为该特定大小的维度上进行规范化。</p>
</li>
<li>
<p><strong>eps</strong> – a value added to the denominator for numerical stability. Default: 1e-5</p>
</li>
<li>
<p><strong>elementwise_affine</strong> –一个布尔值，当设置为<code>True</code>时，此模块具有可学习的按元素仿射参数，分别初始化为 1(用于权重）和零(用于偏差）。 默认值：<code>True</code>。</p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>输入：<img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" /></p>
</li>
<li>
<p>输出：<img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" />(形状与输入相同）</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; input = torch.randn(20, 5, 10, 10)
&gt;&gt;&gt; # With Learnable Parameters
&gt;&gt;&gt; m = nn.LayerNorm(input.size()[1:])
&gt;&gt;&gt; # Without Learnable Parameters
&gt;&gt;&gt; m = nn.LayerNorm(input.size()[1:], elementwise_affine=False)
&gt;&gt;&gt; # Normalize over last two dimensions
&gt;&gt;&gt; m = nn.LayerNorm([10, 10])
&gt;&gt;&gt; # Normalize over last dimension of size 10
&gt;&gt;&gt; m = nn.LayerNorm(10)
&gt;&gt;&gt; # Activating the module
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="localresponsenorm">LocalResponseNorm</h3>
<hr />
<pre><code>class torch.nn.LocalResponseNorm(size, alpha=0.0001, beta=0.75, k=1.0)¶
</code></pre>
<p>在由多个输入平面组成的输入信号上应用本地响应归一化，其中通道占据第二维。 跨通道应用标准化。</p>
<p><img alt="" src="../img/d4ebeebaf82396b8de4c5ac6cabd2b07.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>大小</strong> –用于标准化的相邻信道的数量</p>
</li>
<li>
<p><strong>alpha</strong> –乘法因子。 默认值：0.0001</p>
</li>
<li>
<p><strong>beta</strong> -指数。 默认值：0.75</p>
</li>
<li>
<p><strong>k</strong> –加法因子。 默认值：1</p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/877a6a1f684db60343e3c9847af621cd.jpg" /></p>
</li>
<li>
<p>Output: <img alt="" src="../img/877a6a1f684db60343e3c9847af621cd.jpg" /> (same shape as input)</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; lrn = nn.LocalResponseNorm(2)
&gt;&gt;&gt; signal_2d = torch.randn(32, 5, 24, 24)
&gt;&gt;&gt; signal_4d = torch.randn(16, 5, 7, 7, 7, 7)
&gt;&gt;&gt; output_2d = lrn(signal_2d)
&gt;&gt;&gt; output_4d = lrn(signal_4d)

</code></pre>
<h2 id="_31">循环层</h2>
<h3 id="rnn">RNN 库</h3>
<hr />
<pre><code>class torch.nn.RNNBase(mode, input_size, hidden_size, num_layers=1, bias=True, batch_first=False, dropout=0.0, bidirectional=False)¶
</code></pre>
<hr />
<pre><code>flatten_parameters()¶
</code></pre>
<p>重置参数数据指针，以便它们可以使用更快的代码路径。</p>
<p>现在，仅当模块在 GPU 上并且启用 cuDNN 时，此方法才有效。 否则，这是无人操作。</p>
<h3 id="rnn_1">RNN</h3>
<hr />
<pre><code>class torch.nn.RNN(*args, **kwargs)¶
</code></pre>
<p>将具有<img alt="" src="../img/7354cfd98ddea91e88d945eaeb68b241.jpg" />或<img alt="" src="../img/e71d901380b5ab1438092e7d0a525166.jpg" />非线性的多层 Elman RNN 应用于输入序列。</p>
<p>对于输入序列中的每个元素，每一层都会计算以下功能：</p>
<p><img alt="" src="../img/e2d18160a8db2c019b445fa5040b65e1.jpg" /></p>
<p>其中<img alt="" src="../img/3fd3982fee4410f80dc7f5f61fde4adc.jpg" />是时间 &lt;cite&gt;t&lt;/cite&gt; 的隐藏状态，<img alt="" src="../img/9c89ef5a4fdc4fecb9f6768daaabd7cc.jpg" />是时间 &lt;cite&gt;t&lt;/cite&gt; 的输入，而<img alt="" src="../img/137b2023c97cef7b94bb32d72084c2dd.jpg" />是时间&lt;cite&gt;的上一层的隐藏状态 ] t-1&lt;/cite&gt; 或时间 &lt;cite&gt;0&lt;/cite&gt; 时的初始隐藏状态。 如果<code>nonlinearity</code>为<code>'relu'</code>，则使用 &lt;cite&gt;ReLU&lt;/cite&gt; 代替 &lt;cite&gt;tanh&lt;/cite&gt; 。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input_size</strong> -输入 &lt;cite&gt;x&lt;/cite&gt; 中预期功能的数量</p>
</li>
<li>
<p><strong>hidden_​​size</strong> –处于隐藏状态的特征数 &lt;cite&gt;h&lt;/cite&gt;</p>
</li>
<li>
<p><strong>num_layers</strong> –循环层数。 例如，设置<code>num_layers=2</code>意味着将两个 RNN 堆叠在一起以形成&lt;cite&gt;堆叠的 RNN&lt;/cite&gt; ，而第二个 RNN 则接收第一个 RNN 的输出并计算最终结果。 默认值：1</p>
</li>
<li>
<p><strong>非线性</strong> –使用的非线性。 可以是<code>'tanh'</code>或<code>'relu'</code>。 默认值：<code>'tanh'</code></p>
</li>
<li>
<p><strong>偏置</strong>-如果<code>False</code>，则该层不使用偏置权重 &lt;cite&gt;b_ih&lt;/cite&gt; 和 &lt;cite&gt;b_hh&lt;/cite&gt; 。 默认值：<code>True</code></p>
</li>
<li>
<p><strong>batch_first</strong> –如果为<code>True</code>，则输入和输出张量以&lt;cite&gt;(批，序列，特征）&lt;/cite&gt;的形式提供。 默认值：<code>False</code></p>
</li>
<li>
<p><strong>dropout</strong> –如果不为零，则在除最后一层之外的每个 RNN 层的输出上引入 &lt;cite&gt;Dropout&lt;/cite&gt; 层，其丢弃概率等于<code>dropout</code>。 默认值：0</p>
</li>
<li>
<p><strong>双向</strong> –如果<code>True</code>成为双向 RNN。 默认值：<code>False</code></p>
</li>
</ul>
<pre><code>Inputs: input, h_0
</code></pre>
<ul>
<li>
<p>形状为&lt;cite&gt;(seq_len，批处理，input_size）的<strong>输入</strong>：&lt;/cite&gt;：包含输入序列特征的张量。 输入也可以是打包的可变长度序列。 有关详细信息，请参见 <a href="#torch.nn.utils.rnn.pack_padded_sequence" title="torch.nn.utils.rnn.pack_padded_sequence"><code>torch.nn.utils.rnn.pack_padded_sequence()</code></a> 或 <a href="#torch.nn.utils.rnn.pack_sequence" title="torch.nn.utils.rnn.pack_sequence"><code>torch.nn.utils.rnn.pack_sequence()</code></a> 。</p>
</li>
<li>
<p><strong>h_0 &lt;cite&gt;的形状为&lt;cite&gt;(num_layers * num_directions，批处理，hidden_​​size）&lt;/cite&gt;：张量，包含批处理中每个元素的初始隐藏状态。 如果未提供，则默认为零。 如果 RNN 是双向的，则 num_directions 应该为 2，否则应为 1。&lt;/cite&gt;</strong></p>
</li>
</ul>
<pre><code>Outputs: output, h_n
</code></pre>
<ul>
<li>
<p><strong>输出形状为&lt;cite&gt;(seq_len，批处理，num_directions * hidden_​​size）的&lt;/cite&gt;</strong> ：张量包含来自 RNN 的最后一层的输出特征 (&lt;cite&gt;h_t&lt;/cite&gt; )，对于每个[ &lt;cite&gt;t&lt;/cite&gt; 。 如果已将 <a href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code>torch.nn.utils.rnn.PackedSequence</code></a> 作为输入，则输出也将是打包序列。</p>
<p>对于未包装的情况，可以使用<code>output.view(seq_len, batch, num_directions, hidden_size)</code>分隔方向，向前和向后分别是方向 &lt;cite&gt;0&lt;/cite&gt; 和 &lt;cite&gt;1&lt;/cite&gt; 。 同样，在包装好的情况下，方向也可以分开。</p>
</li>
<li>
<p><strong>h_n &lt;cite&gt;的形状为&lt;cite&gt;(num_layers * num_directions，批处理，hidden_​​size）&lt;/cite&gt;：包含 &lt;cite&gt;t = seq_len&lt;/cite&gt; 的隐藏状态的张量。&lt;/cite&gt;</strong></p>
<p>像_输出_一样，可以使用<code>h_n.view(num_layers, num_directions, batch, hidden_size)</code>分离各层。</p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input1：包含输入特征的<img alt="" src="../img/743afebab2c571c044f834ebc0b49876.jpg" />张量，其中<img alt="" src="../img/5c2d4162cc0e2c310ab131936dd18379.jpg" />和 &lt;cite&gt;L&lt;/cite&gt; 表示序列长度。</p>
</li>
<li>
<p>Input2：<img alt="" src="../img/232b6b3af90c18e2f958b7f31a8149a7.jpg" />张量，包含批次中每个元素的初始隐藏状态。 <img alt="" src="../img/29eef31c436a3d76feb1567024f35397.jpg" />如果未提供，则默认为零。 其中<img alt="" src="../img/673966db2f00f66f67797a55595e3848.jpg" />如果 RNN 是双向的，则 num_directions 应该为 2，否则应为 1。</p>
</li>
<li>
<p>输出 1：<img alt="" src="../img/0d787878e3cb82da31f56105583e2d36.jpg" />，其中<img alt="" src="../img/78483d83fd1d8f56ebd76441c376054b.jpg" /></p>
</li>
<li>
<p>输出 2：<img alt="" src="../img/232b6b3af90c18e2f958b7f31a8149a7.jpg" />张量，包含批次中每个元素的下一个隐藏状态</p>
</li>
</ul>
<pre><code>Variables
</code></pre>
<ul>
<li>
<p><strong>〜RNN.weight_ih_l [k]</strong> –第 k 层的可学习的输入隐藏权重，形状为&lt;cite&gt;(hidden_​​size，input_size）&lt;/cite&gt;，其中 &lt;cite&gt;k = 0&lt;/cite&gt; 。 否则，形状为&lt;cite&gt;(hidden_​​size，num_directions * hidden_​​size）&lt;/cite&gt;</p>
</li>
<li>
<p><strong>〜RNN.weight_hh_l [k]</strong> –第 k 层可学习的隐藏权重，形状为&lt;cite&gt;(hidden_​​size，hidden_​​size）&lt;/cite&gt;</p>
</li>
<li>
<p><strong>〜RNN.bias_ih_l [k]</strong> –第 k 层的可学习的输入隐藏偏差，形状为&lt;cite&gt;(hidden_​​size）&lt;/cite&gt;</p>
</li>
<li>
<p><strong>〜RNN.bias_hh_l [k]</strong> –第 k 层的可学习的隐藏偏差，形状为&lt;cite&gt;(hidden_​​size）&lt;/cite&gt;</p>
</li>
</ul>
<p>Note</p>
<p>所有权重和偏差均从<img alt="" src="../img/b0cc88ecd067ab9281ee698299aa4b54.jpg" />初始化，其中<img alt="" src="../img/e9d93bc41851f68043abf39caaeef4a2.jpg" /></p>
<p>Note</p>
<p>如果满足以下条件：1）启用 cudnn，2）输入数据在 GPU 上 3）输入数据具有 dtype <code>torch.float16</code> 4）使用 V100 GPU，5）输入数据不是<code>PackedSequence</code>格式的持久算法 可以选择以提高性能。</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; rnn = nn.RNN(10, 20, 2)
&gt;&gt;&gt; input = torch.randn(5, 3, 10)
&gt;&gt;&gt; h0 = torch.randn(2, 3, 20)
&gt;&gt;&gt; output, hn = rnn(input, h0)

</code></pre>
<h3 id="lstm">LSTM</h3>
<hr />
<pre><code>class torch.nn.LSTM(*args, **kwargs)¶
</code></pre>
<p>将多层长短期记忆(LSTM）RNN 应用于输入序列。</p>
<p>For each element in the input sequence, each layer computes the following function:</p>
<p><img alt="" src="../img/3fa57b9a1bf16eb1e03938e0aa064e9e.jpg" /></p>
<p>其中<img alt="" src="../img/3fd3982fee4410f80dc7f5f61fde4adc.jpg" />是时间 &lt;cite&gt;t&lt;/cite&gt; 的隐藏状态，<img alt="" src="../img/bff01b04bb6431bafb28b6bd496ca544.jpg" />是时间 &lt;cite&gt;t&lt;/cite&gt; 的单元状态，<img alt="" src="../img/9c89ef5a4fdc4fecb9f6768daaabd7cc.jpg" />是时间 &lt;cite&gt;t&lt;/cite&gt; 的输入 ，<img alt="" src="../img/137b2023c97cef7b94bb32d72084c2dd.jpg" />是时间 &lt;cite&gt;t-1&lt;/cite&gt; 时层的隐藏状态，或者是时间 &lt;cite&gt;0&lt;/cite&gt; 时的初始隐藏状态，以及<img alt="" src="../img/b37e615a4c6a2fafb6810096893be9e3.jpg" />，<img alt="" src="../img/c1b689f3f0426213e1fd65c78264614b.jpg" />，<img alt="" src="../img/c5f99d1d61c7d54e731f9018031dcac2.jpg" />， <img alt="" src="../img/9c77c0e32fc84ee13c79e190bf56ca58.jpg" />分别是输入，忘记，单元和输出门。 <img alt="" src="../img/cc84e998f72a1c5a0a5f736ba6a9ff34.jpg" />是 S 型函数，<img alt="" src="../img/dcef98688866c0d5a21137cf53bf228d.jpg" />是 Hadamard 乘积。</p>
<p>在多层 LSTM 中，第<img alt="" src="../img/af4f4fa6e6abb6653ad3ae9b0801d996.jpg" />层(<img alt="" src="../img/05da1e2fe3f0aeb7e6316b5a46f3b554.jpg" />）的输入<img alt="" src="../img/048ad68d7c3b399538acac5238871e7e.jpg" />是前一层的隐藏状态<img alt="" src="../img/9dfcfd6f6cc7d8f382e98a72e163b5cc.jpg" />乘以压降<img alt="" src="../img/da17e8ef928f0a8812f94847f807dc1d.jpg" />，其中每个<img alt="" src="../img/da17e8ef928f0a8812f94847f807dc1d.jpg" />是伯努利随机变量 概率为<code>dropout</code>的是<img alt="" src="../img/e647c6371faf8b6dd9327515ae95cbdb.jpg" />。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input_size</strong> – The number of expected features in the input &lt;cite&gt;x&lt;/cite&gt;</p>
</li>
<li>
<p><strong>hidden_size</strong> – The number of features in the hidden state &lt;cite&gt;h&lt;/cite&gt;</p>
</li>
<li>
<p><strong>num_layers</strong> –循环层数。 例如，设置<code>num_layers=2</code>意味着将两个 LSTM 堆叠在一起以形成&lt;cite&gt;堆叠的 LSTM&lt;/cite&gt; ，而第二个 LSTM 则接收第一个 LSTM 的输出并计算最终结果。 默认值：1</p>
</li>
<li>
<p><strong>bias</strong> – If <code>False</code>, then the layer does not use bias weights &lt;cite&gt;b_ih&lt;/cite&gt; and &lt;cite&gt;b_hh&lt;/cite&gt;. Default: <code>True</code></p>
</li>
<li>
<p><strong>batch_first</strong> –如果为<code>True</code>，则输入和输出张量按(batch，seq，feature）提供。 默认值：<code>False</code></p>
</li>
<li>
<p><strong>dropout</strong> –如果不为零，则在除最后一层以外的每个 LSTM 层的输出上引入 &lt;cite&gt;Dropout&lt;/cite&gt; 层，其丢弃概率等于<code>dropout</code>。 默认值：0</p>
</li>
<li>
<p><strong>双向</strong> –如果<code>True</code>变为双向 LSTM。 默认值：<code>False</code></p>
</li>
</ul>
<pre><code>Inputs: input, (h_0, c_0)
</code></pre>
<ul>
<li>
<p>形状为&lt;cite&gt;(seq_len，批处理，input_size）的<strong>输入</strong>：&lt;/cite&gt;：包含输入序列特征的张量。 输入也可以是打包的可变长度序列。 有关详细信息，请参见 <a href="#torch.nn.utils.rnn.pack_padded_sequence" title="torch.nn.utils.rnn.pack_padded_sequence"><code>torch.nn.utils.rnn.pack_padded_sequence()</code></a> 或 <a href="#torch.nn.utils.rnn.pack_sequence" title="torch.nn.utils.rnn.pack_sequence"><code>torch.nn.utils.rnn.pack_sequence()</code></a> 。</p>
</li>
<li>
<p><strong>h_0 &lt;cite&gt;的形状为&lt;cite&gt;(num_layers * num_directions，批处理，hidden_​​size）&lt;/cite&gt;：张量，包含批处理中每个元素的初始隐藏状态。 如果 LSTM 是双向的，则 num_directions 应该为 2，否则应为 1。&lt;/cite&gt;</strong></p>
</li>
<li>
<p><strong>c_0</strong> 的形状为&lt;cite&gt;(num_layers * num_directions，批处理，hidden_​​size）&lt;/cite&gt;：张量，包含批处理中每个元素的初始单元状态。</p>
<p>如果未提供&lt;cite&gt;(h_0，c_0）&lt;/cite&gt;，则 <strong>h_0</strong> 和 <strong>c_0</strong> 均默认为零。</p>
</li>
</ul>
<pre><code>Outputs: output, (h_n, c_n)
</code></pre>
<ul>
<li>
<p>每个[[G] &lt;cite&gt;t&lt;/cite&gt; 。 如果已将 <a href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code>torch.nn.utils.rnn.PackedSequence</code></a> 作为输入，则输出也将是打包序列。</p>
<p>For the unpacked case, the directions can be separated using <code>output.view(seq_len, batch, num_directions, hidden_size)</code>, with forward and backward being direction &lt;cite&gt;0&lt;/cite&gt; and &lt;cite&gt;1&lt;/cite&gt; respectively. Similarly, the directions can be separated in the packed case.</p>
</li>
<li>
<p><strong>h_n</strong> of shape &lt;cite&gt;(num_layers * num_directions, batch, hidden_size)&lt;/cite&gt;: tensor containing the hidden state for &lt;cite&gt;t = seq_len&lt;/cite&gt;.</p>
<p>像_输出_一样，可以使用<code>h_n.view(num_layers, num_directions, batch, hidden_size)</code>分隔各层，并且对于 <em>c_n</em> 同样。</p>
</li>
<li>
<p><strong>c_n &lt;cite&gt;的形状为&lt;cite&gt;(num_layers * num_directions，batch，hidden_​​size）&lt;/cite&gt;：张量，其中包含 &lt;cite&gt;t = seq_len&lt;/cite&gt; 的像元状态。&lt;/cite&gt;</strong></p>
</li>
</ul>
<pre><code>Variables
</code></pre>
<ul>
<li>
<p><strong>〜LSTM.weight_ih_l [k]</strong> – &lt;cite&gt;形状为&lt;cite&gt;(4 * &lt;cite&gt;k = 0&lt;/cite&gt; 的[hidden_​​size，input_size）&lt;/cite&gt;。 否则，形状为&lt;cite&gt;(4 * hidden_​​size，num_directions * hidden_​​size）&lt;/cite&gt;&lt;/cite&gt;</p>
</li>
<li>
<p><strong>〜LSTM.weight_hh_l [k]</strong> – <img alt="" src="../img/c9ea8acb2c30b1271d907994c7975114.jpg" />层&lt;cite&gt;(W_hi | W_hf | W_hg | W_ho）&lt;/cite&gt;的可学习的隐藏权重，形状为&lt;cite&gt;(4 * hidden_​​size，hidden_​​size）&lt;/cite&gt;</p>
</li>
<li>
<p><strong>〜LSTM.bias_ih_l [k]</strong> – <img alt="" src="../img/c9ea8acb2c30b1271d907994c7975114.jpg" />层&lt;cite&gt;(b_ii | b_if | b_ig | b_io）&lt;/cite&gt;的可学习输入隐藏偏置，形状为&lt;cite&gt;(4 * hidden_​​size）&lt;/cite&gt;</p>
</li>
<li>
<p><strong>〜LSTM.bias_hh_l [k]</strong> – <img alt="" src="../img/c9ea8acb2c30b1271d907994c7975114.jpg" />层&lt;cite&gt;(b_hi | b_hf | b_hg | b_ho）&lt;/cite&gt;的可学习的隐藏偏置，形状为&lt;cite&gt;(4 * hidden_​​size）&lt;/cite&gt;</p>
</li>
</ul>
<p>Note</p>
<p>All the weights and biases are initialized from <img alt="" src="../img/b0cc88ecd067ab9281ee698299aa4b54.jpg" /> where <img alt="" src="../img/e9d93bc41851f68043abf39caaeef4a2.jpg" /></p>
<p>Note</p>
<p>If the following conditions are satisfied: 1) cudnn is enabled, 2) input data is on the GPU 3) input data has dtype <code>torch.float16</code> 4) V100 GPU is used, 5) input data is not in <code>PackedSequence</code> format persistent algorithm can be selected to improve performance.</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; rnn = nn.LSTM(10, 20, 2)
&gt;&gt;&gt; input = torch.randn(5, 3, 10)
&gt;&gt;&gt; h0 = torch.randn(2, 3, 20)
&gt;&gt;&gt; c0 = torch.randn(2, 3, 20)
&gt;&gt;&gt; output, (hn, cn) = rnn(input, (h0, c0))

</code></pre>
<h3 id="_32">格鲁</h3>
<hr />
<pre><code>class torch.nn.GRU(*args, **kwargs)¶
</code></pre>
<p>将多层门控循环单元(GRU）RNN 应用于输入序列。</p>
<p>For each element in the input sequence, each layer computes the following function:</p>
<p><img alt="" src="../img/dc12b4d7fa269c9f843d3ca76e784967.jpg" /></p>
<p>其中<img alt="" src="../img/3fd3982fee4410f80dc7f5f61fde4adc.jpg" />是时间 &lt;cite&gt;t&lt;/cite&gt; 的隐藏状态，<img alt="" src="../img/9c89ef5a4fdc4fecb9f6768daaabd7cc.jpg" />是时间 &lt;cite&gt;t&lt;/cite&gt; 的输入，<img alt="" src="../img/137b2023c97cef7b94bb32d72084c2dd.jpg" />是时间 &lt;cite&gt;t 时层的隐藏状态 -1&lt;/cite&gt; 或时间 &lt;cite&gt;0&lt;/cite&gt; 时的初始隐藏状态，以及<img alt="" src="../img/7d95a2168ef986b7a2c2dd8c24087216.jpg" />，<img alt="" src="../img/22d07c8b78d5b99de7d2d6b5f23a6d1d.jpg" />和<img alt="" src="../img/55edb01fedd57895a92015f146ac833b.jpg" />分别是复位门，更新门和新门。 <img alt="" src="../img/cc84e998f72a1c5a0a5f736ba6a9ff34.jpg" />是 S 型函数，<img alt="" src="../img/dcef98688866c0d5a21137cf53bf228d.jpg" />是 Hadamard 乘积。</p>
<p>在多层 GRU 中，第<img alt="" src="../img/af4f4fa6e6abb6653ad3ae9b0801d996.jpg" />层(<img alt="" src="../img/05da1e2fe3f0aeb7e6316b5a46f3b554.jpg" />）的输入<img alt="" src="../img/048ad68d7c3b399538acac5238871e7e.jpg" />是前一层的隐藏状态<img alt="" src="../img/9dfcfd6f6cc7d8f382e98a72e163b5cc.jpg" />乘以压降<img alt="" src="../img/da17e8ef928f0a8812f94847f807dc1d.jpg" />，其中每个<img alt="" src="../img/da17e8ef928f0a8812f94847f807dc1d.jpg" />是伯努利随机变量 概率为<code>dropout</code>的是<img alt="" src="../img/e647c6371faf8b6dd9327515ae95cbdb.jpg" />。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input_size</strong> – The number of expected features in the input &lt;cite&gt;x&lt;/cite&gt;</p>
</li>
<li>
<p><strong>hidden_size</strong> – The number of features in the hidden state &lt;cite&gt;h&lt;/cite&gt;</p>
</li>
<li>
<p><strong>num_layers</strong> –循环层数。 例如，设置<code>num_layers=2</code>意味着将两个 GRU 堆叠在一起以形成&lt;cite&gt;堆叠的 GRU&lt;/cite&gt; ，而第二个 GRU 则接收第一个 GRU 的输出并计算最终结果。 默认值：1</p>
</li>
<li>
<p><strong>bias</strong> – If <code>False</code>, then the layer does not use bias weights &lt;cite&gt;b_ih&lt;/cite&gt; and &lt;cite&gt;b_hh&lt;/cite&gt;. Default: <code>True</code></p>
</li>
<li>
<p><strong>batch_first</strong> – If <code>True</code>, then the input and output tensors are provided as (batch, seq, feature). Default: <code>False</code></p>
</li>
<li>
<p><strong>丢失</strong> –如果不为零，则在除最后一层之外的每个 GRU 层的输出上引入&lt;cite&gt;丢失&lt;/cite&gt;层，丢失概率等于<code>dropout</code>。 默认值：0</p>
</li>
<li>
<p><strong>双向</strong> –如果<code>True</code>成为双向 GRU。 默认值：<code>False</code></p>
</li>
</ul>
<pre><code>Inputs: input, h_0
</code></pre>
<ul>
<li>
<p>形状为&lt;cite&gt;(seq_len，批处理，input_size）的<strong>输入</strong>：&lt;/cite&gt;：包含输入序列特征的张量。 输入也可以是打包的可变长度序列。 有关详细信息，请参见 <a href="#torch.nn.utils.rnn.pack_padded_sequence" title="torch.nn.utils.rnn.pack_padded_sequence"><code>torch.nn.utils.rnn.pack_padded_sequence()</code></a> 。</p>
</li>
<li>
<p><strong>h_0</strong> of shape &lt;cite&gt;(num_layers * num_directions, batch, hidden_size)&lt;/cite&gt;: tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided. If the RNN is bidirectional, num_directions should be 2, else it should be 1.</p>
</li>
</ul>
<pre><code>Outputs: output, h_n
</code></pre>
<ul>
<li>
<p><strong>输出形状为&lt;cite&gt;(seq_len，batch，num_directions * hidden_​​size）的&lt;/cite&gt;</strong>：：对于每个 &lt;cite&gt;t&lt;/cite&gt; ，张量包含来自 GRU 最后一层的输出特征 h_t。 如果已给定 <a href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code>torch.nn.utils.rnn.PackedSequence</code></a> ，则输出也将是打包序列。 对于未包装的情况，可以使用<code>output.view(seq_len, batch, num_directions, hidden_size)</code>分离方向，向前和向后分别是方向 &lt;cite&gt;0&lt;/cite&gt; 和 &lt;cite&gt;1&lt;/cite&gt; 。</p>
<p>同样，在包装好的情况下，方向也可以分开。</p>
</li>
<li>
<p><strong>h_n</strong> 的形状为&lt;cite&gt;(num_layers * num_directions，批处理，hidden_​​size）&lt;/cite&gt;：张量，其中包含&lt;cite&gt;的隐藏状态 t = seq_len&lt;/cite&gt;</p>
<p>Like <em>output</em>, the layers can be separated using <code>h_n.view(num_layers, num_directions, batch, hidden_size)</code>.</p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input1: <img alt="" src="../img/743afebab2c571c044f834ebc0b49876.jpg" /> tensor containing input features where <img alt="" src="../img/5c2d4162cc0e2c310ab131936dd18379.jpg" /> and &lt;cite&gt;L&lt;/cite&gt; represents a sequence length.</p>
</li>
<li>
<p>Input2: <img alt="" src="../img/232b6b3af90c18e2f958b7f31a8149a7.jpg" /> tensor containing the initial hidden state for each element in the batch. <img alt="" src="../img/29eef31c436a3d76feb1567024f35397.jpg" /> Defaults to zero if not provided. where <img alt="" src="../img/673966db2f00f66f67797a55595e3848.jpg" /> If the RNN is bidirectional, num_directions should be 2, else it should be 1.</p>
</li>
<li>
<p>Output1: <img alt="" src="../img/0d787878e3cb82da31f56105583e2d36.jpg" /> where <img alt="" src="../img/78483d83fd1d8f56ebd76441c376054b.jpg" /></p>
</li>
<li>
<p>Output2: <img alt="" src="../img/232b6b3af90c18e2f958b7f31a8149a7.jpg" /> tensor containing the next hidden state for each element in the batch</p>
</li>
</ul>
<pre><code>Variables
</code></pre>
<ul>
<li>
<p><strong>〜GRU.weight_ih_l [k]</strong> – <img alt="" src="../img/c9ea8acb2c30b1271d907994c7975114.jpg" />层(W_ir | W_iz | W_in）的可学习的输入隐藏权重，形状为&lt;cite&gt;(3 * hidden_​​size，input_size）&lt;/cite&gt; &lt;cite&gt;k = 0&lt;/cite&gt; 。 否则，形状为&lt;cite&gt;(3 * hidden_​​size，num_directions * hidden_​​size）&lt;/cite&gt;</p>
</li>
<li>
<p><strong>〜GRU.weight_hh_l [k]</strong> – <img alt="" src="../img/c9ea8acb2c30b1271d907994c7975114.jpg" />层(W_hr | W_hz | W_hn）的可学习隐藏权重，形状为&lt;cite&gt;(3 * hidden_​​size，hidden_​​size）&lt;/cite&gt;</p>
</li>
<li>
<p><strong>〜GRU.bias_ih_l [k]</strong> – <img alt="" src="../img/c9ea8acb2c30b1271d907994c7975114.jpg" />层(b_ir | b_iz | b_in）的可学习输入隐藏偏置，形状为&lt;cite&gt;(3 * hidden_​​size）&lt;/cite&gt;</p>
</li>
<li>
<p><strong>〜GRU.bias_hh_l [k]</strong> – <img alt="" src="../img/c9ea8acb2c30b1271d907994c7975114.jpg" />层(b_hr | b_hz | b_hn）的可学习的隐藏偏置，形状为&lt;cite&gt;(3 * hidden_​​size）&lt;/cite&gt;</p>
</li>
</ul>
<p>Note</p>
<p>All the weights and biases are initialized from <img alt="" src="../img/b0cc88ecd067ab9281ee698299aa4b54.jpg" /> where <img alt="" src="../img/e9d93bc41851f68043abf39caaeef4a2.jpg" /></p>
<p>Note</p>
<p>If the following conditions are satisfied: 1) cudnn is enabled, 2) input data is on the GPU 3) input data has dtype <code>torch.float16</code> 4) V100 GPU is used, 5) input data is not in <code>PackedSequence</code> format persistent algorithm can be selected to improve performance.</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; rnn = nn.GRU(10, 20, 2)
&gt;&gt;&gt; input = torch.randn(5, 3, 10)
&gt;&gt;&gt; h0 = torch.randn(2, 3, 20)
&gt;&gt;&gt; output, hn = rnn(input, h0)

</code></pre>
<h3 id="_33">核细胞</h3>
<hr />
<pre><code>class torch.nn.RNNCell(input_size, hidden_size, bias=True, nonlinearity='tanh')¶
</code></pre>
<p>具有 tanh 或 ReLU 非线性的 Elman RNN 单元。</p>
<p><img alt="" src="../img/2655abdf2939ef63b4a059ef41e944ac.jpg" /></p>
<p>如果<code>nonlinearity</code>为&lt;cite&gt;'relu'&lt;/cite&gt;，则使用 ReLU 代替 tanh。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input_size</strong> – The number of expected features in the input &lt;cite&gt;x&lt;/cite&gt;</p>
</li>
<li>
<p><strong>hidden_size</strong> – The number of features in the hidden state &lt;cite&gt;h&lt;/cite&gt;</p>
</li>
<li>
<p><strong>bias</strong> – If <code>False</code>, then the layer does not use bias weights &lt;cite&gt;b_ih&lt;/cite&gt; and &lt;cite&gt;b_hh&lt;/cite&gt;. Default: <code>True</code></p>
</li>
<li>
<p><strong>nonlinearity</strong> – The non-linearity to use. Can be either <code>'tanh'</code> or <code>'relu'</code>. Default: <code>'tanh'</code></p>
</li>
</ul>
<pre><code>Inputs: input, hidden
</code></pre>
<ul>
<li>
<p><strong>形状为&lt;cite&gt;的输入&lt;/cite&gt;</strong>(批处理，input_size）：包含输入特征的张量</p>
</li>
<li>
<p><strong>形状为&lt;cite&gt;(批次，hidden_​​size）&lt;/cite&gt;的隐藏</strong>：张量，其中包含批次中每个元素的初始隐藏状态。 如果未提供，则默认为零。</p>
</li>
</ul>
<pre><code>Outputs: h'
</code></pre>
<ul>
<li><strong>h'</strong>的形状为&lt;cite&gt;(批处理，hidden_​​size）&lt;/cite&gt;：张量，其中包含批次中每个元素的下一个隐藏状态</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input1：包含输入特征的<img alt="" src="../img/dcfed7dba6eb5cb7169c3fe22dc9969e.jpg" />张量，其中<img alt="" src="../img/e1efc9ea97c94c5e931c7c4d1547f70e.jpg" /> = &lt;cite&gt; input_size&lt;/cite&gt;</p>
</li>
<li>
<p>Input2：<img alt="" src="../img/5049d81f92fd7cbb024d94e4efe31404.jpg" />张量，包含批处理中每个元素的初始隐藏状态，其中<img alt="" src="../img/d020d0163e4d7b2ad8f74c3621612972.jpg" /> = &lt;cite&gt;hidden_​​size&lt;/cite&gt; 如果未提供，则默认为零。</p>
</li>
<li>
<p>输出：<img alt="" src="../img/5049d81f92fd7cbb024d94e4efe31404.jpg" />张量包含批处理中每个元素的下一个隐藏状态</p>
</li>
</ul>
<pre><code>Variables
</code></pre>
<ul>
<li>
<p><strong>〜RNNCell.weight_ih</strong> –可学习的输入隐藏权重，形状为&lt;cite&gt;(hidden_​​size，input_size）&lt;/cite&gt;</p>
</li>
<li>
<p><strong>〜RNNCell.weight_hh</strong> –可学习的隐藏权重，形状为&lt;cite&gt;(hidden_​​size，hidden_​​size）&lt;/cite&gt;</p>
</li>
<li>
<p><strong>〜RNNCell.bias_ih</strong> –形状为&lt;cite&gt;(hidden_​​size）&lt;/cite&gt;的可学习的隐藏输入偏差</p>
</li>
<li>
<p><strong>〜RNNCell.bias_hh</strong> –形状为&lt;cite&gt;(hidden_​​size）&lt;/cite&gt;的可学习的隐藏偏差。</p>
</li>
</ul>
<p>Note</p>
<p>All the weights and biases are initialized from <img alt="" src="../img/b0cc88ecd067ab9281ee698299aa4b54.jpg" /> where <img alt="" src="../img/e9d93bc41851f68043abf39caaeef4a2.jpg" /></p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; rnn = nn.RNNCell(10, 20)
&gt;&gt;&gt; input = torch.randn(6, 3, 10)
&gt;&gt;&gt; hx = torch.randn(3, 20)
&gt;&gt;&gt; output = []
&gt;&gt;&gt; for i in range(6):
        hx = rnn(input[i], hx)
        output.append(hx)

</code></pre>
<h3 id="lstm_1">LSTM 单元</h3>
<hr />
<pre><code>class torch.nn.LSTMCell(input_size, hidden_size, bias=True)¶
</code></pre>
<p>长短期记忆(LSTM）单元。</p>
<p><img alt="" src="../img/6b98ecc453bd7da29378e73743b2ade5.jpg" /></p>
<p>其中<img alt="" src="../img/cc84e998f72a1c5a0a5f736ba6a9ff34.jpg" />是 S 型函数，<img alt="" src="../img/dcef98688866c0d5a21137cf53bf228d.jpg" />是 Hadamard 乘积。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input_size</strong> – The number of expected features in the input &lt;cite&gt;x&lt;/cite&gt;</p>
</li>
<li>
<p><strong>hidden_size</strong> – The number of features in the hidden state &lt;cite&gt;h&lt;/cite&gt;</p>
</li>
<li>
<p><strong>偏置</strong>-如果<code>False</code>，则该层不使用偏置权重 &lt;cite&gt;b_ih&lt;/cite&gt; 和 &lt;cite&gt;b_hh&lt;/cite&gt; 。 默认值：<code>True</code></p>
</li>
</ul>
<pre><code>Inputs: input, (h_0, c_0)
</code></pre>
<ul>
<li>
<p><strong>input</strong> of shape &lt;cite&gt;(batch, input_size)&lt;/cite&gt;: tensor containing input features</p>
</li>
<li>
<p><strong>h_0 &lt;cite&gt;的形状为&lt;/cite&gt;</strong>(批处理，hidden_​​size）：张量，其中包含批次中每个元素的初始隐藏状态。</p>
</li>
<li>
<p><strong>c_0</strong> 的形状为&lt;cite&gt;(批处理，hidden_​​size）&lt;/cite&gt;：张量，其中包含批次中每个元素的初始单元状态。</p>
<p>If &lt;cite&gt;(h_0, c_0)&lt;/cite&gt; is not provided, both <strong>h_0</strong> and <strong>c_0</strong> default to zero.</p>
</li>
</ul>
<pre><code>Outputs: (h_1, c_1)
</code></pre>
<ul>
<li>
<p><strong>h_1 &lt;cite&gt;的形状为&lt;cite&gt;(批处理，hidden_​​size）&lt;/cite&gt;：张量，其中包含批次中每个元素的下一个隐藏状态&lt;/cite&gt;</strong></p>
</li>
<li>
<p><strong>c_1 &lt;cite&gt;的形状为&lt;cite&gt;(批处理，hidden_​​size）&lt;/cite&gt;：张量，其中包含批次中每个元素的下一个单元格状态&lt;/cite&gt;</strong></p>
</li>
</ul>
<pre><code>Variables
</code></pre>
<ul>
<li>
<p><strong>〜LSTMCell.weight_ih</strong> –可学习的输入隐藏权重，形状为&lt;cite&gt;(4 * hidden_​​size，input_size）&lt;/cite&gt;</p>
</li>
<li>
<p><strong>〜LSTMCell.weight_hh</strong> –可学习的隐藏权重，形状为&lt;cite&gt;(4 * hidden_​​size，hidden_​​size）&lt;/cite&gt;</p>
</li>
<li>
<p><strong>〜LSTMCell.bias_ih</strong> –形状为&lt;cite&gt;(4 * hidden_​​size）&lt;/cite&gt;的可学习的隐藏输入偏差</p>
</li>
<li>
<p><strong>〜LSTMCell.bias_hh</strong> –形状为&lt;cite&gt;(4 * hidden_​​size）&lt;/cite&gt;的可学习的隐藏偏差。</p>
</li>
</ul>
<p>Note</p>
<p>All the weights and biases are initialized from <img alt="" src="../img/b0cc88ecd067ab9281ee698299aa4b54.jpg" /> where <img alt="" src="../img/e9d93bc41851f68043abf39caaeef4a2.jpg" /></p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; rnn = nn.LSTMCell(10, 20)
&gt;&gt;&gt; input = torch.randn(6, 3, 10)
&gt;&gt;&gt; hx = torch.randn(3, 20)
&gt;&gt;&gt; cx = torch.randn(3, 20)
&gt;&gt;&gt; output = []
&gt;&gt;&gt; for i in range(6):
        hx, cx = rnn(input[i], (hx, cx))
        output.append(hx)

</code></pre>
<h3 id="_34">格鲁塞尔</h3>
<hr />
<pre><code>class torch.nn.GRUCell(input_size, hidden_size, bias=True)¶
</code></pre>
<p>门控循环单元(GRU）单元</p>
<p><img alt="" src="../img/d1206914aae0a6de5bf4bb430136bb5e.jpg" /></p>
<p>where <img alt="" src="../img/cc84e998f72a1c5a0a5f736ba6a9ff34.jpg" /> is the sigmoid function, and <img alt="" src="../img/dcef98688866c0d5a21137cf53bf228d.jpg" /> is the Hadamard product.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input_size</strong> – The number of expected features in the input &lt;cite&gt;x&lt;/cite&gt;</p>
</li>
<li>
<p><strong>hidden_size</strong> – The number of features in the hidden state &lt;cite&gt;h&lt;/cite&gt;</p>
</li>
<li>
<p><strong>bias</strong> – If <code>False</code>, then the layer does not use bias weights &lt;cite&gt;b_ih&lt;/cite&gt; and &lt;cite&gt;b_hh&lt;/cite&gt;. Default: <code>True</code></p>
</li>
</ul>
<pre><code>Inputs: input, hidden
</code></pre>
<ul>
<li>
<p><strong>input</strong> of shape &lt;cite&gt;(batch, input_size)&lt;/cite&gt;: tensor containing input features</p>
</li>
<li>
<p><strong>hidden</strong> of shape &lt;cite&gt;(batch, hidden_size)&lt;/cite&gt;: tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided.</p>
</li>
</ul>
<pre><code>Outputs: h'
</code></pre>
<ul>
<li><strong>h'</strong> of shape &lt;cite&gt;(batch, hidden_size)&lt;/cite&gt;: tensor containing the next hidden state for each element in the batch</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input1: <img alt="" src="../img/dcfed7dba6eb5cb7169c3fe22dc9969e.jpg" /> tensor containing input features where <img alt="" src="../img/e1efc9ea97c94c5e931c7c4d1547f70e.jpg" /> = &lt;cite&gt;input_size&lt;/cite&gt;</p>
</li>
<li>
<p>Input2: <img alt="" src="../img/5049d81f92fd7cbb024d94e4efe31404.jpg" /> tensor containing the initial hidden state for each element in the batch where <img alt="" src="../img/d020d0163e4d7b2ad8f74c3621612972.jpg" /> = &lt;cite&gt;hidden_size&lt;/cite&gt; Defaults to zero if not provided.</p>
</li>
<li>
<p>Output: <img alt="" src="../img/5049d81f92fd7cbb024d94e4efe31404.jpg" /> tensor containing the next hidden state for each element in the batch</p>
</li>
</ul>
<pre><code>Variables
</code></pre>
<ul>
<li>
<p><strong>〜GRUCell.weight_ih</strong> –可学习的输入隐藏权重，形状为&lt;cite&gt;(3 * hidden_​​size，input_size）&lt;/cite&gt;</p>
</li>
<li>
<p><strong>〜GRUCell.weight_hh</strong> –可学习的隐藏权重，形状为&lt;cite&gt;(3 * hidden_​​size，hidden_​​size）&lt;/cite&gt;</p>
</li>
<li>
<p><strong>〜GRUCell.bias_ih</strong> –可学习的输入隐藏偏差，形状为&lt;cite&gt;(3 * hidden_​​size）&lt;/cite&gt;</p>
</li>
<li>
<p><strong>〜GRUCell.bias_hh</strong> –可学习的隐藏偏差，形状为&lt;cite&gt;(3 * hidden_​​size）&lt;/cite&gt;</p>
</li>
</ul>
<p>Note</p>
<p>All the weights and biases are initialized from <img alt="" src="../img/b0cc88ecd067ab9281ee698299aa4b54.jpg" /> where <img alt="" src="../img/e9d93bc41851f68043abf39caaeef4a2.jpg" /></p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; rnn = nn.GRUCell(10, 20)
&gt;&gt;&gt; input = torch.randn(6, 3, 10)
&gt;&gt;&gt; hx = torch.randn(3, 20)
&gt;&gt;&gt; output = []
&gt;&gt;&gt; for i in range(6):
        hx = rnn(input[i], hx)
        output.append(hx)

</code></pre>
<h2 id="_35">变压器层</h2>
<h3 id="_36">变压器</h3>
<hr />
<pre><code>class torch.nn.Transformer(d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, dropout=0.1, activation='relu', custom_encoder=None, custom_decoder=None)¶
</code></pre>
<p>变压器模型。 用户可以根据需要修改属性。 该体系结构基于论文“注意就是您所需要的”。 Ashish Vaswani，Noam Shazeer，Niki Parmar，Jakob Uszkoreit，Llion Jones，Aidan N Gomez，Lukasz Kaiser 和 Illia Polosukhin。 2017 年。您只需要关注即可。 《神经信息处理系统的发展》，第 6000-6010 页。 用户可以使用相应的参数构建 BERT (<a href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805</a>)模型。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>d_model</strong> –编码器/解码器输入中的预期功能数量(默认= 512）。</p>
</li>
<li>
<p><strong>nhead</strong> –多头注意力模型中的头数(默认为 8）。</p>
</li>
<li>
<p><strong>num_encoder_layers</strong> –编码器中的子编码器层数(默认为 6）。</p>
</li>
<li>
<p><strong>num_decoder_layers</strong> –解码器中子解码器层的数量(默认为 6）。</p>
</li>
<li>
<p><strong>dim_feedforward</strong> -前馈网络模型的尺寸(默认= 2048）。</p>
</li>
<li>
<p><strong>dropout</strong> -退出值(默认值= 0.1）。</p>
</li>
<li>
<p><strong>激活</strong> –编码器/解码器中间层 relu 或 gelu 的激活功能(默认= relu）。</p>
</li>
<li>
<p><strong>custom_encoder</strong> –自定义编码器(默认=无）。</p>
</li>
<li>
<p><strong>custom_decoder</strong> -自定义解码器(默认=无）。</p>
</li>
</ul>
<pre><code>Examples::
</code></pre>
<pre><code>&gt;&gt;&gt; transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)
&gt;&gt;&gt; src = torch.rand((10, 32, 512))
&gt;&gt;&gt; tgt = torch.rand((20, 32, 512))
&gt;&gt;&gt; out = transformer_model(src, tgt)

</code></pre>
<p>注意：<a href="https://github.com/pytorch/examples/tree/master/word_language_model">中提供了将 nn.Transformer 模块应用于单词语言模型的完整示例，网址为 https://github.com/pytorch/examples/tree/master/word_language_model</a></p>
<hr />
<pre><code>forward(src, tgt, src_mask=None, tgt_mask=None, memory_mask=None, src_key_padding_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None)¶
</code></pre>
<p>接收并处理屏蔽的源/目标序列。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>src</strong> –编码器的序列(必需）。</p>
</li>
<li>
<p><strong>tgt</strong> –解码器的序列(必需）。</p>
</li>
<li>
<p><strong>src_mask</strong> – src 序列的附加掩码(可选）。</p>
</li>
<li>
<p><strong>tgt_mask</strong> – tgt 序列的附加掩码(可选）。</p>
</li>
<li>
<p><strong>memory_mask</strong> –编码器输出的附加掩码(可选）。</p>
</li>
<li>
<p><strong>src_key_padding_mask</strong> –每批 src 密钥的 ByteTensor 掩码(可选）。</p>
</li>
<li>
<p><strong>tgt_key_padding_mask</strong> –每批 tgt 密钥的 ByteTensor 掩码(可选）。</p>
</li>
<li>
<p><strong>memory_key_padding_mask</strong> –每批存储密钥的 ByteTensor 掩码(可选）。</p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>src：<img alt="" src="../img/5dc2d865ca031350cad0cde77d835371.jpg" />。</p>
</li>
<li>
<p>tgt：<img alt="" src="../img/15fc5cff0a5b74cdea4d70c75d87dc2e.jpg" />。</p>
</li>
<li>
<p>src_mask：<img alt="" src="../img/ba5846b9a626df2ee623bcccf9c9c65f.jpg" />。</p>
</li>
<li>
<p>tgt_mask：<img alt="" src="../img/0cea5e49559d3369ab9e567ed49fc558.jpg" />。</p>
</li>
<li>
<p>memory_mask：<img alt="" src="../img/3093b144c2b4f75932ceb3a5b2cbf85a.jpg" />。</p>
</li>
<li>
<p>src_key_padding_mask：<img alt="" src="../img/8fd771cabec0d2f2ff6ac3992568c469.jpg" />。</p>
</li>
<li>
<p>tgt_key_padding_mask：<img alt="" src="../img/c790b435eb8d1dacc77ae74971c53e12.jpg" />。</p>
</li>
<li>
<p>memory_key_padding_mask：<img alt="" src="../img/8fd771cabec0d2f2ff6ac3992568c469.jpg" />。</p>
</li>
</ul>
<p>注意：[src / tgt / memory] ​​_mask 应该用 float('-inf'）表示被遮盖的位置，而 float(0.0）表示其他。 这些掩码可确保对位置 i 的预测仅取决于未掩码的位置 j，并且对批次中的每个序列均应用相同的预测。 [src / tgt / memory] ​​_key_padding_mask 应该是 ByteTensor，其中 True 值是应该用 float('-inf'）掩盖的位置，而 False 值将保持不变。 此掩码可确保在屏蔽后不会从位置 i 获取任何信息，并且对于批次中的每个序列都有单独的掩码。</p>
<ul>
<li>输出：<img alt="" src="../img/15fc5cff0a5b74cdea4d70c75d87dc2e.jpg" />。</li>
</ul>
<p>注意：由于转换器模型中的多头注意架构，转换器的输出序列长度与解码的输入序列(即目标）长度相同。</p>
<p>其中 S 是源序列长度，T 是目标序列长度，N 是批处理大小，E 是特征编号</p>
<p>Examples</p>
<pre><code>&gt;&gt;&gt; output = transformer_model(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)

</code></pre>
<hr />
<pre><code>generate_square_subsequent_mask(sz)¶
</code></pre>
<p>为该序列生成一个正方形蒙版。 屏蔽的位置填充有 float('-inf'）。 未屏蔽的位置填充有 float(0.0）。</p>
<h3 id="_37">变压器编码器</h3>
<hr />
<pre><code>class torch.nn.TransformerEncoder(encoder_layer, num_layers, norm=None)¶
</code></pre>
<p>TransformerEncoder 是 N 个编码器层的堆栈</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>coder_layer</strong> – TransformerEncoderLayer(）类的实例(必需）。</p>
</li>
<li>
<p><strong>num_layers</strong> –编码器中子编码器层的数量(必填）。</p>
</li>
<li>
<p><strong>规范</strong> –图层归一化组件(可选）。</p>
</li>
</ul>
<pre><code>Examples::
</code></pre>
<pre><code>&gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)
&gt;&gt;&gt; transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)
&gt;&gt;&gt; src = torch.rand(10, 32, 512)
&gt;&gt;&gt; out = transformer_encoder(src)

</code></pre>
<hr />
<pre><code>forward(src, mask=None, src_key_padding_mask=None)¶
</code></pre>
<p>将输入依次通过编码器层。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>src</strong> –编码器的序列(必需）。</p>
</li>
<li>
<p><strong>掩码</strong> – src 序列的掩码(可选）。</p>
</li>
<li>
<p><strong>src_key_padding_mask</strong> –每批 src 密钥的掩码(可选）。</p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<p>请参阅 Transformer 类中的文档。</p>
<h3 id="_38">变压器解码器</h3>
<hr />
<pre><code>class torch.nn.TransformerDecoder(decoder_layer, num_layers, norm=None)¶
</code></pre>
<p>TransformerDecoder 是 N 个解码器层的堆栈</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>coder_layer</strong> – TransformerDecoderLayer(）类的实例(必需）。</p>
</li>
<li>
<p><strong>num_layers</strong> –解码器中子解码器层的数量(必填）。</p>
</li>
<li>
<p><strong>norm</strong> – the layer normalization component (optional).</p>
</li>
</ul>
<pre><code>Examples::
</code></pre>
<pre><code>&gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)
&gt;&gt;&gt; transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)
&gt;&gt;&gt; memory = torch.rand(10, 32, 512)
&gt;&gt;&gt; tgt = torch.rand(20, 32, 512)
&gt;&gt;&gt; out = transformer_decoder(tgt, memory)

</code></pre>
<hr />
<pre><code>forward(tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None)¶
</code></pre>
<p>输入(和掩码）依次通过解码器层。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>tgt</strong> – the sequence to the decoder (required).</p>
</li>
<li>
<p><strong>存储器</strong> –从编码器最后一层开始的顺序(必需）。</p>
</li>
<li>
<p><strong>tgt_mask</strong> – tgt 序列的掩码(可选）。</p>
</li>
<li>
<p><strong>memory_mask</strong> –内存序列的掩码(可选）。</p>
</li>
<li>
<p><strong>tgt_key_padding_mask</strong> –每批 tgt 密钥的掩码(可选）。</p>
</li>
<li>
<p><strong>memory_key_padding_mask</strong> –每批存储密钥的掩码(可选）。</p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<p>see the docs in Transformer class.</p>
<h3 id="transformerencoderlayer">TransformerEncoderLayer</h3>
<hr />
<pre><code>class torch.nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=2048, dropout=0.1, activation='relu')¶
</code></pre>
<p>TransformerEncoderLayer 由自检和前馈网络组成。 此标准编码器层基于“注意就是您所需要的”一文。 Ashish Vaswani，Noam Shazeer，Niki Parmar，Jakob Uszkoreit，Llion Jones，Aidan N Gomez，Lukasz Kaiser 和 Illia Polosukhin。 2017 年。您只需要关注即可。 《神经信息处理系统的发展》，第 6000-6010 页。 用户可以在应用过程中以不同的方式修改或实现。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>d_model</strong> –输入中预期特征的数量(必填）。</p>
</li>
<li>
<p><strong>nhead</strong> –多头注意力模型中的头数(必填）。</p>
</li>
<li>
<p><strong>dim_feedforward</strong> – the dimension of the feedforward network model (default=2048).</p>
</li>
<li>
<p><strong>dropout</strong> – the dropout value (default=0.1).</p>
</li>
<li>
<p><strong>激活</strong> –中间层，relu 或 gelu(默认值= relu）的激活功能。</p>
</li>
</ul>
<pre><code>Examples::
</code></pre>
<pre><code>&gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)
&gt;&gt;&gt; src = torch.rand(10, 32, 512)
&gt;&gt;&gt; out = encoder_layer(src)

</code></pre>
<hr />
<pre><code>forward(src, src_mask=None, src_key_padding_mask=None)¶
</code></pre>
<p>使输入通过编码器层。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>src</strong> –编码器层的序列(必需）。</p>
</li>
<li>
<p><strong>src_mask</strong> – src 序列的掩码(可选）。</p>
</li>
<li>
<p><strong>src_key_padding_mask</strong> – the mask for the src keys per batch (optional).</p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<p>see the docs in Transformer class.</p>
<h3 id="_39">变压器解码器层</h3>
<hr />
<pre><code>class torch.nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward=2048, dropout=0.1, activation='relu')¶
</code></pre>
<p>TransformerDecoderLayer 由自组织，多头组织和前馈网络组成。 这个标准的解码器层基于论文“注意就是全部”。 Ashish Vaswani，Noam Shazeer，Niki Parmar，Jakob Uszkoreit，Llion Jones，Aidan N Gomez，Lukasz Kaiser 和 Illia Polosukhin。 2017 年。您只需要关注即可。 《神经信息处理系统的发展》，第 6000-6010 页。 用户可以在应用过程中以不同的方式修改或实现。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>d_model</strong> – the number of expected features in the input (required).</p>
</li>
<li>
<p><strong>nhead</strong> – the number of heads in the multiheadattention models (required).</p>
</li>
<li>
<p><strong>dim_feedforward</strong> – the dimension of the feedforward network model (default=2048).</p>
</li>
<li>
<p><strong>dropout</strong> – the dropout value (default=0.1).</p>
</li>
<li>
<p><strong>activation</strong> – the activation function of intermediate layer, relu or gelu (default=relu).</p>
</li>
</ul>
<pre><code>Examples::
</code></pre>
<pre><code>&gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)
&gt;&gt;&gt; memory = torch.rand(10, 32, 512)
&gt;&gt;&gt; tgt = torch.rand(20, 32, 512)
&gt;&gt;&gt; out = decoder_layer(tgt, memory)

</code></pre>
<hr />
<pre><code>forward(tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None)¶
</code></pre>
<p>将输入(和掩码）通过解码器层。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>tgt</strong> –解码器层的序列(必需）。</p>
</li>
<li>
<p><strong>memory</strong> – the sequnce from the last layer of the encoder (required).</p>
</li>
<li>
<p><strong>tgt_mask</strong> – the mask for the tgt sequence (optional).</p>
</li>
<li>
<p><strong>memory_mask</strong> – the mask for the memory sequence (optional).</p>
</li>
<li>
<p><strong>tgt_key_padding_mask</strong> – the mask for the tgt keys per batch (optional).</p>
</li>
<li>
<p><strong>memory_key_padding_mask</strong> – the mask for the memory keys per batch (optional).</p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<p>see the docs in Transformer class.</p>
<h2 id="_40">线性层</h2>
<h3 id="_41">身分识别</h3>
<hr />
<pre><code>class torch.nn.Identity(*args, **kwargs)¶
</code></pre>
<p>对参数不敏感的占位符身份运算符。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>args</strong> –任何参数(未使用）</p>
</li>
<li>
<p><strong>kwargs</strong> –任何关键字参数(未使用）</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.Identity(54, unused_argument1=0.1, unused_argument2=False)
&gt;&gt;&gt; input = torch.randn(128, 20)
&gt;&gt;&gt; output = m(input)
&gt;&gt;&gt; print(output.size())
torch.Size([128, 20])

</code></pre>
<h3 id="_42">线性的</h3>
<hr />
<pre><code>class torch.nn.Linear(in_features, out_features, bias=True)¶
</code></pre>
<p>对输入数据应用线性变换：<img alt="" src="../img/d817e6e509f7bee55d97c706b640d0c0.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>in_features</strong> –每个输入样本的大小</p>
</li>
<li>
<p><strong>out_features</strong> –每个输出样本的大小</p>
</li>
<li>
<p><strong>偏差</strong>-如果设置为<code>False</code>，则该图层将不会学习加法偏差。 默认值：<code>True</code></p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>输入：<img alt="" src="../img/6afc9be4f4494a7bc74ddc77348b7250.jpg" />，其中<img alt="" src="../img/dcef98688866c0d5a21137cf53bf228d.jpg" />表示任意数量的附加尺寸，<img alt="" src="../img/1aa48600768716bed7dd605675bcd50b.jpg" /></p>
</li>
<li>
<p>输出：<img alt="" src="../img/3eda73173a678dfce1130a5417d9eba6.jpg" />，除最后一个尺寸外，所有尺寸均与输入和<img alt="" src="../img/554a0055afae539eab69b878ddf1028b.jpg" />相同。</p>
</li>
</ul>
<pre><code>Variables
</code></pre>
<ul>
<li>
<p><strong>〜Linear.weight</strong> -形状为<img alt="" src="../img/83d717f417d6876606ec7528737b7712.jpg" />的模块的可学习重量。 值从<img alt="" src="../img/b0cc88ecd067ab9281ee698299aa4b54.jpg" />初始化，其中<img alt="" src="../img/9ff5cefd7df1a84948a2538c22ebf204.jpg" /></p>
</li>
<li>
<p><strong>〜Linear.bias</strong> -形状<img alt="" src="../img/42b83d0cda78f15594f94984575fbcae.jpg" />的模块的可学习偏差。 如果<code>bias</code>为<code>True</code>，则从<img alt="" src="../img/b0cc88ecd067ab9281ee698299aa4b54.jpg" />初始化值，其中<img alt="" src="../img/9ff5cefd7df1a84948a2538c22ebf204.jpg" /></p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.Linear(20, 30)
&gt;&gt;&gt; input = torch.randn(128, 20)
&gt;&gt;&gt; output = m(input)
&gt;&gt;&gt; print(output.size())
torch.Size([128, 30])

</code></pre>
<h3 id="_43">双线性</h3>
<hr />
<pre><code>class torch.nn.Bilinear(in1_features, in2_features, out_features, bias=True)¶
</code></pre>
<p>对输入数据应用双线性变换：<img alt="" src="../img/fdb467e7acd4534bf5e31c3873ad9648.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>in1_features</strong> –每个第一个输入样本的大小</p>
</li>
<li>
<p><strong>in2_features</strong> –每秒钟输入样本的大小</p>
</li>
<li>
<p><strong>out_features</strong> – size of each output sample</p>
</li>
<li>
<p><strong>偏差</strong>-如果设置为 False，则该图层将不会学习加法偏差。 默认值：<code>True</code></p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>输入 1：<img alt="" src="../img/8973812778d4cc89241a595a46fe285b.jpg" />，其中<img alt="" src="../img/0a2b108d6e502a272856a815696e40bb.jpg" />和<img alt="" src="../img/dcef98688866c0d5a21137cf53bf228d.jpg" />表示任意数量的附加尺寸。 除了最后输入的维度外，其他所有维度均应相同。</p>
</li>
<li>
<p>输入 2：<img alt="" src="../img/aadd52ac21cc3f13e2981beb2ab9d260.jpg" />其中<img alt="" src="../img/a486b505438561b697cbacfe22fff755.jpg" />。</p>
</li>
<li>
<p>输出：<img alt="" src="../img/3eda73173a678dfce1130a5417d9eba6.jpg" />，其中<img alt="" src="../img/0281c5af0f326743c751dd76ab6c8594.jpg" />和除最后一个尺寸外的所有尺寸均与输入相同。</p>
</li>
</ul>
<pre><code>Variables
</code></pre>
<ul>
<li>
<p><strong>〜Bilinear.weight</strong> -形状为<img alt="" src="../img/43e8f5ecbb7c4e156b13d960f0f4ef86.jpg" />的模块的可学习权重。 值从<img alt="" src="../img/b0cc88ecd067ab9281ee698299aa4b54.jpg" />初始化，其中<img alt="" src="../img/30be44cbcd901ad4b764401935e7e977.jpg" /></p>
</li>
<li>
<p><strong>〜Bilinear.bias</strong> -形状<img alt="" src="../img/42b83d0cda78f15594f94984575fbcae.jpg" />的模块的可学习偏差。 如果<code>bias</code>为<code>True</code>，则从<img alt="" src="../img/b0cc88ecd067ab9281ee698299aa4b54.jpg" />初始化值，其中<img alt="" src="../img/30be44cbcd901ad4b764401935e7e977.jpg" /></p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.Bilinear(20, 30, 40)
&gt;&gt;&gt; input1 = torch.randn(128, 20)
&gt;&gt;&gt; input2 = torch.randn(128, 30)
&gt;&gt;&gt; output = m(input1, input2)
&gt;&gt;&gt; print(output.size())
torch.Size([128, 40])

</code></pre>
<h2 id="_44">辍学层</h2>
<h3 id="_45">退出</h3>
<hr />
<pre><code>class torch.nn.Dropout(p=0.5, inplace=False)¶
</code></pre>
<p>在训练期间，使用伯努利分布的样本以概率<code>p</code>将输入张量的某些元素随机归零。 在每个前向呼叫中，每个通道将独立清零。</p>
<p>如论文<a href="https://arxiv.org/abs/1207.0580">中所述，通过防止特征检测器</a>的共同适应来改善神经网络，这已被证明是一种有效的技术，可用于规范化和防止神经元的共同适应。</p>
<p>此外，在训练期间，将输出缩放为<img alt="" src="../img/0464d2eed18b0919c1d68b14e8196f3e.jpg" />。 这意味着在评估期间，模块仅计算身份函数。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>p</strong> –元素归零的概率。 默认值：0.5</p>
</li>
<li>
<p><strong>就地</strong> –如果设置为<code>True</code>，将就地执行此操作。 默认值：<code>False</code></p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>输入：<img alt="" src="../img/39f05b69c0ea6b9b6b228f948944c4f6.jpg" />。 输入可以是任何形状</p>
</li>
<li>
<p>输出：<img alt="" src="../img/39f05b69c0ea6b9b6b228f948944c4f6.jpg" />。 输出与输入的形状相同</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.Dropout(p=0.2)
&gt;&gt;&gt; input = torch.randn(20, 16)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="dropout2d">Dropout2d</h3>
<hr />
<pre><code>class torch.nn.Dropout2d(p=0.5, inplace=False)¶
</code></pre>
<p>随机将整个通道调零(通道是 2D 特征图，例如，批输入中第<img alt="" src="../img/db26d1a59be5965889bd4d5533b7be61.jpg" />个样本的第<img alt="" src="../img/36608d1dd28464666846576485c40a7b.jpg" />个通道是 2D 张量<img alt="" src="../img/917b580efb3f22e40da4844882b18fe8.jpg" />）。 使用伯努利分布中的样本，每个信道将在每次前向呼叫中以概率<code>p</code>独立清零。</p>
<p>通常，输入来自<code>nn.Conv2d</code>模块。</p>
<p>如论文<a href="http://arxiv.org/abs/1411.4280">中所述，使用卷积网络</a>进行有效的对象定位，如果特征图中的相邻像素高度相关(通常是早期卷积层的情况），则 i.i.d。 辍学不会使激活规律化，否则只会导致有效学习率下降。</p>
<p>在这种情况下，<code>nn.Dropout2d()</code>将有助于促进要素地图之间的独立性，应改用<code>nn.Dropout2d()</code>。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>p</strong>  (<em>python：float</em> <em>，</em> <em>可选</em>）–元素归零的概率。</p>
</li>
<li>
<p><strong>原位</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–如果设置为<code>True</code>，则将原位执行此操作</p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/a52adc3e96ebecd26270170745455e66.jpg" /></p>
</li>
<li>
<p>Output: <img alt="" src="../img/a52adc3e96ebecd26270170745455e66.jpg" /> (same shape as input)</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.Dropout2d(p=0.2)
&gt;&gt;&gt; input = torch.randn(20, 16, 32, 32)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="3d_3">辍学 3d</h3>
<hr />
<pre><code>class torch.nn.Dropout3d(p=0.5, inplace=False)¶
</code></pre>
<p>随机将整个通道调零(通道是 3D 特征图，例如，批输入中第<img alt="" src="../img/db26d1a59be5965889bd4d5533b7be61.jpg" />个样本的第<img alt="" src="../img/36608d1dd28464666846576485c40a7b.jpg" />个通道是 3D 张量<img alt="" src="../img/917b580efb3f22e40da4844882b18fe8.jpg" />）。 使用伯努利分布中的样本，每个信道将在每次前向呼叫中以概率<code>p</code>独立清零。</p>
<p>通常，输入来自<code>nn.Conv3d</code>模块。</p>
<p>As described in the paper <a href="http://arxiv.org/abs/1411.4280">Efficient Object Localization Using Convolutional Networks</a> , if adjacent pixels within feature maps are strongly correlated (as is normally the case in early convolution layers) then i.i.d. dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease.</p>
<p>在这种情况下，<code>nn.Dropout3d()</code>将有助于促进要素地图之间的独立性，应改用<code>nn.Dropout3d()</code>。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>p</strong>  (<em>python：float</em> <em>，</em> <em>可选</em>）–元素归零的概率。</p>
</li>
<li>
<p><strong>inplace</strong> (<em>bool__,</em> <em>optional</em>) – If set to <code>True</code>, will do this operation in-place</p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/2bbc80f07f22b3961bbc382bd5a5e8f3.jpg" /></p>
</li>
<li>
<p>Output: <img alt="" src="../img/2bbc80f07f22b3961bbc382bd5a5e8f3.jpg" /> (same shape as input)</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.Dropout3d(p=0.2)
&gt;&gt;&gt; input = torch.randn(20, 16, 4, 32, 32)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="alphadropout">AlphaDropout</h3>
<hr />
<pre><code>class torch.nn.AlphaDropout(p=0.5, inplace=False)¶
</code></pre>
<p>将 Alpha Dropout 应用于输入。</p>
<p>Alpha Dropout 是一种 Dropout，可以维持自我规范化属性。 对于均值为零且单位标准差为零的输入，Alpha Dropout 的输出将保持输入的原始均值和标准差。 Alpha Dropout 与 SELU 激活功能紧密结合，可确保输出具有零均值和单位标准偏差。</p>
<p>在训练期间，它使用来自伯努利分布的样本以概率 <em>p</em> 随机掩盖输入张量的某些元素。 在每个前向调用中，要屏蔽的元素都会随机化，并进行缩放和移位以保持零均值和单位标准差。</p>
<p>在评估过程中，模块仅计算身份函数。</p>
<p>More details can be found in the paper <a href="https://arxiv.org/abs/1706.02515">Self-Normalizing Neural Networks</a> .</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>p</strong>  (<em>python：float</em> )–元素被删除的概率。 默认值：0.5</p>
</li>
<li>
<p><strong>inplace</strong> (<em>bool__,</em> <em>optional</em>) – If set to <code>True</code>, will do this operation in-place</p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/39f05b69c0ea6b9b6b228f948944c4f6.jpg" />. Input can be of any shape</p>
</li>
<li>
<p>Output: <img alt="" src="../img/39f05b69c0ea6b9b6b228f948944c4f6.jpg" />. Output is of the same shape as input</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.AlphaDropout(p=0.2)
&gt;&gt;&gt; input = torch.randn(20, 16)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h2 id="_46">稀疏层</h2>
<h3 id="_47">嵌入</h3>
<hr />
<pre><code>class torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False, _weight=None)¶
</code></pre>
<p>一个简单的查找表，用于存储固定字典和大小的嵌入。</p>
<p>该模块通常用于存储单词嵌入并使用索引检索它们。 模块的输入是索引列表，输出是相应的词嵌入。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>num_embeddings</strong>  (<em>python：int</em> )–嵌入字典的大小</p>
</li>
<li>
<p><strong>embedding_dim</strong>  (<em>python：int</em> )–每个嵌入向量的大小</p>
</li>
<li>
<p><strong>padding_idx</strong>  (<em>python：int</em> <em>，</em> <em>可选</em>）–如果给定，则在<code>padding_idx</code>处嵌入输出以填充输出(初始化为 零）。</p>
</li>
<li>
<p><strong>max_norm</strong>  (<em>python：float</em> <em>，</em> <em>可选</em>）–如果给定，则范数大于<code>max_norm</code>的每个嵌入矢量将重新规范化为具有 规范<code>max_norm</code>。</p>
</li>
<li>
<p><strong>norm_type</strong>  (<em>python：float</em> <em>，</em> <em>可选</em>）–为<code>max_norm</code>选项计算的 p 范数的 p。 默认值<code>2</code>。</p>
</li>
<li>
<p><strong>scale_grad_by_freq</strong> (<em>布尔值</em> <em>，</em> <em>可选</em>））–如果给定，则将按照最小 批量。 默认值<code>False</code>。</p>
</li>
<li>
<p><strong>稀疏</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–如果<code>True</code>，则梯度 w.r.t. <code>weight</code>矩阵将是稀疏张量。 有关稀疏渐变的更多详细信息，请参见注释。</p>
</li>
</ul>
<pre><code>Variables
</code></pre>
<p><strong>〜Embedding.weight</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–形状模块的可学习权重(num_embeddings，embedding_dim）从<img alt="" src="../img/edc1494f2f7b8422181db7ed2917ca8c.jpg" />初始化</p>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>输入：<img alt="" src="../img/39f05b69c0ea6b9b6b228f948944c4f6.jpg" />，任意形状的 LongTensor，包含要提取的索引</p>
</li>
<li>
<p>输出：<img alt="" src="../img/8d4f6906786906a0bc918c85911beaab.jpg" />，其中 &lt;cite&gt;*&lt;/cite&gt; 是输入形状，<img alt="" src="../img/120ecec574f5c971b4d86192aea5bcca.jpg" /></p>
</li>
</ul>
<p>Note</p>
<p>请记住，只有有限数量的优化程序支持稀疏渐变：当前为<code>optim.SGD</code> (&lt;cite&gt;CUDA&lt;/cite&gt; 和 &lt;cite&gt;CPU&lt;/cite&gt; )，<code>optim.SparseAdam</code> (&lt;cite&gt;CUDA&lt;/cite&gt; 和[ &lt;cite&gt;CPU&lt;/cite&gt; )和<code>optim.Adagrad</code> (&lt;cite&gt;CPU&lt;/cite&gt; )</p>
<p>Note</p>
<p>设置<code>padding_idx</code>时，<code>padding_idx</code>的嵌入矢量初始化为全零。 但是，请注意，此向量可以在以后进行修改，例如，使用定制的初始化方法，从而更改用于填充输出的向量。 来自 <a href="#torch.nn.Embedding" title="torch.nn.Embedding"><code>Embedding</code></a> 的矢量的梯度始终为零。</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; # an Embedding module containing 10 tensors of size 3
&gt;&gt;&gt; embedding = nn.Embedding(10, 3)
&gt;&gt;&gt; # a batch of 2 samples of 4 indices each
&gt;&gt;&gt; input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])
&gt;&gt;&gt; embedding(input)
tensor([[[-0.0251, -1.6902,  0.7172],
         [-0.6431,  0.0748,  0.6969],
         [ 1.4970,  1.3448, -0.9685],
         [-0.3677, -2.7265, -0.1685]],

        [[ 1.4970,  1.3448, -0.9685],
         [ 0.4362, -0.4004,  0.9400],
         [-0.6431,  0.0748,  0.6969],
         [ 0.9124, -2.3616,  1.1151]]])

&gt;&gt;&gt; # example with padding_idx
&gt;&gt;&gt; embedding = nn.Embedding(10, 3, padding_idx=0)
&gt;&gt;&gt; input = torch.LongTensor([[0,2,0,5]])
&gt;&gt;&gt; embedding(input)
tensor([[[ 0.0000,  0.0000,  0.0000],
         [ 0.1535, -2.0309,  0.9315],
         [ 0.0000,  0.0000,  0.0000],
         [-0.1655,  0.9897,  0.0635]]])

</code></pre>
<hr />
<pre><code>classmethod from_pretrained(embeddings, freeze=True, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False)¶
</code></pre>
<p>从给定的二维 FloatTensor 创建嵌入实例。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>嵌入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–包含嵌入权重的 FloatTensor。 第一维作为<code>num_embeddings</code>传递给嵌入，第二维作为<code>embedding_dim</code>传递给 Embedding。</p>
</li>
<li>
<p><strong>冻结</strong>(<em>布尔</em> <em>，</em> <em>可选</em>）–如果<code>True</code>，则张量不会在学习过程中更新。 等效于<code>embedding.weight.requires_grad = False</code>。 默认值：<code>True</code></p>
</li>
<li>
<p><strong>padding_idx</strong>  (<em>python：int</em> <em>，</em> <em>可选</em>）–请参阅模块初始化文档。</p>
</li>
<li>
<p><strong>max_norm</strong>  (<em>python：float</em> <em>，</em> <em>可选</em>）–请参阅模块初始化文档。</p>
</li>
<li>
<p><strong>norm_type</strong>  (<em>python：float</em> <em>，</em> <em>可选</em>）–请参阅模块初始化文档。 默认值<code>2</code>。</p>
</li>
<li>
<p><strong>scale_grad_by_freq</strong> (<em>布尔值</em> <em>，</em> <em>可选</em>）–请参见模块初始化文档。 默认值<code>False</code>。</p>
</li>
<li>
<p><strong>稀疏</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–请参阅模块初始化文档。</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; # FloatTensor containing pretrained weights
&gt;&gt;&gt; weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])
&gt;&gt;&gt; embedding = nn.Embedding.from_pretrained(weight)
&gt;&gt;&gt; # Get embeddings for index 1
&gt;&gt;&gt; input = torch.LongTensor([1])
&gt;&gt;&gt; embedding(input)
tensor([[ 4.0000,  5.1000,  6.3000]])

</code></pre>
<h3 id="_48">嵌入袋</h3>
<hr />
<pre><code>class torch.nn.EmbeddingBag(num_embeddings, embedding_dim, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, mode='mean', sparse=False, _weight=None)¶
</code></pre>
<p>计算嵌入“袋”的总和或方法，而无需实例化中间嵌入。</p>
<p>对于恒定长度且没有<code>per_sample_weights</code>的袋子，此类</p>
<blockquote>
<ul>
<li>
<p><code>mode="sum"</code>等于 <a href="#torch.nn.Embedding" title="torch.nn.Embedding"><code>Embedding</code></a> ，然后是<code>torch.sum(dim=0)</code>，</p>
</li>
<li>
<p><code>mode="mean"</code>等于 <a href="#torch.nn.Embedding" title="torch.nn.Embedding"><code>Embedding</code></a> ，然后是<code>torch.mean(dim=0)</code>，</p>
</li>
<li>
<p><code>mode="max"</code>的等效于 <a href="#torch.nn.Embedding" title="torch.nn.Embedding"><code>Embedding</code></a> ，然后是<code>torch.max(dim=0)</code>。</p>
</li>
</ul>
</blockquote>
<p>但是， <a href="#torch.nn.EmbeddingBag" title="torch.nn.EmbeddingBag"><code>EmbeddingBag</code></a> 比使用这些操作链要花费更多的时间和内存。</p>
<p>EmbeddingBag 还支持按样本权重作为正向传递的参数。 在执行<code>mode</code>指定的加权缩减之前，这会缩放嵌入的输出。 如果通过<code>per_sample_weights``，则唯一支持的</code>mode<code>是</code>"sum"<code>，它根据</code>per_sample_weights`计算加权和。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>num_embeddings</strong> (<em>python:int</em>) – size of the dictionary of embeddings</p>
</li>
<li>
<p><strong>embedding_dim</strong> (<em>python:int</em>) – the size of each embedding vector</p>
</li>
<li>
<p><strong>max_norm</strong> (<em>python:float__,</em> <em>optional</em>) – If given, each embedding vector with norm larger than <code>max_norm</code> is renormalized to have norm <code>max_norm</code>.</p>
</li>
<li>
<p><strong>norm_type</strong> (<em>python:float__,</em> <em>optional</em>) – The p of the p-norm to compute for the <code>max_norm</code> option. Default <code>2</code>.</p>
</li>
<li>
<p><strong>scale_grad_by_freq</strong> (<em>布尔</em> <em>，</em> <em>可选</em>）–如果指定，则将按比例缩小坡度中单词的频率 批量。 默认值<code>False</code>。 注意：<code>mode="max"</code>时不支持此选项。</p>
</li>
<li>
<p><strong>模式</strong>(<em>字符串</em> <em>，</em> <em>可选</em>）– <code>"sum"</code>，<code>"mean"</code>或<code>"max"</code>。 指定减少袋子的方式。 <code>"sum"</code>会考虑<code>per_sample_weights</code>来计算加权总和。 <code>"mean"</code>计算袋子中值的平均值，<code>"max"</code>计算每个袋子中的最大值。 默认值：<code>"mean"</code></p>
</li>
<li>
<p><strong>稀疏</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–如果<code>True</code>，则梯度 w.r.t. <code>weight</code>矩阵将是稀疏张量。 有关稀疏渐变的更多详细信息，请参见注释。 注意：<code>mode="max"</code>时不支持此选项。</p>
</li>
</ul>
<pre><code>Variables
</code></pre>
<p><strong>〜EmbeddingBag.weight</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–形状为&lt;cite&gt;的模块的可学习权重(从<img alt="" src="../img/edc1494f2f7b8422181db7ed2917ca8c.jpg" />初始化的 num_embeddings，embedding_dim）&lt;/cite&gt; 。</p>
<pre><code>Inputs: input (LongTensor), offsets (LongTensor, optional), and
</code></pre>
<p><code>per_index_weights</code>(张量，可选）</p>
<ul>
<li>
<p>如果<code>input</code>是形状为&lt;cite&gt;(B，N）&lt;/cite&gt;的二维，</p>
<p>它将被视为<code>B</code>袋(序列），每个袋子的长度都是固定长度<code>N</code>，这将返回<code>B</code>值的汇总值取决于<code>mode</code>。 在这种情况下，<code>offsets</code>被忽略，必须为<code>None</code>。</p>
</li>
<li>
<p>如果<code>input</code>是形状为&lt;cite&gt;(N）&lt;/cite&gt;的 1D，</p>
<p>它将被视为多个包(序列）的串联。 <code>offsets</code>必须是一维张量，其中包含<code>input</code>中每个包的起始索引位置。 因此，对于形状为&lt;cite&gt;(B）&lt;/cite&gt;的<code>offsets</code>，<code>input</code>将被视为具有<code>B</code>袋。 空袋子(即长度为 0 的袋子）将返回由零填充的向量。</p>
</li>
</ul>
<pre><code>per_sample_weights (Tensor, optional): a tensor of float / double weights, or None
</code></pre>
<p>表示所有权重应为<code>1</code>。 如果指定，<code>per_sample_weights</code>必须具有与输入完全相同的形状，并且如果不是<code>None</code>，则将其视为具有相同的<code>offsets</code>。 仅支持<code>mode='sum'</code>。</p>
<p>输出形状：&lt;cite&gt;(B，embedding_dim）&lt;/cite&gt;</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; # an Embedding module containing 10 tensors of size 3
&gt;&gt;&gt; embedding_sum = nn.EmbeddingBag(10, 3, mode='sum')
&gt;&gt;&gt; # a batch of 2 samples of 4 indices each
&gt;&gt;&gt; input = torch.LongTensor([1,2,4,5,4,3,2,9])
&gt;&gt;&gt; offsets = torch.LongTensor([0,4])
&gt;&gt;&gt; embedding_sum(input, offsets)
tensor([[-0.8861, -5.4350, -0.0523],
        [ 1.1306, -2.5798, -1.0044]])

</code></pre>
<hr />
<pre><code>classmethod from_pretrained(embeddings, freeze=True, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, mode='mean', sparse=False)¶
</code></pre>
<p>从给定的二维 FloatTensor 创建 EmbeddingBag 实例。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>嵌入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–包含 EmbeddingBag 权重的 FloatTensor。 第一维正以“ num_embeddings”传递给 EmbeddingBag，第二维以“ embedding_dim”传递。</p>
</li>
<li>
<p><strong>冻结</strong>(<em>布尔</em> <em>，</em> <em>可选</em>）–如果<code>True</code>，则张量不会在学习过程中更新。 等效于<code>embeddingbag.weight.requires_grad = False</code>。 默认值：<code>True</code></p>
</li>
<li>
<p><strong>max_norm</strong>  (<em>python：float</em> <em>，</em> <em>可选</em>）–请参阅模块初始化文档。 默认值：<code>None</code></p>
</li>
<li>
<p><strong>norm_type</strong> (<em>python:float__,</em> <em>optional</em>) – See module initialization documentation. Default <code>2</code>.</p>
</li>
<li>
<p><strong>scale_grad_by_freq</strong> (<em>boolean__,</em> <em>optional</em>) – See module initialization documentation. Default <code>False</code>.</p>
</li>
<li>
<p><strong>模式</strong>(<em>字符串</em> <em>，</em> <em>可选</em>）–请参见模块初始化文档。 默认值：<code>"mean"</code></p>
</li>
<li>
<p><strong>稀疏</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–请参阅模块初始化文档。 默认值：<code>False</code>。</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; # FloatTensor containing pretrained weights
&gt;&gt;&gt; weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])
&gt;&gt;&gt; embeddingbag = nn.EmbeddingBag.from_pretrained(weight)
&gt;&gt;&gt; # Get embeddings for index 1
&gt;&gt;&gt; input = torch.LongTensor([[1, 0]])
&gt;&gt;&gt; embeddingbag(input)
tensor([[ 2.5000,  3.7000,  4.6500]])

</code></pre>
<h2 id="_49">距离功能</h2>
<h3 id="_50">余弦相似度</h3>
<hr />
<pre><code>class torch.nn.CosineSimilarity(dim=1, eps=1e-08)¶
</code></pre>
<p>传回<img alt="" src="../img/6729a6dfa0e7278903c230760f72d93e.jpg" />与<img alt="" src="../img/3723cd7a3980a7833e2a8c9d94af161e.jpg" />之间的余弦相似度(沿 dim 计算）。</p>
<p><img alt="" src="../img/ace7429d7433d94caac88bffe995de16.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>昏暗的</strong> (<em>python：int</em> <em>，</em> <em>可选</em>）–计算余弦相似度的维度。 默认值：1</p>
</li>
<li>
<p><strong>eps</strong>  (<em>python：float</em> <em>，</em> <em>可选</em>）–避免被零除的小值。 默认值：1e-8</p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>输入 1：<img alt="" src="../img/dde3a2edd54b05e44f02300cc17bf1d7.jpg" />，其中 D 在位置&lt;cite&gt;变暗&lt;/cite&gt;</p>
</li>
<li>
<p>Input2：<img alt="" src="../img/dde3a2edd54b05e44f02300cc17bf1d7.jpg" />，形状与 Input1 相同</p>
</li>
<li>
<p>输出：<img alt="" src="../img/e2442d617370dc8d277d04eaaf157733.jpg" /></p>
</li>
</ul>
<pre><code>Examples::
</code></pre>
<pre><code>&gt;&gt;&gt; input1 = torch.randn(100, 128)
&gt;&gt;&gt; input2 = torch.randn(100, 128)
&gt;&gt;&gt; cos = nn.CosineSimilarity(dim=1, eps=1e-6)
&gt;&gt;&gt; output = cos(input1, input2)

</code></pre>
<h3 id="_51">成对距离</h3>
<hr />
<pre><code>class torch.nn.PairwiseDistance(p=2.0, eps=1e-06, keepdim=False)¶
</code></pre>
<p>使用 p 范数计算向量<img alt="" src="../img/d2a3fcb9182c1f6deb94da56191505ed.jpg" />和<img alt="" src="../img/1634f31b5183ed44cb5322e1b65cb8b2.jpg" />之间的成对成对距离：</p>
<p><img alt="" src="../img/5d71b030b5d229d538d0efdb4262896e.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>p</strong> (<em>实数</em>）–规范度。 默认值：2</p>
</li>
<li>
<p><strong>eps</strong>  (<em>python：float</em> <em>，</em> <em>可选</em>）–避免被零除的小值。 默认值：1e-6</p>
</li>
<li>
<p><strong>keepdim</strong>  (<em>bool</em> <em>，</em> <em>可选</em>）–确定是否保留矢量尺寸。 默认值：False</p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>输入 1：<img alt="" src="../img/64d764a4273bfec3a3846f3f0fbff88f.jpg" />，其中 &lt;cite&gt;D =矢量尺寸&lt;/cite&gt;</p>
</li>
<li>
<p>Input2：<img alt="" src="../img/64d764a4273bfec3a3846f3f0fbff88f.jpg" />，形状与 Input1 相同</p>
</li>
<li>
<p>输出：<img alt="" src="../img/a01009d68498c3f4597b15bab746c24d.jpg" />。 如果<code>keepdim</code>为<code>True</code>，则<img alt="" src="../img/fd9bc5819a18e1cc326657302de4adc2.jpg" />。</p>
</li>
</ul>
<pre><code>Examples::
</code></pre>
<pre><code>&gt;&gt;&gt; pdist = nn.PairwiseDistance(p=2)
&gt;&gt;&gt; input1 = torch.randn(100, 128)
&gt;&gt;&gt; input2 = torch.randn(100, 128)
&gt;&gt;&gt; output = pdist(input1, input2)

</code></pre>
<h2 id="_52">损失函数</h2>
<h3 id="l1">L1 损失</h3>
<hr />
<pre><code>class torch.nn.L1Loss(size_average=None, reduce=None, reduction='mean')¶
</code></pre>
<p>创建一个标准，该标准测量输入<img alt="" src="../img/2c2415be487481d652d00c279423c3d0.jpg" />中的每个元素与目标<img alt="" src="../img/faf0bb6f76d3932d159da2a147a17089.jpg" />中的平均绝对误差(MAE）。</p>
<p>未减少的损失(即<code>reduction</code>设置为<code>'none'</code>）的损失可描述为：</p>
<p><img alt="" src="../img/c15aab2b37dd57d298da5a87611a9153.jpg" /></p>
<p>其中<img alt="" src="../img/7ea0d6aa290d37c8bc57957206bafe80.jpg" />是批次大小。 如果<code>reduction</code>不是<code>'none'</code>(默认为<code>'mean'</code>），则：</p>
<p><img alt="" src="../img/e6f7bd0ed7f94854885226b129bb9011.jpg" /></p>
<p><img alt="" src="../img/2c2415be487481d652d00c279423c3d0.jpg" />和<img alt="" src="../img/faf0bb6f76d3932d159da2a147a17089.jpg" />是任意形状的张量，每个张量共有<img alt="" src="../img/5f0051b0454fa75ca446b59b47eff6f6.jpg" />个元素。</p>
<p>求和运算仍对所有元素进行运算，并除以<img alt="" src="../img/5f0051b0454fa75ca446b59b47eff6f6.jpg" />。</p>
<p>如果一组<code>reduction = 'sum'</code>可以避免被<img alt="" src="../img/5f0051b0454fa75ca446b59b47eff6f6.jpg" />划分。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>size_average</strong> (<em>布尔</em> <em>，</em> <em>可选</em>）–已弃用(请参见<code>reduction</code>）。 默认情况下，损失是批次中每个损失元素的平均数。 请注意，对于某些损失，每个样本有多个元素。 如果将字段<code>size_average</code>设置为<code>False</code>，则每个小批量的损失总和。 当 reduce 为<code>False</code>时将被忽略。 默认值：<code>True</code></p>
</li>
<li>
<p><strong>还原</strong>(<em>布尔</em> <em>，</em> <em>可选</em>）–已弃用(请参阅<code>reduction</code>）。 默认情况下，取决于<code>size_average</code>，对每个小批量的观测值求平均或求和。 当<code>reduce</code>为<code>False</code>时，返回每批元素损失，并忽略<code>size_average</code>。 默认值：<code>True</code></p>
</li>
<li>
<p><strong>缩减</strong>(<em>字符串</em> <em>，</em> <em>可选</em>）–指定要应用于输出的缩减：<code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>。 <code>'none'</code>：不应用任何减少量； <code>'mean'</code>：输出的总和除以输出中元素的数量； <code>'sum'</code>：将对输出求和。 注意：<code>size_average</code>和<code>reduce</code>正在淘汰中，与此同时，指定这两个 args 中的任何一个将覆盖<code>reduction</code>。 默认值：<code>'mean'</code></p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>输入：<img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" />其中<img alt="" src="../img/dcef98688866c0d5a21137cf53bf228d.jpg" />表示任意数量的附加尺寸</p>
</li>
<li>
<p>目标：<img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" />，形状与输入相同</p>
</li>
<li>
<p>输出：标量。 如果<code>reduction</code>为<code>'none'</code>，则<img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" />的形状与输入相同</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; loss = nn.L1Loss()
&gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True)
&gt;&gt;&gt; target = torch.randn(3, 5)
&gt;&gt;&gt; output = loss(input, target)
&gt;&gt;&gt; output.backward()

</code></pre>
<h3 id="_53">选配</h3>
<hr />
<pre><code>class torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean')¶
</code></pre>
<p>创建一个标准，该标准测量输入<img alt="" src="../img/2c2415be487481d652d00c279423c3d0.jpg" />和目标<img alt="" src="../img/faf0bb6f76d3932d159da2a147a17089.jpg" />中每个元素之间的均方误差(L2 平方平方）。</p>
<p>The unreduced (i.e. with <code>reduction</code> set to <code>'none'</code>) loss can be described as:</p>
<p><img alt="" src="../img/a1b78fd6bec54165dc29f0a454a09d58.jpg" /></p>
<p>where <img alt="" src="../img/7ea0d6aa290d37c8bc57957206bafe80.jpg" /> is the batch size. If <code>reduction</code> is not <code>'none'</code> (default <code>'mean'</code>), then:</p>
<p><img alt="" src="../img/b6a8a4e8de7de87f1f45cbab3d7a9ab1.jpg" /></p>
<p><img alt="" src="../img/2c2415be487481d652d00c279423c3d0.jpg" /> and <img alt="" src="../img/faf0bb6f76d3932d159da2a147a17089.jpg" /> are tensors of arbitrary shapes with a total of <img alt="" src="../img/5f0051b0454fa75ca446b59b47eff6f6.jpg" /> elements each.</p>
<p>The sum operation still operates over all the elements, and divides by <img alt="" src="../img/5f0051b0454fa75ca446b59b47eff6f6.jpg" />.</p>
<p>The division by <img alt="" src="../img/5f0051b0454fa75ca446b59b47eff6f6.jpg" /> can be avoided if one sets <code>reduction = 'sum'</code>.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>size_average</strong> (<em>bool__,</em> <em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></p>
</li>
<li>
<p><strong>reduce</strong> (<em>bool__,</em> <em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></p>
</li>
<li>
<p><strong>reduction</strong> (<em>string__,</em> <em>optional</em>) – Specifies the reduction to apply to the output: <code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum of the output will be divided by the number of elements in the output, <code>'sum'</code>: the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: <code>'mean'</code></p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" /> where <img alt="" src="../img/dcef98688866c0d5a21137cf53bf228d.jpg" /> means, any number of additional dimensions</p>
</li>
<li>
<p>Target: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" />, same shape as the input</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; loss = nn.MSELoss()
&gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True)
&gt;&gt;&gt; target = torch.randn(3, 5)
&gt;&gt;&gt; output = loss(input, target)
&gt;&gt;&gt; output.backward()

</code></pre>
<h3 id="_54">交叉熵损失</h3>
<hr />
<pre><code>class torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')¶
</code></pre>
<p>此标准将<code>nn.LogSoftmax()</code>和<code>nn.NLLLoss()</code>合并为一个类别。</p>
<p>在训练带有 &lt;cite&gt;C&lt;/cite&gt; 类的分类问题时很有用。 如果提供的话，可选参数<code>weight</code>应该是一维&lt;cite&gt;张量&lt;/cite&gt;，为每个类分配权重。 当您的训练集不平衡时，此功能特别有用。</p>
<p>&lt;cite&gt;输入&lt;/cite&gt;预计将包含每个类别的原始，未标准化的分数。</p>
<p>对于 &lt;cite&gt;K&lt;/cite&gt; -维情况(稍后描述），&lt;cite&gt;输入&lt;/cite&gt;必须为<img alt="" src="../img/170f6fd3c25c71669d79482ca111cb57.jpg" />或<img alt="" src="../img/413c5ecfc8061696d19bdb15824b3bf3.jpg" />和<img alt="" src="../img/81b2f06b62b9cf6d8138187eb833b452.jpg" />大小的张量。</p>
<p>对于一个大小为 &lt;cite&gt;minibatch&lt;/cite&gt; 的 1D 张量张量的每个值，此标准都希望在<img alt="" src="../img/cd248d64767e64a04f1180b6fbf62aba.jpg" />范围内的类别索引作为&lt;cite&gt;目标&lt;/cite&gt;； 如果指定了 &lt;cite&gt;ignore_index&lt;/cite&gt; ，则此条件也接受该类索引(该索引可能不一定在类范围内）。</p>
<p>损失可描述为：</p>
<p><img alt="" src="../img/08a27c373abe34203e7d8dc401bdbb1a.jpg" /></p>
<p>或在指定<code>weight</code>参数的情况下：</p>
<p><img alt="" src="../img/6634e8b3bf37fe6b01a9a74aaeead881.jpg" /></p>
<p>对于每个小批量，损失是通过观察得出的平均值。</p>
<p>通过提供<img alt="" src="../img/81b2f06b62b9cf6d8138187eb833b452.jpg" />大小为<img alt="" src="../img/413c5ecfc8061696d19bdb15824b3bf3.jpg" />的输入(其中<img alt="" src="../img/8ccf33bfd6edc45aa7ca77ecf75bf04a.jpg" />是尺寸的数量）和适当形状的目标，也可以用于更高尺寸的输入(例如 2D 图像）(请参见下文）。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>重量</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>，</em> <em>可选</em>）–为每个类别提供手动缩放比例的重量。 如果给定，则其张量必须为 &lt;cite&gt;C&lt;/cite&gt;</p>
</li>
<li>
<p><strong>size_average</strong> (<em>bool__,</em> <em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></p>
</li>
<li>
<p><strong>ignore_index</strong>  (<em>python：int</em> <em>，</em> <em>可选</em>）–指定目标值，该目标值将被忽略并且不会对输入梯度产生影响。 当<code>size_average</code>为<code>True</code>时，损耗是在不可忽略的目标上平均的。</p>
</li>
<li>
<p><strong>reduce</strong> (<em>bool__,</em> <em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></p>
</li>
<li>
<p><strong>reduction</strong> (<em>string__,</em> <em>optional</em>) – Specifies the reduction to apply to the output: <code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum of the output will be divided by the number of elements in the output, <code>'sum'</code>: the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: <code>'mean'</code></p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>输入：<img alt="" src="../img/e2f1df6fc3cf4eb0f5bee6a89bb2f37b.jpg" />，其中 &lt;cite&gt;C =类&lt;/cite&gt;的数量，或在 &lt;cite&gt;K&lt;/cite&gt; -尺寸损失的情况下，<img alt="" src="../img/d2b291d58abab8c9ef51a99cea539b2b.jpg" />和<img alt="" src="../img/81b2f06b62b9cf6d8138187eb833b452.jpg" />一起使用。</p>
</li>
<li>
<p>目标：<img alt="" src="../img/a01009d68498c3f4597b15bab746c24d.jpg" />，其中每个值为<img alt="" src="../img/35235b20063aefd21abb814e4f037d93.jpg" />，或者在 K 维丢失的情况下为<img alt="" src="../img/88a862c7e57602f20997d7172545b05f.jpg" />和<img alt="" src="../img/81b2f06b62b9cf6d8138187eb833b452.jpg" />。</p>
</li>
<li>
<p>输出：标量。 如果<code>reduction</code>为<code>'none'</code>，则与 K 尺寸相同：<img alt="" src="../img/a01009d68498c3f4597b15bab746c24d.jpg" />或<img alt="" src="../img/88a862c7e57602f20997d7172545b05f.jpg" />，带有<img alt="" src="../img/81b2f06b62b9cf6d8138187eb833b452.jpg" />的对象在 K 维丢失的情况下。</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; loss = nn.CrossEntropyLoss()
&gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True)
&gt;&gt;&gt; target = torch.empty(3, dtype=torch.long).random_(5)
&gt;&gt;&gt; output = loss(input, target)
&gt;&gt;&gt; output.backward()

</code></pre>
<h3 id="ctcloss">CTCLoss</h3>
<hr />
<pre><code>class torch.nn.CTCLoss(blank=0, reduction='mean', zero_infinity=False)¶
</code></pre>
<p>连接主义者的时间分类损失。</p>
<p>计算连续(非分段）时间序列与目标序列之间的损失。 CTCLoss 将输入与目标进行可能的对齐之和加起来，从而产生相对于每个输入节点可微分的损耗值。 假设输入与目标的比对是“多对一”，这限制了目标序列的长度，因此它必须是输入长度的<img alt="" src="../img/4bd7c7209abda12ab057ebbb1be093d4.jpg" />。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>空白</strong> (<em>python：int</em> <em>，</em> <em>可选</em>）–空白标签。 默认值<img alt="" src="../img/e647c6371faf8b6dd9327515ae95cbdb.jpg" />。</p>
</li>
<li>
<p><strong>缩减</strong>(<em>字符串</em> <em>，</em> <em>可选</em>）–指定要应用于输出的缩减：<code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>。 <code>'none'</code>：不应用减少量，<code>'mean'</code>：将输出损失除以目标长度，然后取批次的平均值。 默认值：<code>'mean'</code></p>
</li>
<li>
<p><strong>zero_infinity</strong>  (<em>bool</em> <em>，</em> <em>可选</em>）–是否将无限大损失和相关的梯度归零。 默认值：<code>False</code>无限损失主要发生在输入太短而无法与目标对齐时。</p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Log_probs：大小为<img alt="" src="../img/34467dd7001d4e2248cf56af240c7222.jpg" />的张量，其中<img alt="" src="../img/761ac33d79cdfbc22fb96e0d7dd066a8.jpg" />，<img alt="" src="../img/1acd7062a274efe0dc2498ccccec4bf8.jpg" />和<img alt="" src="../img/96c9110f969cadcc2069d50d80df5809.jpg" />。 输出的对数概率(例如，使用 <a href="nn.functional.html#torch.nn.functional.log_softmax" title="torch.nn.functional.log_softmax"><code>torch.nn.functional.log_softmax()</code></a> 获得的概率）。</p>
</li>
<li>
<p>目标：大小为<img alt="" src="../img/8fd771cabec0d2f2ff6ac3992568c469.jpg" />或<img alt="" src="../img/0c3c3b47c1c4c221019ab2ab9d3159fa.jpg" />的张量，其中<img alt="" src="../img/1acd7062a274efe0dc2498ccccec4bf8.jpg" />和<img alt="" src="../img/91dd2e39fac7daffae54ef330a8b4c01.jpg" />。 它代表靶序列。 目标序列中的每个元素都是一个类索引。 并且目标索引不能为空(默认= 0）。 在<img alt="" src="../img/8fd771cabec0d2f2ff6ac3992568c469.jpg" />形式中，将目标填充到最长序列的长度，然后堆叠。 在<img alt="" src="../img/0c3c3b47c1c4c221019ab2ab9d3159fa.jpg" />格式中，假定目标未填充且在 1 维内连接。</p>
</li>
<li>
<p>Input_lengths：大小为<img alt="" src="../img/a01009d68498c3f4597b15bab746c24d.jpg" />的元组或张量，其中<img alt="" src="../img/1acd7062a274efe0dc2498ccccec4bf8.jpg" />。 它代表输入的长度(每个必须为<img alt="" src="../img/ad3788a1226429a2be718c2d8b9695e1.jpg" />）。 并且在序列被填充为相等长度的假设下，为每个序列指定长度以实现屏蔽。</p>
</li>
<li>
<p>Target_lengths：大小为<img alt="" src="../img/a01009d68498c3f4597b15bab746c24d.jpg" />的元组或张量，其中<img alt="" src="../img/1acd7062a274efe0dc2498ccccec4bf8.jpg" />。 它代表目标的长度。 在将序列填充为相等长度的假设下，为每个序列指定长度以实现屏蔽。 如果目标形状为<img alt="" src="../img/23f48dff28530efc95add243667c14b5.jpg" />，则 target_lengths 实际上是每个目标序列的终止索引<img alt="" src="../img/88f072962ca9ea33a7add0cc52eaaff9.jpg" />，从而使批次中每个目标的<code>target_n = targets[n,0:s_n]</code>。 每个长度都必须为<img alt="" src="../img/c64041774620d366c3ec9ba92f6754d3.jpg" />如果目标是作为单个目标的并置的 1d 张量给出的，则 target_lengths 必须加起来为张量的总长度。</p>
</li>
<li>
<p>输出：标量。 如果<code>reduction</code>为<code>'none'</code>，则为<img alt="" src="../img/a01009d68498c3f4597b15bab746c24d.jpg" />，其中<img alt="" src="../img/1acd7062a274efe0dc2498ccccec4bf8.jpg" />。</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; T = 50      # Input sequence length
&gt;&gt;&gt; C = 20      # Number of classes (including blank)
&gt;&gt;&gt; N = 16      # Batch size
&gt;&gt;&gt; S = 30      # Target sequence length of longest target in batch
&gt;&gt;&gt; S_min = 10  # Minimum target length, for demonstration purposes
&gt;&gt;&gt;
&gt;&gt;&gt; # Initialize random batch of input vectors, for *size = (T,N,C)
&gt;&gt;&gt; input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()
&gt;&gt;&gt;
&gt;&gt;&gt; # Initialize random batch of targets (0 = blank, 1:C = classes)
&gt;&gt;&gt; target = torch.randint(low=1, high=C, size=(N, S), dtype=torch.long)
&gt;&gt;&gt;
&gt;&gt;&gt; input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)
&gt;&gt;&gt; target_lengths = torch.randint(low=S_min, high=S, size=(N,), dtype=torch.long)
&gt;&gt;&gt; ctc_loss = nn.CTCLoss()
&gt;&gt;&gt; loss = ctc_loss(input, target, input_lengths, target_lengths)
&gt;&gt;&gt; loss.backward()

</code></pre>
<pre><code>Reference:
</code></pre>
<p>A. Graves 等人：连接主义者的时间分类：使用循环神经网络标记未分段的序列数据： <a href="https://www.cs.toronto.edu/~graves/icml_2006.pdf">https://www.cs.toronto.edu/~graves/icml_2006.pdf</a></p>
<p>Note</p>
<p>为了使用 CuDNN，必须满足以下条件：<code>targets</code>必须为级联格式，所有<code>input_lengths</code>都必须为 &lt;cite&gt;T&lt;/cite&gt; 。 <img alt="" src="../img/e1ec19c99db8e2127a83a8831880bc11.jpg" />，<code>target_lengths</code> <img alt="" src="../img/d059e54a5245ba6222a051caceae9993.jpg" />，整数参数必须为 dtype <code>torch.int32</code>。</p>
<p>常规实现使用(在 PyTorch 中更常见） &lt;cite&gt;torch.long&lt;/cite&gt; dtype。</p>
<p>Note</p>
<p>In some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting <code>torch.backends.cudnn.deterministic = True</code>. Please see the notes on <a href="notes/randomness.html">Reproducibility</a> for background.</p>
<h3 id="_55">亏损</h3>
<hr />
<pre><code>class torch.nn.NLLLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')¶
</code></pre>
<p>负对数似然损失。 训练带有 &lt;cite&gt;C&lt;/cite&gt; 类的分类问题很有用。</p>
<p>如果提供，则可选参数<code>weight</code>应该是一维 Tensor，为每个类分配权重。 当您的训练集不平衡时，此功能特别有用。</p>
<p>通过前向调用给出的&lt;cite&gt;输入&lt;/cite&gt;预计将包含每个类的对数概率。 对于 &lt;cite&gt;K&lt;/cite&gt; -维情况(稍后描述），&lt;cite&gt;输入&lt;/cite&gt;的张量必须为<img alt="" src="../img/170f6fd3c25c71669d79482ca111cb57.jpg" />或<img alt="" src="../img/413c5ecfc8061696d19bdb15824b3bf3.jpg" />和<img alt="" src="../img/81b2f06b62b9cf6d8138187eb833b452.jpg" />的大小。</p>
<p>通过在网络的最后一层添加 &lt;cite&gt;LogSoftmax&lt;/cite&gt; 层，可以轻松获得神经网络中的对数概率。 如果您不想添加额外的图层，则可以改用 &lt;cite&gt;CrossEntropyLoss&lt;/cite&gt; 。</p>
<p>该损失预期的&lt;cite&gt;目标&lt;/cite&gt;应该是<img alt="" src="../img/cd248d64767e64a04f1180b6fbf62aba.jpg" />范围内的类别索引，其中 &lt;cite&gt;C =类别数&lt;/cite&gt;； 如果指定了 &lt;cite&gt;ignore_index&lt;/cite&gt; ，则此丢失也将接受该类索引(该索引可能不一定在类范围内）。</p>
<p>The unreduced (i.e. with <code>reduction</code> set to <code>'none'</code>) loss can be described as:</p>
<p><img alt="" src="../img/0bec4ddcb003313336a655e62551b25f.jpg" /></p>
<p>其中<img alt="" src="../img/7ea0d6aa290d37c8bc57957206bafe80.jpg" />是批次大小。 如果<code>reduction</code>不是<code>'none'</code>(默认为<code>'mean'</code>），则</p>
<p><img alt="" src="../img/cf0d3050e3974bcc01b8f278062ffb33.jpg" /></p>
<p>通过提供<img alt="" src="../img/81b2f06b62b9cf6d8138187eb833b452.jpg" />大小为<img alt="" src="../img/413c5ecfc8061696d19bdb15824b3bf3.jpg" />的输入(其中<img alt="" src="../img/8ccf33bfd6edc45aa7ca77ecf75bf04a.jpg" />是尺寸的数量）和适当形状的目标，也可以用于更高尺寸的输入(例如 2D 图像）(请参见下文）。 对于图像，它计算每像素 NLL 损耗。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>重量</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>，</em> <em>可选</em>）–为每个类别提供手动缩放比例的重量。 如果给定，则它必须是 &lt;cite&gt;C&lt;/cite&gt; 大小的张量。 否则，将其视为拥有全部。</p>
</li>
<li>
<p><strong>size_average</strong> (<em>bool__,</em> <em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></p>
</li>
<li>
<p><strong>ignore_index</strong>  (<em>python：int</em> <em>，</em> <em>可选</em>）–指定目标值，该目标值将被忽略并且不会对输入梯度产生影响。 当<code>size_average</code>为<code>True</code>时，损耗是在不可忽略的目标上平均的。</p>
</li>
<li>
<p><strong>reduce</strong> (<em>bool__,</em> <em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></p>
</li>
<li>
<p><strong>reduction</strong> (<em>string__,</em> <em>optional</em>) – Specifies the reduction to apply to the output: <code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum of the output will be divided by the number of elements in the output, <code>'sum'</code>: the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: <code>'mean'</code></p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/e2f1df6fc3cf4eb0f5bee6a89bb2f37b.jpg" /> where &lt;cite&gt;C = number of classes&lt;/cite&gt;, or <img alt="" src="../img/d2b291d58abab8c9ef51a99cea539b2b.jpg" /> with <img alt="" src="../img/81b2f06b62b9cf6d8138187eb833b452.jpg" /> in the case of &lt;cite&gt;K&lt;/cite&gt;-dimensional loss.</p>
</li>
<li>
<p>Target: <img alt="" src="../img/a01009d68498c3f4597b15bab746c24d.jpg" /> where each value is <img alt="" src="../img/35235b20063aefd21abb814e4f037d93.jpg" />, or <img alt="" src="../img/88a862c7e57602f20997d7172545b05f.jpg" /> with <img alt="" src="../img/81b2f06b62b9cf6d8138187eb833b452.jpg" /> in the case of K-dimensional loss.</p>
</li>
<li>
<p>输出：标量。 如果<code>reduction</code>为<code>'none'</code>，则与 K 尺寸相同：<img alt="" src="../img/a01009d68498c3f4597b15bab746c24d.jpg" />或<img alt="" src="../img/88a862c7e57602f20997d7172545b05f.jpg" />，带有<img alt="" src="../img/81b2f06b62b9cf6d8138187eb833b452.jpg" />的对象在 K 维丢失的情况下。</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.LogSoftmax(dim=1)
&gt;&gt;&gt; loss = nn.NLLLoss()
&gt;&gt;&gt; # input is of size N x C = 3 x 5
&gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True)
&gt;&gt;&gt; # each element in target has to have 0 &lt;= value &lt; C
&gt;&gt;&gt; target = torch.tensor([1, 0, 4])
&gt;&gt;&gt; output = loss(m(input), target)
&gt;&gt;&gt; output.backward()
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; # 2D loss example (used, for example, with image inputs)
&gt;&gt;&gt; N, C = 5, 4
&gt;&gt;&gt; loss = nn.NLLLoss()
&gt;&gt;&gt; # input is of size N x C x height x width
&gt;&gt;&gt; data = torch.randn(N, 16, 10, 10)
&gt;&gt;&gt; conv = nn.Conv2d(16, C, (3, 3))
&gt;&gt;&gt; m = nn.LogSoftmax(dim=1)
&gt;&gt;&gt; # each element in target has to have 0 &lt;= value &lt; C
&gt;&gt;&gt; target = torch.empty(N, 8, 8, dtype=torch.long).random_(0, C)
&gt;&gt;&gt; output = loss(m(conv(data)), target)
&gt;&gt;&gt; output.backward()

</code></pre>
<h3 id="_56">泊松</h3>
<hr />
<pre><code>class torch.nn.PoissonNLLLoss(log_input=True, full=False, size_average=None, eps=1e-08, reduce=None, reduction='mean')¶
</code></pre>
<p>带有目标泊松分布的负对数似然损失。</p>
<p>The loss can be described as:</p>
<p><img alt="" src="../img/f4999d80656127fee6e9d5790ae76555.jpg" /></p>
<p>最后一项可以省略，也可以用斯特林公式近似。 逼近值用于大于 1 的目标值。对于小于或等于 1 的目标值，零添加到损耗中。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>log_input</strong>  (<em>bool</em> <em>，</em> <em>可选</em>）–如果<code>True</code>的损失计算为<img alt="" src="../img/f01e398709f0bdfcee698abd8a8f2bfc.jpg" />，如果<code>False</code>的损失计算 是<img alt="" src="../img/ff1a6d65e47faa83c018e0b1c5b36b58.jpg" />。</p>
</li>
<li>
<p><strong>完整的</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–</p>
<p>是否计算全部损失； i。 e。 添加斯特林近似项</p>
<p><img alt="" src="../img/1f92ed5d26d82652b53bcfbea85d0c64.jpg" /></p>
</li>
<li>
<p><strong>size_average</strong> (<em>bool__,</em> <em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></p>
</li>
<li>
<p><strong>eps</strong>  (<em>python：float</em> <em>，</em> <em>可选</em>）–较小的值，以避免在<code>log_input = False</code>时评估<img alt="" src="../img/5cc7cca8d8cb4f18a6ce8b7feda1442d.jpg" />。 默认值：1e-8</p>
</li>
<li>
<p><strong>reduce</strong> (<em>bool__,</em> <em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></p>
</li>
<li>
<p><strong>reduction</strong> (<em>string__,</em> <em>optional</em>) – Specifies the reduction to apply to the output: <code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum of the output will be divided by the number of elements in the output, <code>'sum'</code>: the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: <code>'mean'</code></p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; loss = nn.PoissonNLLLoss()
&gt;&gt;&gt; log_input = torch.randn(5, 2, requires_grad=True)
&gt;&gt;&gt; target = torch.randn(5, 2)
&gt;&gt;&gt; output = loss(log_input, target)
&gt;&gt;&gt; output.backward()

</code></pre>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" /> where <img alt="" src="../img/dcef98688866c0d5a21137cf53bf228d.jpg" /> means, any number of additional dimensions</p>
</li>
<li>
<p>Target: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" />, same shape as the input</p>
</li>
<li>
<p>输出：默认情况下为标量。 如果<code>reduction</code>为<code>'none'</code>，则<img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" />的形状与输入相同</p>
</li>
</ul>
<h3 id="_57">损失</h3>
<hr />
<pre><code>class torch.nn.KLDivLoss(size_average=None, reduce=None, reduction='mean')¶
</code></pre>
<p><a href="https://en.wikipedia.org/wiki/Kullback-Leibler_divergence">Kullback-Leibler 散度</a>损失</p>
<p>KL 散度是用于连续分布的有用距离度量，并且在对(离散采样）连续输出分布的空间进行直接回归时通常很有用。</p>
<p>与 <a href="#torch.nn.NLLLoss" title="torch.nn.NLLLoss"><code>NLLLoss</code></a> 一样，给定的&lt;cite&gt;输入&lt;/cite&gt;预期包含_对数概率_，并且不限于 2D 张量。 目标以_概率_给出(即，不取对数）。</p>
<p>该标准要求&lt;cite&gt;目标&lt;/cite&gt; &lt;cite&gt;张量&lt;/cite&gt;与&lt;cite&gt;输入&lt;/cite&gt; &lt;cite&gt;张量&lt;/cite&gt;大小相同。</p>
<p>The unreduced (i.e. with <code>reduction</code> set to <code>'none'</code>) loss can be described as:</p>
<p><img alt="" src="../img/c7d9a672b5ba900c463a18b79205f8ed.jpg" /></p>
<p>其中索引<img alt="" src="../img/7ea0d6aa290d37c8bc57957206bafe80.jpg" />跨越<code>input</code>的所有维度，并且<img alt="" src="../img/0abd5691618d0a16f37deae1aacd79cf.jpg" />具有与<code>input</code>相同的形状。 如果<code>reduction</code>不是<code>'none'</code>(默认为<code>'mean'</code>），则：</p>
<p><img alt="" src="../img/41b138ab42db900c48c18491ef49f077.jpg" /></p>
<p>在默认的<code>reduction</code>模式<code>'mean'</code>中，损耗是对观察值<strong>和尺寸上的</strong>的每个小批量进行平均的。 <code>'batchmean'</code>模式给出正确的 KL 散度，其中损失仅在批次范围内平均。 在下一个主要版本中，<code>'mean'</code>模式的行为将更改为与<code>'batchmean'</code>相同。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>size_average</strong> (<em>bool__,</em> <em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></p>
</li>
<li>
<p><strong>reduce</strong> (<em>bool__,</em> <em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></p>
</li>
<li>
<p><strong>缩减</strong>(<em>字符串</em> <em>，</em> <em>可选</em>）–指定要应用于输出的缩减：<code>'none'</code> | <code>'batchmean'</code> | <code>'sum'</code> | <code>'mean'</code>。 <code>'none'</code>：不应用减少。 <code>'batchmean'</code>：输出的总和将除以 batchsize。 <code>'sum'</code>：将对输出求和。 <code>'mean'</code>：输出将除以输出中的元素数。 默认值：<code>'mean'</code></p>
</li>
</ul>
<p>Note</p>
<p><code>size_average</code>和<code>reduce</code>正在弃用的过程中，与此同时，指定这两个 args 中的任何一个将覆盖<code>reduction</code>。</p>
<p>Note</p>
<p><code>reduction</code> = <code>'mean'</code>未返回真实的 kl 散度值，请使用与 KL 数学定义一致的<code>reduction</code> = <code>'batchmean'</code>。 在下一个主要版本中，<code>'mean'</code>将更改为与<code>'batchmean'</code>相同。</p>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" /> where <img alt="" src="../img/dcef98688866c0d5a21137cf53bf228d.jpg" /> means, any number of additional dimensions</p>
</li>
<li>
<p>Target: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" />, same shape as the input</p>
</li>
<li>
<p>输出：默认情况下为标量。 如果：attr：<code>reduction</code>为<code>'none'</code>，则<img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" />的形状与输入的形状相同</p>
</li>
</ul>
<h3 id="bceloss">BCELoss</h3>
<hr />
<pre><code>class torch.nn.BCELoss(weight=None, size_average=None, reduce=None, reduction='mean')¶
</code></pre>
<p>创建一个衡量目标和输出之间的二进制交叉熵的标准：</p>
<p>The unreduced (i.e. with <code>reduction</code> set to <code>'none'</code>) loss can be described as:</p>
<p><img alt="" src="../img/d1a711f99b0cfe9ad3c4799cccd8bd6c.jpg" /></p>
<p>where <img alt="" src="../img/7ea0d6aa290d37c8bc57957206bafe80.jpg" /> is the batch size. If <code>reduction</code> is not <code>'none'</code> (default <code>'mean'</code>), then</p>
<p><img alt="" src="../img/f990a1e4d7704208d27c62d0eef0cae7.jpg" /></p>
<p>这用于测量例如自动编码器中的重建误差。 请注意，目标<img alt="" src="../img/faf0bb6f76d3932d159da2a147a17089.jpg" />应为 0 到 1 之间的数字。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>重量</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>，</em> <em>可选</em>）–手工重新定标重量，以补偿每个批次元素的损失。 如果给定，则必须是大小为 &lt;cite&gt;nbatch&lt;/cite&gt; 的张量。</p>
</li>
<li>
<p><strong>size_average</strong> (<em>bool__,</em> <em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></p>
</li>
<li>
<p><strong>reduce</strong> (<em>bool__,</em> <em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></p>
</li>
<li>
<p><strong>reduction</strong> (<em>string__,</em> <em>optional</em>) – Specifies the reduction to apply to the output: <code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum of the output will be divided by the number of elements in the output, <code>'sum'</code>: the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: <code>'mean'</code></p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" /> where <img alt="" src="../img/dcef98688866c0d5a21137cf53bf228d.jpg" /> means, any number of additional dimensions</p>
</li>
<li>
<p>Target: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" />, same shape as the input</p>
</li>
<li>
<p>输出：标量。 如果<code>reduction</code>为<code>'none'</code>，则<img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" />与输入形状相同。</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; m = nn.Sigmoid()
&gt;&gt;&gt; loss = nn.BCELoss()
&gt;&gt;&gt; input = torch.randn(3, requires_grad=True)
&gt;&gt;&gt; target = torch.empty(3).random_(2)
&gt;&gt;&gt; output = loss(m(input), target)
&gt;&gt;&gt; output.backward()

</code></pre>
<h3 id="bcewithlogitsloss">BCEWithLogitsLoss</h3>
<hr />
<pre><code>class torch.nn.BCEWithLogitsLoss(weight=None, size_average=None, reduce=None, reduction='mean', pos_weight=None)¶
</code></pre>
<p>这种损失将&lt;cite&gt;乙状结肠&lt;/cite&gt;层和 &lt;cite&gt;BCELoss&lt;/cite&gt; 合并为一个类别。 该版本比使用普通的 &lt;cite&gt;Sigmoid&lt;/cite&gt; 和随后的 &lt;cite&gt;BCELoss&lt;/cite&gt; 在数值上更稳定，因为通过将操作合并为一层，我们利用了 log-sum-exp 技巧进行数值计算 稳定性。</p>
<p>The unreduced (i.e. with <code>reduction</code> set to <code>'none'</code>) loss can be described as:</p>
<p><img alt="" src="../img/95c46c53aea774f55282cdb5abe78927.jpg" /></p>
<p>where <img alt="" src="../img/7ea0d6aa290d37c8bc57957206bafe80.jpg" /> is the batch size. If <code>reduction</code> is not <code>'none'</code> (default <code>'mean'</code>), then</p>
<p><img alt="" src="../img/f990a1e4d7704208d27c62d0eef0cae7.jpg" /></p>
<p>这用于测量例如自动编码器中的重建误差。 请注意，目标 &lt;cite&gt;t [i]&lt;/cite&gt; 应为 0 到 1 之间的数字。</p>
<p>通过在积极的例子中增加权重，可以权衡召回率和准确性。 在多标签分类的情况下，损失可描述为：</p>
<p><img alt="" src="../img/09a292e8112a686e2075cfb6d53a4e77.jpg" /></p>
<p>其中，<img alt="" src="../img/009c69db54af1829f05c23f52be15e24.jpg" />是类别编号(对于多标签二进制分类，<img alt="" src="../img/12598ba91df82673941ba4746b77bc36.jpg" />；对于单标签二进制分类，<img alt="" src="../img/4c32fe3286386aeb33e4c86f793363f7.jpg" />），<img alt="" src="../img/5f0051b0454fa75ca446b59b47eff6f6.jpg" />是批次中样品的数量，<img alt="" src="../img/8cedd20f0917b2b21d3a3c321af3f6c6.jpg" />是样品的重量 课程<img alt="" src="../img/009c69db54af1829f05c23f52be15e24.jpg" />的肯定答案。</p>
<p><img alt="" src="../img/4b11f27d5f1b6aea24f90331f415017f.jpg" />增加查全率，<img alt="" src="../img/a8488b210fba6355dbfbca6d512bde36.jpg" />增加精度。</p>
<p>例如，如果数据集包含一个类的 100 个正例和 300 个负例，则该类的 &lt;cite&gt;pos_weight&lt;/cite&gt; 应等于<img alt="" src="../img/844634e18600681e061114d67832092c.jpg" />。 损失将好像数据集包含<img alt="" src="../img/2cefd1e09f454f29a10f42b2515d7165.jpg" />阳性示例。</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; target = torch.ones([10, 64], dtype=torch.float32)  # 64 classes, batch size = 10
&gt;&gt;&gt; output = torch.full([10, 64], 0.999)  # A prediction (logit)
&gt;&gt;&gt; pos_weight = torch.ones([64])  # All weights are equal to 1
&gt;&gt;&gt; criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)
&gt;&gt;&gt; criterion(output, target)  # -log(sigmoid(0.999))
tensor(0.3135)

</code></pre>
<p>Parameters</p>
<ul>
<li>
<p><strong>weight</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – a manual rescaling weight given to the loss of each batch element. If given, has to be a Tensor of size &lt;cite&gt;nbatch&lt;/cite&gt;.</p>
</li>
<li>
<p><strong>size_average</strong> (<em>bool__,</em> <em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></p>
</li>
<li>
<p><strong>reduce</strong> (<em>bool__,</em> <em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></p>
</li>
<li>
<p><strong>reduction</strong> (<em>string__,</em> <em>optional</em>) – Specifies the reduction to apply to the output: <code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum of the output will be divided by the number of elements in the output, <code>'sum'</code>: the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: <code>'mean'</code></p>
</li>
<li>
<p><strong>pos_weight</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>，</em> <em>可选</em>）–正例的权重。 必须是长度等于类数的向量。</p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<blockquote>
<ul>
<li>
<p>输入：<img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" />其中<img alt="" src="../img/dcef98688866c0d5a21137cf53bf228d.jpg" />表示任意数量的附加尺寸</p>
</li>
<li>
<p>Target: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" />, same shape as the input</p>
</li>
<li>
<p>Output: scalar. If <code>reduction</code> is <code>'none'</code>, then <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" />, same shape as input.</p>
</li>
</ul>
</blockquote>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; loss = nn.BCEWithLogitsLoss()
&gt;&gt;&gt; input = torch.randn(3, requires_grad=True)
&gt;&gt;&gt; target = torch.empty(3).random_(2)
&gt;&gt;&gt; output = loss(input, target)
&gt;&gt;&gt; output.backward()

</code></pre>
<h3 id="_58">保证金排名损失</h3>
<hr />
<pre><code>class torch.nn.MarginRankingLoss(margin=0.0, size_average=None, reduce=None, reduction='mean')¶
</code></pre>
<p>创建一个标准来测量给定输入<img alt="" src="../img/7a04de965c452c1d620631716f34cfc1.jpg" />，<img alt="" src="../img/474c7e0ec0f9040732f72b7bb2993974.jpg" />，两个 1D 迷你批量&lt;cite&gt;张量&lt;/cite&gt;和标签 1D 迷你批量张量<img alt="" src="../img/faf0bb6f76d3932d159da2a147a17089.jpg" />(包含 1 或-1）的损耗。</p>
<p>如果<img alt="" src="../img/2ffd2fd6fbfb4fa26bfd4e1b157ebd29.jpg" />，则假定第一个输入的排名应高于第二个输入(具有更大的值），反之亦然。</p>
<p>迷你批次中每个样本的损失函数为：</p>
<p><img alt="" src="../img/c9d860728daee1e312aef6082bce3a6f.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>边距</strong> (<em>python：float</em> <em>，</em> <em>可选</em>）–默认值为<img alt="" src="../img/e647c6371faf8b6dd9327515ae95cbdb.jpg" />。</p>
</li>
<li>
<p><strong>size_average</strong> (<em>bool__,</em> <em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></p>
</li>
<li>
<p><strong>reduce</strong> (<em>bool__,</em> <em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></p>
</li>
<li>
<p><strong>reduction</strong> (<em>string__,</em> <em>optional</em>) – Specifies the reduction to apply to the output: <code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum of the output will be divided by the number of elements in the output, <code>'sum'</code>: the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: <code>'mean'</code></p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>输入：<img alt="" src="../img/64d764a4273bfec3a3846f3f0fbff88f.jpg" />，其中 &lt;cite&gt;N&lt;/cite&gt; 是批次大小， &lt;cite&gt;D&lt;/cite&gt; 是样本大小。</p>
</li>
<li>
<p>目标：<img alt="" src="../img/a01009d68498c3f4597b15bab746c24d.jpg" /></p>
</li>
<li>
<p>输出：标量。 如果<code>reduction</code>是<code>'none'</code>，则<img alt="" src="../img/a01009d68498c3f4597b15bab746c24d.jpg" />。</p>
</li>
</ul>
<h3 id="_59">嵌入损耗</h3>
<hr />
<pre><code>class torch.nn.HingeEmbeddingLoss(margin=1.0, size_average=None, reduce=None, reduction='mean')¶
</code></pre>
<p>在输入张量<img alt="" src="../img/2c2415be487481d652d00c279423c3d0.jpg" />和标签张量<img alt="" src="../img/faf0bb6f76d3932d159da2a147a17089.jpg" />(包含 1 或-1）的情况下测量损耗。 通常用于测量两个输入是否相似或不相似，例如 使用 L1 成对距离作为<img alt="" src="../img/2c2415be487481d652d00c279423c3d0.jpg" />，通常用于学习非线性嵌入或半监督学习。</p>
<p>微型批次中第<img alt="" src="../img/5f0051b0454fa75ca446b59b47eff6f6.jpg" />个样本的损失函数为</p>
<p><img alt="" src="../img/5cf677c366829215214db864c92c56c8.jpg" /></p>
<p>总损失函数为</p>
<p><img alt="" src="../img/f990a1e4d7704208d27c62d0eef0cae7.jpg" /></p>
<p>其中<img alt="" src="../img/9bd5bf08a96aeb315a7c768795683c7f.jpg" />。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>边距</strong> (<em>python：float</em> <em>，</em> <em>可选</em>）–默认值为 &lt;cite&gt;1&lt;/cite&gt; 。</p>
</li>
<li>
<p><strong>size_average</strong> (<em>bool__,</em> <em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></p>
</li>
<li>
<p><strong>reduce</strong> (<em>bool__,</em> <em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></p>
</li>
<li>
<p><strong>reduction</strong> (<em>string__,</em> <em>optional</em>) – Specifies the reduction to apply to the output: <code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum of the output will be divided by the number of elements in the output, <code>'sum'</code>: the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: <code>'mean'</code></p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>输入：<img alt="" src="../img/39f05b69c0ea6b9b6b228f948944c4f6.jpg" />其中<img alt="" src="../img/dcef98688866c0d5a21137cf53bf228d.jpg" />表示任意数量的尺寸。 求和运算对所有元素进行运算。</p>
</li>
<li>
<p>目标：<img alt="" src="../img/39f05b69c0ea6b9b6b228f948944c4f6.jpg" />，形状与输入相同</p>
</li>
<li>
<p>输出：标量。 如果<code>reduction</code>为<code>'none'</code>，则形状与输入相同</p>
</li>
</ul>
<h3 id="_60">多标签保证金损失</h3>
<hr />
<pre><code>class torch.nn.MultiLabelMarginLoss(size_average=None, reduce=None, reduction='mean')¶
</code></pre>
<p>创建一个标准，以优化输入<img alt="" src="../img/2c2415be487481d652d00c279423c3d0.jpg" />(2D 微型批量&lt;cite&gt;张量&lt;/cite&gt;）和输出<img alt="" src="../img/faf0bb6f76d3932d159da2a147a17089.jpg" />(2D &lt;cite&gt;目标类别索引的张量&lt;/cite&gt;。 对于小批量中的每个样品：</p>
<p><img alt="" src="../img/c3e378022e15c7518f4de635cdd46ca9.jpg" /></p>
<p>其中，<img alt="" src="../img/8a14dc2e02683dd551c08bbaea377f77.jpg" />，<img alt="" src="../img/cd5eb95a60eb440d282684fa96585622.jpg" />，<img alt="" src="../img/4f5d7643e658ec4cd5800f45da60efee.jpg" />和<img alt="" src="../img/e67bc94012bcd52c7bfece1f7a02815e.jpg" />都适用于<img alt="" src="../img/db26d1a59be5965889bd4d5533b7be61.jpg" />和<img alt="" src="../img/36608d1dd28464666846576485c40a7b.jpg" />。</p>
<p><img alt="" src="../img/faf0bb6f76d3932d159da2a147a17089.jpg" />和<img alt="" src="../img/2c2415be487481d652d00c279423c3d0.jpg" />的大小必须相同。</p>
<p>该标准仅考虑从正面开始的非负目标的连续块。</p>
<p>这允许不同的样本具有可变数量的目标类别。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>size_average</strong> (<em>bool__,</em> <em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></p>
</li>
<li>
<p><strong>reduce</strong> (<em>bool__,</em> <em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></p>
</li>
<li>
<p><strong>reduction</strong> (<em>string__,</em> <em>optional</em>) – Specifies the reduction to apply to the output: <code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum of the output will be divided by the number of elements in the output, <code>'sum'</code>: the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: <code>'mean'</code></p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>输入：<img alt="" src="../img/744790ef54cd0ed0e192ed2623260bd6.jpg" />或<img alt="" src="../img/e2f1df6fc3cf4eb0f5bee6a89bb2f37b.jpg" />，其中 &lt;cite&gt;N&lt;/cite&gt; 是批处理大小， &lt;cite&gt;C&lt;/cite&gt; 是类别数。</p>
</li>
<li>
<p>目标：<img alt="" src="../img/744790ef54cd0ed0e192ed2623260bd6.jpg" />或<img alt="" src="../img/e2f1df6fc3cf4eb0f5bee6a89bb2f37b.jpg" />，标有-1 的标签目标确保与输入形状相同。</p>
</li>
<li>
<p>Output: scalar. If <code>reduction</code> is <code>'none'</code>, then <img alt="" src="../img/a01009d68498c3f4597b15bab746c24d.jpg" />.</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; loss = nn.MultiLabelMarginLoss()
&gt;&gt;&gt; x = torch.FloatTensor([[0.1, 0.2, 0.4, 0.8]])
&gt;&gt;&gt; # for target y, only consider labels 3 and 0, not after label -1
&gt;&gt;&gt; y = torch.LongTensor([[3, 0, -1, 1]])
&gt;&gt;&gt; loss(x, y)
&gt;&gt;&gt; # 0.25 * ((1-(0.1-0.2)) + (1-(0.1-0.4)) + (1-(0.8-0.2)) + (1-(0.8-0.4)))
tensor(0.8500)

</code></pre>
<h3 id="l1_1">平滑 L1 损失</h3>
<hr />
<pre><code>class torch.nn.SmoothL1Loss(size_average=None, reduce=None, reduction='mean')¶
</code></pre>
<p>如果每个元素的绝对误差小于 1，则创建一个使用平方项的条件，否则，则使用 L1 项。 它对异常值的敏感性不及 &lt;cite&gt;MSELoss&lt;/cite&gt; ，并且在某些情况下可以防止爆炸梯度(例如，参见 Ross Girshick 的 &lt;cite&gt;Fast R-CNN&lt;/cite&gt; 论文）。 也称为胡贝尔损耗：</p>
<p><img alt="" src="../img/4f6a7627936ea4c9ac9e89ac51ff7fa1.jpg" /></p>
<p>其中<img alt="" src="../img/2a878a86b12deb88bf413bb91f60f6fd.jpg" />的计算公式为：</p>
<p><img alt="" src="../img/50240b936c52aefac52360cae6763cbd.jpg" /></p>
<p>具有总共<img alt="" src="../img/5f0051b0454fa75ca446b59b47eff6f6.jpg" />个元素的<img alt="" src="../img/2c2415be487481d652d00c279423c3d0.jpg" />和<img alt="" src="../img/faf0bb6f76d3932d159da2a147a17089.jpg" />任意形状，求和运算仍对所有元素进行运算，并除以<img alt="" src="../img/5f0051b0454fa75ca446b59b47eff6f6.jpg" />。</p>
<p>如果设置<code>reduction = 'sum'</code>，则可以避免被<img alt="" src="../img/5f0051b0454fa75ca446b59b47eff6f6.jpg" />划分。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>size_average</strong> (<em>bool__,</em> <em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></p>
</li>
<li>
<p><strong>reduce</strong> (<em>bool__,</em> <em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></p>
</li>
<li>
<p><strong>reduction</strong> (<em>string__,</em> <em>optional</em>) – Specifies the reduction to apply to the output: <code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum of the output will be divided by the number of elements in the output, <code>'sum'</code>: the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: <code>'mean'</code></p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" /> where <img alt="" src="../img/dcef98688866c0d5a21137cf53bf228d.jpg" /> means, any number of additional dimensions</p>
</li>
<li>
<p>Target: <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" />, same shape as the input</p>
</li>
<li>
<p>Output: scalar. If <code>reduction</code> is <code>'none'</code>, then <img alt="" src="../img/3dde05be0ba8949db2e0329e2ea1e16e.jpg" />, same shape as the input</p>
</li>
</ul>
<h3 id="_61">软保证金损失</h3>
<hr />
<pre><code>class torch.nn.SoftMarginLoss(size_average=None, reduce=None, reduction='mean')¶
</code></pre>
<p>创建一个标准，以优化输入张量<img alt="" src="../img/2c2415be487481d652d00c279423c3d0.jpg" />与目标张量<img alt="" src="../img/faf0bb6f76d3932d159da2a147a17089.jpg" />(包含 1 或-1）之间的两类分类逻辑损失。</p>
<p><img alt="" src="../img/6f8213e95d62de557ebe8d71130b7d84.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>size_average</strong> (<em>bool__,</em> <em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></p>
</li>
<li>
<p><strong>reduce</strong> (<em>bool__,</em> <em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></p>
</li>
<li>
<p><strong>reduction</strong> (<em>string__,</em> <em>optional</em>) – Specifies the reduction to apply to the output: <code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum of the output will be divided by the number of elements in the output, <code>'sum'</code>: the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: <code>'mean'</code></p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>输入：<img alt="" src="../img/39f05b69c0ea6b9b6b228f948944c4f6.jpg" />其中<img alt="" src="../img/dcef98688866c0d5a21137cf53bf228d.jpg" />表示任意数量的附加尺寸</p>
</li>
<li>
<p>Target: <img alt="" src="../img/39f05b69c0ea6b9b6b228f948944c4f6.jpg" />, same shape as the input</p>
</li>
<li>
<p>Output: scalar. If <code>reduction</code> is <code>'none'</code>, then same shape as the input</p>
</li>
</ul>
<h3 id="multilabelsoftmarginloss">MultiLabelSoftMarginLoss</h3>
<hr />
<pre><code>class torch.nn.MultiLabelSoftMarginLoss(weight=None, size_average=None, reduce=None, reduction='mean')¶
</code></pre>
<p>创建一个标准，该标准基于大小<img alt="" src="../img/e2f1df6fc3cf4eb0f5bee6a89bb2f37b.jpg" />的输入<img alt="" src="../img/2c2415be487481d652d00c279423c3d0.jpg" />和目标<img alt="" src="../img/faf0bb6f76d3932d159da2a147a17089.jpg" />之间的最大熵，优化多标签“一对全”损失。 对于小批量中的每个样品：</p>
<p><img alt="" src="../img/85bc6ff425c3306ec8bd28a156613df4.jpg" /></p>
<p>其中<img alt="" src="../img/e262e5c94008c54384e9ae055b6e6174.jpg" />和<img alt="" src="../img/0ca0b05e9ebba891ac5634078fcee113.jpg" />。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>weight</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – a manual rescaling weight given to each class. If given, it has to be a Tensor of size &lt;cite&gt;C&lt;/cite&gt;. Otherwise, it is treated as if having all ones.</p>
</li>
<li>
<p><strong>size_average</strong> (<em>bool__,</em> <em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></p>
</li>
<li>
<p><strong>reduce</strong> (<em>bool__,</em> <em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></p>
</li>
<li>
<p><strong>reduction</strong> (<em>string__,</em> <em>optional</em>) – Specifies the reduction to apply to the output: <code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum of the output will be divided by the number of elements in the output, <code>'sum'</code>: the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: <code>'mean'</code></p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>输入：<img alt="" src="../img/e2f1df6fc3cf4eb0f5bee6a89bb2f37b.jpg" />，其中 &lt;cite&gt;N&lt;/cite&gt; 是批次大小， &lt;cite&gt;C&lt;/cite&gt; 是类别数量。</p>
</li>
<li>
<p>目标：<img alt="" src="../img/e2f1df6fc3cf4eb0f5bee6a89bb2f37b.jpg" />，用-1 填充的标签目标确保与输入形状相同。</p>
</li>
<li>
<p>Output: scalar. If <code>reduction</code> is <code>'none'</code>, then <img alt="" src="../img/a01009d68498c3f4597b15bab746c24d.jpg" />.</p>
</li>
</ul>
<h3 id="_62">余弦嵌入损失</h3>
<hr />
<pre><code>class torch.nn.CosineEmbeddingLoss(margin=0.0, size_average=None, reduce=None, reduction='mean')¶
</code></pre>
<p>创建一个标准来测量给定输入张量<img alt="" src="../img/6729a6dfa0e7278903c230760f72d93e.jpg" />，<img alt="" src="../img/3723cd7a3980a7833e2a8c9d94af161e.jpg" />和&lt;cite&gt;张量&lt;/cite&gt;标签<img alt="" src="../img/faf0bb6f76d3932d159da2a147a17089.jpg" />的损耗，其值为 1 或-1。 这用于使用余弦距离来测量两个输入是否相似或不相似，并且通常用于学习非线性嵌入或半监督学习。</p>
<p>每个样本的损失函数为：</p>
<p><img alt="" src="../img/ab74feb0932246db4ef00c5d6715aa47.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>保证金</strong> (<em>python：float</em> <em>，</em> <em>可选</em>）–应该是从<img alt="" src="../img/c321026d2f4af8773e890218986290b2.jpg" />到<img alt="" src="../img/21c5bc8d40ee5ab2e18d64aaba8359c7.jpg" />，<img alt="" src="../img/e647c6371faf8b6dd9327515ae95cbdb.jpg" /> 建议使用<img alt="" src="../img/38db6f2552b56977925eb5468a368121.jpg" />。 如果缺少<code>margin</code>，则默认值为<img alt="" src="../img/e647c6371faf8b6dd9327515ae95cbdb.jpg" />。</p>
</li>
<li>
<p><strong>size_average</strong> (<em>bool__,</em> <em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></p>
</li>
<li>
<p><strong>reduce</strong> (<em>bool__,</em> <em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></p>
</li>
<li>
<p><strong>reduction</strong> (<em>string__,</em> <em>optional</em>) – Specifies the reduction to apply to the output: <code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum of the output will be divided by the number of elements in the output, <code>'sum'</code>: the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: <code>'mean'</code></p>
</li>
</ul>
<h3 id="_63">多保证金亏损</h3>
<hr />
<pre><code>class torch.nn.MultiMarginLoss(p=1, margin=1.0, weight=None, size_average=None, reduce=None, reduction='mean')¶
</code></pre>
<p>创建一个标准，以优化输入<img alt="" src="../img/2c2415be487481d652d00c279423c3d0.jpg" />(2D 微型批处理&lt;cite&gt;张量&lt;/cite&gt;）和输出<img alt="" src="../img/faf0bb6f76d3932d159da2a147a17089.jpg" />(目标的 1D 张量）之间的多类分类铰链损耗(基于边距的损耗） 类别索引<img alt="" src="../img/c76963d204cfc4729e04a7fd6af7692d.jpg" />）：</p>
<p>对于每个小批量样品，一维输入<img alt="" src="../img/2c2415be487481d652d00c279423c3d0.jpg" />和标量输出<img alt="" src="../img/faf0bb6f76d3932d159da2a147a17089.jpg" />的损耗为：</p>
<p><img alt="" src="../img/08c26645227d406f615426e74fb92225.jpg" /></p>
<p>其中<img alt="" src="../img/8a14dc2e02683dd551c08bbaea377f77.jpg" />和<img alt="" src="../img/71dfe7d5bb53b4f59833f1b86a24b3a1.jpg" />。</p>
<p>(可选）您可以通过将 1D <code>weight</code>张量传递到构造函数中来对类进行非相等加权。</p>
<p>损失函数将变为：</p>
<p><img alt="" src="../img/e01fa4b9fc534d5fa3972ffe688d951a.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>p</strong>  (<em>python：int</em> <em>，</em> <em>可选</em>）–默认值为<img alt="" src="../img/21c5bc8d40ee5ab2e18d64aaba8359c7.jpg" />。 仅支持<img alt="" src="../img/21c5bc8d40ee5ab2e18d64aaba8359c7.jpg" />和<img alt="" src="../img/627cc35746f2cd015e3dc5b406d31f1b.jpg" />值。</p>
</li>
<li>
<p><strong>边距</strong> (<em>python：float</em> <em>，</em> <em>可选</em>）–默认值为<img alt="" src="../img/21c5bc8d40ee5ab2e18d64aaba8359c7.jpg" />。</p>
</li>
<li>
<p><strong>weight</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – a manual rescaling weight given to each class. If given, it has to be a Tensor of size &lt;cite&gt;C&lt;/cite&gt;. Otherwise, it is treated as if having all ones.</p>
</li>
<li>
<p><strong>size_average</strong> (<em>bool__,</em> <em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></p>
</li>
<li>
<p><strong>reduce</strong> (<em>bool__,</em> <em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></p>
</li>
<li>
<p><strong>reduction</strong> (<em>string__,</em> <em>optional</em>) – Specifies the reduction to apply to the output: <code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum of the output will be divided by the number of elements in the output, <code>'sum'</code>: the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: <code>'mean'</code></p>
</li>
</ul>
<h3 id="_64">三重保证金亏损</h3>
<hr />
<pre><code>class torch.nn.TripletMarginLoss(margin=1.0, p=2.0, eps=1e-06, swap=False, size_average=None, reduce=None, reduction='mean')¶
</code></pre>
<p>创建一个标准，该标准在输入张量<img alt="" src="../img/7a04de965c452c1d620631716f34cfc1.jpg" />，<img alt="" src="../img/474c7e0ec0f9040732f72b7bb2993974.jpg" />，<img alt="" src="../img/a783d5599e651af8c0a017f6879ad350.jpg" />和值大于<img alt="" src="../img/e647c6371faf8b6dd9327515ae95cbdb.jpg" />的边距下测量三重态损失。 这用于测量样本之间的相对相似性。 一个三元组由 &lt;cite&gt;a&lt;/cite&gt; ， &lt;cite&gt;p&lt;/cite&gt; 和 &lt;cite&gt;n&lt;/cite&gt; 组成(即&lt;cite&gt;锚定&lt;/cite&gt;，&lt;cite&gt;阳性示例&lt;/cite&gt;和[HTG14 负面示例）。 所有输入张量的形状应为<img alt="" src="../img/64d764a4273bfec3a3846f3f0fbff88f.jpg" />。</p>
<p>V. Balntas，E. Riba 等人的论文<a href="http://www.bmva.org/bmvc/2016/papers/paper119/index.html">学习具有三重态损失的浅卷积特征描述符</a>中详细描述了距离交换。</p>
<p>The loss function for each sample in the mini-batch is:</p>
<p><img alt="" src="../img/6d83a557052953ecd1441feb9ba26c7b.jpg" /></p>
<p>哪里</p>
<p><img alt="" src="../img/95852bf7a0693a89de2def2d723f1e6f.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>边距</strong> (<em>python：float</em> <em>，</em> <em>可选</em>）–默认值：<img alt="" src="../img/21c5bc8d40ee5ab2e18d64aaba8359c7.jpg" />。</p>
</li>
<li>
<p><strong>p</strong>  (<em>python：int</em> <em>，</em> <em>可选</em>）–成对距离的标准度。 默认值：<img alt="" src="../img/627cc35746f2cd015e3dc5b406d31f1b.jpg" />。</p>
</li>
<li>
<p><strong>交换</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–距离交换在论文中详细描述。&lt;cite&gt;通过以下方法学习浅卷积特征描述符 V. Balntas，E。Riba 等人的三重损失&lt;/cite&gt;。 默认值：<code>False</code>。</p>
</li>
<li>
<p><strong>size_average</strong> (<em>bool__,</em> <em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></p>
</li>
<li>
<p><strong>reduce</strong> (<em>bool__,</em> <em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></p>
</li>
<li>
<p><strong>reduction</strong> (<em>string__,</em> <em>optional</em>) – Specifies the reduction to apply to the output: <code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum of the output will be divided by the number of elements in the output, <code>'sum'</code>: the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: <code>'mean'</code></p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>输入：<img alt="" src="../img/64d764a4273bfec3a3846f3f0fbff88f.jpg" />其中<img alt="" src="../img/bce3c5d2230f3875308e2e3abd32bd05.jpg" />是矢量尺寸。</p>
</li>
<li>
<p>Output: scalar. If <code>reduction</code> is <code>'none'</code>, then <img alt="" src="../img/a01009d68498c3f4597b15bab746c24d.jpg" />.</p>
</li>
</ul>
<pre><code>&gt;&gt;&gt; triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)
&gt;&gt;&gt; anchor = torch.randn(100, 128, requires_grad=True)
&gt;&gt;&gt; positive = torch.randn(100, 128, requires_grad=True)
&gt;&gt;&gt; negative = torch.randn(100, 128, requires_grad=True)
&gt;&gt;&gt; output = triplet_loss(anchor, positive, negative)
&gt;&gt;&gt; output.backward()

</code></pre>
<h2 id="_65">视觉层</h2>
<h3 id="_66">像素随机播放</h3>
<hr />
<pre><code>class torch.nn.PixelShuffle(upscale_factor)¶
</code></pre>
<p>将形状为<img alt="" src="../img/6b7acfa5f011739269f4e6cc57db0c43.jpg" />的张量中的元素重新排列为形状为<img alt="" src="../img/bd008942513578c0f427f1dd01591db9.jpg" />的张量中的元素。</p>
<p>这对于实现跨度为<img alt="" src="../img/f2e455e49ebc0fc06958bcb048f59627.jpg" />的有效子像素卷积很有用。</p>
<p>看论文：Shi 等人的<a href="https://arxiv.org/abs/1609.05158">使用高效的亚像素卷积神经网络</a>进行实时单图像和视频超分辨率。 al(2016）了解更多详情。</p>
<p>Parameters</p>
<p><strong>upscale_factor</strong>  (<em>python：int</em> )–通过提高空间分辨率的因子</p>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>输入：<img alt="" src="../img/2876c61a26fc0f8d37c5780f6b3b3de6.jpg" />，其中<img alt="" src="../img/57aaf57caee8665e53a8655ef8c55edd.jpg" /></p>
</li>
<li>
<p>输出：<img alt="" src="../img/bf0bebeaa2a77220349c5240a79ad798.jpg" />，其中<img alt="" src="../img/17f898793e6e5445cebfc433e50751d7.jpg" />和<img alt="" src="../img/f9b6e0d2200ed50d483987d0eaffbeed.jpg" /></p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; pixel_shuffle = nn.PixelShuffle(3)
&gt;&gt;&gt; input = torch.randn(1, 9, 4, 4)
&gt;&gt;&gt; output = pixel_shuffle(input)
&gt;&gt;&gt; print(output.size())
torch.Size([1, 1, 12, 12])

</code></pre>
<h3 id="_67">上采样</h3>
<hr />
<pre><code>class torch.nn.Upsample(size=None, scale_factor=None, mode='nearest', align_corners=None)¶
</code></pre>
<p>上采样给定的多通道 1D(时间），2D(空间）或 3D(体积）数据。</p>
<p>假定输入数据的形式为&lt;cite&gt;微型批处理 x 通道 x [可选深度] x [可选高度] x 宽度&lt;/cite&gt;。 因此，对于空间输入，我们期望使用 4D 张量；对于体积输入，我们期望使用 5D 张量。</p>
<p>可用于上采样的算法分别是 3D，4D 和 5D 输入张量的最近邻和线性，双线性，双三次和三线性。</p>
<p>可以给出<code>scale_factor</code>或目标输出<code>size</code>来计算输出大小。 (因为模棱两可，您不能同时给出两者）</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>大小</strong> (<em>python：int</em> <em>或</em> <em>元组</em> <em>[</em> <em>python：int</em> <em>]或</em> <em>元组</em> <em>[</em> <em>python：int</em> <em>，</em> <em>python：int</em> <em>]或</em> <em>元组</em> <em>[</em> <em>python：int</em> <em>，</em> <em>python：int</em> <em>，</em> <em>python：int</em> <em>]</em> <em>，</em> <em>可选</em>）–输出空间大小</p>
</li>
<li>
<p><strong>scale_factor</strong>  (<em>python：float</em> <em>或</em> <em>元组</em> <em>[</em> <em>python：float</em> <em>]或</em> <em>元组</em> <em>[</em> <em>python：float</em> <em>，</em> <em>python：float</em> <em>]或</em> <em>元组</em> <em>[</em> <em>python：float</em> <em>，</em> <em>python：float</em> <em>，</em> <em>python：float</em> <em>]</em> <em>，</em> <em>可选</em>）–空间大小的乘数。 如果是元组，则必须匹配输入大小。</p>
</li>
<li>
<p><strong>模式</strong> (<em>str</em> <em>，</em> <em>可选</em>）–上采样算法：<code>'nearest'</code>，<code>'linear'</code>，<code>'bilinear'</code>，[ <code>'bicubic'</code>和<code>'trilinear'</code>。 默认值：<code>'nearest'</code></p>
</li>
<li>
<p><strong>align_corners</strong>  (<em>bool</em> <em>，</em> <em>可选</em>）–如果<code>True</code>，则输入和输出张量的角像素对齐，因此 保留这些像素的值。 仅在<code>mode</code>为<code>'linear'</code>，<code>'bilinear'</code>或<code>'trilinear'</code>时才有效。 默认值：<code>False</code></p>
</li>
</ul>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>输入：<img alt="" src="../img/23913305314c2d86c6063bd9944df485.jpg" />，<img alt="" src="../img/55ac2d313b1b37ea5379f0f94f8d6982.jpg" />或<img alt="" src="../img/fc79fbdaec72bad3be239985d50f2f7c.jpg" /></p>
</li>
<li>
<p>输出：<img alt="" src="../img/6540eac5cf9523bdf619bd561683d713.jpg" />，<img alt="" src="../img/bf0bebeaa2a77220349c5240a79ad798.jpg" />或<img alt="" src="../img/56dd521f825f0c201df32048c532fd13.jpg" />，其中</p>
</li>
</ul>
<p><img alt="" src="../img/0a09a30e1738b87a38a0519135524089.jpg" /></p>
<p><img alt="" src="../img/be1cca954fae6a64da221acd6aa7effb.jpg" /></p>
<p><img alt="" src="../img/4178ffa288e7413addb995e381a61881.jpg" /></p>
<p>Warning</p>
<p>使用<code>align_corners = True</code>时，线性插值模式(&lt;cite&gt;线性&lt;/cite&gt;，&lt;cite&gt;双线性&lt;/cite&gt;，&lt;cite&gt;双三次&lt;/cite&gt;和&lt;cite&gt;三线性&lt;/cite&gt;）不会按比例对齐输出 和输入像素，因此输出值可能取决于输入大小。 这是这些模式(0.3.1 版之前）的默认行为。 从那时起，默认行为是<code>align_corners = False</code>。 有关如何影响输出的具体示例，请参见下文。</p>
<p>Note</p>
<p>如果要缩减采样/调整大小，应使用<code>interpolate()</code>。</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; input = torch.arange(1, 5, dtype=torch.float32).view(1, 1, 2, 2)
&gt;&gt;&gt; input
tensor([[[[ 1.,  2.],
          [ 3.,  4.]]]])

&gt;&gt;&gt; m = nn.Upsample(scale_factor=2, mode='nearest')
&gt;&gt;&gt; m(input)
tensor([[[[ 1.,  1.,  2.,  2.],
          [ 1.,  1.,  2.,  2.],
          [ 3.,  3.,  4.,  4.],
          [ 3.,  3.,  4.,  4.]]]])

&gt;&gt;&gt; m = nn.Upsample(scale_factor=2, mode='bilinear')  # align_corners=False
&gt;&gt;&gt; m(input)
tensor([[[[ 1.0000,  1.2500,  1.7500,  2.0000],
          [ 1.5000,  1.7500,  2.2500,  2.5000],
          [ 2.5000,  2.7500,  3.2500,  3.5000],
          [ 3.0000,  3.2500,  3.7500,  4.0000]]]])

&gt;&gt;&gt; m = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
&gt;&gt;&gt; m(input)
tensor([[[[ 1.0000,  1.3333,  1.6667,  2.0000],
          [ 1.6667,  2.0000,  2.3333,  2.6667],
          [ 2.3333,  2.6667,  3.0000,  3.3333],
          [ 3.0000,  3.3333,  3.6667,  4.0000]]]])

&gt;&gt;&gt; # Try scaling the same data in a larger tensor
&gt;&gt;&gt;
&gt;&gt;&gt; input_3x3 = torch.zeros(3, 3).view(1, 1, 3, 3)
&gt;&gt;&gt; input_3x3[:, :, :2, :2].copy_(input)
tensor([[[[ 1.,  2.],
          [ 3.,  4.]]]])
&gt;&gt;&gt; input_3x3
tensor([[[[ 1.,  2.,  0.],
          [ 3.,  4.,  0.],
          [ 0.,  0.,  0.]]]])

&gt;&gt;&gt; m = nn.Upsample(scale_factor=2, mode='bilinear')  # align_corners=False
&gt;&gt;&gt; # Notice that values in top left corner are the same with the small input (except at boundary)
&gt;&gt;&gt; m(input_3x3)
tensor([[[[ 1.0000,  1.2500,  1.7500,  1.5000,  0.5000,  0.0000],
          [ 1.5000,  1.7500,  2.2500,  1.8750,  0.6250,  0.0000],
          [ 2.5000,  2.7500,  3.2500,  2.6250,  0.8750,  0.0000],
          [ 2.2500,  2.4375,  2.8125,  2.2500,  0.7500,  0.0000],
          [ 0.7500,  0.8125,  0.9375,  0.7500,  0.2500,  0.0000],
          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]])

&gt;&gt;&gt; m = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
&gt;&gt;&gt; # Notice that values in top left corner are now changed
&gt;&gt;&gt; m(input_3x3)
tensor([[[[ 1.0000,  1.4000,  1.8000,  1.6000,  0.8000,  0.0000],
          [ 1.8000,  2.2000,  2.6000,  2.2400,  1.1200,  0.0000],
          [ 2.6000,  3.0000,  3.4000,  2.8800,  1.4400,  0.0000],
          [ 2.4000,  2.7200,  3.0400,  2.5600,  1.2800,  0.0000],
          [ 1.2000,  1.3600,  1.5200,  1.2800,  0.6400,  0.0000],
          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]])

</code></pre>
<h3 id="upsamplingnearest2d">UpsamplingNearest2d</h3>
<hr />
<pre><code>class torch.nn.UpsamplingNearest2d(size=None, scale_factor=None)¶
</code></pre>
<p>将二维最近邻居上采样应用于由多个输入通道组成的输入信号。</p>
<p>要指定比例，请使用<code>size</code>或<code>scale_factor</code>作为构造函数参数。</p>
<p>给定<code>size</code>时，它是图像&lt;cite&gt;(h，w）&lt;/cite&gt;的输出大小。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>大小</strong> (<em>python：int</em> <em>或</em> <em>元组</em> <em>[</em> <em>python：int</em> <em>，</em> <em>python：int</em> <em>]</em> <em>，</em> <em>可选</em>）–输出空间大小</p>
</li>
<li>
<p><strong>scale_factor</strong>  (<em>python：float</em> <em>或</em> <em>元组</em> <em>[</em> <em>python：float</em> <em>，</em> <em>python：float</em> <em>]</em> <em>，</em> <em>可选</em>）–空间大小的乘数。</p>
</li>
</ul>
<p>Warning</p>
<p>不推荐使用此类，而推荐使用<code>interpolate()</code>。</p>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/55ac2d313b1b37ea5379f0f94f8d6982.jpg" /></p>
</li>
<li>
<p>Output: <img alt="" src="../img/bf0bebeaa2a77220349c5240a79ad798.jpg" /> where</p>
</li>
</ul>
<p><img alt="" src="../img/be1cca954fae6a64da221acd6aa7effb.jpg" /></p>
<p><img alt="" src="../img/4178ffa288e7413addb995e381a61881.jpg" /></p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; input = torch.arange(1, 5, dtype=torch.float32).view(1, 1, 2, 2)
&gt;&gt;&gt; input
tensor([[[[ 1.,  2.],
          [ 3.,  4.]]]])

&gt;&gt;&gt; m = nn.UpsamplingNearest2d(scale_factor=2)
&gt;&gt;&gt; m(input)
tensor([[[[ 1.,  1.,  2.,  2.],
          [ 1.,  1.,  2.,  2.],
          [ 3.,  3.,  4.,  4.],
          [ 3.,  3.,  4.,  4.]]]])

</code></pre>
<h3 id="2d_4">上采样双线性 2d</h3>
<hr />
<pre><code>class torch.nn.UpsamplingBilinear2d(size=None, scale_factor=None)¶
</code></pre>
<p>将二维双线性上采样应用于由多个输入通道组成的输入信号。</p>
<p>To specify the scale, it takes either the <code>size</code> or the <code>scale_factor</code> as it's constructor argument.</p>
<p>When <code>size</code> is given, it is the output size of the image &lt;cite&gt;(h, w)&lt;/cite&gt;.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>size</strong> (<em>python:int</em> <em>or</em> <em>Tuple__[__python:int__,</em> <em>python:int__]__,</em> <em>optional</em>) – output spatial sizes</p>
</li>
<li>
<p><strong>scale_factor</strong> (<em>python:float</em> <em>or</em> <em>Tuple__[__python:float__,</em> <em>python:float__]__,</em> <em>optional</em>) – multiplier for spatial size.</p>
</li>
</ul>
<p>Warning</p>
<p>不推荐使用此类，而推荐使用<code>interpolate()</code>。 等效于<code>nn.functional.interpolate(..., mode='bilinear', align_corners=True)</code>。</p>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>Input: <img alt="" src="../img/55ac2d313b1b37ea5379f0f94f8d6982.jpg" /></p>
</li>
<li>
<p>Output: <img alt="" src="../img/bf0bebeaa2a77220349c5240a79ad798.jpg" /> where</p>
</li>
</ul>
<p><img alt="" src="../img/be1cca954fae6a64da221acd6aa7effb.jpg" /></p>
<p><img alt="" src="../img/4178ffa288e7413addb995e381a61881.jpg" /></p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; input = torch.arange(1, 5, dtype=torch.float32).view(1, 1, 2, 2)
&gt;&gt;&gt; input
tensor([[[[ 1.,  2.],
          [ 3.,  4.]]]])

&gt;&gt;&gt; m = nn.UpsamplingBilinear2d(scale_factor=2)
&gt;&gt;&gt; m(input)
tensor([[[[ 1.0000,  1.3333,  1.6667,  2.0000],
          [ 1.6667,  2.0000,  2.3333,  2.6667],
          [ 2.3333,  2.6667,  3.0000,  3.3333],
          [ 3.0000,  3.3333,  3.6667,  4.0000]]]])

</code></pre>
<h2 id="dataparallel-gpu">DataParallel 层(多 GPU，分布式）</h2>
<h3 id="_68">数据并行</h3>
<hr />
<pre><code>class torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0)¶
</code></pre>
<p>在模块级别实现数据并行性。</p>
<p>该容器通过按批处理维度中的组块在指定设备之间划分输入来并行化给定<code>module</code>的应用程序(其他对象将每个设备复制一次）。 在前向传递中，模块在每个设备上复制，每个副本处理输入的一部分。 在向后传递过程中，每个副本的梯度被累加到原始模块中。</p>
<p>批处理大小应大于使用的 GPU 数量。</p>
<p>另请参见：<a href="notes/cuda.html#cuda-nn-dataparallel-instead">使用 nn.DataParallel 而不是并行处理</a></p>
<p>允许将任意位置和关键字输入传递到 DataParallel 中，但是某些类型经过特殊处理。 张量将在指定的昏暗状态(默认值为 0）下分散为。 元组，列表和字典类型将被浅表复制。 其他类型将在不同线程之间共享，并且如果写入模型的前向传递中，则可能会损坏。</p>
<p>运行此 <a href="#torch.nn.DataParallel" title="torch.nn.DataParallel"><code>DataParallel</code></a> 模块之前，并行化的<code>module</code>必须在<code>device_ids[0]</code>上具有其参数和缓冲区。</p>
<p>Warning</p>
<p>在每个转发中，<code>module</code>是在每个设备上复制的<strong>，因此对<code>forward</code>中正在运行的模块的任何更新都将丢失。 例如，如果<code>module</code>具有在每个<code>forward</code>中递增的计数器属性，则它将始终保持在初始值，因为更新是在<code>forward</code>之后销毁的副本上进行的。 但是， <a href="#torch.nn.DataParallel" title="torch.nn.DataParallel"><code>DataParallel</code></a> 保证<code>device[0]</code>上的副本具有与基本并行化<code>module</code>共享存储的参数和缓冲区。 因此，将记录对<code>device[0]</code>上参数或缓冲区的</strong>就地<strong>更新。 例如， <a href="#torch.nn.BatchNorm2d" title="torch.nn.BatchNorm2d"><code>BatchNorm2d</code></a> 和 <a href="#torch.nn.utils.spectral_norm" title="torch.nn.utils.spectral_norm"><code>spectral_norm()</code></a> 依赖此行为来更新缓冲区。</strong></p>
<p>Warning</p>
<p><code>module</code>及其子模块上定义的前向和后向挂钩将被调用<code>len(device_ids)</code>次，每次输入都位于特定设备上。 特别地，仅保证挂钩在相对应的设备上的操作中以正确的顺序执行。 例如，不能保证在所有 <code>len(device_ids)</code> <a href="#torch.nn.Module.forward" title="torch.nn.Module.forward"><code>forward()</code></a> 调用&lt;cite&gt;之前执行通过 <a href="#torch.nn.Module.register_forward_pre_hook" title="torch.nn.Module.register_forward_pre_hook"><code>register_forward_pre_hook()</code></a> 设置的挂钩。 在该设备的相应 <a href="#torch.nn.Module.forward" title="torch.nn.Module.forward"><code>forward()</code></a> 调用之前执行。&lt;/cite&gt;</p>
<p>Warning</p>
<p>当<code>module</code>在<code>forward()</code>中返回标量(即 0 维张量）时，此包装器将返回一个长度等于向量的设备，该矢量等于数据并行性中使用的设备数，其中包含每个设备的结果。</p>
<p>Note</p>
<p>在包裹在 <a href="#torch.nn.DataParallel" title="torch.nn.DataParallel"><code>DataParallel</code></a> 中的 <a href="#torch.nn.Module" title="torch.nn.Module"><code>Module</code></a> 中使用<code>pack sequence -&amp;gt; recurrent network -&amp;gt; unpack sequence</code>模式有个微妙之处。 有关详细信息，请参见<a href="notes/faq.html#pack-rnn-unpack-with-data-parallelism">我的循环网络不适用于数据并行性。</a>部分。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>模块</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><em>模块</em></a>)–要并行化的模块</p>
</li>
<li>
<p><strong>device_ids</strong> (python：int 的_列表：<em> _或</em> <a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>Torch.device</em></a>)– CUDA 设备(默认：所有设备 )</p>
</li>
<li>
<p><strong>output_device</strong>  (<em>python：int</em> <em>或</em> <a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>Torch.device</em></a>)–输出的设备位置(默认值：device_ids [ 0]）</p>
</li>
</ul>
<pre><code>Variables
</code></pre>
<p><strong>〜DataParallel.module</strong>  (<a href="#torch.nn.Module" title="torch.nn.Module"><em>模块</em></a>)–要并行化的模块</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; net = torch.nn.DataParallel(model, device_ids=[0, 1, 2])
&gt;&gt;&gt; output = net(input_var)  # input_var can be on any device, including CPU

</code></pre>
<h3 id="_69">分布式数据并行</h3>
<hr />
<pre><code>class torch.nn.parallel.DistributedDataParallel(module, device_ids=None, output_device=None, dim=0, broadcast_buffers=True, process_group=None, bucket_cap_mb=25, find_unused_parameters=False, check_reduction=False)¶
</code></pre>
<p>在模块级别实现基于<code>torch.distributed</code>包的分布式数据并行性。</p>
<p>该容器通过按批处理维度分块指定设备之间的输入来并行化给定模块的应用程序。 该模块在每台机器和每台设备上复制，并且每个这样的副本处理一部分输入。 在向后传递过程中，将平均每个节点的梯度。</p>
<p>批处理大小应大于本地使用的 GPU 数量。</p>
<p>另请参见：<a href="distributed.html#distributed-basics">基础知识</a>和<a href="notes/cuda.html#cuda-nn-dataparallel-instead">使用 nn.DataParallel 而不是并行处理</a>。 对输入的限制与 <a href="#torch.nn.DataParallel" title="torch.nn.DataParallel"><code>torch.nn.DataParallel</code></a> 相同。</p>
<p>此类的创建要求通过调用 <a href="distributed.html#torch.distributed.init_process_group" title="torch.distributed.init_process_group"><code>torch.distributed.init_process_group()</code></a> 已初始化<code>torch.distributed</code>。</p>
<p><code>DistributedDataParallel</code>可以通过以下两种方式使用：</p>
<ol>
<li>单进程多 GPU</li>
</ol>
<p>在这种情况下，将在每个主机/节点上生成一个进程，并且每个进程将在运行该节点的节点的所有 GPU 上运行。 要以这种方式使用<code>DistributedDataParallel</code>，您可以简单地按以下方式构建模型：</p>
<pre><code>&gt;&gt;&gt; torch.distributed.init_process_group(backend=&quot;nccl&quot;)
&gt;&gt;&gt; model = DistributedDataParallel(model) # device_ids will include all GPU devices by default

</code></pre>
<ol>
<li>多进程单 GPU</li>
</ol>
<p>强烈建议将<code>DistributedDataParallel</code>与多个进程配合使用，每个进程都在单个 GPU 上运行。 这是目前使用 PyTorch 进行数据并行训练的最快方法，适用于单节点(multi-GPU）和多节点数据并行训练。 对于单节点多 GPU 数据并行训练，它被证明比 <a href="#torch.nn.DataParallel" title="torch.nn.DataParallel"><code>torch.nn.DataParallel</code></a> 快得多。</p>
<p>使用方法如下：在具有 N 个 GPU 的每台主机上，应生成 N 个进程，同时确保每个进程在 0 至 N-1 的单个 GPU 上可单独运行。 因此，您的工作是通过调用以下命令来确保您的训练脚本在单个给定的 GPU 上运行：</p>
<pre><code>&gt;&gt;&gt; torch.cuda.set_device(i)

</code></pre>
<p>我从 0 到 N-1。 在每个过程中，应参考以下内容来构造此模块：</p>
<pre><code>&gt;&gt;&gt; torch.distributed.init_process_group(backend='nccl', world_size=4, init_method='...')
&gt;&gt;&gt; model = DistributedDataParallel(model, device_ids=[i], output_device=i)

</code></pre>
<p>为了在每个节点上产生多个进程，可以使用<code>torch.distributed.launch</code>或<code>torch.multiprocessing.spawn</code></p>
<p>Note</p>
<p><code>nccl</code>后端目前是与多进程单 GPU 分布式训练一起使用的最快和强烈推荐的后端，这适用于单节点和多节点分布式训练</p>
<p>Note</p>
<p>该模块还支持混合精度分布式训练。 这意味着您的模型可以具有不同类型的参数，例如 fp16 和 fp32 的混合类型，对这些混合类型的参数进行梯度降低将可以正常工作。 另请注意，<code>nccl</code>后端是目前 fp16 / fp32 混合精度训练中最快，最受推荐的后端。</p>
<p>Note</p>
<p>如果在一个进程上使用<code>torch.save</code>检查模块，在其他进程上使用<code>torch.load</code>对其进行恢复，请确保为每个进程都正确配置了<code>map_location</code>。 没有<code>map_location</code>，<code>torch.load</code>会将模块恢复到保存模块的设备。</p>
<p>Warning</p>
<p>该模块仅适用于<code>gloo</code>和<code>nccl</code>后端。</p>
<p>Warning</p>
<p>构造函数，正向方法和输出的微分(或此模块输出的函数）是分布式同步点。 如果不同的进程可能执行不同的代码，请考虑到这一点。</p>
<p>Warning</p>
<p>该模块假定所有参数在创建时已在模型中注册。 不应添加或删除参数。 同样适用于缓冲区。</p>
<p>Warning</p>
<p>该模块假定所有参数在模型中注册的每个分布式过程都以相同的顺序进行。 模块本身将按照模型注册参数的相反顺序进行梯度全约。 换句话说，确保每个分布式过程具有完全相同的模型并因此具有完全相同的参数注册顺序是用户的责任。</p>
<p>Warning</p>
<p>该模块假定所有缓冲区和渐变都是密集的。</p>
<p>Warning</p>
<p>此模块不适用于 <a href="autograd.html#torch.autograd.grad" title="torch.autograd.grad"><code>torch.autograd.grad()</code></a> (即，仅当要在<code>.grad</code>参数的属性中累积梯度时才适用）。</p>
<p>Warning</p>
<p>如果您打算将此模块与<code>nccl</code>后端或<code>gloo</code>后端(使用 Infiniband）一起使用，以及使用多个工作程序的 DataLoader，请将并行处理启动方法更改为<code>forkserver</code>(仅适用于 Python 3），或者 <code>spawn</code>。 不幸的是，Gloo(使用 Infiniband）和 NCCL2 都不是安全的，如果不更改此设置，您可能会遇到死锁。</p>
<p>Warning</p>
<p>除非在<code>forward()</code>方法中初始化了挂钩，否则将不再调用<code>module</code>及其子模块上定义的向前和向后挂钩。</p>
<p>Warning</p>
<p>在用 DistributedDataParallel 包装模型之后，您永远不要尝试更改模型的参数。 换句话说，当用 DistributedDataParallel 包装模型时，DistributedDataParallel 的构造函数将在构造时在模型本身的所有参数上注册其他梯度减少函数。 如果在构造 DistributedDataParallel 之后更改模型的参数，则不支持此操作，并且可能会发生意外行为，因为可能不会调用某些参数的梯度减小函数。</p>
<p>Note</p>
<p>参数从不在进程之间广播。 该模块对梯度执行全缩减步骤，并假设优化器将在所有过程中以相同方式修改它们。 缓冲区(例如 BatchNorm 统计信息）在每次迭代中从模块广播到第 0 级进程，并广播到系统中的所有其他副本。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>module</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><em>Module</em></a>) – module to be parallelized</p>
</li>
<li>
<p><strong>device_ids</strong> (python：int <em>或</em> <a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>Torch.device</em></a> 的_列表）– CUDA 设备。 仅当输入模块位于单个 CUDA 设备上时才应提供此选项。 对于单设备模块，<code>i``th :attr:</code>module<code>replica is placed on ``device_ids[i]</code>。 对于多设备模块和 CPU 模块，device_ids 必须为 None 或为空列表，并且用于正向传递的输入数据必须放置在正确的设备上。 (默认：单设备模块的所有设备）_</p>
</li>
<li>
<p><strong>output_device</strong>  (<em>python：int</em> <em>或</em> <a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>Torch.device</em></a>)–单设备 CUDA 输出的设备位置 模块。 对于多设备模块和 CPU 模块，它必须为 None(无），并且模块本身指定输出位置。 (对于单设备模块，默认值：device_ids [0]）</p>
</li>
<li>
<p><strong>broadcast_buffers</strong>  (<em>bool</em> )–该标志在转发功能开始时启用模块的同步(广播）缓冲区。 (默认：<code>True</code>）</p>
</li>
<li>
<p><strong>process_group</strong> –用于减少所有分布式数据的进程组。 如果<code>None</code>，将使用由<code>torch.distributed.init_process_group</code>创建的默认进程组。 (默认：<code>None</code>）</p>
</li>
<li>
<p><strong>bucket_cap_mb</strong> – DistributedDataParallel 会将参数存储到多个存储桶中，以便每个存储桶的梯度缩减可能与反向计算重叠。 <code>bucket_cap_mb</code>控制存储桶的大小(以兆字节(MB）为单位）(默认值：25）</p>
</li>
<li>
<p><strong>find_unused_pa​​rameters</strong>  (<em>bool</em> )–遍历包装模块的<code>forward</code>函数的返回值中包含的所有张量的自动梯度图。 在此图表中未接收到渐变的参数会被抢先标记为可以还原。 请注意，从模块参数派生的所有<code>forward</code>输出必须参与计算损耗，然后再参与梯度计算。 如果没有，该包装器将挂起，等待 autograd 为这些参数生成梯度。 可以使用<code>torch.Tensor.detach</code>将来自模块参数的其他未使用的输出从 autograd 图中分离出来。 (默认：<code>False</code>）</p>
</li>
<li>
<p><strong>check_reduction</strong> –设置为<code>True</code>时，它使 DistributedDataParallel 自动检查在每次迭代的正向功能开始时是否成功发布了先前迭代的向后缩减。 通常您不需要启用此选项，除非您观察到奇怪的现象，例如不同的等级得到不同的梯度，如果正确使用 DistributedDataParallel，则不会发生这种情况。 (默认：<code>False</code>）</p>
</li>
</ul>
<pre><code>Variables
</code></pre>
<p><strong>〜DistributedDataParallel.module</strong>  (<a href="#torch.nn.Module" title="torch.nn.Module"><em>Module</em></a>)–要并行化的模块</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.distributed.init_process_group(backend='nccl', world_size=4, init_method='...')
&gt;&gt;&gt; net = torch.nn.DistributedDataParallel(model, pg)

</code></pre>
<hr />
<pre><code>no_sync()¶
</code></pre>
<p>上下文管理器，用于禁用 DDP 进程之间的梯度同步。 在此上下文中，梯度将累积在模块变量上，稍后将在退出上下文的第一个向前-向后传递中进行同步。</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; ddp = torch.nn.DistributedDataParallel(model, pg)
&gt;&gt;&gt; with ddp.no_sync():
...   for input in inputs:
...     ddp(input).backward()  # no synchronization, accumulate grads
... ddp(another_input).backward()  # synchronize grads

</code></pre>
<h2 id="_70">实用工具</h2>
<h3 id="clip_grad_norm_">clip_grad_norm_</h3>
<hr />
<pre><code>torch.nn.utils.clip_grad_norm_(parameters, max_norm, norm_type=2)¶
</code></pre>
<p>裁剪参数可迭代的梯度范数。</p>
<p>范数是在所有梯度上一起计算的，就好像它们被串联到单个矢量中一样。 渐变就地修改。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>参数</strong>(<em>可迭代的</em> <em>[</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>]或</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–张量的迭代或将张量归一化的单个张量</p>
</li>
<li>
<p><strong>max_norm</strong>  (<em>python：float</em> <em>或</em> <em>python：int</em> )–梯度的最大范数</p>
</li>
<li>
<p><strong>norm_type</strong>  (<em>python：float</em> <em>或</em> <em>python：int</em> )–使用的 p 范数的类型。 对于无穷大范数可以为<code>'inf'</code>。</p>
</li>
</ul>
<p>Returns</p>
<p>参数的总范数(视为单个向量）。</p>
<h3 id="clip_grad_value_">clip_grad_value_</h3>
<hr />
<pre><code>torch.nn.utils.clip_grad_value_(parameters, clip_value)¶
</code></pre>
<p>将可迭代参数的梯度剪切为指定值。</p>
<p>渐变就地修改。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>parameters</strong> (<em>Iterable__[</em><a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>] or</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – an iterable of Tensors or a single Tensor that will have gradients normalized</p>
</li>
<li>
<p><strong>clip_value</strong>  (<em>python：float</em> <em>或</em> <em>python：int</em> )–渐变的最大允许值。 渐变被限制在<img alt="" src="../img/d88b48b19821641cdac62209db0bfc01.jpg" />范围内</p>
</li>
</ul>
<h3 id="parameters_to_vector">parameters_to_vector</h3>
<hr />
<pre><code>torch.nn.utils.parameters_to_vector(parameters)¶
</code></pre>
<p>将参数转换为一个向量</p>
<p>Parameters</p>
<p><strong>参数</strong>(<em>可迭代</em> <em>[</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>]</em> )–张量的迭代器 模型的参数。</p>
<p>Returns</p>
<p>单个向量表示的参数</p>
<h3 id="vector_to_parameters">vector_to_parameters</h3>
<hr />
<pre><code>torch.nn.utils.vector_to_parameters(vec, parameters)¶
</code></pre>
<p>将一个向量转换为参数</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>vec</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–单个矢量表示模型的参数。</p>
</li>
<li>
<p><strong>parameters</strong> (<em>Iterable__[</em><a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>]</em>) – an iterator of Tensors that are the parameters of a model.</p>
</li>
</ul>
<h3 id="basepruningmethod">BasePruningMethod</h3>
<hr />
<pre><code>class torch.nn.utils.prune.BasePruningMethod¶
</code></pre>
<p>用于创建新修剪技术的抽象基类。</p>
<p>为需要重写诸如 <a href="#torch.nn.utils.prune.BasePruningMethod.compute_mask" title="torch.nn.utils.prune.BasePruningMethod.compute_mask"><code>compute_mask()</code></a> 和 <a href="#torch.nn.utils.prune.BasePruningMethod.apply" title="torch.nn.utils.prune.BasePruningMethod.apply"><code>apply()</code></a> 等方法的定制提供框架。</p>
<hr />
<pre><code>classmethod apply(module, name, *args, **kwargs)¶
</code></pre>
<p>添加了前向预挂接，可进行即时修剪，并根据原始张量和修剪蒙版对张量进行重新参数化。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>模块</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>)–包含要修剪的张量的模块</p>
</li>
<li>
<p><strong>名称</strong> (<em>str</em> )– <code>module</code>中将对其进行修剪的参数名称。</p>
</li>
<li>
<p><strong>args</strong> –传递给 <a href="#torch.nn.utils.prune.BasePruningMethod" title="torch.nn.utils.prune.BasePruningMethod"><code>BasePruningMethod</code></a> 子类的参数</p>
</li>
<li>
<p><strong>kwargs</strong> –传递给 <a href="#torch.nn.utils.prune.BasePruningMethod" title="torch.nn.utils.prune.BasePruningMethod"><code>BasePruningMethod</code></a> 子类的关键字参数</p>
</li>
</ul>
<hr />
<pre><code>apply_mask(module)¶
</code></pre>
<p>只需处理要修剪的参数和生成的掩码之间的乘法。 从模块中获取遮罩和原始张量，然后返回该张量的修剪版本。</p>
<p>Parameters</p>
<p><strong>module</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) – module containing the tensor to prune</p>
<p>Returns</p>
<p>输入张量的修剪版本</p>
<p>Return type</p>
<p>pruned_tensor (<a href="tensors.html#torch.Tensor" title="torch.Tensor">Torch.Tensor</a>)</p>
<hr />
<pre><code>abstract compute_mask(t, default_mask)¶
</code></pre>
<p>计算并返回输入张量<code>t</code>的掩码。 从基础<code>default_mask</code>(如果尚未修剪张量，应为 1 的掩码）开始，根据特定的修剪方法配方，生成一个随机掩码以应用于<code>default_mask</code>的顶部。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>t</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>torch张量</em></a>)–代表修剪参数的张量</p>
</li>
<li>
<p><strong>default_mask</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>torch张量</em></a>)–先前修剪迭代中的基础掩码，在应用新掩码后需要加以注意。 与<code>t</code>相同。</p>
</li>
</ul>
<p>Returns</p>
<p>应用于<code>t</code>的遮罩，其亮度与<code>t</code>相同</p>
<p>Return type</p>
<p>面罩(<a href="tensors.html#torch.Tensor" title="torch.Tensor">torch。张量</a>）</p>
<hr />
<pre><code>prune(t, default_mask=None)¶
</code></pre>
<p>根据 <a href="#torch.nn.utils.prune.BasePruningMethod.compute_mask" title="torch.nn.utils.prune.BasePruningMethod.compute_mask"><code>compute_mask()</code></a> 中指定的修剪规则，计算并返回输入张量<code>t</code>的修剪版本。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>t</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>torch张量</em></a>)–张量到修剪(与<code>default_mask</code>相同的尺寸）。</p>
</li>
<li>
<p><strong>default_mask</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>炬管张量</em></a> <em>，</em> <em>可选</em>）–先前修剪迭代的掩码(如果有）。 在确定应对张量的哪一部分进行修剪时考虑。 如果为 None，则默认为 1 的掩码。</p>
</li>
</ul>
<p>Returns</p>
<p>张量的修剪版本<code>t</code>。</p>
<hr />
<pre><code>remove(module)¶
</code></pre>
<p>从模块中删除修剪重新参数化。 被修剪的名为<code>name</code>的参数将被永久修剪，并且将从参数列表中删除名为<code>name+'_orig'</code>的参数。 同样，从缓冲区中删除名为<code>name+'_mask'</code>的缓冲区。</p>
<p>Note</p>
<p>修剪本身不会撤消或撤消！</p>
<h3 id="_71">修剪容器</h3>
<hr />
<pre><code>class torch.nn.utils.prune.PruningContainer(*args)¶
</code></pre>
<p>容器，其中包含一系列用于迭代修剪的修剪方法。 跟踪修剪方法的应用顺序，并处理合并的连续修剪调用。</p>
<p>接受 BasePruningMethod 的实例或它们的可迭代实例作为参数。</p>
<hr />
<pre><code>add_pruning_method(method)¶
</code></pre>
<p>将子修剪<code>method</code>添加到容器中。</p>
<p>Parameters</p>
<p><strong>方法</strong>(BasePruningMethod 的_子类）–要添加到容器的子修剪方法。_</p>
<hr />
<pre><code>classmethod apply(module, name, *args, **kwargs)¶
</code></pre>
<p>Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>module</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) – module containing the tensor to prune</p>
</li>
<li>
<p><strong>name</strong> (<em>str</em>) – parameter name within <code>module</code> on which pruning will act.</p>
</li>
<li>
<p><strong>args</strong> – arguments passed on to a subclass of <a href="#torch.nn.utils.prune.BasePruningMethod" title="torch.nn.utils.prune.BasePruningMethod"><code>BasePruningMethod</code></a></p>
</li>
<li>
<p><strong>kwargs</strong> – keyword arguments passed on to a subclass of a <a href="#torch.nn.utils.prune.BasePruningMethod" title="torch.nn.utils.prune.BasePruningMethod"><code>BasePruningMethod</code></a></p>
</li>
</ul>
<hr />
<pre><code>apply_mask(module)¶
</code></pre>
<p>Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.</p>
<p>Parameters</p>
<p><strong>module</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) – module containing the tensor to prune</p>
<p>Returns</p>
<p>pruned version of the input tensor</p>
<p>Return type</p>
<p>pruned_tensor (<a href="tensors.html#torch.Tensor" title="torch.Tensor">torch.Tensor</a>)</p>
<hr />
<pre><code>compute_mask(t, default_mask)¶
</code></pre>
<p>通过计算新的局部掩码并返回其与<code>default_mask</code>的组合来应用最新的<code>method</code>。 新的部分掩码应在<code>default_mask</code>未归零的条目或通道上计算。 将根据<code>PRUNING_TYPE</code>(由类型处理程序处理）来计算新掩码的张量<code>t</code>的哪一部分：</p>
<blockquote>
<ul>
<li>对于“非结构化”，蒙版将根据乱码计算</li>
</ul>
<p>非屏蔽条目列表；</p>
<ul>
<li>对于“结构化”，遮罩将根据非遮罩计算</li>
</ul>
<p>张量中的通道；</p>
<ul>
<li>对于“全局”，将在所有条目中计算掩码。</li>
</ul>
</blockquote>
<p>Parameters</p>
<ul>
<li>
<p><strong>t</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>torch张量</em></a>)–代表要修剪的参数的张量(与<code>default_mask</code>尺寸相同）。</p>
</li>
<li>
<p><strong>default_mask</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>torch张量</em></a>)–先前修剪迭代的掩码。</p>
</li>
</ul>
<p>Returns</p>
<p>合并了<code>default_mask</code>和来自当前修剪<code>method</code>的新蒙版(与<code>default_mask</code>和<code>t</code>尺寸相同）的新蒙版的新蒙版。</p>
<p>Return type</p>
<p>mask (<a href="tensors.html#torch.Tensor" title="torch.Tensor">torch.Tensor</a>)</p>
<hr />
<pre><code>prune(t, default_mask=None)¶
</code></pre>
<p>根据 <a href="#torch.nn.utils.prune.PruningContainer.compute_mask" title="torch.nn.utils.prune.PruningContainer.compute_mask"><code>compute_mask()</code></a> 中指定的修剪规则，计算并返回输入张量<code>t</code>的修剪版本。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>t</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>torch.Tensor</em></a>) – tensor to prune (of same dimensions as <code>default_mask</code>).</p>
</li>
<li>
<p><strong>default_mask</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>torch.Tensor</em></a><em>,</em> <em>optional</em>) – mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.</p>
</li>
</ul>
<p>Returns</p>
<p>pruned version of tensor <code>t</code>.</p>
<hr />
<pre><code>remove(module)¶
</code></pre>
<p>Removes the pruning reparameterization from a module. The pruned parameter named <code>name</code> remains permanently pruned, and the parameter named <code>name+'_orig'</code> is removed from the parameter list. Similarly, the buffer named <code>name+'_mask'</code> is removed from the buffers.</p>
<p>Note</p>
<p>Pruning itself is NOT undone or reversed!</p>
<h3 id="identity">Identity</h3>
<hr />
<pre><code>class torch.nn.utils.prune.Identity¶
</code></pre>
<p>实用修剪方法，不修剪任何单位，而是用一个“ 1”的掩码生成修剪参数。</p>
<hr />
<pre><code>classmethod apply(module, name)¶
</code></pre>
<p>Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>module</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) – module containing the tensor to prune</p>
</li>
<li>
<p><strong>name</strong> (<em>str</em>) – parameter name within <code>module</code> on which pruning will act.</p>
</li>
</ul>
<hr />
<pre><code>apply_mask(module)¶
</code></pre>
<p>Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.</p>
<p>Parameters</p>
<p><strong>module</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) – module containing the tensor to prune</p>
<p>Returns</p>
<p>pruned version of the input tensor</p>
<p>Return type</p>
<p>pruned_tensor (<a href="tensors.html#torch.Tensor" title="torch.Tensor">torch.Tensor</a>)</p>
<hr />
<pre><code>prune(t, default_mask=None)¶
</code></pre>
<p>根据<code>compute_mask()</code>中指定的修剪规则，计算并返回输入张量<code>t</code>的修剪版本。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>t</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>torch.Tensor</em></a>) – tensor to prune (of same dimensions as <code>default_mask</code>).</p>
</li>
<li>
<p><strong>default_mask</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>torch.Tensor</em></a><em>,</em> <em>optional</em>) – mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.</p>
</li>
</ul>
<p>Returns</p>
<p>pruned version of tensor <code>t</code>.</p>
<hr />
<pre><code>remove(module)¶
</code></pre>
<p>Removes the pruning reparameterization from a module. The pruned parameter named <code>name</code> remains permanently pruned, and the parameter named <code>name+'_orig'</code> is removed from the parameter list. Similarly, the buffer named <code>name+'_mask'</code> is removed from the buffers.</p>
<p>Note</p>
<p>Pruning itself is NOT undone or reversed!</p>
<h3 id="_72">随机非结构化</h3>
<hr />
<pre><code>class torch.nn.utils.prune.RandomUnstructured(amount)¶
</code></pre>
<p>在张量中随机修剪(当前未修剪）单位。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>name</strong> (<em>str</em>) – parameter name within <code>module</code> on which pruning will act.</p>
</li>
<li>
<p><strong>数量</strong> (<em>python：int</em> <em>或</em> <em>python：float</em> )–修剪参数的数量。 如果<code>float</code>，则应在 0.0 到 1.0 之间，并且代表要修剪的参数的分数。 如果<code>int</code>，则表示要修剪的参数的绝对数量。</p>
</li>
</ul>
<hr />
<pre><code>classmethod apply(module, name, amount)¶
</code></pre>
<p>Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>module</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) – module containing the tensor to prune</p>
</li>
<li>
<p><strong>name</strong> (<em>str</em>) – parameter name within <code>module</code> on which pruning will act.</p>
</li>
<li>
<p><strong>amount</strong> (<em>python:int</em> <em>or</em> <em>python:float</em>) – quantity of parameters to prune. If <code>float</code>, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If <code>int</code>, it represents the absolute number of parameters to prune.</p>
</li>
</ul>
<hr />
<pre><code>apply_mask(module)¶
</code></pre>
<p>Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.</p>
<p>Parameters</p>
<p><strong>module</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) – module containing the tensor to prune</p>
<p>Returns</p>
<p>pruned version of the input tensor</p>
<p>Return type</p>
<p>pruned_tensor (<a href="tensors.html#torch.Tensor" title="torch.Tensor">torch.Tensor</a>)</p>
<hr />
<pre><code>prune(t, default_mask=None)¶
</code></pre>
<p>Computes and returns a pruned version of input tensor <code>t</code> according to the pruning rule specified in <code>compute_mask()</code>.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>t</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>torch.Tensor</em></a>) – tensor to prune (of same dimensions as <code>default_mask</code>).</p>
</li>
<li>
<p><strong>default_mask</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>torch.Tensor</em></a><em>,</em> <em>optional</em>) – mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.</p>
</li>
</ul>
<p>Returns</p>
<p>pruned version of tensor <code>t</code>.</p>
<hr />
<pre><code>remove(module)¶
</code></pre>
<p>Removes the pruning reparameterization from a module. The pruned parameter named <code>name</code> remains permanently pruned, and the parameter named <code>name+'_orig'</code> is removed from the parameter list. Similarly, the buffer named <code>name+'_mask'</code> is removed from the buffers.</p>
<p>Note</p>
<p>Pruning itself is NOT undone or reversed!</p>
<h3 id="l1_2">L1 非结构化</h3>
<hr />
<pre><code>class torch.nn.utils.prune.L1Unstructured(amount)¶
</code></pre>
<p>通过将具有最低 L1 范数的单元归零，在张量中修剪(当前未修剪的）单元。</p>
<p>Parameters</p>
<p><strong>amount</strong> (<em>python:int</em> <em>or</em> <em>python:float</em>) – quantity of parameters to prune. If <code>float</code>, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If <code>int</code>, it represents the absolute number of parameters to prune.</p>
<hr />
<pre><code>classmethod apply(module, name, amount)¶
</code></pre>
<p>Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>module</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) – module containing the tensor to prune</p>
</li>
<li>
<p><strong>name</strong> (<em>str</em>) – parameter name within <code>module</code> on which pruning will act.</p>
</li>
<li>
<p><strong>amount</strong> (<em>python:int</em> <em>or</em> <em>python:float</em>) – quantity of parameters to prune. If <code>float</code>, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If <code>int</code>, it represents the absolute number of parameters to prune.</p>
</li>
</ul>
<hr />
<pre><code>apply_mask(module)¶
</code></pre>
<p>Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.</p>
<p>Parameters</p>
<p><strong>module</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) – module containing the tensor to prune</p>
<p>Returns</p>
<p>pruned version of the input tensor</p>
<p>Return type</p>
<p>pruned_tensor (<a href="tensors.html#torch.Tensor" title="torch.Tensor">torch.Tensor</a>)</p>
<hr />
<pre><code>prune(t, default_mask=None)¶
</code></pre>
<p>Computes and returns a pruned version of input tensor <code>t</code> according to the pruning rule specified in <code>compute_mask()</code>.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>t</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>torch.Tensor</em></a>) – tensor to prune (of same dimensions as <code>default_mask</code>).</p>
</li>
<li>
<p><strong>default_mask</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>torch.Tensor</em></a><em>,</em> <em>optional</em>) – mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.</p>
</li>
</ul>
<p>Returns</p>
<p>pruned version of tensor <code>t</code>.</p>
<hr />
<pre><code>remove(module)¶
</code></pre>
<p>Removes the pruning reparameterization from a module. The pruned parameter named <code>name</code> remains permanently pruned, and the parameter named <code>name+'_orig'</code> is removed from the parameter list. Similarly, the buffer named <code>name+'_mask'</code> is removed from the buffers.</p>
<p>Note</p>
<p>Pruning itself is NOT undone or reversed!</p>
<h3 id="_73">随机结构</h3>
<hr />
<pre><code>class torch.nn.utils.prune.RandomStructured(amount, dim=-1)¶
</code></pre>
<p>在张量中随机修剪整个(当前未修剪的）通道。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>amount</strong> (<em>python:int</em> <em>or</em> <em>python:float</em>) – quantity of parameters to prune. If <code>float</code>, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If <code>int</code>, it represents the absolute number of parameters to prune.</p>
</li>
<li>
<p><strong>暗淡</strong> (<em>python：int</em> <em>，</em> <em>可选</em>）–暗淡的索引，我们沿着该暗淡定义了修剪通道。 默认值：-1。</p>
</li>
</ul>
<hr />
<pre><code>classmethod apply(module, name, amount, dim=-1)¶
</code></pre>
<p>Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>module</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) – module containing the tensor to prune</p>
</li>
<li>
<p><strong>name</strong> (<em>str</em>) – parameter name within <code>module</code> on which pruning will act.</p>
</li>
<li>
<p><strong>amount</strong> (<em>python:int</em> <em>or</em> <em>python:float</em>) – quantity of parameters to prune. If <code>float</code>, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If <code>int</code>, it represents the absolute number of parameters to prune.</p>
</li>
<li>
<p><strong>dim</strong> (<em>python:int__,</em> <em>optional</em>) – index of the dim along which we define channels to prune. Default: -1.</p>
</li>
</ul>
<hr />
<pre><code>apply_mask(module)¶
</code></pre>
<p>Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.</p>
<p>Parameters</p>
<p><strong>module</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) – module containing the tensor to prune</p>
<p>Returns</p>
<p>pruned version of the input tensor</p>
<p>Return type</p>
<p>pruned_tensor (<a href="tensors.html#torch.Tensor" title="torch.Tensor">torch.Tensor</a>)</p>
<hr />
<pre><code>compute_mask(t, default_mask)¶
</code></pre>
<p>计算并返回输入张量<code>t</code>的掩码。 从基础<code>default_mask</code>(如果尚未修剪张量，应为 1 的掩码）开始，通过沿张量的指定暗部随机清零通道，生成随机掩码以应用于<code>default_mask</code>顶部 。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>t</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>torch.Tensor</em></a>) – tensor representing the parameter to prune</p>
</li>
<li>
<p><strong>default_mask</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>torch.Tensor</em></a>) – Base mask from previous pruning iterations, that need to be respected after the new mask is applied. Same dims as <code>t</code>.</p>
</li>
</ul>
<p>Returns</p>
<p>mask to apply to <code>t</code>, of same dims as <code>t</code></p>
<p>Return type</p>
<p>mask (<a href="tensors.html#torch.Tensor" title="torch.Tensor">torch.Tensor</a>)</p>
<pre><code>Raises
</code></pre>
<p><strong>IndexError</strong> –如果<code>self.dim &amp;gt;= len(t.shape)</code></p>
<hr />
<pre><code>prune(t, default_mask=None)¶
</code></pre>
<p>根据 <a href="#torch.nn.utils.prune.RandomStructured.compute_mask" title="torch.nn.utils.prune.RandomStructured.compute_mask"><code>compute_mask()</code></a> 中指定的修剪规则，计算并返回输入张量<code>t</code>的修剪版本。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>t</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>torch.Tensor</em></a>) – tensor to prune (of same dimensions as <code>default_mask</code>).</p>
</li>
<li>
<p><strong>default_mask</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>torch.Tensor</em></a><em>,</em> <em>optional</em>) – mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.</p>
</li>
</ul>
<p>Returns</p>
<p>pruned version of tensor <code>t</code>.</p>
<hr />
<pre><code>remove(module)¶
</code></pre>
<p>Removes the pruning reparameterization from a module. The pruned parameter named <code>name</code> remains permanently pruned, and the parameter named <code>name+'_orig'</code> is removed from the parameter list. Similarly, the buffer named <code>name+'_mask'</code> is removed from the buffers.</p>
<p>Note</p>
<p>Pruning itself is NOT undone or reversed!</p>
<h3 id="ln">Ln 结构化</h3>
<hr />
<pre><code>class torch.nn.utils.prune.LnStructured(amount, n, dim=-1)¶
</code></pre>
<p>根据张量的 Ln 范数在张量中修剪整个(当前未修剪的）通道。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>数量</strong> (<em>python：int</em> <em>或</em> <em>python：float</em> )–修剪通道的数量。 如果<code>float</code>，则应在 0.0 到 1.0 之间，并且代表要修剪的参数的分数。 如果<code>int</code>，则表示要修剪的参数的绝对数量。</p>
</li>
<li>
<p><strong>n</strong>  (<em>python：int</em> <em>，</em> <em>python：float</em> <em>，</em> <em>inf</em> <em>，</em> <em>-inf</em> <em>，</em> <em>'来回</em> <em>，</em> <em>'nuc'</em>）–请参阅有效的文档 <a href="torch.html#torch.norm" title="torch.norm"><code>torch.norm()</code></a> 中参数<code>p</code>的条目。</p>
</li>
<li>
<p><strong>dim</strong> (<em>python:int__,</em> <em>optional</em>) – index of the dim along which we define channels to prune. Default: -1.</p>
</li>
</ul>
<hr />
<pre><code>classmethod apply(module, name, amount, n, dim)¶
</code></pre>
<p>Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>module</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) – module containing the tensor to prune</p>
</li>
<li>
<p><strong>name</strong> (<em>str</em>) – parameter name within <code>module</code> on which pruning will act.</p>
</li>
<li>
<p><strong>amount</strong> (<em>python:int</em> <em>or</em> <em>python:float</em>) – quantity of parameters to prune. If <code>float</code>, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If <code>int</code>, it represents the absolute number of parameters to prune.</p>
</li>
<li>
<p><strong>n</strong> (<em>python:int__,</em> <em>python:float__,</em> <em>inf__,</em> <em>-inf__,</em> <em>'fro'__,</em> <em>'nuc'</em>) – See documentation of valid entries for argument <code>p</code> in <a href="torch.html#torch.norm" title="torch.norm"><code>torch.norm()</code></a>.</p>
</li>
<li>
<p><strong>暗淡</strong> (<em>python：int</em> )–暗淡的索引，我们沿其定义修剪的通道。</p>
</li>
</ul>
<hr />
<pre><code>apply_mask(module)¶
</code></pre>
<p>Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.</p>
<p>Parameters</p>
<p><strong>module</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) – module containing the tensor to prune</p>
<p>Returns</p>
<p>pruned version of the input tensor</p>
<p>Return type</p>
<p>pruned_tensor (<a href="tensors.html#torch.Tensor" title="torch.Tensor">torch.Tensor</a>)</p>
<hr />
<pre><code>compute_mask(t, default_mask)¶
</code></pre>
<p>计算并返回输入张量<code>t</code>的掩码。 从基本<code>default_mask</code>(如果尚未修剪张量，应为 1 的掩码）开始，通过将沿指定的暗角(具有最低 Ln 的通道）归零，生成一个掩码以应用于<code>default_mask</code>顶部 -规范。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>t</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>torch.Tensor</em></a>) – tensor representing the parameter to prune</p>
</li>
<li>
<p><strong>default_mask</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>torch张量</em></a>)–先前修剪迭代中的基础掩码，在应用新掩码后需要加以注意。 与<code>t</code>相同。</p>
</li>
</ul>
<p>Returns</p>
<p>mask to apply to <code>t</code>, of same dims as <code>t</code></p>
<p>Return type</p>
<p>mask (<a href="tensors.html#torch.Tensor" title="torch.Tensor">torch.Tensor</a>)</p>
<pre><code>Raises
</code></pre>
<p><strong>IndexError</strong> – if <code>self.dim &amp;gt;= len(t.shape)</code></p>
<hr />
<pre><code>prune(t, default_mask=None)¶
</code></pre>
<p>根据 <a href="#torch.nn.utils.prune.LnStructured.compute_mask" title="torch.nn.utils.prune.LnStructured.compute_mask"><code>compute_mask()</code></a> 中指定的修剪规则，计算并返回输入张量<code>t</code>的修剪版本。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>t</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>torch.Tensor</em></a>) – tensor to prune (of same dimensions as <code>default_mask</code>).</p>
</li>
<li>
<p><strong>default_mask</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>torch.Tensor</em></a><em>,</em> <em>optional</em>) – mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.</p>
</li>
</ul>
<p>Returns</p>
<p>pruned version of tensor <code>t</code>.</p>
<hr />
<pre><code>remove(module)¶
</code></pre>
<p>Removes the pruning reparameterization from a module. The pruned parameter named <code>name</code> remains permanently pruned, and the parameter named <code>name+'_orig'</code> is removed from the parameter list. Similarly, the buffer named <code>name+'_mask'</code> is removed from the buffers.</p>
<p>Note</p>
<p>Pruning itself is NOT undone or reversed!</p>
<h3 id="customfrommask">CustomFromMask</h3>
<hr />
<pre><code>class torch.nn.utils.prune.CustomFromMask(mask)¶
</code></pre>
<hr />
<pre><code>classmethod apply(module, name, mask)¶
</code></pre>
<p>Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>module</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) – module containing the tensor to prune</p>
</li>
<li>
<p><strong>name</strong> (<em>str</em>) – parameter name within <code>module</code> on which pruning will act.</p>
</li>
</ul>
<hr />
<pre><code>apply_mask(module)¶
</code></pre>
<p>Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.</p>
<p>Parameters</p>
<p><strong>module</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) – module containing the tensor to prune</p>
<p>Returns</p>
<p>pruned version of the input tensor</p>
<p>Return type</p>
<p>pruned_tensor (<a href="tensors.html#torch.Tensor" title="torch.Tensor">torch.Tensor</a>)</p>
<hr />
<pre><code>prune(t, default_mask=None)¶
</code></pre>
<p>Computes and returns a pruned version of input tensor <code>t</code> according to the pruning rule specified in <code>compute_mask()</code>.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>t</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>torch.Tensor</em></a>) – tensor to prune (of same dimensions as <code>default_mask</code>).</p>
</li>
<li>
<p><strong>default_mask</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>torch.Tensor</em></a><em>,</em> <em>optional</em>) – mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.</p>
</li>
</ul>
<p>Returns</p>
<p>pruned version of tensor <code>t</code>.</p>
<hr />
<pre><code>remove(module)¶
</code></pre>
<p>Removes the pruning reparameterization from a module. The pruned parameter named <code>name</code> remains permanently pruned, and the parameter named <code>name+'_orig'</code> is removed from the parameter list. Similarly, the buffer named <code>name+'_mask'</code> is removed from the buffers.</p>
<p>Note</p>
<p>Pruning itself is NOT undone or reversed!</p>
<h3 id="_74">身份</h3>
<hr />
<pre><code>torch.nn.utils.prune.identity(module, name)¶
</code></pre>
<p>将修剪重新参数化应用于与<code>module</code>中称为<code>name</code>的参数相对应的张量，而无需实际修剪任何单位。 通过以下方式就地修改模块(并返回修改后的模块）：1）添加一个名为<code>name+'_mask'</code>的命名缓冲区，该缓冲区与通过修剪方法应用于参数<code>name</code>的二进制掩码相对应。 2）用已修剪版本替换参数<code>name</code>，而原始(未修剪）参数存储在名为<code>name+'_orig'</code>的新参数中。</p>
<p>Note</p>
<p>掩码是一个张量。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>模块</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>)–包含要修剪的张量的模块。</p>
</li>
<li>
<p><strong>name</strong> (<em>str</em>) – parameter name within <code>module</code> on which pruning will act.</p>
</li>
</ul>
<p>Returns</p>
<p>输入模块的修改(即修剪）版本</p>
<p>Return type</p>
<p>模块 (<a href="#torch.nn.Module" title="torch.nn.Module">nn.Module</a>)</p>
<p>Examples</p>
<pre><code>&gt;&gt;&gt; m = prune.identity(nn.Linear(2, 3), 'bias')
&gt;&gt;&gt; print(m.bias_mask)
tensor([1., 1., 1.])

</code></pre>
<h3 id="random_unstructured">random_unstructured</h3>
<hr />
<pre><code>torch.nn.utils.prune.random_unstructured(module, name, amount)¶
</code></pre>
<p>通过删除随机选择的(当前未修剪的）单位的指定<code>amount</code>来修剪与<code>module</code>中称为<code>name</code>的参数相对应的张量。 通过以下方式就地修改模块(并返回修改后的模块）：1）添加一个名为<code>name+'_mask'</code>的命名缓冲区，该缓冲区与通过修剪方法应用于参数&lt;cite&gt;名称&lt;/cite&gt;的二进制掩码相对应。 2）用已修剪版本替换参数<code>name</code>，而原始(未修剪）参数存储在名为<code>name+'_orig'</code>的新参数中。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>module</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) – module containing the tensor to prune</p>
</li>
<li>
<p><strong>name</strong> (<em>str</em>) – parameter name within <code>module</code> on which pruning will act.</p>
</li>
<li>
<p><strong>amount</strong> (<em>python:int</em> <em>or</em> <em>python:float</em>) – quantity of parameters to prune. If <code>float</code>, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If <code>int</code>, it represents the absolute number of parameters to prune.</p>
</li>
</ul>
<p>Returns</p>
<p>modified (i.e. pruned) version of the input module</p>
<p>Return type</p>
<p>module (<a href="#torch.nn.Module" title="torch.nn.Module">nn.Module</a>)</p>
<p>Examples</p>
<pre><code>&gt;&gt;&gt; m = prune.random_unstructured(nn.Linear(2, 3), 'weight', amount=1)
&gt;&gt;&gt; torch.sum(m.weight_mask == 0)
tensor(1)

</code></pre>
<h3 id="l1_unstructured">l1_unstructured</h3>
<hr />
<pre><code>torch.nn.utils.prune.l1_unstructured(module, name, amount)¶
</code></pre>
<p>通过删除 L1 范数最低的(当前未修剪）单位的指定&lt;cite&gt;量&lt;/cite&gt;，修剪与<code>module</code>中称为<code>name</code>的参数相对应的张量。 通过以下方式就地修改模块(并返回修改后的模块）：1）添加一个名为<code>name+'_mask'</code>的命名缓冲区，该缓冲区与通过修剪方法应用于参数<code>name</code>的二进制掩码相对应。 2）用已修剪版本替换参数<code>name</code>，而原始(未修剪）参数存储在名为<code>name+'_orig'</code>的新参数中。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>module</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) – module containing the tensor to prune</p>
</li>
<li>
<p><strong>name</strong> (<em>str</em>) – parameter name within <code>module</code> on which pruning will act.</p>
</li>
<li>
<p><strong>amount</strong> (<em>python:int</em> <em>or</em> <em>python:float</em>) – quantity of parameters to prune. If <code>float</code>, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If <code>int</code>, it represents the absolute number of parameters to prune.</p>
</li>
</ul>
<p>Returns</p>
<p>modified (i.e. pruned) version of the input module</p>
<p>Return type</p>
<p>module (<a href="#torch.nn.Module" title="torch.nn.Module">nn.Module</a>)</p>
<p>Examples</p>
<pre><code>&gt;&gt;&gt; m = prune.l1_unstructured(nn.Linear(2, 3), 'weight', amount=0.2)
&gt;&gt;&gt; m.state_dict().keys()
odict_keys(['bias', 'weight_orig', 'weight_mask'])

</code></pre>
<h3 id="_75">随机结构</h3>
<hr />
<pre><code>torch.nn.utils.prune.random_structured(module, name, amount, dim)¶
</code></pre>
<p>通过沿着随机选择的指定<code>dim</code>移除(当前未修剪的）通道的指定<code>amount</code>来修剪与<code>module</code>中称为<code>name</code>的参数相对应的张量。 通过以下方式就地修改模块(并返回修改后的模块）：1）添加一个名为<code>name+'_mask'</code>的命名缓冲区，该缓冲区与通过修剪方法应用于参数<code>name</code>的二进制掩码相对应。 2）用已修剪版本替换参数<code>name</code>，而原始(未修剪）参数存储在名为<code>name+'_orig'</code>的新参数中。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>module</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) – module containing the tensor to prune</p>
</li>
<li>
<p><strong>name</strong> (<em>str</em>) – parameter name within <code>module</code> on which pruning will act.</p>
</li>
<li>
<p><strong>amount</strong> (<em>python:int</em> <em>or</em> <em>python:float</em>) – quantity of parameters to prune. If <code>float</code>, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If <code>int</code>, it represents the absolute number of parameters to prune.</p>
</li>
<li>
<p><strong>暗淡</strong> (<em>python：int</em> )–暗淡的索引，我们沿其定义修剪的通道。</p>
</li>
</ul>
<p>Returns</p>
<p>modified (i.e. pruned) version of the input module</p>
<p>Return type</p>
<p>module (<a href="#torch.nn.Module" title="torch.nn.Module">nn.Module</a>)</p>
<p>Examples</p>
<pre><code>&gt;&gt;&gt; m = prune.random_structured(
        nn.Linear(5, 3), 'weight', amount=3, dim=1
    )
&gt;&gt;&gt; columns_pruned = int(sum(torch.sum(m.weight, dim=0) == 0))
&gt;&gt;&gt; print(columns_pruned)
3

</code></pre>
<h3 id="ln_">ln_ 结构化</h3>
<hr />
<pre><code>torch.nn.utils.prune.ln_structured(module, name, amount, n, dim)¶
</code></pre>
<p>通过沿着具有最低 L''n<code>`范数的指定</code>dim<code>移除(当前未修剪的）通道的指定</code>amount<code>来修剪与</code>module<code>中称为</code>name<code>的参数相对应的张量。 通过以下方式就地修改模块(并返回修改后的模块）：1）添加一个名为</code>name+'_mask'<code>的命名缓冲区，该缓冲区与通过修剪方法应用于参数</code>name<code>的二进制掩码相对应。 2）用已修剪版本替换参数</code>name<code>，而原始(未修剪）参数存储在名为</code>name+'_orig'`的新参数中。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>module</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) – module containing the tensor to prune</p>
</li>
<li>
<p><strong>name</strong> (<em>str</em>) – parameter name within <code>module</code> on which pruning will act.</p>
</li>
<li>
<p><strong>amount</strong> (<em>python:int</em> <em>or</em> <em>python:float</em>) – quantity of parameters to prune. If <code>float</code>, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If <code>int</code>, it represents the absolute number of parameters to prune.</p>
</li>
<li>
<p><strong>n</strong> (<em>python:int__,</em> <em>python:float__,</em> <em>inf__,</em> <em>-inf__,</em> <em>'fro'__,</em> <em>'nuc'</em>) – See documentation of valid entries for argument <code>p</code> in <a href="torch.html#torch.norm" title="torch.norm"><code>torch.norm()</code></a>.</p>
</li>
<li>
<p><strong>dim</strong> (<em>python:int</em>) – index of the dim along which we define channels to prune.</p>
</li>
</ul>
<p>Returns</p>
<p>modified (i.e. pruned) version of the input module</p>
<p>Return type</p>
<p>module (<a href="#torch.nn.Module" title="torch.nn.Module">nn.Module</a>)</p>
<p>Examples</p>
<pre><code>&gt;&gt;&gt; m = prune.ln_structured(
       nn.Conv2d(5, 3, 2), 'weight', amount=0.3, dim=1, n=float('-inf')
    )

</code></pre>
<h3 id="global_unstructured">global_unstructured</h3>
<hr />
<pre><code>torch.nn.utils.prune.global_unstructured(parameters, pruning_method, **kwargs)¶
</code></pre>
<p>通过应用指定的<code>pruning_method</code>全局修剪与<code>parameters</code>中所有参数相对应的张量。 通过以下方式修改模块：1）添加一个名为<code>name+'_mask'</code>的命名缓冲区，该缓冲区与通过修剪方法应用于参数<code>name</code>的二进制掩码相对应。 2）用已修剪版本替换参数<code>name</code>，而原始(未修剪）参数存储在名为<code>name+'_orig'</code>的新参数中。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>参数</strong> (<em>(</em> <em>模块</em> <em>，</em> <em>名称</em> <em>）的迭代</em> ] <em>元组</em>）–以全局方式修剪的模型参数，即在决定要修剪的权重之前，先汇总所有权重。 模块必须为<code>nn.Module</code>类型，名称必须为字符串。</p>
</li>
<li>
<p><strong>pruning_method</strong> (<em>函数</em>）–该模块中的有效修剪函数，或者是由用户实现的，满足实施准则并具有<code>PRUNING_TYPE='unstructured'</code>的自定义函数。</p>
</li>
<li>
<p><strong>kwargs</strong> –其他关键字参数，例如：amount(整数或浮点数）：跨指定参数修剪的参数数量。 如果<code>float</code>，则应在 0.0 到 1.0 之间，并且代表要修剪的参数的分数。 如果<code>int</code>，则表示要修剪的参数的绝对数量。</p>
</li>
</ul>
<pre><code>Raises
</code></pre>
<p><strong>TypeError</strong> –如果<code>PRUNING_TYPE != 'unstructured'</code></p>
<p>Note</p>
<p>由于除非通过参数的大小对规范进行规范化，否则全局结构化修剪没有多大意义，因此我们现在将全局修剪的范围限制为非结构化方法。</p>
<p>Examples</p>
<pre><code>&gt;&gt;&gt; net = nn.Sequential(OrderedDict([
        ('first', nn.Linear(10, 4)),
        ('second', nn.Linear(4, 1)),
    ]))
&gt;&gt;&gt; parameters_to_prune = (
        (net.first, 'weight'),
        (net.second, 'weight'),
    )
&gt;&gt;&gt; prune.global_unstructured(
        parameters_to_prune,
        pruning_method=prune.L1Unstructured,
        amount=10,
    )
&gt;&gt;&gt; print(sum(torch.nn.utils.parameters_to_vector(net.buffers()) == 0))
tensor(10, dtype=torch.uint8)

</code></pre>
<h3 id="custom_from_mask">custom_from_mask</h3>
<hr />
<pre><code>torch.nn.utils.prune.custom_from_mask(module, name, mask)¶
</code></pre>
<p>通过在<code>mask</code>中应用预先计算的掩码，修剪与<code>module</code>中称为<code>name</code>的参数相对应的张量。 通过以下方式就地修改模块(并返回修改后的模块）：1）添加一个名为<code>name+'_mask'</code>的命名缓冲区，该缓冲区与通过修剪方法应用于参数<code>name</code>的二进制掩码相对应。 2）用已修剪版本替换参数<code>name</code>，而原始(未修剪）参数存储在名为<code>name+'_orig'</code>的新参数中。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>module</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) – module containing the tensor to prune</p>
</li>
<li>
<p><strong>name</strong> (<em>str</em>) – parameter name within <code>module</code> on which pruning will act.</p>
</li>
<li>
<p><strong>掩码</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–应用于参数的二进制掩码。</p>
</li>
</ul>
<p>Returns</p>
<p>modified (i.e. pruned) version of the input module</p>
<p>Return type</p>
<p>module (<a href="#torch.nn.Module" title="torch.nn.Module">nn.Module</a>)</p>
<p>Examples</p>
<pre><code>&gt;&gt;&gt; m = prune.custom_from_mask(
        nn.Linear(5, 3), name='bias', mask=torch.Tensor([0, 1, 0])
    )
&gt;&gt;&gt; print(m.bias_mask)
tensor([0., 1., 0.])

</code></pre>
<h3 id="_76">去掉</h3>
<hr />
<pre><code>torch.nn.utils.prune.remove(module, name)¶
</code></pre>
<p>从模块中删除修剪重新参数化，并从前向挂钩中删除修剪方法。 被修剪的名为<code>name</code>的参数将被永久修剪，并且将从参数列表中删除名为<code>name+'_orig'</code>的参数。 同样，从缓冲区中删除名为<code>name+'_mask'</code>的缓冲区。</p>
<p>Note</p>
<p>Pruning itself is NOT undone or reversed!</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>module</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) – module containing the tensor to prune</p>
</li>
<li>
<p><strong>name</strong> (<em>str</em>) – parameter name within <code>module</code> on which pruning will act.</p>
</li>
</ul>
<p>Examples</p>
<pre><code>&gt;&gt;&gt; m = random_pruning(nn.Linear(5, 7), name='weight', amount=0.2)
&gt;&gt;&gt; m = remove_pruning(m, name='weight')

</code></pre>
<h3 id="is_pruned">is_pruned</h3>
<hr />
<pre><code>torch.nn.utils.prune.is_pruned(module)¶
</code></pre>
<p>通过在从 <a href="#torch.nn.utils.prune.BasePruningMethod" title="torch.nn.utils.prune.BasePruningMethod"><code>BasePruningMethod</code></a> 继承的模块中查找<code>forward_pre_hooks</code>，检查是否修剪了<code>module</code>。</p>
<p>Parameters</p>
<p><strong>模块</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>)–已修剪或未修剪的对象</p>
<p>Returns</p>
<p>是否修剪<code>module</code>的二进制答案。</p>
<p>Examples</p>
<pre><code>&gt;&gt;&gt; m = nn.Linear(5, 7)
&gt;&gt;&gt; print(prune.is_pruned(m))
False
&gt;&gt;&gt; prune.random_pruning(m, name='weight', amount=0.2)
&gt;&gt;&gt; print(prune.is_pruned(m))
True

</code></pre>
<h3 id="weight_norm">weight_norm</h3>
<hr />
<pre><code>torch.nn.utils.weight_norm(module, name='weight', dim=0)¶
</code></pre>
<p>将权重归一化应用于给定模块中的参数。</p>
<p><img alt="" src="../img/61d8b3f7ac0fe0b0855ef7876ec5c6a2.jpg" /></p>
<p>权重归一化是将权重张量的大小与其方向解耦的重新参数化。 这用两个参数替换了<code>name</code>指定的参数(例如<code>'weight'</code>）：一个指定幅度(例如<code>'weight_g'</code>）和一个指定方向(例如<code>'weight_v'</code>）。 权重归一化是通过一个挂钩实现的，该挂钩在每次<code>forward()</code>调用之前从幅度和方向重新计算权重张量。</p>
<p>默认情况下，使用<code>dim=0</code>，将针对每个输出通道/平面独立计算范数。 要计算整个重量张量的范数，请使用<code>dim=None</code>。</p>
<p>参见 <a href="https://arxiv.org/abs/1602.07868">https://arxiv.org/abs/1602.07868</a></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>模块</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><em>模块</em></a>)–包含模块</p>
</li>
<li>
<p><strong>名称</strong> (<em>str</em> <em>，</em> <em>可选</em>）–重量参数的名称</p>
</li>
<li>
<p><strong>昏暗的</strong> (<em>python：int</em> <em>，</em> <em>可选</em>）–计算范数的维度</p>
</li>
</ul>
<p>Returns</p>
<p>带有重量标准钩的原始模块</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; m = weight_norm(nn.Linear(20, 40), name='weight')
&gt;&gt;&gt; m
Linear(in_features=20, out_features=40, bias=True)
&gt;&gt;&gt; m.weight_g.size()
torch.Size([40, 1])
&gt;&gt;&gt; m.weight_v.size()
torch.Size([40, 20])

</code></pre>
<h3 id="remove_weight_norm">remove_weight_norm</h3>
<hr />
<pre><code>torch.nn.utils.remove_weight_norm(module, name='weight')¶
</code></pre>
<p>从模块中删除权重归一化重新参数化。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>module</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><em>Module</em></a>) – containing module</p>
</li>
<li>
<p><strong>name</strong> (<em>str__,</em> <em>optional</em>) – name of weight parameter</p>
</li>
</ul>
<p>例</p>
<pre><code>&gt;&gt;&gt; m = weight_norm(nn.Linear(20, 40))
&gt;&gt;&gt; remove_weight_norm(m)

</code></pre>
<h3 id="spectrum_norm">Spectrum_norm</h3>
<hr />
<pre><code>torch.nn.utils.spectral_norm(module, name='weight', n_power_iterations=1, eps=1e-12, dim=None)¶
</code></pre>
<p>将频谱归一化应用于给定模块中的参数。</p>
<p><img alt="" src="../img/6c2eac98972ccaf01bc0944df73bf9d9.jpg" /></p>
<p>频谱归一化通过使用权重矩阵计算的权重矩阵的频谱范数<img alt="" src="../img/cc84e998f72a1c5a0a5f736ba6a9ff34.jpg" />重新调整权重张量，从而稳定了生成对抗网络(GAN）中鉴别器(批评家）的训练。 如果权重张量的尺寸大于 2，则在幂迭代方法中将其重塑为 2D 以获得频谱范数。 这是通过一个挂钩实现的，该挂钩在每次<code>forward()</code>调用之前计算频谱范数并重新调整权重。</p>
<p>请参阅生成对抗网络的<a href="https://arxiv.org/abs/1802.05957">频谱归一化。</a></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>module</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) – containing module</p>
</li>
<li>
<p><strong>name</strong> (<em>str__,</em> <em>optional</em>) – name of weight parameter</p>
</li>
<li>
<p><strong>n_power_iterations</strong>  (<em>python：int</em> <em>，</em> <em>可选</em>）–计算频谱范数的功率迭代次数</p>
</li>
<li>
<p><strong>eps</strong>  (<em>python：float</em> <em>，</em> <em>可选</em>）– epsilon 在计算范数时具有数值稳定性</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> <em>，</em> <em>可选</em>）–尺寸对应于输出数量，默认为<code>0</code>，除了模块 是 ConvTranspose {1,2,3} d 的实例，当它是<code>1</code>时</p>
</li>
</ul>
<p>Returns</p>
<p>带有频谱范数挂钩的原始模块</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; m = spectral_norm(nn.Linear(20, 40))
&gt;&gt;&gt; m
Linear(in_features=20, out_features=40, bias=True)
&gt;&gt;&gt; m.weight_u.size()
torch.Size([40])

</code></pre>
<h3 id="remove_spectral_norm">remove_spectral_norm</h3>
<hr />
<pre><code>torch.nn.utils.remove_spectral_norm(module, name='weight')¶
</code></pre>
<p>从模块中删除频谱归一化重新参数化。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>module</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><em>Module</em></a>) – containing module</p>
</li>
<li>
<p><strong>name</strong> (<em>str__,</em> <em>optional</em>) – name of weight parameter</p>
</li>
</ul>
<p>Example</p>
<pre><code>&gt;&gt;&gt; m = spectral_norm(nn.Linear(40, 10))
&gt;&gt;&gt; remove_spectral_norm(m)

</code></pre>
<h3 id="_77">打包序列</h3>
<hr />
<pre><code>torch.nn.utils.rnn.PackedSequence(data, batch_sizes=None, sorted_indices=None, unsorted_indices=None)¶
</code></pre>
<p>保存打包序列的<code>batch_sizes</code>的数据和列表。</p>
<p>所有 RNN 模块都将打包序列作为输入。</p>
<p>Note</p>
<p>此类的实例永远不要手动创建。 它们应通过 <a href="#torch.nn.utils.rnn.pack_padded_sequence" title="torch.nn.utils.rnn.pack_padded_sequence"><code>pack_padded_sequence()</code></a> 之类的函数实例化。</p>
<p>批次大小代表批次中每个序列步骤的数量元素，而不是传递给 <a href="#torch.nn.utils.rnn.pack_padded_sequence" title="torch.nn.utils.rnn.pack_padded_sequence"><code>pack_padded_sequence()</code></a> 的变化序列长度。 例如，给定数据<code>abc</code>和<code>x</code>， <a href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code>PackedSequence</code></a> 将包含数据<code>axbc</code>和<code>batch_sizes=[2,1,1]</code>。</p>
<pre><code>Variables
</code></pre>
<ul>
<li>
<p><strong>〜PackedSequence.data</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–包含压缩序列的张量</p>
</li>
<li>
<p><strong>〜PackedSequence.batch_sizes</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–整数张量，用于保存有关每个序列步骤的批次大小信息</p>
</li>
<li>
<p><strong>〜PackedSequence.sorted_indices</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>，</em> <em>可选</em>）–保持此 [<a href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"> <code>PackedSequence</code></a> 由序列构建。</p>
</li>
<li>
<p><strong>〜PackedSequence.unsorted_indices</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>，</em> <em>可选</em>）–整数的张量，表示如何恢复原始值 顺序正确的序列。</p>
</li>
</ul>
<p>Note</p>
<p><code>data</code>可以在任意设备上和任意 dtype 上。 <code>sorted_indices</code>和<code>unsorted_indices</code>必须是与<code>data</code>在同一设备上的<code>torch.int64</code>张量。</p>
<p>但是，<code>batch_sizes</code>应该始终是 CPU <code>torch.int64</code>张量。</p>
<p>这个不变量在整个 <a href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code>PackedSequence</code></a> 类中保持不变，并且所有在 PyTorch 中构造&lt;cite&gt;：class：PackedSequence&lt;/cite&gt; 的函数都保持不变(即，它们仅传递符合此约束的张量）。</p>
<h3 id="pack_padded_sequence">pack_padded_sequence</h3>
<hr />
<pre><code>torch.nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=False, enforce_sorted=True)¶
</code></pre>
<p>打包一个 Tensor，其中包含可变长度的填充序列。</p>
<p><code>input</code>的大小可以为<code>T x B x *</code>，其中 &lt;cite&gt;T&lt;/cite&gt; 是最长序列的长度(等于<code>lengths[0]</code>），<code>B</code>是批处理大小，<code>*</code>是任意数量的尺寸 (包括 0）。 如果<code>batch_first</code>为<code>True</code>，则预期为<code>B x T x *</code> <code>input</code>。</p>
<p>对于未排序的序列，请使用 &lt;cite&gt;force_sorted = False&lt;/cite&gt; 。 如果<code>enforce_sorted</code>为<code>True</code>，则序列应按长度降序排列，即<code>input[:,0]</code>应为最长序列，而<code>input[:,B-1]</code>为最短序列。 &lt;cite&gt;forcen_sorted = True&lt;/cite&gt; 仅对于 ONNX 导出是必需的。</p>
<p>Note</p>
<p>此函数接受至少具有二维的任何输入。 您可以将其应用于包装标签，并与它们一起使用 RNN 的输出直接计算损失。 可以通过访问 <a href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code>.data</code>属性来从</a> <a href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code>PackedSequence</code></a> 对象中检索张量。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–填充了可变长度序列的批处理。</p>
</li>
<li>
<p><strong>长度</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–每个批处理元素的序列长度列表。</p>
</li>
<li>
<p><strong>batch_first</strong>  (<em>bool</em> <em>，</em> <em>可选</em>）–如果为<code>True</code>，则输入应为<code>B x T x *</code>格式。</p>
</li>
<li>
<p><strong>强制排序的</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–如果<code>True</code>，则输入应包含按长度降序排列的序列 。 如果为<code>False</code>，则不检查此条件。 默认值：<code>True</code>。</p>
</li>
</ul>
<p>Returns</p>
<p><a href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code>PackedSequence</code></a> 对象</p>
<h3 id="pad_packed_sequence">pad_packed_sequence</h3>
<hr />
<pre><code>torch.nn.utils.rnn.pad_packed_sequence(sequence, batch_first=False, padding_value=0.0, total_length=None)¶
</code></pre>
<p>填充打包的可变长度序列批次。</p>
<p>这是 <a href="#torch.nn.utils.rnn.pack_padded_sequence" title="torch.nn.utils.rnn.pack_padded_sequence"><code>pack_padded_sequence()</code></a> 的逆运算。</p>
<p>返回的 Tensor 数据大小为<code>T x B x *</code>，其中 &lt;cite&gt;T&lt;/cite&gt; 是最长序列的长度， &lt;cite&gt;B&lt;/cite&gt; 是批处理大小。 如果<code>batch_first</code>为 True，则数据将转置为<code>B x T x *</code>格式。</p>
<p>批处理元素将按其长度顺序减小。</p>
<p>Note</p>
<p><code>total_length</code>可用于在包裹在 <a href="#torch.nn.DataParallel" title="torch.nn.DataParallel"><code>DataParallel</code></a> 中的 <a href="#torch.nn.Module" title="torch.nn.Module"><code>Module</code></a> 中实现<code>pack sequence -&amp;gt; recurrent network -&amp;gt; unpack sequence</code>模式。 有关详细信息，请参见<a href="notes/faq.html#pack-rnn-unpack-with-data-parallelism">此常见问题解答部分</a>。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>序列</strong> (<em>PackedSequence</em> )–批量填充</p>
</li>
<li>
<p><strong>batch_first</strong> (<em>布尔</em> <em>，</em> <em>可选</em>）–如果为<code>True</code>，则输出为<code>B x T x *</code>格式。</p>
</li>
<li>
<p><strong>padding_value</strong>  (<em>python：float</em> <em>，</em> <em>可选</em>）–填充元素的值。</p>
</li>
<li>
<p><strong>total_length</strong>  (<em>python：int</em> <em>，</em> <em>可选</em>）–如果不是<code>None</code>，则输出将被填充为长度<code>total_length</code> 。 如果<code>total_length</code>小于<code>sequence</code>中的最大序列长度，则此方法将抛出<code>ValueError</code>。</p>
</li>
</ul>
<p>Returns</p>
<p>张量元组包含填充的序列，张量包含批处理中每个序列的长度列表。</p>
<h3 id="pad_sequence">pad_sequence</h3>
<hr />
<pre><code>torch.nn.utils.rnn.pad_sequence(sequences, batch_first=False, padding_value=0)¶
</code></pre>
<p>用<code>padding_value</code>填充可变长度张量的列表</p>
<p><code>pad_sequence</code>沿新维度堆叠张量列表，并将它们填充为相等的长度。 例如，如果输入是大小为<code>L x *</code>的序列列表，并且 batch_first 为 False，否则为<code>T x B x *</code>。</p>
<p>&lt;cite&gt;B&lt;/cite&gt; 是批处理大小。 它等于<code>sequences</code>中的元素数。 &lt;cite&gt;T&lt;/cite&gt; 是最长序列的长度。 &lt;cite&gt;L&lt;/cite&gt; 是序列的长度。 &lt;cite&gt;*&lt;/cite&gt; 是任意数量的尾随尺寸，包括无。</p>
<p>Example</p>
<pre><code>&gt;&gt;&gt; from torch.nn.utils.rnn import pad_sequence
&gt;&gt;&gt; a = torch.ones(25, 300)
&gt;&gt;&gt; b = torch.ones(22, 300)
&gt;&gt;&gt; c = torch.ones(15, 300)
&gt;&gt;&gt; pad_sequence([a, b, c]).size()
torch.Size([25, 3, 300])

</code></pre>
<p>Note</p>
<p>此函数返回张量为<code>T x B x *</code>或<code>B x T x *</code>的张量，其中 &lt;cite&gt;T&lt;/cite&gt; 是最长序列的长度。 此函数假定序列中所有张量的尾随尺寸和类型相同。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>序列</strong>(<em>列表</em> <em>[</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>]</em> )–可变长度序列的列表。</p>
</li>
<li>
<p><strong>batch_first</strong> (<em>布尔</em> <em>，</em> <em>可选</em>）–如果为 True，则输出为<code>B x T x *</code>，否则为<code>T x B x *</code></p>
</li>
<li>
<p><strong>padding_value</strong>  (<em>python：float</em> <em>，</em> <em>可选</em>）–填充元素的值。 默认值：0</p>
</li>
</ul>
<p>Returns</p>
<p>如果<code>batch_first</code>为<code>False</code>，则大小为<code>T x B x *</code>的张量。 否则大小为<code>B x T x *</code>的张量</p>
<h3 id="pack_sequence">pack_sequence</h3>
<hr />
<pre><code>torch.nn.utils.rnn.pack_sequence(sequences, enforce_sorted=True)¶
</code></pre>
<p>打包可变长度张量的列表</p>
<p><code>sequences</code>应该是大小为<code>L x *</code>的张量的列表，其中 &lt;cite&gt;L&lt;/cite&gt; 是序列的长度， &lt;cite&gt;*&lt;/cite&gt; 是任意数量的尾随尺寸，包括零。</p>
<p>对于未排序的序列，请使用 &lt;cite&gt;force_sorted = False&lt;/cite&gt; 。 如果<code>enforce_sorted</code>为<code>True</code>，则序列应按长度减小的顺序排序。 <code>enforce_sorted = True</code>仅对于 ONNX 导出是必需的。</p>
<p>Example</p>
<pre><code>&gt;&gt;&gt; from torch.nn.utils.rnn import pack_sequence
&gt;&gt;&gt; a = torch.tensor([1,2,3])
&gt;&gt;&gt; b = torch.tensor([4,5])
&gt;&gt;&gt; c = torch.tensor([6])
&gt;&gt;&gt; pack_sequence([a, b, c])
PackedSequence(data=tensor([ 1,  4,  6,  2,  5,  3]), batch_sizes=tensor([ 3,  2,  1]))

</code></pre>
<p>Parameters</p>
<ul>
<li>
<p><strong>序列</strong>(<em>列表</em> <em>[</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>]</em> )–递减序列的列表 长度。</p>
</li>
<li>
<p><strong>强制排序的</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–如果<code>True</code>，则检查输入是否包含按长度排序的降序序列。 如果为<code>False</code>，则不检查此条件。 默认值：<code>True</code>。</p>
</li>
</ul>
<p>Returns</p>
<p>a <a href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code>PackedSequence</code></a> object</p>
<h3 id="_78">展平</h3>
<hr />
<pre><code>class torch.nn.Flatten(start_dim=1, end_dim=-1)¶
</code></pre>
<p>将连续范围的暗角展平为张量。 用于<code>Sequential</code>。 ：param start_dim：首先变暗到变平(默认= 1）。 ：param end_dim：最后变暗到变平(默认= -1）。</p>
<pre><code>Shape:
</code></pre>
<ul>
<li>
<p>输入：<img alt="" src="../img/a6609b240fa9d46868c91f222f0a204b.jpg" /></p>
</li>
<li>
<p>输出：<img alt="" src="../img/f514c9d5c6d07970753ccd23f7cd161e.jpg" />(默认情况下）。</p>
</li>
</ul>
<pre><code>Examples::
</code></pre>
<pre><code>&gt;&gt;&gt; m = nn.Sequential(
&gt;&gt;&gt;     nn.Conv2d(1, 32, 5, 1, 1),
&gt;&gt;&gt;     nn.Flatten()
&gt;&gt;&gt; )

</code></pre>
<h2 id="_79">量化功能</h2>
<p>量化是指用于执行计算并以低于浮点精度的位宽存储张量的技术。 PyTorch 支持每个张量和每个通道非对称线性量化。 要了解更多如何在 PyTorch 中使用量化函数的信息，请参阅<a href="quantization.html#quantization-doc">量化</a>文档。</p>
<hr/>
<div align="center">
  <p><a href="https://www.apachecn.org/" target="_blank"><font face="KaiTi" size="6" color="red">我们一直在努力</font></a><p>
  <p><a href="https://github.com/apachecn/pytorch-doc-zh" target="_blank">apachecn/pytorch-doc-zh</a></p>
  <p><a target="_blank" href="https://qm.qq.com/cgi-bin/qm/qr?k=5u_aAU-YlY3fH-m8meXTJzBEo2boQIUs&jump_from=webapi&authKey=CVZcReMt/vKdTXZBQ8ly+jWncXiSzzWOlrx5hybX5pSrKu6s0fvGX54+vHHlgYNt"><img border="0" src="https://pub.idqqimg.com/wpa/images/group.png" alt="【布客】中文翻译组" title="【布客】中文翻译组"></a></p>
  <p><span id="cnzz_stat_icon_1275211409"></span></p>
  <!-- <p><a href="https://get.brightdata.com/apachecn" target="_blank"><img src="/assets/images/partnerstack.gif" /></a><p> -->
  <div class="wwads-cn wwads-horizontal" data-id="206" style="max-width:680px"></div>
  <div style="text-align:center;margin:0 0 10.5px;">
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-3565452474788507" crossorigin="anonymous"></script>
    <!-- ApacheCNWide -->
    <ins class="adsbygoogle"
        style="display:inline-block;width:680px;height:90px"
        data-ad-client="ca-pub-3565452474788507"
        data-ad-slot="2543897000"></ins>
    <script>
        (adsbygoogle = window.adsbygoogle || []).push({});
    </script>
  </div>
</div>
<hr/>
<!-- 来必力City版安装代码 -->
<div id="lv-container" data-id="city" data-uid="MTAyMC81ODA2NC8zNDUyNw==">
  <script type="text/javascript">
  (function(d, s) {
      var j, e = d.getElementsByTagName(s)[0];

      if (typeof LivereTower === 'function') { return; }

      j = d.createElement(s);
      j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
      j.async = true;

      e.parentNode.insertBefore(j, e);
  })(document, 'script');
  </script>
<noscript> 为正常使用来必力评论功能请激活JavaScript</noscript>
</div>
<!-- City版安装代码已完成 -->






                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../74/" class="md-footer__link md-footer__link--prev" aria-label="Previous: torch" rel="prev">
            <div class="md-footer__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                torch
              </div>
            </div>
          </a>
        
        
          
          <a href="../76/" class="md-footer__link md-footer__link--next" aria-label="Next: torch功能" rel="next">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                torch功能
              </div>
            </div>
            <div class="md-footer__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright" style="text-align: center; width: 100%;">
  
  
    <div>
      <div style="margin:0 0 10.5px;"><script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1275211409'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s5.cnzz.com/z_stat.php%3Fid%3D1275211409%26online%3D1%26show%3Dline' type='text/javascript'%3E%3C/script%3E"));</script></div>
      <p>Copyright © 2023 学习网站 <a href="http://beian.miit.gov.cn" target="_blank">京ICP备19016010号-1</a><br/>网站由 <a href="https://apachecn.org/cooperate/">@片刻小哥哥</a> 提供支持 | 联系QQ/微信: 529815144 请注明来意！</p>
    </div>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["content.code.copy", "content.action.edit", "content.action.view", "navigation.footer"], "search": "../../assets/javascripts/workers/search.208ed371.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
  
      <script src="../../assets/javascripts/bundle.b425cdc4.min.js"></script>
      
        
          <script src="../../javascripts/mathjax.js"></script>
        
      
        
          <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        
      
        
          <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        
      
    
  <script src="../../assets/javascripts/custom.a7283b5f.min.js"></script>

  </body>
</html>