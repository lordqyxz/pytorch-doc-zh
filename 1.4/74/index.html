
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://pytorch.apachecn.org/1.4/74/">
      
      
        <link rel="prev" href="../72/">
      
      
        <link rel="next" href="../75/">
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.4.3, mkdocs-material-9.1.17">
    
    
      
        <title>torch - 【布客】PyTorch 中文翻译</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.26e3688c.min.css">
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  <link rel="stylesheet" href="../../assets/stylesheets/custom.bea7efe8.min.css">
  <!-- google ads -->
  <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-3565452474788507" crossorigin="anonymous"></script>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-8DP4GX97XY"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-8DP4GX97XY');
  </script>
  <!-- google webmaster -->
  <meta name="google-site-verification" content="pyo9N70ZWyh8JB43bIu633mhxesJ1IcwWCZlM3jUfFo" />

  <!-- wwads-cn union -->
  <meta name="wwads-cn-verify" content="03c6b06952c750899bb03d998e631860" />
  <script type="text/javascript" charset="UTF-8" src="https://cdn.wwads.cn/js/makemoney.js" async></script>

  </head>
  
  
    <body dir="ltr">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#torch" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
        <aside class="md-banner">
          <div class="md-banner__inner md-grid md-typeset">
            
            
  => 组织无偿提供 中文版本（免费，秒级响应）
  <a target="_blank" href="https://chat.ibooker.org.cn/chat" style="color: red;">
    <span class="twemoji mastodon">
      <img src="https://data.apachecn.org/img/icon/ROBOT_TXT.svg" alt="ChatGPT - ailake.top">
    </span>
    <strong>ChatGPT - ailake.top</strong>
  </a> 一起来白嫖叭～！

          </div>
          
        </aside>
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="【布客】PyTorch 中文翻译" class="md-header__button md-logo" aria-label="【布客】PyTorch 中文翻译" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            【布客】PyTorch 中文翻译
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              torch
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/apachecn/pytorch-doc-zh" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    apachecn/pytorch-doc-zh
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="【布客】PyTorch 中文翻译" class="md-nav__button md-logo" aria-label="【布客】PyTorch 中文翻译" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    【布客】PyTorch 中文翻译
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/apachecn/pytorch-doc-zh" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    apachecn/pytorch-doc-zh
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        PyTorch 中文文档 & 教程
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
      
      
      
        <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
          PyTorch 2.0 中文文档 & 教程
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 2.0 中文文档 & 教程
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1" >
      
      
      
        <label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="0">
          中文教程
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_1">
          <span class="md-nav__icon md-icon"></span>
          中文教程
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/README.md" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_2" >
      
      
      
        <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="0">
          中文文档
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_2">
          <span class="md-nav__icon md-icon"></span>
          中文文档
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/docs/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
      
      
      
        <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
          PyTorch 1.7 中文文档
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 1.7 中文文档
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" >
      
      
      
        <label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="0">
          学习 PyTorch
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_1">
          <span class="md-nav__icon md-icon"></span>
          学习 PyTorch
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_1" >
      
      
      
        <label class="md-nav__link" for="__nav_3_1_1" id="__nav_3_1_1_label" tabindex="0">
          PyTorch 深度学习：60 分钟的突击
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_1_1">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 深度学习：60 分钟的突击
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/02/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/03/" class="md-nav__link">
        张量
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/04/" class="md-nav__link">
        torch.autograd的简要介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/05/" class="md-nav__link">
        神经网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/06/" class="md-nav__link">
        训练分类器
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_2" >
      
      
      
        <label class="md-nav__link" for="__nav_3_1_2" id="__nav_3_1_2_label" tabindex="0">
          通过示例学习 PyTorch
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_1_2">
          <span class="md-nav__icon md-icon"></span>
          通过示例学习 PyTorch
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/07/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/08/" class="md-nav__link">
        热身：NumPy
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/09/" class="md-nav__link">
        PyTorch：张量
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/10/" class="md-nav__link">
        PyTorch：张量和 Autograd
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/11/" class="md-nav__link">
        PyTorch：定义新的 Autograd 函数
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/12/" class="md-nav__link">
        PyTorch：nn
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/13/" class="md-nav__link">
        PyTorch：optim
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/14/" class="md-nav__link">
        PyTorch：自定义nn模块
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/15/" class="md-nav__link">
        PyTorch：控制流 - 权重共享
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/16/" class="md-nav__link">
        torch.nn到底是什么？
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/17/" class="md-nav__link">
        使用 TensorBoard 可视化模型，数据和训练
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
      
      
      
        <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
          图片/视频
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_2">
          <span class="md-nav__icon md-icon"></span>
          图片/视频
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/19/" class="md-nav__link">
        torchvision对象检测微调教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/20/" class="md-nav__link">
        计算机视觉的迁移学习教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/21/" class="md-nav__link">
        对抗示例生成
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/22/" class="md-nav__link">
        DCGAN 教程
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_3" >
      
      
      
        <label class="md-nav__link" for="__nav_3_3" id="__nav_3_3_label" tabindex="0">
          音频
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_3">
          <span class="md-nav__icon md-icon"></span>
          音频
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/24/" class="md-nav__link">
        音频 I/O 和torchaudio的预处理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/25/" class="md-nav__link">
        使用torchaudio的语音命令识别
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4" id="__nav_3_4_label" tabindex="0">
          文本
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4">
          <span class="md-nav__icon md-icon"></span>
          文本
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/27/" class="md-nav__link">
        使用nn.Transformer和torchtext的序列到序列建模
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/28/" class="md-nav__link">
        从零开始的 NLP：使用字符级 RNN 分类名称
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/29/" class="md-nav__link">
        从零开始的 NLP：使用字符级 RNN 生成名称
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/30/" class="md-nav__link">
        从零开始的 NLP：使用序列到序列网络和注意力的翻译
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/31/" class="md-nav__link">
        使用torchtext的文本分类
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/32/" class="md-nav__link">
        torchtext语言翻译
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_5" >
      
      
      
        <label class="md-nav__link" for="__nav_3_5" id="__nav_3_5_label" tabindex="0">
          强化学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_5">
          <span class="md-nav__icon md-icon"></span>
          强化学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/34/" class="md-nav__link">
        强化学习（DQN）教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/35/" class="md-nav__link">
        训练玩马里奥的 RL 智能体
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_6" >
      
      
      
        <label class="md-nav__link" for="__nav_3_6" id="__nav_3_6_label" tabindex="0">
          在生产中部署 PyTorch 模型
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_6">
          <span class="md-nav__icon md-icon"></span>
          在生产中部署 PyTorch 模型
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/37/" class="md-nav__link">
        通过使用 Flask 的 REST API 在 Python 中部署 PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/38/" class="md-nav__link">
        TorchScript 简介
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/39/" class="md-nav__link">
        在 C-- 中加载 TorchScript 模型
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/40/" class="md-nav__link">
        将模型从 PyTorch 导出到 ONNX 并使用 ONNX 运行时运行它（可选）
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_7" >
      
      
      
        <label class="md-nav__link" for="__nav_3_7" id="__nav_3_7_label" tabindex="0">
          前端 API
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_7_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_7">
          <span class="md-nav__icon md-icon"></span>
          前端 API
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/42/" class="md-nav__link">
        PyTorch 中的命名张量简介（原型）
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/43/" class="md-nav__link">
        PyTorch 中通道在最后的内存格式（beta）
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/44/" class="md-nav__link">
        使用 PyTorch C-- 前端
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/45/" class="md-nav__link">
        自定义 C-- 和 CUDA 扩展
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/46/" class="md-nav__link">
        使用自定义 C-- 运算符扩展 TorchScript
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/47/" class="md-nav__link">
        使用自定义 C-- 类扩展 TorchScript
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/48/" class="md-nav__link">
        TorchScript 中的动态并行性
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/49/" class="md-nav__link">
        C-- 前端中的 Autograd
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/50/" class="md-nav__link">
        在 C-- 中注册调度运算符
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_8" >
      
      
      
        <label class="md-nav__link" for="__nav_3_8" id="__nav_3_8_label" tabindex="0">
          模型优化
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_8_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_8">
          <span class="md-nav__icon md-icon"></span>
          模型优化
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/52/" class="md-nav__link">
        分析您的 PyTorch 模块
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/53/" class="md-nav__link">
        使用 Ray Tune 的超参数调整
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/54/" class="md-nav__link">
        模型剪裁教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/55/" class="md-nav__link">
        LSTM 单词语言模型上的动态量化（beta）
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/56/" class="md-nav__link">
        BERT 上的动态量化（Beta）
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/57/" class="md-nav__link">
        PyTorch 中使用 Eager 模式的静态量化（beta）
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/58/" class="md-nav__link">
        计算机视觉的量化迁移学习教程（beta）
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_9" >
      
      
      
        <label class="md-nav__link" for="__nav_3_9" id="__nav_3_9_label" tabindex="0">
          并行和分布式训练
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_9_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_9">
          <span class="md-nav__icon md-icon"></span>
          并行和分布式训练
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/60/" class="md-nav__link">
        PyTorch 分布式概述
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/61/" class="md-nav__link">
        单机模型并行最佳实践
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/62/" class="md-nav__link">
        分布式数据并行入门
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/63/" class="md-nav__link">
        用 PyTorch 编写分布式应用
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/64/" class="md-nav__link">
        分布式 RPC 框架入门
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/65/" class="md-nav__link">
        使用分布式 RPC 框架实现参数服务器
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/66/" class="md-nav__link">
        使用 RPC 的分布式管道并行化
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/67/" class="md-nav__link">
        使用异步执行实现批量 RPC 处理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/68/" class="md-nav__link">
        将分布式DataParallel与分布式 RPC 框架相结合
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
      
      
      
        <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
          PyTorch 1.4 中文文档 & 教程
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 1.4 中文文档 & 教程
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1" >
      
      
      
        <label class="md-nav__link" for="__nav_4_1" id="__nav_4_1_label" tabindex="0">
          入门
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_1">
          <span class="md-nav__icon md-icon"></span>
          入门
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1_1" >
      
      
      
        <label class="md-nav__link" for="__nav_4_1_1" id="__nav_4_1_1_label" tabindex="0">
          使用 PyTorch 进行深度学习：60 分钟的闪电战
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_4_1_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_1_1">
          <span class="md-nav__icon md-icon"></span>
          使用 PyTorch 进行深度学习：60 分钟的闪电战
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../4/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../blitz/tensor_tutorial/" class="md-nav__link">
        什么是PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../blitz/autograd_tutorial/" class="md-nav__link">
        Autograd：自动求导
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../blitz/neural_networks_tutorial/" class="md-nav__link">
        神经网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../blitz/cifar10_tutorial/" class="md-nav__link">
        训练分类器
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../blitz/data_parallel_tutorial/" class="md-nav__link">
        可选：数据并行
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../5/" class="md-nav__link">
        编写自定义数据集，数据加载器和转换
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../6/" class="md-nav__link">
        使用 TensorBoard 可视化模型，数据和训练
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_2" >
      
      
      
        <label class="md-nav__link" for="__nav_4_2" id="__nav_4_2_label" tabindex="0">
          图片
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_2">
          <span class="md-nav__icon md-icon"></span>
          图片
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../8/" class="md-nav__link">
        TorchVision 对象检测微调教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../9/" class="md-nav__link">
        转移学习的计算机视觉教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../10/" class="md-nav__link">
        空间变压器网络教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../11/" class="md-nav__link">
        使用 PyTorch 进行神经传递
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../12/" class="md-nav__link">
        对抗示例生成
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../13/" class="md-nav__link">
        DCGAN 教程
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_3" >
      
      
      
        <label class="md-nav__link" for="__nav_4_3" id="__nav_4_3_label" tabindex="0">
          音频
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_3">
          <span class="md-nav__icon md-icon"></span>
          音频
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../15/" class="md-nav__link">
        torchaudio 教程
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_4" >
      
      
      
        <label class="md-nav__link" for="__nav_4_4" id="__nav_4_4_label" tabindex="0">
          文本
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_4">
          <span class="md-nav__icon md-icon"></span>
          文本
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../17/" class="md-nav__link">
        NLP From Scratch: 使用char-RNN对姓氏进行分类
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../18/" class="md-nav__link">
        NLP From Scratch: 生成名称与字符级RNN
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../19/" class="md-nav__link">
        NLP From Scratch: 基于注意力机制的 seq2seq 神经网络翻译
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../20/" class="md-nav__link">
        使用 TorchText 进行文本分类
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../21/" class="md-nav__link">
        使用 TorchText 进行语言翻译
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../22/" class="md-nav__link">
        使用 nn.Transformer 和 TorchText 进行序列到序列建模
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_5" >
      
      
      
        <label class="md-nav__link" for="__nav_4_5" id="__nav_4_5_label" tabindex="0">
          命名为 Tensor(实验性）
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_5">
          <span class="md-nav__icon md-icon"></span>
          命名为 Tensor(实验性）
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../24/" class="md-nav__link">
        (实验性)PyTorch 中的命名张量简介
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_6" >
      
      
      
        <label class="md-nav__link" for="__nav_4_6" id="__nav_4_6_label" tabindex="0">
          强化学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_6">
          <span class="md-nav__icon md-icon"></span>
          强化学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../26/" class="md-nav__link">
        强化学习(DQN)教程
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_7" >
      
      
      
        <label class="md-nav__link" for="__nav_4_7" id="__nav_4_7_label" tabindex="0">
          在生产中部署 PyTorch 模型
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_7_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_7">
          <span class="md-nav__icon md-icon"></span>
          在生产中部署 PyTorch 模型
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../28/" class="md-nav__link">
        通过带有 Flask 的 REST API 在 Python 中部署 PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../29/" class="md-nav__link">
        TorchScript 简介
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../30/" class="md-nav__link">
        在 C --中加载 TorchScript 模型
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../31/" class="md-nav__link">
        (可选）将模型从 PyTorch 导出到 ONNX 并使用 ONNX Runtime 运行
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_8" >
      
      
      
        <label class="md-nav__link" for="__nav_4_8" id="__nav_4_8_label" tabindex="0">
          并行和分布式训练
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_8_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_8">
          <span class="md-nav__icon md-icon"></span>
          并行和分布式训练
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../33/" class="md-nav__link">
        单机模型并行最佳实践
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../34/" class="md-nav__link">
        分布式数据并行入门
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../35/" class="md-nav__link">
        用 PyTorch 编写分布式应用程序
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../36/" class="md-nav__link">
        分布式 RPC 框架入门
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../37/" class="md-nav__link">
        (高级）带有 Amazon AWS 的 PyTorch 1.0 分布式训练师
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_9" >
      
      
      
        <label class="md-nav__link" for="__nav_4_9" id="__nav_4_9_label" tabindex="0">
          扩展 PyTorch
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_9_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_9">
          <span class="md-nav__icon md-icon"></span>
          扩展 PyTorch
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../39/" class="md-nav__link">
        使用自定义 C --运算符扩展 TorchScript
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../40/" class="md-nav__link">
        使用自定义 C --类扩展 TorchScript
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../41/" class="md-nav__link">
        使用 numpy 和 scipy 创建扩展
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../42/" class="md-nav__link">
        自定义 C --和 CUDA 扩展
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_10" >
      
      
      
        <label class="md-nav__link" for="__nav_4_10" id="__nav_4_10_label" tabindex="0">
          模型优化
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_10_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_10">
          <span class="md-nav__icon md-icon"></span>
          模型优化
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../44/" class="md-nav__link">
        LSTM Word 语言模型上的(实验）动态量化
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../45/" class="md-nav__link">
        (实验性）在 PyTorch 中使用 Eager 模式进行静态量化
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../46/" class="md-nav__link">
        (实验性）计算机视觉教程的量化转移学习
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../47/" class="md-nav__link">
        (实验）BERT 上的动态量化
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../48/" class="md-nav__link">
        修剪教程
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_11" >
      
      
      
        <label class="md-nav__link" for="__nav_4_11" id="__nav_4_11_label" tabindex="0">
          PyTorch 用其他语言
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_11_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_11">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 用其他语言
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../50/" class="md-nav__link">
        使用 PyTorch C --前端
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_12" >
      
      
      
        <label class="md-nav__link" for="__nav_4_12" id="__nav_4_12_label" tabindex="0">
          PyTorch 基础知识
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_12_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_12">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 基础知识
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../52/" class="md-nav__link">
        通过示例学习 PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../53/" class="md-nav__link">
        torch.nn 到底是什么？
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_13" >
      
      
      
        <label class="md-nav__link" for="__nav_4_13" id="__nav_4_13_label" tabindex="0">
          笔记
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_13_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_13">
          <span class="md-nav__icon md-icon"></span>
          笔记
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../56/" class="md-nav__link">
        自动求导机制
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../57/" class="md-nav__link">
        广播语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../58/" class="md-nav__link">
        CPU 线程和 TorchScript 推断
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../59/" class="md-nav__link">
        CUDA 语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../60/" class="md-nav__link">
        分布式 Autograd 设计
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../61/" class="md-nav__link">
        扩展 PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../62/" class="md-nav__link">
        经常问的问题
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../63/" class="md-nav__link">
        大规模部署的功能
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../64/" class="md-nav__link">
        并行处理最佳实践
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../65/" class="md-nav__link">
        重现性
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../66/" class="md-nav__link">
        远程参考协议
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../67/" class="md-nav__link">
        序列化语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../68/" class="md-nav__link">
        Windows 常见问题
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../69/" class="md-nav__link">
        XLA 设备上的 PyTorch
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_14" >
      
      
      
        <label class="md-nav__link" for="__nav_4_14" id="__nav_4_14_label" tabindex="0">
          语言绑定
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_14_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_14">
          <span class="md-nav__icon md-icon"></span>
          语言绑定
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../71/" class="md-nav__link">
        PyTorch C -- API
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../72/" class="md-nav__link">
        PyTorch Java API
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_15" checked>
      
      
      
        <label class="md-nav__link" for="__nav_4_15" id="__nav_4_15_label" tabindex="0">
          Python API
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_15_label" aria-expanded="true">
        <label class="md-nav__title" for="__nav_4_15">
          <span class="md-nav__icon md-icon"></span>
          Python API
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          torch
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        torch
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    张量
  </a>
  
    <nav class="md-nav" aria-label="张量">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    创作行动
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    索引，切片，联接，操作变更
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    发电机
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    随机抽样
  </a>
  
    <nav class="md-nav" aria-label="随机抽样">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    就地随机抽样
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    准随机抽样
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    序列化
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_9" class="md-nav__link">
    平行性
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_10" class="md-nav__link">
    局部禁用梯度计算
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_11" class="md-nav__link">
    数学运算
  </a>
  
    <nav class="md-nav" aria-label="数学运算">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_12" class="md-nav__link">
    逐点操作
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_13" class="md-nav__link">
    减少操作
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_14" class="md-nav__link">
    比较行动
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_15" class="md-nav__link">
    光谱操作
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_16" class="md-nav__link">
    其他作业
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#blas-lapack" class="md-nav__link">
    BLAS 和 LAPACK 操作
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_17" class="md-nav__link">
    实用工具
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../75/" class="md-nav__link">
        torch.nn
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../76/" class="md-nav__link">
        torch功能
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../77/" class="md-nav__link">
        torch张量
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../78/" class="md-nav__link">
        张量属性
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../79/" class="md-nav__link">
        自动差分包-Torch.Autograd
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../80/" class="md-nav__link">
        torch.cuda
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../81/" class="md-nav__link">
        分布式通讯包-Torch.Distributed
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../82/" class="md-nav__link">
        概率分布-torch分布
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../83/" class="md-nav__link">
        torch.hub
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../84/" class="md-nav__link">
        torch脚本
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../85/" class="md-nav__link">
        torch.nn.init
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../86/" class="md-nav__link">
        torch.onnx
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../87/" class="md-nav__link">
        torch.optim
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../88/" class="md-nav__link">
        量化
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../89/" class="md-nav__link">
        分布式 RPC 框架
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../90/" class="md-nav__link">
        torch随机
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../91/" class="md-nav__link">
        torch稀疏
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../92/" class="md-nav__link">
        torch存储
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../93/" class="md-nav__link">
        torch.utils.bottleneck
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../94/" class="md-nav__link">
        torch.utils.checkpoint
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../95/" class="md-nav__link">
        torch.utils.cpp_extension
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../96/" class="md-nav__link">
        torch.utils.data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../97/" class="md-nav__link">
        torch.utils.dlpack
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../98/" class="md-nav__link">
        torch.utils.model_zoo
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../99/" class="md-nav__link">
        torch.utils.tensorboard
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../100/" class="md-nav__link">
        类型信息
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../101/" class="md-nav__link">
        命名张量
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../102/" class="md-nav__link">
        命名为 Tensors 操作员范围
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../103/" class="md-nav__link">
        糟糕！
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_16" >
      
      
      
        <label class="md-nav__link" for="__nav_4_16" id="__nav_4_16_label" tabindex="0">
          torchvision参考
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_16_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_16">
          <span class="md-nav__icon md-icon"></span>
          torchvision参考
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../105/" class="md-nav__link">
        torchvision
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_17" >
      
      
      
        <label class="md-nav__link" for="__nav_4_17" id="__nav_4_17_label" tabindex="0">
          音频参考
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_17_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_17">
          <span class="md-nav__icon md-icon"></span>
          音频参考
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../107/" class="md-nav__link">
        torchaudio
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_18" >
      
      
      
        <label class="md-nav__link" for="__nav_4_18" id="__nav_4_18_label" tabindex="0">
          torchtext参考
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_18_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_18">
          <span class="md-nav__icon md-icon"></span>
          torchtext参考
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../109/" class="md-nav__link">
        torchtext
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_19" >
      
      
      
        <label class="md-nav__link" for="__nav_4_19" id="__nav_4_19_label" tabindex="0">
          社区
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_19_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_19">
          <span class="md-nav__icon md-icon"></span>
          社区
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../111/" class="md-nav__link">
        PyTorch 贡献指南
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../112/" class="md-nav__link">
        PyTorch 治理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../113/" class="md-nav__link">
        PyTorch 治理| 感兴趣的人
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
      
      
      
        <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
          PyTorch 1.0 中文文档 & 教程
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 1.0 中文文档 & 教程
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/" class="md-nav__link">
        目录
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2" id="__nav_5_2_label" tabindex="0">
          中文教程
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2">
          <span class="md-nav__icon md-icon"></span>
          中文教程
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2_1" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2_1" id="__nav_5_2_1_label" tabindex="0">
          入门
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_2_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2_1">
          <span class="md-nav__icon md-icon"></span>
          入门
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2_1_1" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2_1_1" id="__nav_5_2_1_1_label" tabindex="0">
          PyTorch 深度学习: 60 分钟极速入门
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_5_2_1_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2_1_1">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 深度学习: 60 分钟极速入门
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/deep_learning_60min_blitz/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/blitz_tensor_tutorial/" class="md-nav__link">
        什么是 PyTorch？
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/blitz_autograd_tutorial/" class="md-nav__link">
        Autograd：自动求导
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/blitz_neural_networks_tutorial/" class="md-nav__link">
        神经网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/blitz_cifar10_tutorial/" class="md-nav__link">
        训练分类器
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/blitz_data_parallel_tutorial/" class="md-nav__link">
        可选：数据并行处理
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/data_loading_tutorial/" class="md-nav__link">
        数据加载和处理教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/pytorch_with_examples/" class="md-nav__link">
        用例子学习 PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/transfer_learning_tutorial/" class="md-nav__link">
        迁移学习教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/deploy_seq2seq_hybrid_frontend_tutorial/" class="md-nav__link">
        混合前端的 seq2seq 模型部署
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/saving_loading_models/" class="md-nav__link">
        Saving and Loading Models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/nn_tutorial/" class="md-nav__link">
        What is torch.nn really?
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2_2" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2_2" id="__nav_5_2_2_label" tabindex="0">
          图像
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_2_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2_2">
          <span class="md-nav__icon md-icon"></span>
          图像
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/finetuning_torchvision_models_tutorial/" class="md-nav__link">
        Torchvision 模型微调
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/spatial_transformer_tutorial/" class="md-nav__link">
        空间变换器网络教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/neural_style_tutorial/" class="md-nav__link">
        使用 PyTorch 进行图像风格转换
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/fgsm_tutorial/" class="md-nav__link">
        对抗性示例生成
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/super_resolution_with_caffe2/" class="md-nav__link">
        使用 ONNX 将模型从 PyTorch 传输到 Caffe2 和移动端
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2_3" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2_3" id="__nav_5_2_3_label" tabindex="0">
          文本
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_2_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2_3">
          <span class="md-nav__icon md-icon"></span>
          文本
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/chatbot_tutorial/" class="md-nav__link">
        聊天机器人教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/char_rnn_generation_tutorial/" class="md-nav__link">
        使用字符级别特征的 RNN 网络生成姓氏
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/char_rnn_classification_tutorial/" class="md-nav__link">
        使用字符级别特征的 RNN 网络进行姓氏分类
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2_3_4" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2_3_4" id="__nav_5_2_3_4_label" tabindex="0">
          Deep Learning for NLP with Pytorch
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_5_2_3_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2_3_4">
          <span class="md-nav__icon md-icon"></span>
          Deep Learning for NLP with Pytorch
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/deep_learning_nlp_tutorial/" class="md-nav__link">
        在深度学习和 NLP 中使用 Pytorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/nlp_pytorch_tutorial/" class="md-nav__link">
        PyTorch 介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/nlp_deep_learning_tutorial/" class="md-nav__link">
        使用 PyTorch 进行深度学习
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/nlp_word_embeddings_tutorial/" class="md-nav__link">
        Word Embeddings: Encoding Lexical Semantics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/nlp_sequence_models_tutorial/" class="md-nav__link">
        序列模型和 LSTM 网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/nlp_advanced_tutorial/" class="md-nav__link">
        Advanced: Making Dynamic Decisions and the Bi-LSTM CRF
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/seq2seq_translation_tutorial/" class="md-nav__link">
        基于注意力机制的 seq2seq 神经网络翻译
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2_4" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2_4" id="__nav_5_2_4_label" tabindex="0">
          生成
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_2_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2_4">
          <span class="md-nav__icon md-icon"></span>
          生成
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/dcgan_faces_tutorial/" class="md-nav__link">
        DCGAN Tutorial
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2_5" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2_5" id="__nav_5_2_5_label" tabindex="0">
          强化学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_2_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2_5">
          <span class="md-nav__icon md-icon"></span>
          强化学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/reinforcement_q_learning/" class="md-nav__link">
        Reinforcement Learning (DQN) Tutorial
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2_6" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2_6" id="__nav_5_2_6_label" tabindex="0">
          扩展 PyTorch
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_2_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2_6">
          <span class="md-nav__icon md-icon"></span>
          扩展 PyTorch
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/numpy_extensions_tutorial/" class="md-nav__link">
        用 numpy 和 scipy 创建扩展
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/cpp_extension/" class="md-nav__link">
        Custom C-- and CUDA Extensions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torch_script_custom_ops/" class="md-nav__link">
        Extending TorchScript with Custom C-- Operators
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2_7" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2_7" id="__nav_5_2_7_label" tabindex="0">
          生产性使用
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_2_7_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2_7">
          <span class="md-nav__icon md-icon"></span>
          生产性使用
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/dist_tuto/" class="md-nav__link">
        Writing Distributed Applications with PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/aws_distributed_training_tutorial/" class="md-nav__link">
        使用 Amazon AWS 进行分布式训练
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/ONNXLive/" class="md-nav__link">
        ONNX 现场演示教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/cpp_export/" class="md-nav__link">
        在 C-- 中加载 PYTORCH 模型
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2_8" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2_8" id="__nav_5_2_8_label" tabindex="0">
          其它语言中的 PyTorch
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_2_8_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2_8">
          <span class="md-nav__icon md-icon"></span>
          其它语言中的 PyTorch
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/cpp_frontend/" class="md-nav__link">
        使用 PyTorch C-- 前端
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3" >
      
      
      
        <label class="md-nav__link" for="__nav_5_3" id="__nav_5_3_label" tabindex="0">
          中文文档
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_3">
          <span class="md-nav__icon md-icon"></span>
          中文文档
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3_1" >
      
      
      
        <label class="md-nav__link" for="__nav_5_3_1" id="__nav_5_3_1_label" tabindex="0">
          注解
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_3_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_3_1">
          <span class="md-nav__icon md-icon"></span>
          注解
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/notes_autograd/" class="md-nav__link">
        自动求导机制
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/notes_broadcasting/" class="md-nav__link">
        广播语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/notes_cuda/" class="md-nav__link">
        CUDA 语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/notes_extending/" class="md-nav__link">
        Extending PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/notes_faq/" class="md-nav__link">
        Frequently Asked Questions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/notes_multiprocessing/" class="md-nav__link">
        Multiprocessing best practices
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/notes_randomness/" class="md-nav__link">
        Reproducibility
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/notes_serialization/" class="md-nav__link">
        Serialization semantics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/notes_windows/" class="md-nav__link">
        Windows FAQ
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3_2" >
      
      
      
        <label class="md-nav__link" for="__nav_5_3_2" id="__nav_5_3_2_label" tabindex="0">
          包参考
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_3_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_3_2">
          <span class="md-nav__icon md-icon"></span>
          包参考
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3_2_1" >
      
      
      
        <label class="md-nav__link" for="__nav_5_3_2_1" id="__nav_5_3_2_1_label" tabindex="0">
          torch
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_5_3_2_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_3_2_1">
          <span class="md-nav__icon md-icon"></span>
          torch
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torch/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torch_tensors/" class="md-nav__link">
        Tensors
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torch_random_sampling/" class="md-nav__link">
        Random sampling
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torch_serialization_parallelism_utilities/" class="md-nav__link">
        Serialization, Parallelism, Utilities
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3_2_1_5" >
      
      
      
        <label class="md-nav__link" for="__nav_5_3_2_1_5" id="__nav_5_3_2_1_5_label" tabindex="0">
          Math operations
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="5" aria-labelledby="__nav_5_3_2_1_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_3_2_1_5">
          <span class="md-nav__icon md-icon"></span>
          Math operations
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torch_math_operations_pointwise_ops/" class="md-nav__link">
        Pointwise Ops
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torch_math_operations_reduction_ops/" class="md-nav__link">
        Reduction Ops
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torch_math_operations_comparison_ops/" class="md-nav__link">
        Comparison Ops
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torch_math_operations_spectral_ops/" class="md-nav__link">
        Spectral Ops
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torch_math_operations_other_ops/" class="md-nav__link">
        Other Operations
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torch_math_operations_blas_lapack_ops/" class="md-nav__link">
        BLAS and LAPACK Operations
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/tensors/" class="md-nav__link">
        torch.Tensor
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/tensor_attributes/" class="md-nav__link">
        Tensor Attributes
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/type_info/" class="md-nav__link">
        数据类型信息
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/sparse/" class="md-nav__link">
        torch.sparse
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/cuda/" class="md-nav__link">
        torch.cuda
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/storage/" class="md-nav__link">
        torch.Storage
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/nn/" class="md-nav__link">
        torch.nn
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/nn_functional/" class="md-nav__link">
        torch.nn.functional
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/nn_init/" class="md-nav__link">
        torch.nn.init
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/optim/" class="md-nav__link">
        torch.optim
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/autograd/" class="md-nav__link">
        Automatic differentiation package - torch.autograd
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/distributed/" class="md-nav__link">
        Distributed communication package - torch.distributed
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/distributions/" class="md-nav__link">
        Probability distributions - torch.distributions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/jit/" class="md-nav__link">
        Torch Script
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/multiprocessing/" class="md-nav__link">
        多进程包 - torch.multiprocessing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/bottleneck/" class="md-nav__link">
        torch.utils.bottleneck
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/checkpoint/" class="md-nav__link">
        torch.utils.checkpoint
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/docs_cpp_extension/" class="md-nav__link">
        torch.utils.cpp_extension
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/data/" class="md-nav__link">
        torch.utils.data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/dlpack/" class="md-nav__link">
        torch.utils.dlpack
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/hub/" class="md-nav__link">
        torch.hub
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/model_zoo/" class="md-nav__link">
        torch.utils.model_zoo
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/onnx/" class="md-nav__link">
        torch.onnx
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/distributed_deprecated/" class="md-nav__link">
        Distributed communication package (deprecated) - torch.distributed.deprecated
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3_3" >
      
      
      
        <label class="md-nav__link" for="__nav_5_3_3" id="__nav_5_3_3_label" tabindex="0">
          torchvision 参考
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_3_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_3_3">
          <span class="md-nav__icon md-icon"></span>
          torchvision 参考
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/docs_torchvision_ref/" class="md-nav__link">
        目录
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torchvision_datasets/" class="md-nav__link">
        torchvision.datasets
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torchvision_models/" class="md-nav__link">
        torchvision.models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torchvision_transforms/" class="md-nav__link">
        torchvision.transforms
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torchvision_utils/" class="md-nav__link">
        torchvision.utils
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
      
      
      
        <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
          PyTorch 0.4 中文文档
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 0.4 中文文档
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_1" >
      
      
      
        <label class="md-nav__link" for="__nav_6_1" id="__nav_6_1_label" tabindex="0">
          笔记
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_6_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_6_1">
          <span class="md-nav__icon md-icon"></span>
          笔记
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/1/" class="md-nav__link">
        自动求导机制
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/2/" class="md-nav__link">
        广播语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/3/" class="md-nav__link">
        CUDA 语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/4/" class="md-nav__link">
        扩展 PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/5/" class="md-nav__link">
        常见问题
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/6/" class="md-nav__link">
        多进程最佳实践
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/7/" class="md-nav__link">
        序列化语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/8/" class="md-nav__link">
        Windows 常见问题
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_2" >
      
      
      
        <label class="md-nav__link" for="__nav_6_2" id="__nav_6_2_label" tabindex="0">
          包参考
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_6_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_6_2">
          <span class="md-nav__icon md-icon"></span>
          包参考
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/10/" class="md-nav__link">
        Torch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/11/" class="md-nav__link">
        torch.Tensor
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/12/" class="md-nav__link">
        Tensor Attributes
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/13/" class="md-nav__link">
        torch.sparse
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/14/" class="md-nav__link">
        torch.cuda
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/15/" class="md-nav__link">
        torch.Storage
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/16/" class="md-nav__link">
        torch.nn
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/17/" class="md-nav__link">
        torch.nn.functional
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/18/" class="md-nav__link">
        自动差异化包 - torch.autograd
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/19/" class="md-nav__link">
        torch.optim
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/20/" class="md-nav__link">
        torch.nn.init
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/21/" class="md-nav__link">
        torch.distributions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/22/" class="md-nav__link">
        Multiprocessing 包 - torch.multiprocessing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/23/" class="md-nav__link">
        分布式通讯包 - torch.distributed
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/24/" class="md-nav__link">
        torch.utils.bottleneck
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/25/" class="md-nav__link">
        torch.utils.checkpoint
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/26/" class="md-nav__link">
        torch.utils.cpp_extension
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/27/" class="md-nav__link">
        torch.utils.data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/28/" class="md-nav__link">
        torch.utils.ffi
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/29/" class="md-nav__link">
        torch.utils.model_zoo
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/30/" class="md-nav__link">
        torch.onnx
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/31/" class="md-nav__link">
        遗留包 - torch.legacy
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_3" >
      
      
      
        <label class="md-nav__link" for="__nav_6_3" id="__nav_6_3_label" tabindex="0">
          torchvision 参考
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_6_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_6_3">
          <span class="md-nav__icon md-icon"></span>
          torchvision 参考
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/33/" class="md-nav__link">
        torchvision
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/34/" class="md-nav__link">
        torchvision.datasets
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/35/" class="md-nav__link">
        torchvision.models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/36/" class="md-nav__link">
        torchvision.transform
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/37/" class="md-nav__link">
        torchvision.utils
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
      
      
      
        <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
          PyTorch 0.3 中文文档 & 教程
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 0.3 中文文档 & 教程
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/" class="md-nav__link">
        目录
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_2" >
      
      
      
        <label class="md-nav__link" for="__nav_7_2" id="__nav_7_2_label" tabindex="0">
          中文教程
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_7_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_2">
          <span class="md-nav__icon md-icon"></span>
          中文教程
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_2_1" >
      
      
      
        <label class="md-nav__link" for="__nav_7_2_1" id="__nav_7_2_1_label" tabindex="0">
          初学者教程
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_7_2_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_2_1">
          <span class="md-nav__icon md-icon"></span>
          初学者教程
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_2_1_1" >
      
      
      
        <label class="md-nav__link" for="__nav_7_2_1_1" id="__nav_7_2_1_1_label" tabindex="0">
          PyTorch 深度学习: 60 分钟极速入门教程
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_7_2_1_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_2_1_1">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 深度学习: 60 分钟极速入门教程
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/deep_learning_60min_blitz/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/blitz_tensor_tutorial/" class="md-nav__link">
        PyTorch 是什么？
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/blitz_autograd_tutorial/" class="md-nav__link">
        自动求导: 自动微分
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/blitz_neural_networks_tutorial/" class="md-nav__link">
        神经网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/blitz_cifar10_tutorial/" class="md-nav__link">
        训练一个分类器
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/blitz_data_parallel_tutorial/" class="md-nav__link">
        可选: 数据并行
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_2_1_2" >
      
      
      
        <label class="md-nav__link" for="__nav_7_2_1_2" id="__nav_7_2_1_2_label" tabindex="0">
          PyTorch for former Torch users
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_7_2_1_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_2_1_2">
          <span class="md-nav__icon md-icon"></span>
          PyTorch for former Torch users
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/former_torchies_tutorial/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/former_torchies_tensor_tutorial/" class="md-nav__link">
        Tensors
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/former_torchies_autograd_tutorial/" class="md-nav__link">
        Autograd (自动求导)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/former_torchies_nn_tutorial/" class="md-nav__link">
        nn package
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/former_torchies_parallelism_tutorial/" class="md-nav__link">
        Multi-GPU examples
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_2_1_3" >
      
      
      
        <label class="md-nav__link" for="__nav_7_2_1_3" id="__nav_7_2_1_3_label" tabindex="0">
          跟着例子学习 PyTorch
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_7_2_1_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_2_1_3">
          <span class="md-nav__icon md-icon"></span>
          跟着例子学习 PyTorch
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/pytorch_with_examples/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/pytorch_with_examples_warm-up-numpy/" class="md-nav__link">
        Warm-up: numpy
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/pytorch_with_examples_pytorch-tensors/" class="md-nav__link">
        PyTorch: Tensors
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/pytorch_with_examples_pytorch-variables-and-autograd/" class="md-nav__link">
        PyTorch: 变量和autograd
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/pytorch_with_examples_pytorch-defining-new-autograd-functions/" class="md-nav__link">
        PyTorch: 定义新的autograd函数
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/pytorch_with_examples_tensorflow-static-graphs/" class="md-nav__link">
        TensorFlow: 静态图
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/pytorch_with_examples_pytorch-nn/" class="md-nav__link">
        PyTorch: nn包
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/pytorch_with_examples_pytorch-optim/" class="md-nav__link">
        PyTorch: optim包
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/pytorch_with_examples_pytorch-custom-nn-modules/" class="md-nav__link">
        PyTorch: 定制化nn模块
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/pytorch_with_examples_pytorch-control-flow-weight-sharing/" class="md-nav__link">
        PyTorch: 动态控制流程 - 权重共享
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/transfer_learning_tutorial/" class="md-nav__link">
        迁移学习教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/data_loading_tutorial/" class="md-nav__link">
        数据加载和处理教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_2_1_6" >
      
      
      
        <label class="md-nav__link" for="__nav_7_2_1_6" id="__nav_7_2_1_6_label" tabindex="0">
          针对NLP的Pytorch深度学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_7_2_1_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_2_1_6">
          <span class="md-nav__icon md-icon"></span>
          针对NLP的Pytorch深度学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/deep_learning_nlp_tutorial/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/nlp_pytorch_tutorial/" class="md-nav__link">
        PyTorch介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/nlp_deep_learning_tutorial/" class="md-nav__link">
        PyTorch深度学习
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/nlp_word_embeddings_tutorial/" class="md-nav__link">
        词汇嵌入:编码词汇语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/nlp_sequence_models_tutorial/" class="md-nav__link">
        序列模型和 LSTM 网络(长短记忆网络）
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/nlp_advanced_tutorial/" class="md-nav__link">
        高级教程: 作出动态决策和 Bi-LSTM CRF
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_2_2" >
      
      
      
        <label class="md-nav__link" for="__nav_7_2_2" id="__nav_7_2_2_label" tabindex="0">
          中级教程
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_7_2_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_2_2">
          <span class="md-nav__icon md-icon"></span>
          中级教程
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/char_rnn_classification_tutorial/" class="md-nav__link">
        用字符级RNN分类名称
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/char_rnn_generation_tutorial/" class="md-nav__link">
        基与字符级RNN(Char-RNN）的人名生成
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/seq2seq_translation_tutorial/" class="md-nav__link">
        用基于注意力机制的seq2seq神经网络进行翻译
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/reinforcement_q_learning/" class="md-nav__link">
        强化学习(DQN）教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/dist_tuto/" class="md-nav__link">
        Writing Distributed Applications with PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/spatial_transformer_tutorial/" class="md-nav__link">
        空间转换网络 (Spatial Transformer Networks) 教程
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_2_3" >
      
      
      
        <label class="md-nav__link" for="__nav_7_2_3" id="__nav_7_2_3_label" tabindex="0">
          高级教程
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_7_2_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_2_3">
          <span class="md-nav__icon md-icon"></span>
          高级教程
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/neural_style_tutorial/" class="md-nav__link">
        用 PyTorch 做 神经转换 (Neural Transfer)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/numpy_extensions_tutorial/" class="md-nav__link">
        使用 numpy 和 scipy 创建扩展
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/super_resolution_with_caffe2/" class="md-nav__link">
        使用 ONNX 将模型从 PyTorch 迁移到 Caffe2 和 Mobile
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/c_extension/" class="md-nav__link">
        为 pytorch 自定义 C 扩展
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_3" >
      
      
      
        <label class="md-nav__link" for="__nav_7_3" id="__nav_7_3_label" tabindex="0">
          中文文档
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_7_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_3">
          <span class="md-nav__icon md-icon"></span>
          中文文档
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_3_1" >
      
      
      
        <label class="md-nav__link" for="__nav_7_3_1" id="__nav_7_3_1_label" tabindex="0">
          介绍
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_7_3_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_3_1">
          <span class="md-nav__icon md-icon"></span>
          介绍
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/notes_autograd/" class="md-nav__link">
        自动求导机制
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/notes_broadcasting/" class="md-nav__link">
        广播语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/notes_cuda/" class="md-nav__link">
        CUDA 语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/notes_extending/" class="md-nav__link">
        扩展 PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/notes_multiprocessing/" class="md-nav__link">
        多进程的最佳实践
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/notes_serialization/" class="md-nav__link">
        序列化语义
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_3_2" >
      
      
      
        <label class="md-nav__link" for="__nav_7_3_2" id="__nav_7_3_2_label" tabindex="0">
          Package 参考
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_7_3_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_3_2">
          <span class="md-nav__icon md-icon"></span>
          Package 参考
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/torch/" class="md-nav__link">
        torch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/tensors/" class="md-nav__link">
        torch.Tensor
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/sparse/" class="md-nav__link">
        torch.sparse
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/storage/" class="md-nav__link">
        torch.Storage
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/nn/" class="md-nav__link">
        torch.nn
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/optim/" class="md-nav__link">
        torch.optim
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/autograd/" class="md-nav__link">
        Automatic differentiation package - torch.autograd
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/distributions/" class="md-nav__link">
        Probability distributions - torch.distributions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/multiprocessing/" class="md-nav__link">
        Multiprocessing package - torch.multiprocessing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/distributed/" class="md-nav__link">
        Distributed communication package - torch.distributed
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/legacy/" class="md-nav__link">
        Legacy package - torch.legacy
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/cuda/" class="md-nav__link">
        torch.cuda
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/ffi/" class="md-nav__link">
        torch.utils.ffi
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/data/" class="md-nav__link">
        torch.utils.data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/model_zoo/" class="md-nav__link">
        torch.utils.model_zoo
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/onnx/" class="md-nav__link">
        torch.onnx
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_3_3" >
      
      
      
        <label class="md-nav__link" for="__nav_7_3_3" id="__nav_7_3_3_label" tabindex="0">
          torchvision 参考
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_7_3_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_3_3">
          <span class="md-nav__icon md-icon"></span>
          torchvision 参考
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/torchvision/" class="md-nav__link">
        torchvision
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/datasets/" class="md-nav__link">
        torchvision.datasets
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/models/" class="md-nav__link">
        torchvision.models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/transforms/" class="md-nav__link">
        torchvision.transforms
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.3/utils/" class="md-nav__link">
        torchvision.utils
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8" >
      
      
      
        <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0">
          PyTorch 0.2 中文文档
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_8">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 0.2 中文文档
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8_2" >
      
      
      
        <label class="md-nav__link" for="__nav_8_2" id="__nav_8_2_label" tabindex="0">
          说明
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_8_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_8_2">
          <span class="md-nav__icon md-icon"></span>
          说明
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/notes/autograd/" class="md-nav__link">
        自动求导机制
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/notes/cuda/" class="md-nav__link">
        CUDA语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/notes/extending/" class="md-nav__link">
        扩展PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/notes/multiprocessing/" class="md-nav__link">
        多进程最佳实践
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/notes/serialization/" class="md-nav__link">
        序列化语义
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8_3" >
      
      
      
        <label class="md-nav__link" for="__nav_8_3" id="__nav_8_3_label" tabindex="0">
          PACKAGE参考
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_8_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_8_3">
          <span class="md-nav__icon md-icon"></span>
          PACKAGE参考
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/package_references/torch/" class="md-nav__link">
        torch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/package_references/Tensor/" class="md-nav__link">
        torch.Tensor
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/package_references/Storage/" class="md-nav__link">
        torch.Storage
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/package_references/torch-nn/" class="md-nav__link">
        torch.nn
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/package_references/functional/" class="md-nav__link">
        torch.nn.functional
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/package_references/torch-autograd/" class="md-nav__link">
        torch.autograd
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/package_references/torch-optim/" class="md-nav__link">
        torch.optim
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/package_references/nn_init/" class="md-nav__link">
        torch.nn.init
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/package_references/torch-multiprocessing/" class="md-nav__link">
        torch.multiprocessing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/package_references/legacy/" class="md-nav__link">
        torch.legacy
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/package_references/torch-cuda/" class="md-nav__link">
        torch.cuda
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/package_references/ffi/" class="md-nav__link">
        torch.utils.ffi
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/package_references/data/" class="md-nav__link">
        torch.utils.data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/package_references/model_zoo/" class="md-nav__link">
        torch.utils.model_zoo
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8_4" >
      
      
      
        <label class="md-nav__link" for="__nav_8_4" id="__nav_8_4_label" tabindex="0">
          TORCHVISION参考
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_8_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_8_4">
          <span class="md-nav__icon md-icon"></span>
          TORCHVISION参考
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/torchvision/torchvision/" class="md-nav__link">
        torchvision
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/torchvision/torchvision-datasets/" class="md-nav__link">
        torchvision.datasets
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/torchvision/torchvision-models/" class="md-nav__link">
        torchvision.models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/torchvision/torchvision-transform/" class="md-nav__link">
        torchvision.transforms
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/torchvision/torchvision-utils/" class="md-nav__link">
        torchvision.utils
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/acknowledgement/" class="md-nav__link">
        致谢
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../contrib/" class="md-nav__link">
        贡献者
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="https://www.apachecn.org/about/" class="md-nav__link">
        关于我们
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="https://docs.apachecn.org" class="md-nav__link">
        中文资源合集
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    张量
  </a>
  
    <nav class="md-nav" aria-label="张量">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    创作行动
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    索引，切片，联接，操作变更
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    发电机
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    随机抽样
  </a>
  
    <nav class="md-nav" aria-label="随机抽样">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    就地随机抽样
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    准随机抽样
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    序列化
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_9" class="md-nav__link">
    平行性
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_10" class="md-nav__link">
    局部禁用梯度计算
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_11" class="md-nav__link">
    数学运算
  </a>
  
    <nav class="md-nav" aria-label="数学运算">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_12" class="md-nav__link">
    逐点操作
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_13" class="md-nav__link">
    减少操作
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_14" class="md-nav__link">
    比较行动
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_15" class="md-nav__link">
    光谱操作
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_16" class="md-nav__link">
    其他作业
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#blas-lapack" class="md-nav__link">
    BLAS 和 LAPACK 操作
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_17" class="md-nav__link">
    实用工具
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
    <a href="https://github.com/apachecn/pytorch-doc-zh/edit/master/docs/1.4/74.md" title="Edit this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4v-2m10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1 2.1 2.1Z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/apachecn/pytorch-doc-zh/raw/master/docs/1.4/74.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.15 8.15 0 0 1-1.23-2Z"/></svg>
    </a>
  


<div class="wwads-cn wwads-horizontal" data-id="206" style="max-width:680px"></div>
<h1 id="torch">torch</h1>
<blockquote>
<p>原文： <a href="https://pytorch.org/docs/stable/torch.html">https://pytorch.org/docs/stable/torch.html</a></p>
</blockquote>
<p>torch程序包包含多维张量的数据结构，并定义了这些数据的数学运算。 此外，它提供了许多实用程序，可用于有效地序列化张量和任意类型，以及其他有用的实用程序。</p>
<p>它具有 CUDA 对应项，使您能够在具有计算能力&gt; = 3.0 的 NVIDIA GPU 上运行张量计算。</p>
<h2 id="_1">张量</h2>
<hr />
<pre><code>torch.is_tensor(obj)¶
</code></pre>
<p>如果 &lt;cite&gt;obj&lt;/cite&gt; 是 PyTorch 张量，则返回 True。</p>
<p>参数</p>
<p><strong>obj</strong> (<em>对象</em>）–要测试的对象</p>
<hr />
<pre><code>torch.is_storage(obj)¶
</code></pre>
<p>如果 &lt;cite&gt;obj&lt;/cite&gt; 是 PyTorch 存储对象，则返回 True。</p>
<p>Parameters</p>
<p><strong>obj</strong> (<em>Object</em>) – Object to test</p>
<hr />
<pre><code>torch.is_floating_point(input) -&gt; (bool)¶
</code></pre>
<p>如果<code>input</code>的数据类型是浮点数据类型，即<code>torch.float64</code>，<code>torch.float32</code>和<code>torch.float16</code>之一，则返回 True。</p>
<p>Parameters</p>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要测试的 PyTorch 张量</p>
<hr />
<pre><code>torch.set_default_dtype(d)¶
</code></pre>
<p>将默认浮点 dtype 设置为<code>d</code>。 该类型将用作 <a href="#torch.tensor" title="torch.tensor"><code>torch.tensor()</code></a> 中类型推断的默认浮点类型。</p>
<p>默认浮点 dtype 最初为<code>torch.float32</code>。</p>
<p>Parameters</p>
<p><strong>d</strong>  (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>)–浮点 dtype，使其成为默认值</p>
<p>例：</p>
<pre><code>&gt;&gt;&gt; torch.tensor([1.2, 3]).dtype           # initial default for floating point is torch.float32
torch.float32
&gt;&gt;&gt; torch.set_default_dtype(torch.float64)
&gt;&gt;&gt; torch.tensor([1.2, 3]).dtype           # a new floating point tensor
torch.float64

</code></pre>
<hr />
<pre><code>torch.get_default_dtype() → torch.dtype¶
</code></pre>
<p>获取当前的默认浮点数 <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> 。</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.get_default_dtype()  # initial default for floating point is torch.float32
torch.float32
&gt;&gt;&gt; torch.set_default_dtype(torch.float64)
&gt;&gt;&gt; torch.get_default_dtype()  # default is now changed to torch.float64
torch.float64
&gt;&gt;&gt; torch.set_default_tensor_type(torch.FloatTensor)  # setting tensor type also affects this
&gt;&gt;&gt; torch.get_default_dtype()  # changed to torch.float32, the dtype for torch.FloatTensor
torch.float32

</code></pre>
<hr />
<pre><code>torch.set_default_tensor_type(t)¶
</code></pre>
<p>将默认的<code>torch.Tensor</code>类型设置为浮点张量类型<code>t</code>。 该类型还将用作 <a href="#torch.tensor" title="torch.tensor"><code>torch.tensor()</code></a> 中类型推断的默认浮点类型。</p>
<p>默认的浮点张量类型最初为<code>torch.FloatTensor</code>。</p>
<p>Parameters</p>
<p><strong>t</strong>  (<em>python：type</em> <em>或</em> <em>字符串</em>）–浮点张量类型或其名称</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.tensor([1.2, 3]).dtype    # initial default for floating point is torch.float32
torch.float32
&gt;&gt;&gt; torch.set_default_tensor_type(torch.DoubleTensor)
&gt;&gt;&gt; torch.tensor([1.2, 3]).dtype    # a new floating point tensor
torch.float64

</code></pre>
<hr />
<pre><code>torch.numel(input) → int¶
</code></pre>
<p>返回<code>input</code>张量中的元素总数。</p>
<p>Parameters</p>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–输入张量。</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(1, 2, 3, 4, 5)
&gt;&gt;&gt; torch.numel(a)
120
&gt;&gt;&gt; a = torch.zeros(4,4)
&gt;&gt;&gt; torch.numel(a)
16

</code></pre>
<hr />
<pre><code>torch.set_printoptions(precision=None, threshold=None, edgeitems=None, linewidth=None, profile=None, sci_mode=None)¶
</code></pre>
<p>设置打印选项。 从 NumPy 无耻地拿走的物品</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>precision</strong> –浮点输出的精度位数(默认= 4）。</p>
</li>
<li>
<p><strong>阈值</strong> –触发汇总而不是完整的 &lt;cite&gt;repr&lt;/cite&gt; 的数组元素总数(默认= 1000）。</p>
</li>
<li>
<p><strong>edgeitems</strong> -每个维的开始和结束处摘要中数组项目的数量(默认= 3）。</p>
</li>
<li>
<p><strong>线宽</strong> –用于插入换行符的每行字符数(默认= 80）。 阈值矩阵将忽略此参数。</p>
</li>
<li>
<p><strong>配置文件</strong> – Sane 默认用于漂亮的打印。 可以使用以上任何选项覆盖。 (&lt;cite&gt;默认&lt;/cite&gt;，&lt;cite&gt;短&lt;/cite&gt;，&lt;cite&gt;完整&lt;/cite&gt;中的任何一种）</p>
</li>
<li>
<p><strong>sci_mode</strong> –启用(真）或禁用(假）科学计数法。 如果指定了 None(默认），则该值由 &lt;cite&gt;_Formatter&lt;/cite&gt; 定义</p>
</li>
</ul>
<hr />
<pre><code>torch.set_flush_denormal(mode) → bool¶
</code></pre>
<p>禁用 CPU 上的非正常浮​​点数。</p>
<p>如果您的系统支持刷新非正规数并且已成功配置刷新非正规模式，则返回<code>True</code>。 <a href="#torch.set_flush_denormal" title="torch.set_flush_denormal"><code>set_flush_denormal()</code></a> 仅在支持 SSE3 的 x86 架构上受支持。</p>
<p>Parameters</p>
<p><strong>模式</strong> (<em>bool</em> )–控制是否启用冲洗非正常模式</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.set_flush_denormal(True)
True
&gt;&gt;&gt; torch.tensor([1e-323], dtype=torch.float64)
tensor([ 0.], dtype=torch.float64)
&gt;&gt;&gt; torch.set_flush_denormal(False)
True
&gt;&gt;&gt; torch.tensor([1e-323], dtype=torch.float64)
tensor(9.88131e-324 *
       [ 1.0000], dtype=torch.float64)

</code></pre>
<h3 id="_2">创作行动</h3>
<p>注意</p>
<p>随机抽样创建操作列在<a href="#random-sampling">随机抽样</a>下，包括： <a href="#torch.rand" title="torch.rand"><code>torch.rand()</code></a> <a href="#torch.rand_like" title="torch.rand_like"><code>torch.rand_like()</code></a> <a href="#torch.randn" title="torch.randn"><code>torch.randn()</code></a> <a href="#torch.randn_like" title="torch.randn_like"><code>torch.randn_like()</code></a> <a href="#torch.randint" title="torch.randint"><code>torch.randint()</code></a> <a href="#torch.randint_like" title="torch.randint_like"><code>torch.randint_like()</code></a> <a href="#torch.randperm" title="torch.randperm"><code>torch.randperm()</code></a> 您也可以将 <a href="#torch.empty" title="torch.empty"><code>torch.empty()</code></a> 与<a href="#inplace-random-sampling">输入一起使用 位随机抽样</a>方法来创建 <a href="tensors.html#torch.Tensor" title="torch.Tensor"><code>torch.Tensor</code></a> ，并从更广泛的分布范围内采样值。</p>
<hr />
<pre><code>torch.tensor(data, dtype=None, device=None, requires_grad=False, pin_memory=False) → Tensor¶
</code></pre>
<p>用<code>data</code>构造一个张量。</p>
<p>警告</p>
<p><a href="#torch.tensor" title="torch.tensor"><code>torch.tensor()</code></a> 始终复制<code>data</code>。 如果您具有张量<code>data</code>并希望避免复制，请使用 <a href="tensors.html#torch.Tensor.requires_grad_" title="torch.Tensor.requires_grad_"><code>torch.Tensor.requires_grad_()</code></a> 或 <a href="autograd.html#torch.Tensor.detach" title="torch.Tensor.detach"><code>torch.Tensor.detach()</code></a> 。 如果您有 NumPy <code>ndarray</code>并想避免复制，请使用 <a href="#torch.as_tensor" title="torch.as_tensor"><code>torch.as_tensor()</code></a> 。</p>
<p>Warning</p>
<p>当数据是张量 &lt;cite&gt;x&lt;/cite&gt; 时， <a href="#torch.tensor" title="torch.tensor"><code>torch.tensor()</code></a> 从传递的任何数据中读出“数据”，并构造一个叶子变量。 因此，<code>torch.tensor(x)</code>等同于<code>x.clone().detach()</code>，<code>torch.tensor(x, requires_grad=True)</code>等同于<code>x.clone().detach().requires_grad_(True)</code>。 建议使用<code>clone()</code>和<code>detach()</code>的等效项。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>数据</strong> (<em>array_like</em> )–张量的初始数据。 可以是列表，元组，NumPy <code>ndarray</code>，标量和其他类型。</p>
</li>
<li>
<p><strong>dtype</strong>  (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> ，可选）–返回张量的所需数据类型。 默认值：如果<code>None</code>，则从<code>data</code>推断数据类型。</p>
</li>
<li>
<p><strong>设备</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a> ，可选）–返回张量的所需设备。 默认值：如果<code>None</code>，则使用当前设备作为默认张量类型(请参见 <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>)。 <code>device</code>将是用于 CPU 张量类型的 CPU，并且是用于 CUDA 张量类型的当前 CUDA 设备。</p>
</li>
<li>
<p><strong>require_grad</strong>  (<em>bool</em> <em>，</em> <em>可选</em>）–如果 autograd 应该在返回的张量上记录操作。 默认值：<code>False</code>。</p>
</li>
<li>
<p><strong>pin_memory</strong>  (<em>bool</em> <em>，</em> <em>可选</em>）–如果设置，返回的张量将分配在固定的内存中。 仅适用于 CPU 张量。 默认值：<code>False</code>。</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]])
tensor([[ 0.1000,  1.2000],
        [ 2.2000,  3.1000],
        [ 4.9000,  5.2000]])

&gt;&gt;&gt; torch.tensor([0, 1])  # Type inference on data
tensor([ 0,  1])

&gt;&gt;&gt; torch.tensor([[0.11111, 0.222222, 0.3333333]],
                 dtype=torch.float64,
                 device=torch.device('cuda:0'))  # creates a torch.cuda.DoubleTensor
tensor([[ 0.1111,  0.2222,  0.3333]], dtype=torch.float64, device='cuda:0')

&gt;&gt;&gt; torch.tensor(3.14159)  # Create a scalar (zero-dimensional tensor)
tensor(3.1416)

&gt;&gt;&gt; torch.tensor([])  # Create an empty tensor (of size (0,))
tensor([])

</code></pre>
<hr />
<pre><code>torch.sparse_coo_tensor(indices, values, size=None, dtype=None, device=None, requires_grad=False) → Tensor¶
</code></pre>
<p>在给定<code>values</code>和给定<code>values</code>的情况下，以非零元素构造 COO(rdinate）格式的稀疏张量。 稀疏张量可以是&lt;cite&gt;而不是&lt;/cite&gt;，在那种情况下，索引中有重复的坐标，并且该索引处的值是所有重复值条目的总和： <a href="https://pytorch.org/docs/stable/sparse.html">torch.sparse</a> 。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>索引</strong> (<em>array_like</em> )–张量的初始数据。 可以是列表，元组，NumPy <code>ndarray</code>，标量和其他类型。 将在内部强制转换为<code>torch.LongTensor</code>。 索引是矩阵中非零值的坐标，因此应为二维，其中第一维是张量维数，第二维是非零值数。</p>
</li>
<li>
<p><strong>值</strong> (<em>array_like</em> )–张量的初始值。 可以是列表，元组，NumPy <code>ndarray</code>，标量和其他类型。</p>
</li>
<li>
<p><strong>大小</strong>(列表，元组或<code>torch.Size</code>，可选）–稀疏张量的大小。 如果未提供，则将推断大小为足以容纳所有非零元素的最小大小。</p>
</li>
<li>
<p><strong>dtype</strong>  (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> ，可选）–返回张量的所需数据类型。 默认值：如果为 None，则从<code>values</code>推断数据类型。</p>
</li>
<li>
<p><strong>设备</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a> ，可选）–返回张量的所需设备。 默认值：如果为 None，则使用当前设备作为默认张量类型(请参见 <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>)。 <code>device</code>将是用于 CPU 张量类型的 CPU，是用于 CUDA 张量类型的当前 CUDA 设备。</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; i = torch.tensor([[0, 1, 1],
                      [2, 0, 2]])
&gt;&gt;&gt; v = torch.tensor([3, 4, 5], dtype=torch.float32)
&gt;&gt;&gt; torch.sparse_coo_tensor(i, v, [2, 4])
tensor(indices=tensor([[0, 1, 1],
                       [2, 0, 2]]),
       values=tensor([3., 4., 5.]),
       size=(2, 4), nnz=3, layout=torch.sparse_coo)

&gt;&gt;&gt; torch.sparse_coo_tensor(i, v)  # Shape inference
tensor(indices=tensor([[0, 1, 1],
                       [2, 0, 2]]),
       values=tensor([3., 4., 5.]),
       size=(2, 3), nnz=3, layout=torch.sparse_coo)

&gt;&gt;&gt; torch.sparse_coo_tensor(i, v, [2, 4],
                            dtype=torch.float64,
                            device=torch.device('cuda:0'))
tensor(indices=tensor([[0, 1, 1],
                       [2, 0, 2]]),
       values=tensor([3., 4., 5.]),
       device='cuda:0', size=(2, 4), nnz=3, dtype=torch.float64,
       layout=torch.sparse_coo)

# Create an empty sparse tensor with the following invariants:
#   1\. sparse_dim + dense_dim = len(SparseTensor.shape)
#   2\. SparseTensor._indices().shape = (sparse_dim, nnz)
#   3\. SparseTensor._values().shape = (nnz, SparseTensor.shape[sparse_dim:])
#
# For instance, to create an empty sparse tensor with nnz = 0, dense_dim = 0 and
# sparse_dim = 1 (hence indices is a 2D tensor of shape = (1, 0))
&gt;&gt;&gt; S = torch.sparse_coo_tensor(torch.empty([1, 0]), [], [1])
tensor(indices=tensor([], size=(1, 0)),
       values=tensor([], size=(0,)),
       size=(1,), nnz=0, layout=torch.sparse_coo)

# and to create an empty sparse tensor with nnz = 0, dense_dim = 1 and
# sparse_dim = 1
&gt;&gt;&gt; S = torch.sparse_coo_tensor(torch.empty([1, 0]), torch.empty([0, 2]), [1, 2])
tensor(indices=tensor([], size=(1, 0)),
       values=tensor([], size=(0, 2)),
       size=(1, 2), nnz=0, layout=torch.sparse_coo)

</code></pre>
<hr />
<pre><code>torch.as_tensor(data, dtype=None, device=None) → Tensor¶
</code></pre>
<p>将数据转换为&lt;cite&gt;torch。张量&lt;/cite&gt;。 如果数据已经是具有相同 &lt;cite&gt;dtype&lt;/cite&gt; 和&lt;cite&gt;设备&lt;/cite&gt;的&lt;cite&gt;张量&lt;/cite&gt;，则不会执行任何复制，否则将使用新的&lt;cite&gt;张量&lt;/cite&gt;。 如果数据&lt;cite&gt;张量&lt;/cite&gt;具有<code>requires_grad=True</code>，则返回保留计算图的计算图。 同样，如果数据是对应的 &lt;cite&gt;dtype&lt;/cite&gt; 的<code>ndarray</code>，并且&lt;cite&gt;设备&lt;/cite&gt;是 cpu，则不会执行任何复制。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>data</strong> (<em>array_like</em>) – Initial data for the tensor. Can be a list, tuple, NumPy <code>ndarray</code>, scalar, and other types.</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. Default: if <code>None</code>, infers data type from <code>data</code>.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = numpy.array([1, 2, 3])
&gt;&gt;&gt; t = torch.as_tensor(a)
&gt;&gt;&gt; t
tensor([ 1,  2,  3])
&gt;&gt;&gt; t[0] = -1
&gt;&gt;&gt; a
array([-1,  2,  3])

&gt;&gt;&gt; a = numpy.array([1, 2, 3])
&gt;&gt;&gt; t = torch.as_tensor(a, device=torch.device('cuda'))
&gt;&gt;&gt; t
tensor([ 1,  2,  3])
&gt;&gt;&gt; t[0] = -1
&gt;&gt;&gt; a
array([1,  2,  3])

</code></pre>
<hr />
<pre><code>torch.as_strided(input, size, stride, storage_offset=0) → Tensor¶
</code></pre>
<p>创建具有指定<code>size</code>，<code>stride</code>和<code>storage_offset</code>的现有&lt;cite&gt;炬管&lt;/cite&gt; <code>input</code>的视图。</p>
<p>Warning</p>
<p>创建的张量中的一个以上元素可以引用单个存储位置。 结果，就地操作(尤其是矢量化的操作）可能会导致错误的行为。 如果需要写张量，请先克隆它们。</p>
<p>许多 PyTorch 函数可返回张量视图，并在此函数内部实现。 这些功能，例如 <a href="tensors.html#torch.Tensor.expand" title="torch.Tensor.expand"><code>torch.Tensor.expand()</code></a> ，更易于阅读，因此更可取。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>大小</strong>(<em>元组</em> <em>或</em> <em>python：ints</em> )–输出张量的形状</p>
</li>
<li>
<p><strong>跨度</strong>(<em>元组</em> <em>或</em> <em>python：ints</em> )–输出张量的跨度</p>
</li>
<li>
<p><strong>storage_offset</strong>  (<em>python：int</em> <em>，</em> <em>可选</em>）–输出张量的基础存储中的偏移量</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(3, 3)
&gt;&gt;&gt; x
tensor([[ 0.9039,  0.6291,  1.0795],
        [ 0.1586,  2.1939, -0.4900],
        [-0.1909, -0.7503,  1.9355]])
&gt;&gt;&gt; t = torch.as_strided(x, (2, 2), (1, 2))
&gt;&gt;&gt; t
tensor([[0.9039, 1.0795],
        [0.6291, 0.1586]])
&gt;&gt;&gt; t = torch.as_strided(x, (2, 2), (1, 2), 1)
tensor([[0.6291, 0.1586],
        [1.0795, 2.1939]])

</code></pre>
<hr />
<pre><code>torch.from_numpy(ndarray) → Tensor¶
</code></pre>
<p>从<code>numpy.ndarray</code>创建 <a href="tensors.html#torch.Tensor" title="torch.Tensor"><code>Tensor</code></a> 。</p>
<p>返回的张量和<code>ndarray</code>共享相同的内存。 对张量的修改将反映在<code>ndarray</code>中，反之亦然。 返回的张量不可调整大小。</p>
<p>当前它接受具有<code>numpy.float64</code>，<code>numpy.float32</code>，<code>numpy.float16</code>，<code>numpy.int64</code>，<code>numpy.int32</code>，<code>numpy.int16</code>，<code>numpy.int8</code>，<code>numpy.uint8</code>和<code>numpy.bool</code> d 类型的<code>ndarray</code>。</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = numpy.array([1, 2, 3])
&gt;&gt;&gt; t = torch.from_numpy(a)
&gt;&gt;&gt; t
tensor([ 1,  2,  3])
&gt;&gt;&gt; t[0] = -1
&gt;&gt;&gt; a
array([-1,  2,  3])

</code></pre>
<hr />
<pre><code>torch.zeros(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor¶
</code></pre>
<p>返回一个由标量值 &lt;cite&gt;0&lt;/cite&gt; 填充的张量，其形状由变量参数<code>size</code>定义。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>大小</strong> (<em>python：int ...</em> )–定义输出张量形状的整数序列。 可以是可变数量的参数，也可以是列表或元组之类的集合。</p>
</li>
<li>
<p><strong>输出</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>，</em> <em>可选</em>）–输出张量。</p>
</li>
<li>
<p><strong>dtype</strong>  (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> ，可选）–返回张量的所需数据类型。 默认值：如果<code>None</code>使用全局默认值(请参见 <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>)。</p>
</li>
<li>
<p><strong>布局</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a> ，可选）–返回的 Tensor 所需的布局。 默认值：<code>torch.strided</code>。</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.zeros(2, 3)
tensor([[ 0.,  0.,  0.],
        [ 0.,  0.,  0.]])

&gt;&gt;&gt; torch.zeros(5)
tensor([ 0.,  0.,  0.,  0.,  0.])

</code></pre>
<hr />
<pre><code>torch.zeros_like(input, dtype=None, layout=None, device=None, requires_grad=False) → Tensor¶
</code></pre>
<p>返回一个由标量值 &lt;cite&gt;0&lt;/cite&gt; 填充的张量，其大小与<code>input</code>相同。 <code>torch.zeros_like(input)</code>等效于<code>torch.zeros(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)</code>。</p>
<p>Warning</p>
<p>从 0.4 开始，此功能不支持<code>out</code>关键字。 作为替代，旧的<code>torch.zeros_like(input, out=output)</code>等效于<code>torch.zeros(input.size(), out=output)</code>。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)– <code>input</code>的大小将确定输出张量的大小。</p>
</li>
<li>
<p><strong>dtype</strong>  (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> ，可选）–返回的 Tensor 的所需数据类型。 默认值：如果为<code>None</code>，则默认为<code>input</code>的 dtype。</p>
</li>
<li>
<p><strong>布局</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a> ，可选）–返回张量的所需布局。 默认值：如果为<code>None</code>，则默认为<code>input</code>的布局。</p>
</li>
<li>
<p><strong>设备</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a> ，可选）–返回张量的所需设备。 默认值：如果为<code>None</code>，则默认为<code>input</code>的设备。</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; input = torch.empty(2, 3)
&gt;&gt;&gt; torch.zeros_like(input)
tensor([[ 0.,  0.,  0.],
        [ 0.,  0.,  0.]])

</code></pre>
<hr />
<pre><code>torch.ones(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor¶
</code></pre>
<p>返回一个由标量值 &lt;cite&gt;1&lt;/cite&gt; 填充的张量，其形状由变量自变量<code>size</code>定义。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>size</strong> (<em>python:int...</em>) – a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>).</p>
</li>
<li>
<p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) – the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.ones(2, 3)
tensor([[ 1.,  1.,  1.],
        [ 1.,  1.,  1.]])

&gt;&gt;&gt; torch.ones(5)
tensor([ 1.,  1.,  1.,  1.,  1.])

</code></pre>
<hr />
<pre><code>torch.ones_like(input, dtype=None, layout=None, device=None, requires_grad=False) → Tensor¶
</code></pre>
<p>返回一个由标量值 &lt;cite&gt;1&lt;/cite&gt; 填充的张量，其大小与<code>input</code>相同。 <code>torch.ones_like(input)</code>等效于<code>torch.ones(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)</code>。</p>
<p>Warning</p>
<p>从 0.4 开始，此功能不支持<code>out</code>关键字。 作为替代，旧的<code>torch.ones_like(input, out=output)</code>等效于<code>torch.ones(input.size(), out=output)</code>。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the size of <code>input</code> will determine size of the output tensor.</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned Tensor. Default: if <code>None</code>, defaults to the dtype of <code>input</code>.</p>
</li>
<li>
<p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) – the desired layout of returned tensor. Default: if <code>None</code>, defaults to the layout of <code>input</code>.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, defaults to the device of <code>input</code>.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; input = torch.empty(2, 3)
&gt;&gt;&gt; torch.ones_like(input)
tensor([[ 1.,  1.,  1.],
        [ 1.,  1.,  1.]])

</code></pre>
<hr />
<pre><code>torch.arange(start=0, end, step=1, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor¶
</code></pre>
<p>返回大小为<img alt="" src="../img/fb2784cdfbfdd3f567366d5157f05b62.jpg" />的一维张量，该值具有从&lt;cite&gt;开始&lt;/cite&gt;开始具有公共差<code>step</code>的间隔<code>[start, end)</code>的值。</p>
<p>请注意，与<code>end</code>比较时，非整数<code>step</code>会出现浮点舍入错误； 为了避免不一致，在这种情况下，建议在<code>end</code>中添加一个小的ε。</p>
<p><img alt="" src="../img/e3f9924ddb1c14b63336e10cb955b09f.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>起始</strong>(<em>编号</em>）–点集的起始值。 默认值：<code>0</code>。</p>
</li>
<li>
<p><strong>结束</strong>(<em>编号</em>）–点集的结束值</p>
</li>
<li>
<p><strong>步骤</strong>(<em>编号</em>）–每对相邻点之间的间隙。 默认值：<code>1</code>。</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
<li>
<p><strong>dtype</strong>  (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> ，可选）–返回张量的所需数据类型。 默认值：如果<code>None</code>使用全局默认值(请参阅 <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>)。 如果未提供 &lt;cite&gt;dtype&lt;/cite&gt; ，则从其他输入参数推断数据类型。 如果&lt;cite&gt;开始&lt;/cite&gt;，&lt;cite&gt;结束&lt;/cite&gt;或&lt;cite&gt;停止&lt;/cite&gt;中的任何一个是浮点，则推断 &lt;cite&gt;dtype&lt;/cite&gt; 为默认 dtype，请参见[ <a href="#torch.get_default_dtype" title="torch.get_default_dtype"><code>get_default_dtype()</code></a> 。 否则，将 &lt;cite&gt;dtype&lt;/cite&gt; 推断为 &lt;cite&gt;torch.int64&lt;/cite&gt; 。</p>
</li>
<li>
<p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) – the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.arange(5)
tensor([ 0,  1,  2,  3,  4])
&gt;&gt;&gt; torch.arange(1, 4)
tensor([ 1,  2,  3])
&gt;&gt;&gt; torch.arange(1, 2.5, 0.5)
tensor([ 1.0000,  1.5000,  2.0000])

</code></pre>
<hr />
<pre><code>torch.range(start=0, end, step=1, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor¶
</code></pre>
<p>在步骤<code>step</code>中返回大小为<img alt="" src="../img/c04824f45400f61b164b031c61248a15.jpg" />的一维张量，其值从<code>start</code>到<code>end</code>。 阶跃是张量中两个值之间的差距。</p>
<p><img alt="" src="../img/ce7cb189b2d3908fa0a51507da3efd58.jpg" /></p>
<p>Warning</p>
<p>不推荐使用此功能，而推荐使用 <a href="#torch.arange" title="torch.arange"><code>torch.arange()</code></a> 。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>start</strong>  (<em>python：float</em> )–点集的起始值。 默认值：<code>0</code>。</p>
</li>
<li>
<p><strong>end</strong>  (<em>python：float</em> )–点集的结束值</p>
</li>
<li>
<p><strong>步骤</strong> (<em>python：float</em> )–每对相邻点之间的间隙。 默认值：<code>1</code>。</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). If &lt;cite&gt;dtype&lt;/cite&gt; is not given, infer the data type from the other input arguments. If any of &lt;cite&gt;start&lt;/cite&gt;, &lt;cite&gt;end&lt;/cite&gt;, or &lt;cite&gt;stop&lt;/cite&gt; are floating-point, the &lt;cite&gt;dtype&lt;/cite&gt; is inferred to be the default dtype, see <a href="#torch.get_default_dtype" title="torch.get_default_dtype"><code>get_default_dtype()</code></a>. Otherwise, the &lt;cite&gt;dtype&lt;/cite&gt; is inferred to be &lt;cite&gt;torch.int64&lt;/cite&gt;.</p>
</li>
<li>
<p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) – the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.range(1, 4)
tensor([ 1.,  2.,  3.,  4.])
&gt;&gt;&gt; torch.range(1, 4, 0.5)
tensor([ 1.0000,  1.5000,  2.0000,  2.5000,  3.0000,  3.5000,  4.0000])

</code></pre>
<hr />
<pre><code>torch.linspace(start, end, steps=100, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor¶
</code></pre>
<p>返回<code>start</code>和<code>end</code>之间等距点的<code>steps</code>的一维张量。</p>
<p>输出张量为<code>steps</code>大小的 1-D。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>开始</strong> (<em>python：float</em> )–点集的起始值</p>
</li>
<li>
<p><strong>end</strong> (<em>python:float</em>) – the ending value for the set of points</p>
</li>
<li>
<p><strong>步骤</strong> (<em>python：int</em> )–在<code>start</code>和<code>end</code>之间采样的点数。 默认值：<code>100</code>。</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>).</p>
</li>
<li>
<p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) – the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.linspace(3, 10, steps=5)
tensor([  3.0000,   4.7500,   6.5000,   8.2500,  10.0000])
&gt;&gt;&gt; torch.linspace(-10, 10, steps=5)
tensor([-10.,  -5.,   0.,   5.,  10.])
&gt;&gt;&gt; torch.linspace(start=-10, end=10, steps=5)
tensor([-10.,  -5.,   0.,   5.,  10.])
&gt;&gt;&gt; torch.linspace(start=-10, end=10, steps=1)
tensor([-10.])

</code></pre>
<hr />
<pre><code>torch.logspace(start, end, steps=100, base=10.0, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor¶
</code></pre>
<p>返回与<img alt="" src="../img/6a2c1af1d373a2b11ada878076f5b2a5.jpg" />和<img alt="" src="../img/6dcd2d5bb84df471dd6574611a280f9d.jpg" />之间的底数<code>base</code>对数间隔的<code>steps</code>点的一维张量。</p>
<p>The output tensor is 1-D of size <code>steps</code>.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>start</strong> (<em>python:float</em>) – the starting value for the set of points</p>
</li>
<li>
<p><strong>end</strong> (<em>python:float</em>) – the ending value for the set of points</p>
</li>
<li>
<p><strong>steps</strong> (<em>python:int</em>) – number of points to sample between <code>start</code> and <code>end</code>. Default: <code>100</code>.</p>
</li>
<li>
<p><strong>基数</strong> (<em>python：float</em> )–对数函数的基数。 默认值：<code>10.0</code>。</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>).</p>
</li>
<li>
<p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) – the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.logspace(start=-10, end=10, steps=5)
tensor([ 1.0000e-10,  1.0000e-05,  1.0000e+00,  1.0000e+05,  1.0000e+10])
&gt;&gt;&gt; torch.logspace(start=0.1, end=1.0, steps=5)
tensor([  1.2589,   2.1135,   3.5481,   5.9566,  10.0000])
&gt;&gt;&gt; torch.logspace(start=0.1, end=1.0, steps=1)
tensor([1.2589])
&gt;&gt;&gt; torch.logspace(start=2, end=2, steps=1, base=2)
tensor([4.0])

</code></pre>
<hr />
<pre><code>torch.eye(n, m=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor¶
</code></pre>
<p>返回一个二维张量，对角线上有一个，其他位置为零。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>n</strong>  (<em>python：int</em> )–行数</p>
</li>
<li>
<p><strong>m</strong>  (<em>python：int</em> <em>，</em> <em>可选</em>）–默认为<code>n</code>的列数</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>).</p>
</li>
<li>
<p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) – the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>退货</p>
<p>二维张量，对角线上有一个，其他位置为零</p>
<p>返回类型</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">张量</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.eye(3)
tensor([[ 1.,  0.,  0.],
        [ 0.,  1.,  0.],
        [ 0.,  0.,  1.]])

</code></pre>
<hr />
<pre><code>torch.empty(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False) → Tensor¶
</code></pre>
<p>返回填充有未初始化数据的张量。 张量的形状由变量参数<code>size</code>定义。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>size</strong> (<em>python:int...</em>) – a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>).</p>
</li>
<li>
<p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) – the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
<li>
<p><strong>pin_memory</strong> (<em>bool__,</em> <em>optional</em>) – If set, returned tensor would be allocated in the pinned memory. Works only for CPU tensors. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.empty(2, 3)
tensor(1.00000e-08 *
       [[ 6.3984,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000]])

</code></pre>
<hr />
<pre><code>torch.empty_like(input, dtype=None, layout=None, device=None, requires_grad=False) → Tensor¶
</code></pre>
<p>返回与<code>input</code>相同大小的未初始化张量。 <code>torch.empty_like(input)</code>等效于<code>torch.empty(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)</code>。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the size of <code>input</code> will determine size of the output tensor.</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned Tensor. Default: if <code>None</code>, defaults to the dtype of <code>input</code>.</p>
</li>
<li>
<p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) – the desired layout of returned tensor. Default: if <code>None</code>, defaults to the layout of <code>input</code>.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, defaults to the device of <code>input</code>.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.empty((2,3), dtype=torch.int64)
tensor([[ 9.4064e+13,  2.8000e+01,  9.3493e+13],
        [ 7.5751e+18,  7.1428e+18,  7.5955e+18]])

</code></pre>
<hr />
<pre><code>torch.empty_strided(size, stride, dtype=None, layout=None, device=None, requires_grad=False, pin_memory=False) → Tensor¶
</code></pre>
<p>返回填充有未初始化数据的张量。 张量的形状和步幅分别由变量参数<code>size</code>和<code>stride</code>定义。 <code>torch.empty_strided(size, stride)</code>等同于<code>torch.empty(size).as_strided(size, stride)</code>。</p>
<p>Warning</p>
<p>创建的张量中的一个以上元素可以引用单个存储位置。 结果，就地操作(尤其是矢量化的操作）可能会导致错误的行为。 如果需要写张量，请先克隆它们。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>大小</strong>(python：ints 的_元组）–输出张量的形状_</p>
</li>
<li>
<p><strong>跨度</strong>(python：ints 的_元组）–输出张量的跨度_</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>).</p>
</li>
<li>
<p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) – the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
<li>
<p><strong>pin_memory</strong> (<em>bool__,</em> <em>optional</em>) – If set, returned tensor would be allocated in the pinned memory. Works only for CPU tensors. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.empty_strided((2, 3), (1, 2))
&gt;&gt;&gt; a
tensor([[8.9683e-44, 4.4842e-44, 5.1239e+07],
        [0.0000e+00, 0.0000e+00, 3.0705e-41]])
&gt;&gt;&gt; a.stride()
(1, 2)
&gt;&gt;&gt; a.size()
torch.Size([2, 3])

</code></pre>
<hr />
<pre><code>torch.full(size, fill_value, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor¶
</code></pre>
<p>返回大小为<code>size</code>的张量，其中填充了<code>fill_value</code>。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>大小</strong> (<em>python：int ...</em> )–定义输出张量形状的整数列表，元组或<code>torch.Size</code>。</p>
</li>
<li>
<p><strong>fill_value</strong> –用来填充输出张量的数字。</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>).</p>
</li>
<li>
<p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) – the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.full((2, 3), 3.141592)
tensor([[ 3.1416,  3.1416,  3.1416],
        [ 3.1416,  3.1416,  3.1416]])

</code></pre>
<hr />
<pre><code>torch.full_like(input, fill_value, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor¶
</code></pre>
<p>返回与填充有<code>fill_value</code>的<code>input</code>大小相同的张量。 <code>torch.full_like(input, fill_value)</code>等同于<code>torch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device)</code>。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the size of <code>input</code> will determine size of the output tensor.</p>
</li>
<li>
<p><strong>fill_value</strong> – the number to fill the output tensor with.</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned Tensor. Default: if <code>None</code>, defaults to the dtype of <code>input</code>.</p>
</li>
<li>
<p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) – the desired layout of returned tensor. Default: if <code>None</code>, defaults to the layout of <code>input</code>.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, defaults to the device of <code>input</code>.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<hr />
<pre><code>torch.quantize_per_tensor(input, scale, zero_point, dtype) → Tensor¶
</code></pre>
<p>将浮点张量转换为具有给定比例和零点的量化张量。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–浮点张量进行量化</p>
</li>
<li>
<p><strong>标度</strong> (<em>python：float</em> )–适用于量化公式的标度</p>
</li>
<li>
<p><strong>zero_point</strong>  (<em>python：int</em> )–映射为浮点零的整数值偏移</p>
</li>
<li>
<p><strong>dtype</strong>  (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>)–返回张量的所需数据类型。 必须是量化的 dtypes 之一：<code>torch.quint8</code>，<code>torch.qint8</code>和<code>torch.qint32</code></p>
</li>
</ul>
<p>Returns</p>
<p>新量化的张量</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8)
tensor([-1.,  0.,  1.,  2.], size=(4,), dtype=torch.quint8,
       quantization_scheme=torch.per_tensor_affine, scale=0.1, zero_point=10)
&gt;&gt;&gt; torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8).int_repr()
tensor([ 0, 10, 20, 30], dtype=torch.uint8)

</code></pre>
<hr />
<pre><code>torch.quantize_per_channel(input, scales, zero_points, axis, dtype) → Tensor¶
</code></pre>
<p>将浮点张量转换为具有给定比例和零点的每通道量化张量。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – float tensor to quantize</p>
</li>
<li>
<p><strong>秤</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要使用的一维浮标秤，尺寸应匹配<code>input.size(axis)</code></p>
</li>
<li>
<p><strong>zero_points</strong>  (<em>python：int</em> )–要使用的整数 1D 张量偏移量，大小应与<code>input.size(axis)</code>相匹配</p>
</li>
<li>
<p><strong>轴</strong> (<em>python：int</em> )–应用每个通道量化的维度</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>) – the desired data type of returned tensor. Has to be one of the quantized dtypes: <code>torch.quint8</code>, <code>torch.qint8</code>, <code>torch.qint32</code></p>
</li>
</ul>
<p>Returns</p>
<p>A newly quantized tensor</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.tensor([[-1.0, 0.0], [1.0, 2.0]])
&gt;&gt;&gt; torch.quantize_per_channel(x, torch.tensor([0.1, 0.01]), torch.tensor([10, 0]), 0, torch.quint8)
tensor([[-1.,  0.],
        [ 1.,  2.]], size=(2, 2), dtype=torch.quint8,
       quantization_scheme=torch.per_channel_affine,
       scale=tensor([0.1000, 0.0100], dtype=torch.float64),
       zero_point=tensor([10,  0]), axis=0)
&gt;&gt;&gt; torch.quantize_per_channel(x, torch.tensor([0.1, 0.01]), torch.tensor([10, 0]), 0, torch.quint8).int_repr()
tensor([[  0,  10],
        [100, 200]], dtype=torch.uint8)

</code></pre>
<h3 id="_3">索引，切片，联接，操作变更</h3>
<hr />
<pre><code>torch.cat(tensors, dim=0, out=None) → Tensor¶
</code></pre>
<p>在给定维度上连接<code>seq</code>张量的给定序列。 所有张量必须具有相同的形状(在连接维中除外）或为空。</p>
<p><a href="#torch.cat" title="torch.cat"><code>torch.cat()</code></a> 可以看作是 <a href="#torch.split" title="torch.split"><code>torch.split()</code></a> 和 <a href="#torch.chunk" title="torch.chunk"><code>torch.chunk()</code></a> 的逆运算。</p>
<p>通过示例可以更好地理解 <a href="#torch.cat" title="torch.cat"><code>torch.cat()</code></a> 。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>张量</strong>(张量_序列_）–同一类型的任何 python 张量序列。 提供的非空张量必须具有相同的形状，但猫的尺寸除外。</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> <em>，</em> <em>可选</em>）–张量级联的尺寸</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(2, 3)
&gt;&gt;&gt; x
tensor([[ 0.6580, -1.0969, -0.4614],
        [-0.1034, -0.5790,  0.1497]])
&gt;&gt;&gt; torch.cat((x, x, x), 0)
tensor([[ 0.6580, -1.0969, -0.4614],
        [-0.1034, -0.5790,  0.1497],
        [ 0.6580, -1.0969, -0.4614],
        [-0.1034, -0.5790,  0.1497],
        [ 0.6580, -1.0969, -0.4614],
        [-0.1034, -0.5790,  0.1497]])
&gt;&gt;&gt; torch.cat((x, x, x), 1)
tensor([[ 0.6580, -1.0969, -0.4614,  0.6580, -1.0969, -0.4614,  0.6580,
         -1.0969, -0.4614],
        [-0.1034, -0.5790,  0.1497, -0.1034, -0.5790,  0.1497, -0.1034,
         -0.5790,  0.1497]])

</code></pre>
<hr />
<pre><code>torch.chunk(input, chunks, dim=0) → List of Tensors¶
</code></pre>
<p>将张量拆分为特定数量的块。</p>
<p>如果沿给定维度<code>dim</code>的张量大小不能被<code>chunks</code>整除，则最后一块将较小。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要分割的张量</p>
</li>
<li>
<p><strong>块</strong> (<em>python：int</em> )–要返回的块数</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> )–沿其张量分裂的尺寸</p>
</li>
</ul>
<hr />
<pre><code>torch.gather(input, dim, index, out=None, sparse_grad=False) → Tensor¶
</code></pre>
<p>沿&lt;cite&gt;昏暗&lt;/cite&gt;指定的轴收集值。</p>
<p>对于 3-D 张量，输出指定为：</p>
<pre><code>out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0
out[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1
out[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2

</code></pre>
<p>如果<code>input</code>是大小为<img alt="" src="../img/a89ebd4ba33f6e8d4b992302696fcb6b.jpg" />和<code>dim = i</code>的 n 维张量，则<code>index</code>必须是大小为<img alt="" src="../img/0998ac39fb5e46e656f2261d0af181b6.jpg" />的<img alt="" src="../img/5f0051b0454fa75ca446b59b47eff6f6.jpg" />-维张量，其中<img alt="" src="../img/5890f3c556cb9391345ace3ce2c49ff9.jpg" />和<code>out</code>具有相同的大小 大小为<code>index</code>。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–源张量</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> )–沿其索引的轴</p>
</li>
<li>
<p><strong>索引</strong> (<em>LongTensor</em> )–要收集的元素的索引</p>
</li>
<li>
<p><strong>输出</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>，</em> <em>可选</em>）–目标张量</p>
</li>
<li>
<p><strong>sparse_grad</strong>  (<em>bool</em> <em>，</em> <em>可选</em>）–如果<code>True</code>，则梯度 w.r.t. <code>input</code>将是一个稀疏张量。</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; t = torch.tensor([[1,2],[3,4]])
&gt;&gt;&gt; torch.gather(t, 1, torch.tensor([[0,0],[1,0]]))
tensor([[ 1,  1],
        [ 4,  3]])

</code></pre>
<hr />
<pre><code>torch.index_select(input, dim, index, out=None) → Tensor¶
</code></pre>
<p>返回一个新张量，该张量使用<code>index</code> LongTensor 中的<code>index</code>中的条目沿维度<code>dim</code>索引<code>input</code>张量。</p>
<p>返回的张量具有与原始张量(<code>input</code>）相同的维数。 <code>dim</code>的尺寸与<code>index</code>的长度相同； 其他尺寸与原始张量中的尺寸相同。</p>
<p>Note</p>
<p>返回的张量不与原始张量使用相同的存储空间<strong>而不是</strong>。 如果<code>out</code>的形状与预期的形状不同，我们将默默地将其更改为正确的形状，并在必要时重新分配基础存储。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> )–我们索引的维度</p>
</li>
<li>
<p><strong>索引</strong> (<em>LongTensor</em> )–包含要索引的索引的一维张量</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(3, 4)
&gt;&gt;&gt; x
tensor([[ 0.1427,  0.0231, -0.5414, -1.0009],
        [-0.4664,  0.2647, -0.1228, -1.1068],
        [-1.1734, -0.6571,  0.7230, -0.6004]])
&gt;&gt;&gt; indices = torch.tensor([0, 2])
&gt;&gt;&gt; torch.index_select(x, 0, indices)
tensor([[ 0.1427,  0.0231, -0.5414, -1.0009],
        [-1.1734, -0.6571,  0.7230, -0.6004]])
&gt;&gt;&gt; torch.index_select(x, 1, indices)
tensor([[ 0.1427, -0.5414],
        [-0.4664, -0.1228],
        [-1.1734,  0.7230]])

</code></pre>
<hr />
<pre><code>torch.masked_select(input, mask, out=None) → Tensor¶
</code></pre>
<p>返回一个新的一维张量，该张量根据布尔值掩码<code>mask</code>为其 &lt;cite&gt;BoolTensor&lt;/cite&gt; 索引<code>input</code>张量。</p>
<p><code>mask</code>张量和<code>input</code>张量的形状不需要匹配，但它们必须是<a href="notes/broadcasting.html#broadcasting-semantics">可广播的</a>。</p>
<p>Note</p>
<p>返回的张量是否<strong>而不是</strong>使用与原始张量相同的存储</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>掩码</strong> (<em>ByteTensor</em> )–包含二进制掩码的张量，以使用</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(3, 4)
&gt;&gt;&gt; x
tensor([[ 0.3552, -2.3825, -0.8297,  0.3477],
        [-1.2035,  1.2252,  0.5002,  0.6248],
        [ 0.1307, -2.0608,  0.1244,  2.0139]])
&gt;&gt;&gt; mask = x.ge(0.5)
&gt;&gt;&gt; mask
tensor([[False, False, False, False],
        [False, True, True, True],
        [False, False, False, True]])
&gt;&gt;&gt; torch.masked_select(x, mask)
tensor([ 1.2252,  0.5002,  0.6248,  2.0139])

</code></pre>
<hr />
<pre><code>torch.narrow(input, dim, start, length) → Tensor¶
</code></pre>
<p>返回一个新的张量，该张量是<code>input</code>张量的缩小版本。 尺寸<code>dim</code>从<code>start</code>输入到<code>start + length</code>。 返回的张量和<code>input</code>张量共享相同的基础存储。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–张量变窄</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> )–缩小范围</p>
</li>
<li>
<p><strong>开始</strong> (<em>python：int</em> )–起始尺寸</p>
</li>
<li>
<p><strong>长度</strong> (<em>python：int</em> )–到最终尺寸的距离</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
&gt;&gt;&gt; torch.narrow(x, 0, 0, 2)
tensor([[ 1,  2,  3],
        [ 4,  5,  6]])
&gt;&gt;&gt; torch.narrow(x, 1, 1, 2)
tensor([[ 2,  3],
        [ 5,  6],
        [ 8,  9]])

</code></pre>
<hr />
<pre><code>torch.nonzero(input, *, out=None, as_tuple=False) → LongTensor or tuple of LongTensors¶
</code></pre>
<p>Note</p>
<p><a href="#torch.nonzero" title="torch.nonzero"><code>torch.nonzero(..., as_tuple=False)</code></a> (默认值）返回一个二维张量，其中每一行都是非零值的索引。</p>
<p><a href="#torch.nonzero" title="torch.nonzero"><code>torch.nonzero(..., as_tuple=True)</code></a> 返回一维索引张量的元组，允许进行高级索引，因此<code>x[x.nonzero(as_tuple=True)]</code>给出张量<code>x</code>的所有非零值。 在返回的元组中，每个索引张量都包含特定维度的非零索引。</p>
<p>有关这两种行为的更多详细信息，请参见下文。</p>
<p><strong>当</strong> <code>as_tuple</code> <strong>为“ False”(默认）</strong>时：</p>
<p>返回一个张量，该张量包含<code>input</code>所有非零元素的索引。 结果中的每一行都包含<code>input</code>中非零元素的索引。 结果按字典顺序排序，最后一个索引更改最快(C 样式）。</p>
<p>如果<code>input</code>具有<img alt="" src="../img/5f0051b0454fa75ca446b59b47eff6f6.jpg" />尺寸，则所得索引张量<code>out</code>的大小为<img alt="" src="../img/b6080a6ed8a7dd471ec6fd4b1023bc23.jpg" />，其中<img alt="" src="../img/e1bf4c09825257a3ffbe6bddb254bcb6.jpg" />是<code>input</code>张量中非零元素的总数。</p>
<p><strong>当</strong> <code>as_tuple</code> <strong>为“ True”</strong> 时：</p>
<p>返回一维张量的元组，在<code>input</code>中每个维度一个张量，每个张量包含<code>input</code>所有非零元素的索引(在该维度中）。</p>
<p>如果<code>input</code>具有<img alt="" src="../img/5f0051b0454fa75ca446b59b47eff6f6.jpg" />尺寸，则生成的元组包含<img alt="" src="../img/e1bf4c09825257a3ffbe6bddb254bcb6.jpg" />大小的<img alt="" src="../img/5f0051b0454fa75ca446b59b47eff6f6.jpg" />张量，其中<img alt="" src="../img/e1bf4c09825257a3ffbe6bddb254bcb6.jpg" />是<code>input</code>张量中非零元素的总数。</p>
<p>作为一种特殊情况，当<code>input</code>具有零维和非零标量值时，会将其视为具有一个元素的一维张量。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong>  (<em>LongTensor</em> <em>，</em> <em>可选</em>）–包含索引的输出张量</p>
</li>
</ul>
<p>Returns</p>
<p>如果<code>as_tuple</code>为<code>False</code>，则包含索引的输出张量。 如果<code>as_tuple</code>为<code>True</code>，则每个维度都有一个 1-D 张量，其中包含沿着该维度的每个非零元素的索引。</p>
<p>Return type</p>
<p>LongTensor 或 LongTensor 的元组</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.nonzero(torch.tensor([1, 1, 1, 0, 1]))
tensor([[ 0],
        [ 1],
        [ 2],
        [ 4]])
&gt;&gt;&gt; torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0],
                                [0.0, 0.4, 0.0, 0.0],
                                [0.0, 0.0, 1.2, 0.0],
                                [0.0, 0.0, 0.0,-0.4]]))
tensor([[ 0,  0],
        [ 1,  1],
        [ 2,  2],
        [ 3,  3]])
&gt;&gt;&gt; torch.nonzero(torch.tensor([1, 1, 1, 0, 1]), as_tuple=True)
(tensor([0, 1, 2, 4]),)
&gt;&gt;&gt; torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0],
                                [0.0, 0.4, 0.0, 0.0],
                                [0.0, 0.0, 1.2, 0.0],
                                [0.0, 0.0, 0.0,-0.4]]), as_tuple=True)
(tensor([0, 1, 2, 3]), tensor([0, 1, 2, 3]))
&gt;&gt;&gt; torch.nonzero(torch.tensor(5), as_tuple=True)
(tensor([0]),)

</code></pre>
<hr />
<pre><code>torch.reshape(input, shape) → Tensor¶
</code></pre>
<p>返回具有与<code>input</code>相同的数据和元素数量，但具有指定形状的张量。 如果可能，返回的张量将是<code>input</code>的视图。 否则，它将是副本。 连续输入和具有兼容步幅的输入可以在不复制的情况下进行重塑，但是您不应该依赖复制与查看行为。</p>
<p>当可以返回视图时，请参见 <a href="tensors.html#torch.Tensor.view" title="torch.Tensor.view"><code>torch.Tensor.view()</code></a> 。</p>
<p>单个尺寸可能为-1，在这种情况下，它是根据<code>input</code>中的其余尺寸和元素数量推断出来的。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要重塑的张量</p>
</li>
<li>
<p><strong>形状</strong> (<em>python：ints</em> 的元组）–新形状</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.arange(4.)
&gt;&gt;&gt; torch.reshape(a, (2, 2))
tensor([[ 0.,  1.],
        [ 2.,  3.]])
&gt;&gt;&gt; b = torch.tensor([[0, 1], [2, 3]])
&gt;&gt;&gt; torch.reshape(b, (-1,))
tensor([ 0,  1,  2,  3])

</code></pre>
<hr />
<pre><code>torch.split(tensor, split_size_or_sections, dim=0)¶
</code></pre>
<p>将张量拆分为多个块。</p>
<p>如果<code>split_size_or_sections</code>是整数类型，则 <a href="#torch.tensor" title="torch.tensor"><code>tensor</code></a> 将被拆分为大小相等的块(如果可能）。 如果沿给定维度<code>dim</code>的张量大小不能被<code>split_size</code>整除，则最后一个块将较小。</p>
<p>如果<code>split_size_or_sections</code>是列表，则根据<code>split_size_or_sections</code>将 <a href="#torch.tensor" title="torch.tensor"><code>tensor</code></a> 拆分为<code>dim</code>，大小为<code>dim</code>。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>张量</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–张量分裂。</p>
</li>
<li>
<p><strong>split_size_or_sections</strong>  (<em>python：int</em> <em>）或</em> <em>(</em> <em>列表</em> <em>(</em> <em>python ：int</em> <em>）</em>）–单个块的大小或每个块的大小列表</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> )–沿其张量分裂的尺寸。</p>
</li>
</ul>
<hr />
<pre><code>torch.squeeze(input, dim=None, out=None) → Tensor¶
</code></pre>
<p>返回一个张量，其中所有尺寸为 &lt;cite&gt;1&lt;/cite&gt; 的<code>input</code>尺寸均被删除。</p>
<p>例如，如果&lt;cite&gt;输入&lt;/cite&gt;的形状为：<img alt="" src="../img/1f976021505083151a3b5d3311ab04c2.jpg" />，则张量中的&lt;cite&gt;张量将为：<img alt="" src="../img/7af8285a40441ae3080550e9267b63f8.jpg" />。&lt;/cite&gt;</p>
<p>给定<code>dim</code>时，仅在给定尺寸上执行挤压操作。 如果&lt;cite&gt;输入&lt;/cite&gt;的形状为：<img alt="" src="../img/fb1a3c771fc7be02c64e40a24e873471.jpg" />，<code>squeeze(input, 0)</code>保持张量不变，但是<code>squeeze(input, 1)</code>会将张量压缩为<img alt="" src="../img/e48d9b756847043d64d7565153ad4faf.jpg" />形状。</p>
<p>Note</p>
<p>返回的张量与输入张量共享存储，因此更改一个张量的内容将更改另一个张量的内容。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> <em>，</em> <em>可选</em>）–如果给定，则仅在此维度上压缩输入</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.zeros(2, 1, 2, 1, 2)
&gt;&gt;&gt; x.size()
torch.Size([2, 1, 2, 1, 2])
&gt;&gt;&gt; y = torch.squeeze(x)
&gt;&gt;&gt; y.size()
torch.Size([2, 2, 2])
&gt;&gt;&gt; y = torch.squeeze(x, 0)
&gt;&gt;&gt; y.size()
torch.Size([2, 1, 2, 1, 2])
&gt;&gt;&gt; y = torch.squeeze(x, 1)
&gt;&gt;&gt; y.size()
torch.Size([2, 2, 1, 2])

</code></pre>
<hr />
<pre><code>torch.stack(tensors, dim=0, out=None) → Tensor¶
</code></pre>
<p>将张量的序列沿新维度连接起来。</p>
<p>所有张量都必须具有相同的大小。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>张量</strong>(<em>张量序列</em>）–连接的张量序列</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> )–插入的尺寸。 必须介于 0 和级联张量的维数之间(含）</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<hr />
<pre><code>torch.t(input) → Tensor¶
</code></pre>
<p>期望<code>input</code>为&lt; = 2-D 张量，并转置尺寸 0 和 1。</p>
<p>将按原样返回 0-D 和 1-D 张量，并且可以将 2-D 张量视为<code>transpose(input, 0, 1)</code>的简写函数。</p>
<p>Parameters</p>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(())
&gt;&gt;&gt; x
tensor(0.1995)
&gt;&gt;&gt; torch.t(x)
tensor(0.1995)
&gt;&gt;&gt; x = torch.randn(3)
&gt;&gt;&gt; x
tensor([ 2.4320, -0.4608,  0.7702])
&gt;&gt;&gt; torch.t(x)
tensor([.2.4320,.-0.4608,..0.7702])
&gt;&gt;&gt; x = torch.randn(2, 3)
&gt;&gt;&gt; x
tensor([[ 0.4875,  0.9158, -0.5872],
        [ 0.3938, -0.6929,  0.6932]])
&gt;&gt;&gt; torch.t(x)
tensor([[ 0.4875,  0.3938],
        [ 0.9158, -0.6929],
        [-0.5872,  0.6932]])

</code></pre>
<hr />
<pre><code>torch.take(input, index) → Tensor¶
</code></pre>
<p>返回给定索引处带有<code>input</code>元素的新张量。 将输入张量视为视为一维张量。 结果采用与索引相同的形状。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>索引</strong> (<em>LongTensor</em> )–张量索引</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; src = torch.tensor([[4, 3, 5],
                        [6, 7, 8]])
&gt;&gt;&gt; torch.take(src, torch.tensor([0, 2, 5]))
tensor([ 4,  5,  8])

</code></pre>
<hr />
<pre><code>torch.transpose(input, dim0, dim1) → Tensor¶
</code></pre>
<p>返回一个张量，该张量是<code>input</code>的转置版本。 给定的尺寸<code>dim0</code>和<code>dim1</code>被交换。</p>
<p>产生的<code>out</code>张量与<code>input</code>张量共享其基础存储，因此更改一个内容将更改另一个内容。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>dim0</strong>  (<em>python：int</em> )–要转置的第一个维度</p>
</li>
<li>
<p><strong>dim1</strong>  (<em>python：int</em> )–要转置的第二维</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(2, 3)
&gt;&gt;&gt; x
tensor([[ 1.0028, -0.9893,  0.5809],
        [-0.1669,  0.7299,  0.4942]])
&gt;&gt;&gt; torch.transpose(x, 0, 1)
tensor([[ 1.0028, -0.1669],
        [-0.9893,  0.7299],
        [ 0.5809,  0.4942]])

</code></pre>
<hr />
<pre><code>torch.unbind(input, dim=0) → seq¶
</code></pre>
<p>删除张量尺寸。</p>
<p>返回给定维度上所有切片的元组，已经没有它。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要解除绑定的张量</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> )–要移除的尺寸</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.unbind(torch.tensor([[1, 2, 3],
&gt;&gt;&gt;                            [4, 5, 6],
&gt;&gt;&gt;                            [7, 8, 9]]))
(tensor([1, 2, 3]), tensor([4, 5, 6]), tensor([7, 8, 9]))

</code></pre>
<hr />
<pre><code>torch.unsqueeze(input, dim, out=None) → Tensor¶
</code></pre>
<p>返回在指定位置插入的尺寸为 1 的新张量。</p>
<p>返回的张量与此张量共享相同的基础数据。</p>
<p>可以使用<code>[-input.dim() - 1, input.dim() + 1)</code>范围内的<code>dim</code>值。 负的<code>dim</code>对应于<code>dim</code> = <code>dim + input.dim() + 1</code>处应用的 <a href="#torch.unsqueeze" title="torch.unsqueeze"><code>unsqueeze()</code></a> 。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> )–插入单例尺寸的索引</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.tensor([1, 2, 3, 4])
&gt;&gt;&gt; torch.unsqueeze(x, 0)
tensor([[ 1,  2,  3,  4]])
&gt;&gt;&gt; torch.unsqueeze(x, 1)
tensor([[ 1],
        [ 2],
        [ 3],
        [ 4]])

</code></pre>
<hr />
<pre><code>torch.where()¶
</code></pre>
<hr />
<pre><code>torch.where(condition, x, y) → Tensor
</code></pre>
<p>返回从<code>x</code>或<code>y</code>中选择的元素的张量，具体取决于<code>condition</code>。</p>
<p>该操作定义为：</p>
<p><img alt="" src="../img/12ee903e238f296681b1cef26fef0f8f.jpg" /></p>
<p>Note</p>
<p>张量<code>condition</code>，<code>x</code>和<code>y</code>必须是<a href="notes/broadcasting.html#broadcasting-semantics">可广播的</a>。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>条件</strong> (<a href="tensors.html#torch.BoolTensor" title="torch.BoolTensor"><em>BoolTensor</em></a>)–当为 True(非零）时，产生 x，否则产生 y</p>
</li>
<li>
<p><strong>x</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–在<code>condition</code>为<code>True</code>的索引处选择的值</p>
</li>
<li>
<p><strong>y</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–在<code>condition</code>为<code>False</code>的索引处选择的值</p>
</li>
</ul>
<p>Returns</p>
<p>形状张量等于<code>condition</code>，<code>x</code>，<code>y</code>的广播形状</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(3, 2)
&gt;&gt;&gt; y = torch.ones(3, 2)
&gt;&gt;&gt; x
tensor([[-0.4620,  0.3139],
        [ 0.3898, -0.7197],
        [ 0.0478, -0.1657]])
&gt;&gt;&gt; torch.where(x &gt; 0, x, y)
tensor([[ 1.0000,  0.3139],
        [ 0.3898,  1.0000],
        [ 0.0478,  1.0000]])

</code></pre>
<hr />
<pre><code>torch.where(condition) → tuple of LongTensor
</code></pre>
<p><code>torch.where(condition)</code>与<code>torch.nonzero(condition, as_tuple=True)</code>相同。</p>
<p>Note</p>
<p>另请参见 <a href="#torch.nonzero" title="torch.nonzero"><code>torch.nonzero()</code></a> 。</p>
<h2 id="_4">发电机</h2>
<hr />
<pre><code>class torch._C.Generator(device='cpu') → Generator¶
</code></pre>
<p>创建并返回一个生成器对象，该对象管理产生伪随机数的算法的状态。 在许多<a href="#inplace-random-sampling">就地随机采样</a>函数中用作关键字参数。</p>
<p>Parameters</p>
<p><strong>设备</strong>(<code>torch.device</code>，可选）–生成器所需的设备。</p>
<p>Returns</p>
<p>一个 torch.Generator 对象。</p>
<p>Return type</p>
<p><a href="#torch._C.Generator" title="torch._C.Generator">生成器</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; g_cpu = torch.Generator()
&gt;&gt;&gt; g_cuda = torch.Generator(device='cuda')

</code></pre>
<pre><code>device¶
</code></pre>
<p>Generator.device-&gt;设备</p>
<p>获取生成器的当前设备。</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; g_cpu = torch.Generator()
&gt;&gt;&gt; g_cpu.device
device(type='cpu')

</code></pre>
<hr />
<pre><code>get_state() → Tensor¶
</code></pre>
<p>返回生成器状态为<code>torch.ByteTensor</code>。</p>
<p>Returns</p>
<p>一个<code>torch.ByteTensor</code>，其中包含将生成器还原到特定时间点的所有必要位。</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; g_cpu = torch.Generator()
&gt;&gt;&gt; g_cpu.get_state()

</code></pre>
<hr />
<pre><code>initial_seed() → int¶
</code></pre>
<p>返回用于生成随机数的初始种子。</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; g_cpu = torch.Generator()
&gt;&gt;&gt; g_cpu.initial_seed()
2147483647

</code></pre>
<hr />
<pre><code>manual_seed(seed) → Generator¶
</code></pre>
<p>设置用于生成随机数的种子。 返回一个&lt;cite&gt;torch.生成器&lt;/cite&gt;对象。 建议设置一个大种子，即一个具有 0 和 1 位平衡的数字。 避免在种子中包含许多 0 位。</p>
<p>Parameters</p>
<p><strong>种子</strong> (<em>python：int</em> )–所需的种子。</p>
<p>Returns</p>
<p>An torch.Generator object.</p>
<p>Return type</p>
<p><a href="#torch._C.Generator" title="torch._C.Generator">Generator</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; g_cpu = torch.Generator()
&gt;&gt;&gt; g_cpu.manual_seed(2147483647)

</code></pre>
<hr />
<pre><code>seed() → int¶
</code></pre>
<p>从 std :: random_device 或当前时间获取不确定的随机数，并将其用作生成器的种子。</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; g_cpu = torch.Generator()
&gt;&gt;&gt; g_cpu.seed()
1516516984916

</code></pre>
<hr />
<pre><code>set_state(new_state) → void¶
</code></pre>
<p>设置生成器状态。</p>
<p>Parameters</p>
<p><strong>new_state</strong>  (<em>Torch.ByteTensor</em> )–所需状态。</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; g_cpu = torch.Generator()
&gt;&gt;&gt; g_cpu_other = torch.Generator()
&gt;&gt;&gt; g_cpu.set_state(g_cpu_other.get_state())

</code></pre>
<h2 id="_5">随机抽样</h2>
<hr />
<pre><code>torch.seed()¶
</code></pre>
<p>将用于生成随机数的种子设置为不确定的随机数。 返回用于播种 RNG 的 64 位数字。</p>
<hr />
<pre><code>torch.manual_seed(seed)¶
</code></pre>
<p>设置用于生成随机数的种子。 返回一个&lt;cite&gt;torch.生成器&lt;/cite&gt;对象。</p>
<p>Parameters</p>
<p><strong>seed</strong> (<em>python:int</em>) – The desired seed.</p>
<hr />
<pre><code>torch.initial_seed()¶
</code></pre>
<p>返回长为 Python &lt;cite&gt;long&lt;/cite&gt; 的用于生成随机数的初始种子。</p>
<hr />
<pre><code>torch.get_rng_state()¶
</code></pre>
<p>以 &lt;cite&gt;torch.ByteTensor&lt;/cite&gt; 的形式返回随机数生成器状态。</p>
<hr />
<pre><code>torch.set_rng_state(new_state)¶
</code></pre>
<p>设置随机数生成器状态。</p>
<p>Parameters</p>
<p><strong>new_state</strong>  (<em>torch.ByteTensor</em> )–所需状态</p>
<pre><code>torch.default_generator Returns the default CPU torch.Generator¶
</code></pre>
<hr />
<pre><code>torch.bernoulli(input, *, generator=None, out=None) → Tensor¶
</code></pre>
<p>从伯努利分布中提取二进制随机数(0 或 1）。</p>
<p><code>input</code>张量应为包含用于绘制二进制随机数的概率的张量。 因此，<code>input</code>中的所有值都必须在以下范围内：<img alt="" src="../img/b96a669c7fd5b0f5ea99f4373d6eab2e.jpg" />。</p>
<p>输出张量的<img alt="" src="../img/55a50814a1cfd1ab95d71ca3a7f311fb.jpg" />元素将根据<code>input</code>中给出的<img alt="" src="../img/55a50814a1cfd1ab95d71ca3a7f311fb.jpg" />概率值绘制一个<img alt="" src="../img/21c5bc8d40ee5ab2e18d64aaba8359c7.jpg" />值。</p>
<p><img alt="" src="../img/f1fc132b2ecf782642591904c4f9b7de.jpg" /></p>
<p>返回的<code>out</code>张量仅具有值 0 或 1，并且具有与<code>input</code>相同的形状。</p>
<p><code>out</code>可以具有整数<code>dtype</code>，但是<code>input</code>必须具有浮点<code>dtype</code>。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–伯努利分布的概率值的输入张量</p>
</li>
<li>
<p><strong>生成器</strong>(<code>torch.Generator</code>，可选）–用于采样的伪随机数生成器</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.empty(3, 3).uniform_(0, 1)  # generate a uniform random matrix with range [0, 1]
&gt;&gt;&gt; a
tensor([[ 0.1737,  0.0950,  0.3609],
        [ 0.7148,  0.0289,  0.2676],
        [ 0.9456,  0.8937,  0.7202]])
&gt;&gt;&gt; torch.bernoulli(a)
tensor([[ 1.,  0.,  0.],
        [ 0.,  0.,  0.],
        [ 1.,  1.,  1.]])

&gt;&gt;&gt; a = torch.ones(3, 3) # probability of drawing &quot;1&quot; is 1
&gt;&gt;&gt; torch.bernoulli(a)
tensor([[ 1.,  1.,  1.],
        [ 1.,  1.,  1.],
        [ 1.,  1.,  1.]])
&gt;&gt;&gt; a = torch.zeros(3, 3) # probability of drawing &quot;1&quot; is 0
&gt;&gt;&gt; torch.bernoulli(a)
tensor([[ 0.,  0.,  0.],
        [ 0.,  0.,  0.],
        [ 0.,  0.,  0.]])

</code></pre>
<hr />
<pre><code>torch.multinomial(input, num_samples, replacement=False, *, generator=None, out=None) → LongTensor¶
</code></pre>
<p>返回一个张量，其中每行包含<code>num_samples</code>索引，这些索引是从位于张量<code>input</code>的相应行中的多项式概率分布中采样的。</p>
<p>Note</p>
<p><code>input</code>的行不需要加总为 1(在这种情况下，我们将这些值用作权重），但必须为非负数，有限且总和为非零。</p>
<p>根据每个样本的采样时间，索引从左到右排序(第一个样本放在第一列中）。</p>
<p>如果<code>input</code>是向量，则<code>out</code>是大小<code>num_samples</code>的向量。</p>
<p>如果<code>input</code>是具有 &lt;cite&gt;m&lt;/cite&gt; 行的矩阵，则<code>out</code>是形状<img alt="" src="../img/5d2de3653458e6a14a8a4f0fa87b3ad1.jpg" />的矩阵。</p>
<p>如果替换为<code>True</code>，则抽取样本进行替换。</p>
<p>如果没有，则它们将被替换而不会被绘制，这意味着当为一行绘制样本索引时，无法为该行再次绘制它。</p>
<p>Note</p>
<p>如果绘制时不进行替换，则<code>num_samples</code>必须小于<code>input</code>中非零元素的数目(如果是矩阵，则必须小于<code>input</code>每行中非零元素的最小数目）。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–包含概率的输入张量</p>
</li>
<li>
<p><strong>num_samples</strong>  (<em>python：int</em> )–要绘制的样本数</p>
</li>
<li>
<p><strong>替换</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–是否使用替换绘制</p>
</li>
<li>
<p><strong>generator</strong> (<code>torch.Generator</code>, optional) – a pseudorandom number generator for sampling</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; weights = torch.tensor([0, 10, 3, 0], dtype=torch.float) # create a tensor of weights
&gt;&gt;&gt; torch.multinomial(weights, 2)
tensor([1, 2])
&gt;&gt;&gt; torch.multinomial(weights, 4) # ERROR!
RuntimeError: invalid argument 2: invalid multinomial distribution (with replacement=False,
not enough non-negative category to sample) at ../aten/src/TH/generic/THTensorRandom.cpp:320
&gt;&gt;&gt; torch.multinomial(weights, 4, replacement=True)
tensor([ 2,  1,  1,  1])

</code></pre>
<hr />
<pre><code>torch.normal()¶
</code></pre>
<hr />
<pre><code>torch.normal(mean, std, *, generator=None, out=None) → Tensor
</code></pre>
<p>返回从均值和标准差给出的独立正态分布中得出的随机数张量。</p>
<p><a href="#torch.mean" title="torch.mean"><code>mean</code></a> 是一个张量，每个输出元素的正态分布均值</p>
<p><a href="#torch.std" title="torch.std"><code>std</code></a> 是一个张量，每个输出元素的正态分布的标准偏差</p>
<p><a href="#torch.mean" title="torch.mean"><code>mean</code></a> 和 <a href="#torch.std" title="torch.std"><code>std</code></a> 的形状不需要匹配，但是每个张量中元素的总数必须相同。</p>
<p>Note</p>
<p>当形状不匹配时，将 <a href="#torch.mean" title="torch.mean"><code>mean</code></a> 的形状用作返回的输出张量的形状</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>均值</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–每个元素均值的张量</p>
</li>
<li>
<p><strong>std</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–每个元素的标准偏差张量</p>
</li>
<li>
<p><strong>generator</strong> (<code>torch.Generator</code>, optional) – a pseudorandom number generator for sampling</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.normal(mean=torch.arange(1., 11.), std=torch.arange(1, 0, -0.1))
tensor([  1.0425,   3.5672,   2.7969,   4.2925,   4.7229,   6.2134,
          8.0505,   8.1408,   9.0563,  10.0566])

</code></pre>
<hr />
<pre><code>torch.normal(mean=0.0, std, out=None) → Tensor
</code></pre>
<p>与上面的功能相似，但均值在所有绘制的元素之间共享。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>平均值</strong> (<em>python：float</em> <em>，</em> <em>可选</em>）–所有分布的平均值</p>
</li>
<li>
<p><strong>std</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor of per-element standard deviations</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.normal(mean=0.5, std=torch.arange(1., 6.))
tensor([-1.2793, -1.0732, -2.0687,  5.1177, -1.2303])

</code></pre>
<hr />
<pre><code>torch.normal(mean, std=1.0, out=None) → Tensor
</code></pre>
<p>与上面的函数相似，但是标准偏差在所有绘制的元素之间共享。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>mean</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor of per-element means</p>
</li>
<li>
<p><strong>std</strong>  (<em>python：float</em> <em>，</em> <em>可选</em>）–所有发行版的标准差</p>
</li>
<li>
<p><strong>out</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>，</em> <em>可选</em>）–输出张量</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.normal(mean=torch.arange(1., 6.))
tensor([ 1.1552,  2.6148,  2.6535,  5.8318,  4.2361])

</code></pre>
<hr />
<pre><code>torch.normal(mean, std, size, *, out=None) → Tensor
</code></pre>
<p>与上述功能相似，但均值和标准差在所有绘制的元素之间共享。 所得张量的大小由<code>size</code>给出。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>平均值</strong> (<em>python：float</em> )–所有分布的平均值</p>
</li>
<li>
<p><strong>std</strong>  (<em>python：float</em> )–所有分布的标准偏差</p>
</li>
<li>
<p><strong>大小</strong> (<em>python：int ...</em> )–定义输出张量形状的整数序列。</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.normal(2, 3, size=(1, 4))
tensor([[-1.3987, -1.9544,  3.6048,  0.7909]])

</code></pre>
<hr />
<pre><code>torch.rand(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor¶
</code></pre>
<p>从区间<img alt="" src="../img/22e975ba7fcff9173338360452b96179.jpg" />返回均匀分布的随机张量</p>
<p>张量的形状由变量参数<code>size</code>定义。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>size</strong> (<em>python:int...</em>) – a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>).</p>
</li>
<li>
<p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) – the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.rand(4)
tensor([ 0.5204,  0.2503,  0.3525,  0.5673])
&gt;&gt;&gt; torch.rand(2, 3)
tensor([[ 0.8237,  0.5781,  0.6879],
        [ 0.3816,  0.7249,  0.0998]])

</code></pre>
<hr />
<pre><code>torch.rand_like(input, dtype=None, layout=None, device=None, requires_grad=False) → Tensor¶
</code></pre>
<p>返回与<code>input</code>大小相同的张量，该张量由间隔<img alt="" src="../img/22e975ba7fcff9173338360452b96179.jpg" />上均匀分布的随机数填充。 <code>torch.rand_like(input)</code>等效于<code>torch.rand(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)</code>。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the size of <code>input</code> will determine size of the output tensor.</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned Tensor. Default: if <code>None</code>, defaults to the dtype of <code>input</code>.</p>
</li>
<li>
<p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) – the desired layout of returned tensor. Default: if <code>None</code>, defaults to the layout of <code>input</code>.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, defaults to the device of <code>input</code>.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<hr />
<pre><code>torch.randint(low=0, high, size, *, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor¶
</code></pre>
<p>返回一个由在<code>low</code>(包括）和<code>high</code>(不包括）之间均匀生成的随机整数填充的张量。</p>
<p>The shape of the tensor is defined by the variable argument <code>size</code>.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>低</strong> (<em>python：int</em> <em>，</em> <em>可选</em>）–从分布中得出的最低整数。 默认值：0</p>
</li>
<li>
<p><strong>高</strong> (<em>python：int</em> )–从分布中得出的最高整数之上一个。</p>
</li>
<li>
<p><strong>大小</strong>(<em>元组</em>）–定义输出张量形状的元组。</p>
</li>
<li>
<p><strong>generator</strong> (<code>torch.Generator</code>, optional) – a pseudorandom number generator for sampling</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>).</p>
</li>
<li>
<p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) – the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.randint(3, 5, (3,))
tensor([4, 3, 4])

&gt;&gt;&gt; torch.randint(10, (2, 2))
tensor([[0, 2],
        [5, 5]])

&gt;&gt;&gt; torch.randint(3, 10, (2, 2))
tensor([[4, 5],
        [6, 7]])

</code></pre>
<hr />
<pre><code>torch.randint_like(input, low=0, high, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor¶
</code></pre>
<p>返回具有与张量<code>input</code>相同形状的张量，其中填充了在<code>low</code>(包括）和<code>high</code>(排除）之间均匀生成的随机整数。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the size of <code>input</code> will determine size of the output tensor.</p>
</li>
<li>
<p><strong>low</strong> (<em>python:int__,</em> <em>optional</em>) – Lowest integer to be drawn from the distribution. Default: 0.</p>
</li>
<li>
<p><strong>high</strong> (<em>python:int</em>) – One above the highest integer to be drawn from the distribution.</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned Tensor. Default: if <code>None</code>, defaults to the dtype of <code>input</code>.</p>
</li>
<li>
<p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) – the desired layout of returned tensor. Default: if <code>None</code>, defaults to the layout of <code>input</code>.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, defaults to the device of <code>input</code>.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<hr />
<pre><code>torch.randn(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor¶
</code></pre>
<p>从平均值为 &lt;cite&gt;0&lt;/cite&gt; ，方差为 &lt;cite&gt;1&lt;/cite&gt; 的正态分布中返回一个填充有随机数的张量(也称为标准正态分布）。</p>
<p><img alt="" src="../img/6a5aeab1deaf496af03eb65c0690d32b.jpg" /></p>
<p>The shape of the tensor is defined by the variable argument <code>size</code>.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>size</strong> (<em>python:int...</em>) – a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>).</p>
</li>
<li>
<p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) – the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.randn(4)
tensor([-2.1436,  0.9966,  2.3426, -0.6366])
&gt;&gt;&gt; torch.randn(2, 3)
tensor([[ 1.5954,  2.8929, -1.0923],
        [ 1.1719, -0.4709, -0.1996]])

</code></pre>
<hr />
<pre><code>torch.randn_like(input, dtype=None, layout=None, device=None, requires_grad=False) → Tensor¶
</code></pre>
<p>返回一个与<code>input</code>相同大小的张量，该张量由均值 0 和方差 1 的正态分布的随机数填充。<code>torch.randn_like(input)</code>等效于<code>torch.randn(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)</code>。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the size of <code>input</code> will determine size of the output tensor.</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned Tensor. Default: if <code>None</code>, defaults to the dtype of <code>input</code>.</p>
</li>
<li>
<p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) – the desired layout of returned tensor. Default: if <code>None</code>, defaults to the layout of <code>input</code>.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, defaults to the device of <code>input</code>.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<hr />
<pre><code>torch.randperm(n, out=None, dtype=torch.int64, layout=torch.strided, device=None, requires_grad=False) → LongTensor¶
</code></pre>
<p>返回从<code>0</code>到<code>n - 1</code>的整数的随机排列。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>n</strong>  (<em>python：int</em> )–上限(不包括）</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
<li>
<p><strong>dtype</strong>  (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> ，可选）–返回张量的所需数据类型。 默认值：<code>torch.int64</code>。</p>
</li>
<li>
<p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) – the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.randperm(4)
tensor([2, 1, 0, 3])

</code></pre>
<h3 id="_6">就地随机抽样</h3>
<p>在 Tensor 上还定义了一些就地随机采样函数。 单击以查看其文档：</p>
<ul>
<li>
<p><a href="tensors.html#torch.Tensor.bernoulli_" title="torch.Tensor.bernoulli_"><code>torch.Tensor.bernoulli_()</code></a> - <a href="#torch.bernoulli" title="torch.bernoulli"><code>torch.bernoulli()</code></a> 的就地版本</p>
</li>
<li>
<p><a href="tensors.html#torch.Tensor.cauchy_" title="torch.Tensor.cauchy_"><code>torch.Tensor.cauchy_()</code></a> -从柯西分布中得出的数字</p>
</li>
<li>
<p><a href="tensors.html#torch.Tensor.exponential_" title="torch.Tensor.exponential_"><code>torch.Tensor.exponential_()</code></a> -从指数分布中得出的数字</p>
</li>
<li>
<p><a href="tensors.html#torch.Tensor.geometric_" title="torch.Tensor.geometric_"><code>torch.Tensor.geometric_()</code></a> -从几何分布中绘制的元素</p>
</li>
<li>
<p><a href="tensors.html#torch.Tensor.log_normal_" title="torch.Tensor.log_normal_"><code>torch.Tensor.log_normal_()</code></a> -来自对数正态分布的样本</p>
</li>
<li>
<p><a href="tensors.html#torch.Tensor.normal_" title="torch.Tensor.normal_"><code>torch.Tensor.normal_()</code></a> - <a href="#torch.normal" title="torch.normal"><code>torch.normal()</code></a> 的就地版本</p>
</li>
<li>
<p><a href="tensors.html#torch.Tensor.random_" title="torch.Tensor.random_"><code>torch.Tensor.random_()</code></a> -从离散均匀分布中采样的数字</p>
</li>
<li>
<p><a href="tensors.html#torch.Tensor.uniform_" title="torch.Tensor.uniform_"><code>torch.Tensor.uniform_()</code></a> -从连续均匀分布中采样的数字</p>
</li>
</ul>
<h3 id="_7">准随机抽样</h3>
<hr />
<pre><code>class torch.quasirandom.SobolEngine(dimension, scramble=False, seed=None)¶
</code></pre>
<p><a href="#torch.quasirandom.SobolEngine" title="torch.quasirandom.SobolEngine"><code>torch.quasirandom.SobolEngine</code></a> 是用于生成(加扰）Sobol 序列的引擎。 Sobol 序列是低差异准随机序列的一个示例。</p>
<p>用于 Sobol 序列的引擎的这种实现方式能够对最大维度为 1111 的序列进行采样。它使用方向编号生成这些序列，并且这些编号已从<a href="http://web.maths.unsw.edu.au/~fkuo/sobol/joe-kuo-old.1111">此处</a>改编而来。</p>
<p>参考文献</p>
<ul>
<li>
<p>Art B. Owen。 争夺 Sobol 和 Niederreiter-Xing 点。 复杂性杂志，14(4）：466-489，1998 年 12 月。</p>
</li>
<li>
<p>I. M. Sobol。 立方体中点的分布和积分的准确评估。 嗯 Vychisl。 垫。 我在。 Phys。，7：784-802，1967。</p>
</li>
</ul>
<p>Parameters</p>
<ul>
<li>
<p><strong>尺寸</strong> (<em>Int</em> )–要绘制的序列的尺寸</p>
</li>
<li>
<p><strong>扰乱</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–将其设置为<code>True</code>将产生扰乱的 Sobol 序列。 加扰能够产生更好的 Sobol 序列。 默认值：<code>False</code>。</p>
</li>
<li>
<p><strong>种子</strong> (<em>Int</em> <em>，</em> <em>可选</em>）–这是加扰的种子。 如果指定，则将随机数生成器的种子设置为此。 否则，它将使用随机种子。 默认值：<code>None</code></p>
</li>
</ul>
<p>例子：</p>
<pre><code>&gt;&gt;&gt; soboleng = torch.quasirandom.SobolEngine(dimension=5)
&gt;&gt;&gt; soboleng.draw(3)
tensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
        [0.7500, 0.2500, 0.7500, 0.2500, 0.7500],
        [0.2500, 0.7500, 0.2500, 0.7500, 0.2500]])

</code></pre>
<hr />
<pre><code>draw(n=1, out=None, dtype=torch.float32)¶
</code></pre>
<p>从 Sobol 序列中绘制<code>n</code>点序列的功能。 请注意，样本取决于先前的样本。 结果的大小为<img alt="" src="../img/4a928fac0225cfecb1270d6afba7caf9.jpg" />。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>n</strong>  (<em>Int</em> <em>，</em> <em>可选</em>）–绘制点序列的长度。 默认值：1</p>
</li>
<li>
<p><strong>out</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>，</em> <em>可选</em>）–输出张量</p>
</li>
<li>
<p><strong>dtype</strong> (<code>torch.dtype</code>，可选）–返回的张量的所需数据类型。 默认值：<code>torch.float32</code></p>
</li>
</ul>
<hr />
<pre><code>fast_forward(n)¶
</code></pre>
<p>通过<code>n</code>步骤快速前进<code>SobolEngine</code>状态的功能。 这等效于不使用样本绘制<code>n</code>样本。</p>
<p>Parameters</p>
<p><strong>n</strong>  (<em>Int</em> )–快进的步数。</p>
<hr />
<pre><code>reset()¶
</code></pre>
<p>将<code>SobolEngine</code>重置为基本状态的功能。</p>
<h2 id="_8">序列化</h2>
<hr />
<pre><code>torch.save(obj, f, pickle_module=&lt;module 'pickle' from '/opt/conda/lib/python3.6/pickle.py'&gt;, pickle_protocol=2, _use_new_zipfile_serialization=False)¶
</code></pre>
<p>将对象保存到磁盘文件。</p>
<p>另请参见：<a href="notes/serialization.html#recommend-saving-models">推荐的模型保存方法</a></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>obj</strong> –保存的对象</p>
</li>
<li>
<p><strong>f</strong> –类似于文件的对象(必须实现写入和刷新）或包含文件名的字符串</p>
</li>
<li>
<p><strong>pickle_module</strong> –用于腌制元数据和对象的模块</p>
</li>
<li>
<p><strong>pickle_protocol</strong> –可以指定为覆盖默认协议</p>
</li>
</ul>
<p>Warning</p>
<p>如果使用的是 Python 2，则 <a href="#torch.save" title="torch.save"><code>torch.save()</code></a> 不支持<code>StringIO.StringIO</code>作为有效的类似文件的对象。 这是因为 write 方法应返回写入的字节数； <code>StringIO.write()</code>不这样做。</p>
<p>请改用<code>io.BytesIO</code>之类的东西。</p>
<p>例</p>
<pre><code>&gt;&gt;&gt; # Save to file
&gt;&gt;&gt; x = torch.tensor([0, 1, 2, 3, 4])
&gt;&gt;&gt; torch.save(x, 'tensor.pt')
&gt;&gt;&gt; # Save to io.BytesIO buffer
&gt;&gt;&gt; buffer = io.BytesIO()
&gt;&gt;&gt; torch.save(x, buffer)

</code></pre>
<hr />
<pre><code>torch.load(f, map_location=None, pickle_module=&lt;module 'pickle' from '/opt/conda/lib/python3.6/pickle.py'&gt;, **pickle_load_args)¶
</code></pre>
<p>从文件加载用 <a href="#torch.save" title="torch.save"><code>torch.save()</code></a> 保存的对象。</p>
<p><a href="#torch.load" title="torch.load"><code>torch.load()</code></a> 使用 Python 的解开工具，但会特别处理位于张量之下的存储。 它们首先在 CPU 上反序列化，然后移到保存它们的设备上。 如果失败(例如，因为运行时系统没有某些设备），则会引发异常。 但是，可以使用<code>map_location</code>参数将存储动态重新映射到一组备用设备。</p>
<p>如果<code>map_location</code>是可调用的，则将为每个序列化存储调用一次，并带有两个参数：storage 和 location。 storage 参数将是驻留在 CPU 上的存储的初始反序列化。 每个序列化存储都有一个与之关联的位置标签，该标签标识了从中进行保存的设备，该标签是传递给<code>map_location</code>的第二个参数。 内置位置标签是用于 CPU 张量的<code>'cpu'</code>和用于 CUDA 张量的<code>'cuda:device_id'</code>(例如<code>'cuda:2'</code>）。 <code>map_location</code>应该返回<code>None</code>或存储。 如果<code>map_location</code>返回存储，它将用作最终反序列化的对象，已经移至正确的设备。 否则， <a href="#torch.load" title="torch.load"><code>torch.load()</code></a> 将退回到默认行为，就像未指定<code>map_location</code>一样。</p>
<p>如果<code>map_location</code>是 <a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a> 对象或与设备标签冲突的字符串，则它指示应加载所有张量的位置。</p>
<p>否则，如果<code>map_location</code>是字典，它将用于将文件(键）中出现的位置标签重新映射到指定将存储位置(值）放置的位置标签。</p>
<p>用户扩展可以使用<code>torch.serialization.register_package()</code>注册自己的位置标签以及标记和反序列化方法。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>f</strong> –类似于文件的对象(必须实现<code>read()</code>，：meth<code>readline</code>，：meth<code>tell</code>和：meth<code>seek</code>）或包含文件名的字符串</p>
</li>
<li>
<p><strong>map_location</strong> –函数， <a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a> ，字符串或指定如何重新映射存储位置的字典</p>
</li>
<li>
<p><strong>pickle_module</strong> –用于解开元数据和对象的模块(必须与用于序列化文件的<code>pickle_module</code>匹配）</p>
</li>
<li>
<p><strong>pickle_load_args</strong> –(仅适用于 Python 3）可选关键字参数传递给<code>pickle_module.load()</code>和<code>pickle_module.Unpickler()</code>，例如<code>errors=...</code>。</p>
</li>
</ul>
<p>Note</p>
<p>当您在包含 GPU 张量的文件上调用 <a href="#torch.load" title="torch.load"><code>torch.load()</code></a> 时，这些张量将默认加载到 GPU。 您可以先调用<code>torch.load(.., map_location='cpu')</code>，然后再调用<code>load_state_dict()</code>，以避免在加载模型检查点时 GPU RAM 激增。</p>
<p>Note</p>
<p>默认情况下，我们将字节字符串解码为<code>utf-8</code>。 这是为了避免在 Python 3 中加载 Python 2 保存的文件时出现常见错误情况<code>UnicodeDecodeError: 'ascii' codec can't decode byte 0x...</code>。如果此默认设置不正确，则可以使用额外的<code>encoding</code>关键字参数来指定应如何加载这些对象，例如<code>encoding='latin1'</code>使用<code>latin1</code>编码将它们解码为字符串，<code>encoding='bytes'</code>将它们保留为字节数组，以后可以使用<code>byte_array.decode(...)</code>进行解码。</p>
<p>Example</p>
<pre><code>&gt;&gt;&gt; torch.load('tensors.pt')
# Load all tensors onto the CPU
&gt;&gt;&gt; torch.load('tensors.pt', map_location=torch.device('cpu'))
# Load all tensors onto the CPU, using a function
&gt;&gt;&gt; torch.load('tensors.pt', map_location=lambda storage, loc: storage)
# Load all tensors onto GPU 1
&gt;&gt;&gt; torch.load('tensors.pt', map_location=lambda storage, loc: storage.cuda(1))
# Map tensors from GPU 1 to GPU 0
&gt;&gt;&gt; torch.load('tensors.pt', map_location={'cuda:1':'cuda:0'})
# Load tensor from io.BytesIO object
&gt;&gt;&gt; with open('tensor.pt', 'rb') as f:
        buffer = io.BytesIO(f.read())
&gt;&gt;&gt; torch.load(buffer)
# Load a module with 'ascii' encoding for unpickling
&gt;&gt;&gt; torch.load('module.pt', encoding='ascii')

</code></pre>
<h2 id="_9">平行性</h2>
<hr />
<pre><code>torch.get_num_threads() → int¶
</code></pre>
<p>返回用于并行化 CPU 操作的线程数</p>
<hr />
<pre><code>torch.set_num_threads(int)¶
</code></pre>
<p>设置用于 CPU 上的内部运算并行的线程数。 警告：为确保使用正确的线程数，必须在运行 eager，JIT 或 autograd 代码之前调用 set_num_threads。</p>
<hr />
<pre><code>torch.get_num_interop_threads() → int¶
</code></pre>
<p>返回用于 CPU 上的互操作并行的线程数(例如，在 JIT 解释器中）</p>
<hr />
<pre><code>torch.set_num_interop_threads(int)¶
</code></pre>
<p>设置用于 CPU 上的互操作并行性(例如，在 JIT 解释器中）的线程数。 警告：只能在一次操作间并行工作开始之前(例如 JIT 执行）调用一次。</p>
<h2 id="_10">局部禁用梯度计算</h2>
<p>上下文管理器<code>torch.no_grad()</code>，<code>torch.enable_grad()</code>和<code>torch.set_grad_enabled()</code>有助于局部禁用和启用梯度计算。 有关其用法的更多详细信息，请参见<a href="autograd.html#locally-disable-grad">局部禁用梯度计算</a>。 这些上下文管理器是线程本地的，因此如果您使用<code>threading</code>模块等将工作发送到另一个线程，它们将无法工作。</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; x = torch.zeros(1, requires_grad=True)
&gt;&gt;&gt; with torch.no_grad():
...     y = x * 2
&gt;&gt;&gt; y.requires_grad
False

&gt;&gt;&gt; is_train = False
&gt;&gt;&gt; with torch.set_grad_enabled(is_train):
...     y = x * 2
&gt;&gt;&gt; y.requires_grad
False

&gt;&gt;&gt; torch.set_grad_enabled(True)  # this can also be used as a function
&gt;&gt;&gt; y = x * 2
&gt;&gt;&gt; y.requires_grad
True

&gt;&gt;&gt; torch.set_grad_enabled(False)
&gt;&gt;&gt; y = x * 2
&gt;&gt;&gt; y.requires_grad
False

</code></pre>
<h2 id="_11">数学运算</h2>
<h3 id="_12">逐点操作</h3>
<hr />
<pre><code>torch.abs(input, out=None) → Tensor¶
</code></pre>
<p>计算给定<code>input</code>张量的按元素的绝对值。</p>
<p><img alt="" src="../img/726d369abb9c76e751e626cbfef10220.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.abs(torch.tensor([-1, -2, 3]))
tensor([ 1,  2,  3])

</code></pre>
<hr />
<pre><code>torch.acos(input, out=None) → Tensor¶
</code></pre>
<p>返回带有<code>input</code>元素的反余弦的新张量。</p>
<p><img alt="" src="../img/25323a5363649a36549d717d5a3cbc3e.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([ 0.3348, -0.5889,  0.2005, -0.1584])
&gt;&gt;&gt; torch.acos(a)
tensor([ 1.2294,  2.2004,  1.3690,  1.7298])

</code></pre>
<hr />
<pre><code>torch.add()¶
</code></pre>
<hr />
<pre><code>torch.add(input, other, out=None)
</code></pre>
<p>将标量<code>other</code>添加到输入<code>input</code>的每个元素中，并返回一个新的结果张量。</p>
<p><img alt="" src="../img/1e18f010799098c127bab9465476d1fb.jpg" /></p>
<p>如果<code>input</code>的类型为 FloatTensor 或 DoubleTensor，则<code>other</code>必须为实数，否则应为整数。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>值</strong>(<em>编号</em>）–要添加到<code>input</code>每个元素的编号</p>
</li>
</ul>
<pre><code>Keyword Arguments
</code></pre>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([ 0.0202,  1.0985,  1.3506, -0.6056])
&gt;&gt;&gt; torch.add(a, 20)
tensor([ 20.0202,  21.0985,  21.3506,  19.3944])

</code></pre>
<hr />
<pre><code>torch.add(input, alpha=1, other, out=None)
</code></pre>
<p>张量<code>other</code>的每个元素乘以标量<code>alpha</code>，然后加到张量<code>input</code>的每个元素上。 返回结果张量。</p>
<p><code>input</code>和<code>other</code>的形状必须是<a href="notes/broadcasting.html#broadcasting-semantics">可广播的</a>。</p>
<p><img alt="" src="../img/5a5bc06d446342edf134fb0e87f755ec.jpg" /></p>
<p>如果<code>other</code>的类型为 FloatTensor 或 DoubleTensor，则<code>alpha</code>必须为实数，否则应为整数。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–第一个输入张量</p>
</li>
<li>
<p><strong>alpha</strong> (<em>数字</em>）– <code>other</code>的标量乘法器</p>
</li>
<li>
<p><strong>其他</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–第二个输入张量</p>
</li>
</ul>
<pre><code>Keyword Arguments
</code></pre>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([-0.9732, -0.3497,  0.6245,  0.4022])
&gt;&gt;&gt; b = torch.randn(4, 1)
&gt;&gt;&gt; b
tensor([[ 0.3743],
        [-1.7724],
        [-0.5811],
        [-0.8017]])
&gt;&gt;&gt; torch.add(a, 10, b)
tensor([[  2.7695,   3.3930,   4.3672,   4.1450],
        [-18.6971, -18.0736, -17.0994, -17.3216],
        [ -6.7845,  -6.1610,  -5.1868,  -5.4090],
        [ -8.9902,  -8.3667,  -7.3925,  -7.6147]])

</code></pre>
<hr />
<pre><code>torch.addcdiv(input, value=1, tensor1, tensor2, out=None) → Tensor¶
</code></pre>
<p>执行<code>tensor1</code>除以<code>tensor2</code>的元素，将结果乘以标量<code>value</code>并将其加到<code>input</code>上。</p>
<p><img alt="" src="../img/69053b1e1ce2faac8aae0ee4370775b9.jpg" /></p>
<p><code>input</code>，<code>tensor1</code>和<code>tensor2</code>的形状必须是<a href="notes/broadcasting.html#broadcasting-semantics">可广播</a>。</p>
<p>对于类型为 &lt;cite&gt;FloatTensor&lt;/cite&gt; 或 &lt;cite&gt;DoubleTensor&lt;/cite&gt; 的输入，<code>value</code>必须为实数，否则为整数。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要添加的张量</p>
</li>
<li>
<p><strong>值</strong>(<em>编号</em> <em>，</em> <em>可选</em>）– <img alt="" src="../img/7adafc45ae24ad65f276f988f5b53f3f.jpg" />的乘数</p>
</li>
<li>
<p><strong>张量 1</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–分子张量</p>
</li>
<li>
<p><strong>张量 2</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–分母张量</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; t = torch.randn(1, 3)
&gt;&gt;&gt; t1 = torch.randn(3, 1)
&gt;&gt;&gt; t2 = torch.randn(1, 3)
&gt;&gt;&gt; torch.addcdiv(t, 0.1, t1, t2)
tensor([[-0.2312, -3.6496,  0.1312],
        [-1.0428,  3.4292, -0.1030],
        [-0.5369, -0.9829,  0.0430]])

</code></pre>
<hr />
<pre><code>torch.addcmul(input, value=1, tensor1, tensor2, out=None) → Tensor¶
</code></pre>
<p>对<code>tensor1</code>与<code>tensor2</code>进行元素逐项乘法，将结果与标量<code>value</code>相乘，然后将其与<code>input</code>相加。</p>
<p><img alt="" src="../img/6bfdc94f1a8a13ee1a6ebc9af2d74fda.jpg" /></p>
<p><a href="#torch.tensor" title="torch.tensor"><code>tensor</code></a> ，<code>tensor1</code>和<code>tensor2</code>的形状必须是<a href="notes/broadcasting.html#broadcasting-semantics">可广播的</a>。</p>
<p>For inputs of type &lt;cite&gt;FloatTensor&lt;/cite&gt; or &lt;cite&gt;DoubleTensor&lt;/cite&gt;, <code>value</code> must be a real number, otherwise an integer.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor to be added</p>
</li>
<li>
<p><strong>值</strong>(<em>编号</em> <em>，</em> <em>可选</em>）– <img alt="" src="../img/25f6a482a153e2aed6c16040677c1f5e.jpg" />的乘数</p>
</li>
<li>
<p><strong>张量 1</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要相乘的张量</p>
</li>
<li>
<p><strong>张量 2</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要相乘的张量</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; t = torch.randn(1, 3)
&gt;&gt;&gt; t1 = torch.randn(3, 1)
&gt;&gt;&gt; t2 = torch.randn(1, 3)
&gt;&gt;&gt; torch.addcmul(t, 0.1, t1, t2)
tensor([[-0.8635, -0.6391,  1.6174],
        [-0.7617, -0.5879,  1.7388],
        [-0.8353, -0.6249,  1.6511]])

</code></pre>
<hr />
<pre><code>torch.angle(input, out=None) → Tensor¶
</code></pre>
<p>计算给定<code>input</code>张量的元素方向角(以弧度为单位）。</p>
<p><img alt="" src="../img/5af8c34b3d4c490c508caf11a458d5d6.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.angle(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))*180/3.14159
tensor([ 135.,  135,  -45])

</code></pre>
<hr />
<pre><code>torch.asin(input, out=None) → Tensor¶
</code></pre>
<p>返回带有<code>input</code>元素的反正弦值的新张量。</p>
<p><img alt="" src="../img/fa8ed00ae85106eea8e8a11b6b66d898.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([-0.5962,  1.4985, -0.4396,  1.4525])
&gt;&gt;&gt; torch.asin(a)
tensor([-0.6387,     nan, -0.4552,     nan])

</code></pre>
<hr />
<pre><code>torch.atan(input, out=None) → Tensor¶
</code></pre>
<p>返回带有<code>input</code>元素的反正切的新张量。</p>
<p><img alt="" src="../img/8614369d5e7413ce21e9a5d6bcf786cb.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([ 0.2341,  0.2539, -0.6256, -0.6448])
&gt;&gt;&gt; torch.atan(a)
tensor([ 0.2299,  0.2487, -0.5591, -0.5727])

</code></pre>
<hr />
<pre><code>torch.atan2(input, other, out=None) → Tensor¶
</code></pre>
<p>考虑象限的<img alt="" src="../img/c45e59e7ab5c2f7cff1ff37748c0d62b.jpg" />元素逐级反正切。 返回一个新的张量，其矢量<img alt="" src="../img/2cb0a971340ffd46996a0cf9eb81d376.jpg" />与矢量<img alt="" src="../img/8456c6ac83b24dbe917ff5a29a771bd7.jpg" />之间的弧度为符号角。 (请注意，第二个参数<img alt="" src="../img/4e409083ee35ea2eeab094ea4f9bf730.jpg" />是 x 坐标，而第一个参数<img alt="" src="../img/0c7468da87ed4da7f144e93eb6e7e60e.jpg" />是 y 坐标。）</p>
<p><code>input</code>和<code>other</code>的形状必须是<a href="notes/broadcasting.html#broadcasting-semantics">可广播的</a>。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the first input tensor</p>
</li>
<li>
<p><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the second input tensor</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([ 0.9041,  0.0196, -0.3108, -2.4423])
&gt;&gt;&gt; torch.atan2(a, torch.randn(4))
tensor([ 0.9833,  0.0811, -1.9743, -1.4151])

</code></pre>
<hr />
<pre><code>torch.bitwise_not(input, out=None) → Tensor¶
</code></pre>
<p>计算给定输入张量的按位非。 输入张量必须是整数或布尔类型。 对于布尔张量，它计算逻辑非。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example</p>
<pre><code>&gt;&gt;&gt; torch.bitwise_not(torch.tensor([-1, -2, 3], dtype=torch.int8))
tensor([ 0,  1, -4], dtype=torch.int8)

</code></pre>
<hr />
<pre><code>torch.bitwise_xor(input, other, out=None) → Tensor¶
</code></pre>
<p>计算<code>input</code>和<code>other</code>的按位 XOR。 输入张量必须是整数或布尔类型。 对于布尔张量，它计算逻辑 XOR。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> –第一个输入张量</p>
</li>
<li>
<p><strong>其他</strong> –第二个输入张量</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example</p>
<pre><code>&gt;&gt;&gt; torch.bitwise_xor(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))
tensor([-2, -2,  0], dtype=torch.int8)
&gt;&gt;&gt; torch.bitwise_xor(torch.tensor([True, True, False]), torch.tensor([False, True, False]))
tensor([ True, False, False])

</code></pre>
<hr />
<pre><code>torch.ceil(input, out=None) → Tensor¶
</code></pre>
<p>返回带有<code>input</code>元素的 ceil 的新张量，该元素大于或等于每个元素的最小整数。</p>
<p><img alt="" src="../img/3425145dda63005bbc6ff0b2f0331aa7.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([-0.6341, -1.4208, -1.0900,  0.5826])
&gt;&gt;&gt; torch.ceil(a)
tensor([-0., -1., -1.,  1.])

</code></pre>
<hr />
<pre><code>torch.clamp(input, min, max, out=None) → Tensor¶
</code></pre>
<p>将<code>input</code>中的所有元素限制在 &lt;cite&gt;[&lt;/cite&gt; <a href="#torch.min" title="torch.min"><code>min</code></a> ， <a href="#torch.max" title="torch.max"><code>max</code></a> &lt;cite&gt;]&lt;/cite&gt; 范围内，并返回结果张量：</p>
<p><img alt="" src="../img/597a5259be5b1a330eaa6858b12b8994.jpg" /></p>
<p>如果<code>input</code>的类型为 &lt;cite&gt;FloatTensor&lt;/cite&gt; 或 &lt;cite&gt;DoubleTensor&lt;/cite&gt; ，则参数 <a href="#torch.min" title="torch.min"><code>min</code></a> 和 <a href="#torch.max" title="torch.max"><code>max</code></a> 必须为实数，否则为实数 应该是整数。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>min</strong> (<em>编号</em>）–要钳制的范围的下限</p>
</li>
<li>
<p><strong>最大</strong>(<em>编号</em>）–要钳位的范围的上限</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([-1.7120,  0.1734, -0.0478, -0.0922])
&gt;&gt;&gt; torch.clamp(a, min=-0.5, max=0.5)
tensor([-0.5000,  0.1734, -0.0478, -0.0922])

</code></pre>
<hr />
<pre><code>torch.clamp(input, *, min, out=None) → Tensor
</code></pre>
<p>将<code>input</code>中的所有元素限制为大于或等于 <a href="#torch.min" title="torch.min"><code>min</code></a> 。</p>
<p>如果<code>input</code>的类型为 &lt;cite&gt;FloatTensor&lt;/cite&gt; 或 &lt;cite&gt;DoubleTensor&lt;/cite&gt; ，则<code>value</code>应为实数，否则应为整数。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>值</strong>(<em>编号</em>）–输出中每个元素的最小值</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([-0.0299, -2.3184,  2.1593, -0.8883])
&gt;&gt;&gt; torch.clamp(a, min=0.5)
tensor([ 0.5000,  0.5000,  2.1593,  0.5000])

</code></pre>
<hr />
<pre><code>torch.clamp(input, *, max, out=None) → Tensor
</code></pre>
<p>将<code>input</code>中的所有元素限制为小于或等于 <a href="#torch.max" title="torch.max"><code>max</code></a> 。</p>
<p>If <code>input</code> is of type &lt;cite&gt;FloatTensor&lt;/cite&gt; or &lt;cite&gt;DoubleTensor&lt;/cite&gt;, <code>value</code> should be a real number, otherwise it should be an integer.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>值</strong>(<em>编号</em>）–输出中每个元素的最大值</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([ 0.7753, -0.4702, -0.4599,  1.1899])
&gt;&gt;&gt; torch.clamp(a, max=0.5)
tensor([ 0.5000, -0.4702, -0.4599,  0.5000])

</code></pre>
<hr />
<pre><code>torch.conj(input, out=None) → Tensor¶
</code></pre>
<p>计算给定<code>input</code>张量的逐元素共轭。</p>
<p><img alt="" src="../img/334473ca71a76cc4b45b56e05f9113be.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.conj(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))
tensor([-1 - 1j, -2 - 2j, 3 + 3j])

</code></pre>
<hr />
<pre><code>torch.cos(input, out=None) → Tensor¶
</code></pre>
<p>返回带有<code>input</code>元素的余弦的新张量。</p>
<p><img alt="" src="../img/a54751040e4e9bbc318ce13170d3575a.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([ 1.4309,  1.2706, -0.8562,  0.9796])
&gt;&gt;&gt; torch.cos(a)
tensor([ 0.1395,  0.2957,  0.6553,  0.5574])

</code></pre>
<hr />
<pre><code>torch.cosh(input, out=None) → Tensor¶
</code></pre>
<p>返回具有<code>input</code>元素的双曲余弦的新张量。</p>
<p><img alt="" src="../img/01d5c509dd9312bb70bb293dffb6ee74.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([ 0.1632,  1.1835, -0.6979, -0.7325])
&gt;&gt;&gt; torch.cosh(a)
tensor([ 1.0133,  1.7860,  1.2536,  1.2805])

</code></pre>
<hr />
<pre><code>torch.div()¶
</code></pre>
<hr />
<pre><code>torch.div(input, other, out=None) → Tensor
</code></pre>
<p>将输入<code>input</code>的每个元素除以标量<code>other</code>，然后返回一个新的结果张量。</p>
<p><img alt="" src="../img/b67b84d63ecd57a1b4241eca13fb1661.jpg" /></p>
<p>如果<code>input</code>和<code>other</code>的 <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> 不同，则根据类型提升<a href="tensor_attributes.html#type-promotion-doc">文档</a>中所述的规则确定结果张量的 <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> 。 ]。 如果指定了<code>out</code>，则结果必须是<a href="tensor_attributes.html#type-promotion-doc">可转换为</a>到指定输出张量的 <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> 。 整数除以零会导致不确定的行为。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>其他</strong>(<em>编号</em>）–要划分为<code>input</code>每个元素的编号</p>
</li>
</ul>
<pre><code>Keyword Arguments
</code></pre>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(5)
&gt;&gt;&gt; a
tensor([ 0.3810,  1.2774, -0.2972, -0.3719,  0.4637])
&gt;&gt;&gt; torch.div(a, 0.5)
tensor([ 0.7620,  2.5548, -0.5944, -0.7439,  0.9275])

</code></pre>
<hr />
<pre><code>torch.div(input, other, out=None) → Tensor
</code></pre>
<p>张量<code>input</code>的每个元素除以张量<code>other</code>的每个元素。 返回结果张量。</p>
<p><img alt="" src="../img/9fd0198f0d9df7531b6ea4ae384fbd53.jpg" /></p>
<p><code>input</code>和<code>other</code>的形状必须是<a href="notes/broadcasting.html#broadcasting-semantics">可广播</a>。 如果<code>input</code>和<code>other</code>的 <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> 不同，则根据类型提升<a href="tensor_attributes.html#type-promotion-doc">文档</a>中描述的规则确定结果张量的 <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> 。 ]。 如果指定了<code>out</code>，则结果必须是<a href="tensor_attributes.html#type-promotion-doc">可转换为</a>到指定输出张量的 <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> 。 整数除以零会导致不确定的行为。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–分子张量</p>
</li>
<li>
<p><strong>其他</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–分母张量</p>
</li>
</ul>
<pre><code>Keyword Arguments
</code></pre>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a
tensor([[-0.3711, -1.9353, -0.4605, -0.2917],
        [ 0.1815, -1.0111,  0.9805, -1.5923],
        [ 0.1062,  1.4581,  0.7759, -1.2344],
        [-0.1830, -0.0313,  1.1908, -1.4757]])
&gt;&gt;&gt; b = torch.randn(4)
&gt;&gt;&gt; b
tensor([ 0.8032,  0.2930, -0.8113, -0.2308])
&gt;&gt;&gt; torch.div(a, b)
tensor([[-0.4620, -6.6051,  0.5676,  1.2637],
        [ 0.2260, -3.4507, -1.2086,  6.8988],
        [ 0.1322,  4.9764, -0.9564,  5.3480],
        [-0.2278, -0.1068, -1.4678,  6.3936]])

</code></pre>
<hr />
<pre><code>torch.digamma(input, out=None) → Tensor¶
</code></pre>
<p>计算&lt;cite&gt;输入&lt;/cite&gt;上伽马函数的对数导数。</p>
<p><img alt="" src="../img/00133aac1a20067edb1ffa26d46a0311.jpg" /></p>
<p>Parameters</p>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–用于计算 digamma 函数的张量</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.tensor([1, 0.5])
&gt;&gt;&gt; torch.digamma(a)
tensor([-0.5772, -1.9635])

</code></pre>
<hr />
<pre><code>torch.erf(input, out=None) → Tensor¶
</code></pre>
<p>计算每个元素的误差函数。 错误函数定义如下：</p>
<p><img alt="" src="../img/8d655f076653c2f1d5a1b07d3f7437ea.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.erf(torch.tensor([0, -1., 10.]))
tensor([ 0.0000, -0.8427,  1.0000])

</code></pre>
<hr />
<pre><code>torch.erfc(input, out=None) → Tensor¶
</code></pre>
<p>计算<code>input</code>的每个元素的互补误差函数。 互补误差函数定义如下：</p>
<p><img alt="" src="../img/a58cfc335ee9ca516258ccbad689a9fd.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.erfc(torch.tensor([0, -1., 10.]))
tensor([ 1.0000, 1.8427,  0.0000])

</code></pre>
<hr />
<pre><code>torch.erfinv(input, out=None) → Tensor¶
</code></pre>
<p>计算<code>input</code>的每个元素的反误差函数。 逆误差函数在<img alt="" src="../img/cef0a53226b8b835c974e021239fb845.jpg" />范围内定义为：</p>
<p><img alt="" src="../img/9e2a92a8e980b6525bc4f1a045bc6f7e.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.erfinv(torch.tensor([0, 0.5, -1.]))
tensor([ 0.0000,  0.4769,    -inf])

</code></pre>
<hr />
<pre><code>torch.exp(input, out=None) → Tensor¶
</code></pre>
<p>返回具有输入张量<code>input</code>的元素指数的新张量。</p>
<p><img alt="" src="../img/89fdb5873c4e62755139600cbda25b05.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.exp(torch.tensor([0, math.log(2.)]))
tensor([ 1.,  2.])

</code></pre>
<hr />
<pre><code>torch.expm1(input, out=None) → Tensor¶
</code></pre>
<p>返回一个新的张量，其元素的指数为<code>input</code>的负 1。</p>
<p><img alt="" src="../img/87b2bec9f68b2f04352bccfeb8c06838.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.expm1(torch.tensor([0, math.log(2.)]))
tensor([ 0.,  1.])

</code></pre>
<hr />
<pre><code>torch.floor(input, out=None) → Tensor¶
</code></pre>
<p>返回一个新的张量，该张量的元素为<code>input</code>的下限，即小于或等于每个元素的最大整数。</p>
<p><img alt="" src="../img/660d522c6d1931985ef3cb2c38e0f843.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([-0.8166,  1.5308, -0.2530, -0.2091])
&gt;&gt;&gt; torch.floor(a)
tensor([-1.,  1., -1., -1.])

</code></pre>
<hr />
<pre><code>torch.fmod(input, other, out=None) → Tensor¶
</code></pre>
<p>计算除法元素的余数。</p>
<p>被除数和除数可以同时包含整数和浮点数。 其余部分与股息<code>input</code>具有相同的符号。</p>
<p>当<code>other</code>是张量时，<code>input</code>和<code>other</code>的形状必须是<a href="notes/broadcasting.html#broadcasting-semantics">可广播的</a>。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–股息</p>
</li>
<li>
<p><strong>其他</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>或</em> <em>python：float</em> )–除数，可以是数字或整数 与股息形状相同的张量</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.fmod(torch.tensor([-3., -2, -1, 1, 2, 3]), 2)
tensor([-1., -0., -1.,  1.,  0.,  1.])
&gt;&gt;&gt; torch.fmod(torch.tensor([1., 2, 3, 4, 5]), 1.5)
tensor([ 1.0000,  0.5000,  0.0000,  1.0000,  0.5000])

</code></pre>
<hr />
<pre><code>torch.frac(input, out=None) → Tensor¶
</code></pre>
<p>计算<code>input</code>中每个元素的分数部分。</p>
<p><img alt="" src="../img/fdd683e9c79ed73d211526708d082e20.jpg" /></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.frac(torch.tensor([1, 2.5, -3.2]))
tensor([ 0.0000,  0.5000, -0.2000])

</code></pre>
<hr />
<pre><code>torch.imag(input, out=None) → Tensor¶
</code></pre>
<p>计算给定<code>input</code>张量的逐元素 imag 值。</p>
<p><img alt="" src="../img/23bb4f65e036b1aa01e948c93438b146.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.imag(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))
tensor([ 1,  2,  -3])

</code></pre>
<hr />
<pre><code>torch.lerp(input, end, weight, out=None)¶
</code></pre>
<p>根据标量或张量<code>weight</code>对两个张量<code>start</code>(由<code>input</code>给出）和<code>end</code>进行线性插值，并返回所得的<code>out</code>张量。</p>
<p><img alt="" src="../img/02b2be2accc587016c8fb3f0c79c2b03.jpg" /></p>
<p><code>start</code>和<code>end</code>的形状必须是<a href="notes/broadcasting.html#broadcasting-semantics">可广播的</a>。 如果<code>weight</code>是张量，则<code>weight</code>，<code>start</code>和<code>end</code>的形状必须是<a href="notes/broadcasting.html#broadcasting-semantics">可广播的</a>。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–具有起点的张量</p>
</li>
<li>
<p><strong>末端</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–具有终点的张量</p>
</li>
<li>
<p><strong>权重</strong> (<em>python：float</em> <em>或</em> <em>tensor</em>）–插值公式的权重</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; start = torch.arange(1., 5.)
&gt;&gt;&gt; end = torch.empty(4).fill_(10)
&gt;&gt;&gt; start
tensor([ 1.,  2.,  3.,  4.])
&gt;&gt;&gt; end
tensor([ 10.,  10.,  10.,  10.])
&gt;&gt;&gt; torch.lerp(start, end, 0.5)
tensor([ 5.5000,  6.0000,  6.5000,  7.0000])
&gt;&gt;&gt; torch.lerp(start, end, torch.full_like(start, 0.5))
tensor([ 5.5000,  6.0000,  6.5000,  7.0000])

</code></pre>
<hr />
<pre><code>torch.lgamma(input, out=None) → Tensor¶
</code></pre>
<p>计算<code>input</code>上伽马函数的对数。</p>
<p><img alt="" src="../img/5bc7af83352d4911e4360a1a8b2c3427.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.arange(0.5, 2, 0.5)
&gt;&gt;&gt; torch.lgamma(a)
tensor([ 0.5724,  0.0000, -0.1208])

</code></pre>
<hr />
<pre><code>torch.log(input, out=None) → Tensor¶
</code></pre>
<p>返回具有<code>input</code>元素的自然对数的新张量。</p>
<p><img alt="" src="../img/6b7510d341734226d626b3362506887e.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(5)
&gt;&gt;&gt; a
tensor([-0.7168, -0.5471, -0.8933, -1.4428, -0.1190])
&gt;&gt;&gt; torch.log(a)
tensor([ nan,  nan,  nan,  nan,  nan])

</code></pre>
<hr />
<pre><code>torch.log10(input, out=None) → Tensor¶
</code></pre>
<p>返回以<code>input</code>元素的底数为底的对数的新张量。</p>
<p><img alt="" src="../img/585d59455ec4e6d03ca7d2f072b94ebf.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.rand(5)
&gt;&gt;&gt; a
tensor([ 0.5224,  0.9354,  0.7257,  0.1301,  0.2251])

&gt;&gt;&gt; torch.log10(a)
tensor([-0.2820, -0.0290, -0.1392, -0.8857, -0.6476])

</code></pre>
<hr />
<pre><code>torch.log1p(input, out=None) → Tensor¶
</code></pre>
<p>返回自然对数为(1 + <code>input</code>）的新张量。</p>
<p><img alt="" src="../img/a728d50025ad41634df088b6e1222f73.jpg" /></p>
<p>Note</p>
<p>对于较小的<code>input</code>值，此功能比 <a href="#torch.log" title="torch.log"><code>torch.log()</code></a> 更准确。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(5)
&gt;&gt;&gt; a
tensor([-1.0090, -0.9923,  1.0249, -0.5372,  0.2492])
&gt;&gt;&gt; torch.log1p(a)
tensor([    nan, -4.8653,  0.7055, -0.7705,  0.2225])

</code></pre>
<hr />
<pre><code>torch.log2(input, out=None) → Tensor¶
</code></pre>
<p>返回以<code>input</code>元素的底数为对数的新张量。</p>
<p><img alt="" src="../img/893d53e193443b3650c0541772aa5216.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.rand(5)
&gt;&gt;&gt; a
tensor([ 0.8419,  0.8003,  0.9971,  0.5287,  0.0490])

&gt;&gt;&gt; torch.log2(a)
tensor([-0.2483, -0.3213, -0.0042, -0.9196, -4.3504])

</code></pre>
<hr />
<pre><code>torch.logical_not(input, out=None) → Tensor¶
</code></pre>
<p>计算给定输入张量的按元素逻辑非。 如果未指定，则输出张量将具有 bool dtype。 如果输入张量不是布尔张量，则将零视为<code>False</code>，将非零视为<code>True</code>。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.logical_not(torch.tensor([True, False]))
tensor([ False,  True])
&gt;&gt;&gt; torch.logical_not(torch.tensor([0, 1, -10], dtype=torch.int8))
tensor([ True, False, False])
&gt;&gt;&gt; torch.logical_not(torch.tensor([0., 1.5, -10.], dtype=torch.double))
tensor([ True, False, False])
&gt;&gt;&gt; torch.logical_not(torch.tensor([0., 1., -10.], dtype=torch.double), out=torch.empty(3, dtype=torch.int16))
tensor([1, 0, 0], dtype=torch.int16)

</code></pre>
<hr />
<pre><code>torch.logical_xor(input, other, out=None) → Tensor¶
</code></pre>
<p>计算给定输入张量的逐元素逻辑 XOR。 零被视为<code>False</code>，非零被视为<code>True</code>。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>其他</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–用于计算 XOR 的张量</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.logical_xor(torch.tensor([True, False, True]), torch.tensor([True, False, False]))
tensor([ False, False,  True])
&gt;&gt;&gt; a = torch.tensor([0, 1, 10, 0], dtype=torch.int8)
&gt;&gt;&gt; b = torch.tensor([4, 0, 1, 0], dtype=torch.int8)
&gt;&gt;&gt; torch.logical_xor(a, b)
tensor([ True,  True, False, False])
&gt;&gt;&gt; torch.logical_xor(a.double(), b.double())
tensor([ True,  True, False, False])
&gt;&gt;&gt; torch.logical_xor(a.double(), b)
tensor([ True,  True, False, False])
&gt;&gt;&gt; torch.logical_xor(a, b, out=torch.empty(4, dtype=torch.bool))
tensor([ True,  True, False, False])

</code></pre>
<hr />
<pre><code>torch.mul()¶
</code></pre>
<hr />
<pre><code>torch.mul(input, other, out=None)
</code></pre>
<p>将输入<code>input</code>的每个元素与标量<code>other</code>相乘，并返回一个新的结果张量。</p>
<p><img alt="" src="../img/b9353a390a47f91fc123b9db37178994.jpg" /></p>
<p>如果<code>input</code>的类型为 &lt;cite&gt;FloatTensor&lt;/cite&gt; 或 &lt;cite&gt;DoubleTensor&lt;/cite&gt; ，则<code>other</code>应为实数，否则应为整数</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>{输入}</strong> –</p>
</li>
<li>
<p><strong>值</strong>(<em>数字</em>）–要与<code>input</code>的每个元素相乘的数字</p>
</li>
<li>
<p><strong>{out}</strong> –</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(3)
&gt;&gt;&gt; a
tensor([ 0.2015, -0.4255,  2.6087])
&gt;&gt;&gt; torch.mul(a, 100)
tensor([  20.1494,  -42.5491,  260.8663])

</code></pre>
<hr />
<pre><code>torch.mul(input, other, out=None)
</code></pre>
<p>张量<code>input</code>的每个元素乘以张量<code>other</code>的相应元素。 返回结果张量。</p>
<p>The shapes of <code>input</code> and <code>other</code> must be <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a>.</p>
<p><img alt="" src="../img/e7c84cbd2cee400eafabf79e53ed4a59.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–第一个被乘张量</p>
</li>
<li>
<p><strong>其他</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–第二个被乘张量</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 1)
&gt;&gt;&gt; a
tensor([[ 1.1207],
        [-0.3137],
        [ 0.0700],
        [ 0.8378]])
&gt;&gt;&gt; b = torch.randn(1, 4)
&gt;&gt;&gt; b
tensor([[ 0.5146,  0.1216, -0.5244,  2.2382]])
&gt;&gt;&gt; torch.mul(a, b)
tensor([[ 0.5767,  0.1363, -0.5877,  2.5083],
        [-0.1614, -0.0382,  0.1645, -0.7021],
        [ 0.0360,  0.0085, -0.0367,  0.1567],
        [ 0.4312,  0.1019, -0.4394,  1.8753]])

</code></pre>
<hr />
<pre><code>torch.mvlgamma(input, p) → Tensor¶
</code></pre>
<p>计算元素对数为维度<img alt="" src="../img/28f674762c8a83d24f3018848e6314d5.jpg" />的多元对数伽马函数 (<a href="https://en.wikipedia.org/wiki/Multivariate_gamma_function">[reference]</a>)，公式为</p>
<p><img alt="" src="../img/e04b3e1a381314038fdb290092d532c3.jpg" /></p>
<p>其中<img alt="" src="../img/608c3ee390b6bc4569f2210e1e011ed4.jpg" />和<img alt="" src="../img/f7fbd871ee76d8efb8317c342dd5ed9a.jpg" />是伽玛函数。</p>
<p>如果任何元素小于或等于<img alt="" src="../img/095b4338ff8c0e8670fe99e82033b339.jpg" />，那么将引发错误。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–用于计算多元对数伽马函数的张量</p>
</li>
<li>
<p><strong>p</strong>  (<em>python：int</em> )–尺寸数</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.empty(2, 3).uniform_(1, 2)
&gt;&gt;&gt; a
tensor([[1.6835, 1.8474, 1.1929],
        [1.0475, 1.7162, 1.4180]])
&gt;&gt;&gt; torch.mvlgamma(a, 2)
tensor([[0.3928, 0.4007, 0.7586],
        [1.0311, 0.3901, 0.5049]])

</code></pre>
<hr />
<pre><code>torch.neg(input, out=None) → Tensor¶
</code></pre>
<p>返回带有<code>input</code>元素负数的新张量。</p>
<p><img alt="" src="../img/95cc4605d1ad5dd05793898a56b9b7c4.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(5)
&gt;&gt;&gt; a
tensor([ 0.0090, -0.2262, -0.0682, -0.2866,  0.3940])
&gt;&gt;&gt; torch.neg(a)
tensor([-0.0090,  0.2262,  0.0682,  0.2866, -0.3940])

</code></pre>
<hr />
<pre><code>torch.polygamma(n, input, out=None) → Tensor¶
</code></pre>
<p>计算<code>input</code>上的 digamma 函数的<img alt="" src="../img/e790323c220b523037f6493b45a6be2f.jpg" />导数。 <img alt="" src="../img/5131c6ffc9046f0af6229a8f78fea321.jpg" />被称为多伽玛函数的阶数。</p>
<p><img alt="" src="../img/b6aa0cf4b0167dab157397df97cd44ad.jpg" /></p>
<p>Note</p>
<p><img alt="" src="../img/f08d8837b69470f2939526fe3f47dbf5.jpg" />未实现此功能。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>n</strong>  (<em>python：int</em> )– polygamma 函数的顺序</p>
</li>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<pre><code>Example::
</code></pre>
<pre><code>&gt;&gt;&gt; a = torch.tensor([1, 0.5])
&gt;&gt;&gt; torch.polygamma(1, a)
tensor([1.64493, 4.9348])

</code></pre>
<hr />
<pre><code>torch.pow()¶
</code></pre>
<hr />
<pre><code>torch.pow(input, exponent, out=None) → Tensor
</code></pre>
<p>用<code>exponent</code>取<code>input</code>中每个元素的幂，并返回张量与结果。</p>
<p><code>exponent</code>可以是单个<code>float</code>数字，也可以是具有与<code>input</code>相同元素数的&lt;cite&gt;张量&lt;/cite&gt;。</p>
<p>当<code>exponent</code>为标量值时，应用的运算为：</p>
<p><img alt="" src="../img/71ad83ee7a70b4a097c1393f30ffdf2f.jpg" /></p>
<p>当<code>exponent</code>是张量时，应用的运算是：</p>
<p><img alt="" src="../img/2f65f5d5f3468da870a791f8985d4dba.jpg" /></p>
<p>当<code>exponent</code>是张量时，<code>input</code>和<code>exponent</code>的形状必须是<a href="notes/broadcasting.html#broadcasting-semantics">可广播的</a>。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>指数</strong> (<em>python：float</em> <em>或</em> <em>tensor</em>）–指数值</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([ 0.4331,  1.2475,  0.6834, -0.2791])
&gt;&gt;&gt; torch.pow(a, 2)
tensor([ 0.1875,  1.5561,  0.4670,  0.0779])
&gt;&gt;&gt; exp = torch.arange(1., 5.)

&gt;&gt;&gt; a = torch.arange(1., 5.)
&gt;&gt;&gt; a
tensor([ 1.,  2.,  3.,  4.])
&gt;&gt;&gt; exp
tensor([ 1.,  2.,  3.,  4.])
&gt;&gt;&gt; torch.pow(a, exp)
tensor([   1.,    4.,   27.,  256.])

</code></pre>
<hr />
<pre><code>torch.pow(self, exponent, out=None) → Tensor
</code></pre>
<p><code>self</code>是标量<code>float</code>值，<code>exponent</code>是张量。 返回的张量<code>out</code>与<code>exponent</code>的形状相同</p>
<p>应用的操作是：</p>
<p><img alt="" src="../img/8bfb21a8eb8a28ff6ce228b2b58f69e5.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>自我</strong> (<em>python：float</em> )–幂运算的标量基值</p>
</li>
<li>
<p><strong>指数</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–指数张量</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; exp = torch.arange(1., 5.)
&gt;&gt;&gt; base = 2
&gt;&gt;&gt; torch.pow(base, exp)
tensor([  2.,   4.,   8.,  16.])

</code></pre>
<hr />
<pre><code>torch.real(input, out=None) → Tensor¶
</code></pre>
<p>计算给定<code>input</code>张量的逐元素实数值。</p>
<p><img alt="" src="../img/831fe304d759480b4e6403bb0dce7b41.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.real(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))
tensor([ -1,  -2,  3])

</code></pre>
<hr />
<pre><code>torch.reciprocal(input, out=None) → Tensor¶
</code></pre>
<p>返回带有<code>input</code>元素倒数的新张量</p>
<p><img alt="" src="../img/53b2c49d0eb01403c24924c5088215a4.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([-0.4595, -2.1219, -1.4314,  0.7298])
&gt;&gt;&gt; torch.reciprocal(a)
tensor([-2.1763, -0.4713, -0.6986,  1.3702])

</code></pre>
<hr />
<pre><code>torch.remainder(input, other, out=None) → Tensor¶
</code></pre>
<p>Computes the element-wise remainder of division.</p>
<p>除数和除数可以同时包含整数和浮点数。 其余部分与除数的符号相同。</p>
<p>When <code>other</code> is a tensor, the shapes of <code>input</code> and <code>other</code> must be <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a>.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the dividend</p>
</li>
<li>
<p><strong>其他</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>或</em> <em>python：float</em> )–除数可以是数字或张量 与股息形状相同</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.remainder(torch.tensor([-3., -2, -1, 1, 2, 3]), 2)
tensor([ 1.,  0.,  1.,  1.,  0.,  1.])
&gt;&gt;&gt; torch.remainder(torch.tensor([1., 2, 3, 4, 5]), 1.5)
tensor([ 1.0000,  0.5000,  0.0000,  1.0000,  0.5000])

</code></pre>
<p>也可以看看</p>
<p><a href="#torch.fmod" title="torch.fmod"><code>torch.fmod()</code></a> ，它等效于 C 库函数<code>fmod()</code>来计算元素的除法余数。</p>
<hr />
<pre><code>torch.round(input, out=None) → Tensor¶
</code></pre>
<p>返回一个新的张量，其中<code>input</code>的每个元素都舍入到最接近的整数。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([ 0.9920,  0.6077,  0.9734, -1.0362])
&gt;&gt;&gt; torch.round(a)
tensor([ 1.,  1.,  1., -1.])

</code></pre>
<hr />
<pre><code>torch.rsqrt(input, out=None) → Tensor¶
</code></pre>
<p>返回带有<code>input</code>的每个元素的平方根的倒数的新张量。</p>
<p><img alt="" src="../img/b203a1112169097b0c446f64347eee67.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([-0.0370,  0.2970,  1.5420, -0.9105])
&gt;&gt;&gt; torch.rsqrt(a)
tensor([    nan,  1.8351,  0.8053,     nan])

</code></pre>
<hr />
<pre><code>torch.sigmoid(input, out=None) → Tensor¶
</code></pre>
<p>返回具有<code>input</code>元素的 S 形的新张量。</p>
<p><img alt="" src="../img/9580d259e0be66d0dc07cf0c8c19fc80.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([ 0.9213,  1.0887, -0.8858, -1.7683])
&gt;&gt;&gt; torch.sigmoid(a)
tensor([ 0.7153,  0.7481,  0.2920,  0.1458])

</code></pre>
<hr />
<pre><code>torch.sign(input, out=None) → Tensor¶
</code></pre>
<p>返回带有<code>input</code>元素符号的新张量。</p>
<p><img alt="" src="../img/b1970c66011a7d9a1f7a3f76d56b69a5.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.tensor([0.7, -1.2, 0., 2.3])
&gt;&gt;&gt; a
tensor([ 0.7000, -1.2000,  0.0000,  2.3000])
&gt;&gt;&gt; torch.sign(a)
tensor([ 1., -1.,  0.,  1.])

</code></pre>
<hr />
<pre><code>torch.sin(input, out=None) → Tensor¶
</code></pre>
<p>返回带有<code>input</code>元素正弦值的新张量。</p>
<p><img alt="" src="../img/b2949ef2a0b48170c3ee143bbcbf9575.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([-0.5461,  0.1347, -2.7266, -0.2746])
&gt;&gt;&gt; torch.sin(a)
tensor([-0.5194,  0.1343, -0.4032, -0.2711])

</code></pre>
<hr />
<pre><code>torch.sinh(input, out=None) → Tensor¶
</code></pre>
<p>返回具有<code>input</code>元素的双曲正弦值的新张量。</p>
<p><img alt="" src="../img/f701007992bbd41bad0c0d47fa17cff9.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([ 0.5380, -0.8632, -0.1265,  0.9399])
&gt;&gt;&gt; torch.sinh(a)
tensor([ 0.5644, -0.9744, -0.1268,  1.0845])

</code></pre>
<hr />
<pre><code>torch.sqrt(input, out=None) → Tensor¶
</code></pre>
<p>返回具有<code>input</code>元素平方根的新张量。</p>
<p><img alt="" src="../img/8d503a40d472f4e83ed2f136b2d59121.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([-2.0755,  1.0226,  0.0831,  0.4806])
&gt;&gt;&gt; torch.sqrt(a)
tensor([    nan,  1.0112,  0.2883,  0.6933])

</code></pre>
<hr />
<pre><code>torch.tan(input, out=None) → Tensor¶
</code></pre>
<p>返回带有<code>input</code>元素的切线的新张量。</p>
<p><img alt="" src="../img/2b43ec9d6d179b58d128269ab29a3a9e.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([-1.2027, -1.7687,  0.4412, -1.3856])
&gt;&gt;&gt; torch.tan(a)
tensor([-2.5930,  4.9859,  0.4722, -5.3366])

</code></pre>
<hr />
<pre><code>torch.tanh(input, out=None) → Tensor¶
</code></pre>
<p>返回具有<code>input</code>元素的双曲正切值的新张量。</p>
<p><img alt="" src="../img/7d427ae6124c99de58f1f36b2ae6d5f6.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([ 0.8986, -0.7279,  1.1745,  0.2611])
&gt;&gt;&gt; torch.tanh(a)
tensor([ 0.7156, -0.6218,  0.8257,  0.2553])

</code></pre>
<hr />
<pre><code>torch.trunc(input, out=None) → Tensor¶
</code></pre>
<p>返回一个新的张量，该张量具有<code>input</code>元素的截断的整数值。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([ 3.4742,  0.5466, -0.8008, -0.9079])
&gt;&gt;&gt; torch.trunc(a)
tensor([ 3.,  0., -0., -0.])

</code></pre>
<h3 id="_13">减少操作</h3>
<hr />
<pre><code>torch.argmax()¶
</code></pre>
<hr />
<pre><code>torch.argmax(input) → LongTensor
</code></pre>
<p>返回<code>input</code>张量中所有元素的最大值的索引。</p>
<p>这是 <a href="#torch.max" title="torch.max"><code>torch.max()</code></a> 返回的第二个值。 有关此方法的确切语义，请参见其文档。</p>
<p>Parameters</p>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a
tensor([[ 1.3398,  0.2663, -0.2686,  0.2450],
        [-0.7401, -0.8805, -0.3402, -1.1936],
        [ 0.4907, -1.3948, -1.0691, -0.3132],
        [-1.6092,  0.5419, -0.2993,  0.3195]])
&gt;&gt;&gt; torch.argmax(a)
tensor(0)

</code></pre>
<hr />
<pre><code>torch.argmax(input, dim, keepdim=False) → LongTensor
</code></pre>
<p>返回一个维度上张量最大值的索引。</p>
<p>This is the second value returned by <a href="#torch.max" title="torch.max"><code>torch.max()</code></a>. See its documentation for the exact semantics of this method.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> )–缩小的尺寸。 如果<code>None</code>，则返回扁平化输入的 argmax。</p>
</li>
<li>
<p><strong>keepdim</strong>  (<em>bool</em> )–输出张量是否保留<code>dim</code>。 忽略<code>dim=None</code>。</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a
tensor([[ 1.3398,  0.2663, -0.2686,  0.2450],
        [-0.7401, -0.8805, -0.3402, -1.1936],
        [ 0.4907, -1.3948, -1.0691, -0.3132],
        [-1.6092,  0.5419, -0.2993,  0.3195]])
&gt;&gt;&gt; torch.argmax(a, dim=1)
tensor([ 0,  2,  0,  1])

</code></pre>
<hr />
<pre><code>torch.argmin()¶
</code></pre>
<hr />
<pre><code>torch.argmin(input) → LongTensor
</code></pre>
<p>返回<code>input</code>张量中所有元素的最小值的索引。</p>
<p>这是 <a href="#torch.min" title="torch.min"><code>torch.min()</code></a> 返回的第二个值。 有关此方法的确切语义，请参见其文档。</p>
<p>Parameters</p>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a
tensor([[ 0.1139,  0.2254, -0.1381,  0.3687],
        [ 1.0100, -1.1975, -0.0102, -0.4732],
        [-0.9240,  0.1207, -0.7506, -1.0213],
        [ 1.7809, -1.2960,  0.9384,  0.1438]])
&gt;&gt;&gt; torch.argmin(a)
tensor(13)

</code></pre>
<hr />
<pre><code>torch.argmin(input, dim, keepdim=False, out=None) → LongTensor
</code></pre>
<p>返回整个维度上张量的最小值的索引。</p>
<p>This is the second value returned by <a href="#torch.min" title="torch.min"><code>torch.min()</code></a>. See its documentation for the exact semantics of this method.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> )–缩小的尺寸。 如果<code>None</code>，则返回扁平化输入的 argmin。</p>
</li>
<li>
<p><strong>keepdim</strong> (<em>bool</em>) – whether the output tensor has <code>dim</code> retained or not. Ignored if <code>dim=None</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a
tensor([[ 0.1139,  0.2254, -0.1381,  0.3687],
        [ 1.0100, -1.1975, -0.0102, -0.4732],
        [-0.9240,  0.1207, -0.7506, -1.0213],
        [ 1.7809, -1.2960,  0.9384,  0.1438]])
&gt;&gt;&gt; torch.argmin(a, dim=1)
tensor([ 2,  1,  3,  1])

</code></pre>
<hr />
<pre><code>torch.dist(input, other, p=2) → Tensor¶
</code></pre>
<p>返回(<code>input</code>-<code>other</code>）的 p 范数</p>
<p>The shapes of <code>input</code> and <code>other</code> must be <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a>.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>其他</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–右侧输入张量</p>
</li>
<li>
<p><strong>p</strong>  (<em>python：float</em> <em>，</em> <em>可选</em>）–要计算的范数</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(4)
&gt;&gt;&gt; x
tensor([-1.5393, -0.8675,  0.5916,  1.6321])
&gt;&gt;&gt; y = torch.randn(4)
&gt;&gt;&gt; y
tensor([ 0.0967, -1.0511,  0.6295,  0.8360])
&gt;&gt;&gt; torch.dist(x, y, 3.5)
tensor(1.6727)
&gt;&gt;&gt; torch.dist(x, y, 3)
tensor(1.6973)
&gt;&gt;&gt; torch.dist(x, y, 0)
tensor(inf)
&gt;&gt;&gt; torch.dist(x, y, 1)
tensor(2.6537)

</code></pre>
<hr />
<pre><code>torch.logsumexp(input, dim, keepdim=False, out=None)¶
</code></pre>
<p>返回给定维度<code>dim</code>中<code>input</code>张量的每一行的总指数对数。 该计算在数值上是稳定的。</p>
<p>对于由&lt;cite&gt;昏暗&lt;/cite&gt;给出的总和指数<img alt="" src="../img/36608d1dd28464666846576485c40a7b.jpg" />和其他指数<img alt="" src="../img/db26d1a59be5965889bd4d5533b7be61.jpg" />，结果为</p>
<blockquote>
<p><img alt="" src="../img/9a755b15a82f2dcda096a2a363073062.jpg" /></p>
</blockquote>
<p>如果<code>keepdim</code>为<code>True</code>，则输出张量的大小与<code>input</code>相同，但尺寸为<code>dim</code>的大小为 1。否则，压缩<code>dim</code>(请参见 <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a>)，导致输出张量的尺寸减少 1(或<code>len(dim)</code>）。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> <em>或</em> <em>python：ints</em> 的元组）–要减小的尺寸。</p>
</li>
<li>
<p><strong>keepdim</strong>  (<em>bool</em> )–输出张量是否保留<code>dim</code>。</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<pre><code>Example::
</code></pre>
<pre><code>&gt;&gt;&gt; a = torch.randn(3, 3)
&gt;&gt;&gt; torch.logsumexp(a, 1)
tensor([ 0.8442,  1.4322,  0.8711])

</code></pre>
<hr />
<pre><code>torch.mean()¶
</code></pre>
<hr />
<pre><code>torch.mean(input) → Tensor
</code></pre>
<p>返回<code>input</code>张量中所有元素的平均值。</p>
<p>Parameters</p>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a
tensor([[ 0.2294, -0.5481,  1.3288]])
&gt;&gt;&gt; torch.mean(a)
tensor(0.3367)

</code></pre>
<hr />
<pre><code>torch.mean(input, dim, keepdim=False, out=None) → Tensor
</code></pre>
<p>返回给定维度<code>dim</code>中<code>input</code>张量的每一行的平均值。 如果<code>dim</code>是尺寸列表，请缩小所有尺寸。</p>
<p>If <code>keepdim</code> is <code>True</code>, the output tensor is of the same size as <code>input</code> except in the dimension(s) <code>dim</code> where it is of size 1. Otherwise, <code>dim</code> is squeezed (see <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a>), resulting in the output tensor having 1 (or <code>len(dim)</code>) fewer dimension(s).</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>dim</strong> (<em>python:int</em> <em>or</em> <em>tuple of python:ints</em>) – the dimension or dimensions to reduce.</p>
</li>
<li>
<p><strong>keepdim</strong> (<em>bool</em>) – whether the output tensor has <code>dim</code> retained or not.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a
tensor([[-0.3841,  0.6320,  0.4254, -0.7384],
        [-0.9644,  1.0131, -0.6549, -1.4279],
        [-0.2951, -1.3350, -0.7694,  0.5600],
        [ 1.0842, -0.9580,  0.3623,  0.2343]])
&gt;&gt;&gt; torch.mean(a, 1)
tensor([-0.0163, -0.5085, -0.4599,  0.1807])
&gt;&gt;&gt; torch.mean(a, 1, True)
tensor([[-0.0163],
        [-0.5085],
        [-0.4599],
        [ 0.1807]])

</code></pre>
<hr />
<pre><code>torch.median()¶
</code></pre>
<hr />
<pre><code>torch.median(input) → Tensor
</code></pre>
<p>返回<code>input</code>张量中所有元素的中值。</p>
<p>Parameters</p>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a
tensor([[ 1.5219, -1.5212,  0.2202]])
&gt;&gt;&gt; torch.median(a)
tensor(0.2202)

</code></pre>
<hr />
<pre><code>torch.median(input, dim=-1, keepdim=False, values=None, indices=None) -&gt; (Tensor, LongTensor)
</code></pre>
<p>返回一个命名元组<code>(values, indices)</code>，其中<code>values</code>是在给定维度<code>dim</code>中<code>input</code>张量的每一行的中值。 <code>indices</code>是找到的每个中值的索引位置。</p>
<p>默认情况下，<code>dim</code>是<code>input</code>张量的最后一个尺寸。</p>
<p>如果<code>keepdim</code>为<code>True</code>，则输出张量的大小与<code>input</code>相同，只是尺寸为 1 的尺寸为<code>dim</code>。否则，将压缩<code>dim</code>(请参见 <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a>)，导致输出张量的尺寸比<code>input</code>小 1。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> )–缩小的尺寸。</p>
</li>
<li>
<p><strong>keepdim</strong> (<em>bool</em>) – whether the output tensor has <code>dim</code> retained or not.</p>
</li>
<li>
<p><strong>值</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>，</em> <em>可选</em>）–输出张量</p>
</li>
<li>
<p><strong>索引</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>，</em> <em>可选</em>）–输出索引张量</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 5)
&gt;&gt;&gt; a
tensor([[ 0.2505, -0.3982, -0.9948,  0.3518, -1.3131],
        [ 0.3180, -0.6993,  1.0436,  0.0438,  0.2270],
        [-0.2751,  0.7303,  0.2192,  0.3321,  0.2488],
        [ 1.0778, -1.9510,  0.7048,  0.4742, -0.7125]])
&gt;&gt;&gt; torch.median(a, 1)
torch.return_types.median(values=tensor([-0.3982,  0.2270,  0.2488,  0.4742]), indices=tensor([1, 4, 4, 3]))

</code></pre>
<hr />
<pre><code>torch.mode(input, dim=-1, keepdim=False, values=None, indices=None) -&gt; (Tensor, LongTensor)¶
</code></pre>
<p>返回一个命名元组<code>(values, indices)</code>，其中<code>values</code>是给定维度<code>dim</code>中<code>input</code>张量的每一行的众数值，即该行中最常出现的值，而<code>indices</code>是索引位置 找到的每个模式值。</p>
<p>By default, <code>dim</code> is the last dimension of the <code>input</code> tensor.</p>
<p>如果<code>keepdim</code>为<code>True</code>，则输出张量的大小与<code>input</code>相同，只是尺寸为 1 的尺寸为<code>dim</code>。否则，将压缩<code>dim</code>(请参见 <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a>)，导致输出张量的尺寸比<code>input</code>小 1。</p>
<p>Note</p>
<p>尚未为<code>torch.cuda.Tensor</code>定义此功能。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>dim</strong> (<em>python:int</em>) – the dimension to reduce.</p>
</li>
<li>
<p><strong>keepdim</strong> (<em>bool</em>) – whether the output tensor has <code>dim</code> retained or not.</p>
</li>
<li>
<p><strong>values</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor</p>
</li>
<li>
<p><strong>indices</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output index tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randint(10, (5,))
&gt;&gt;&gt; a
tensor([6, 5, 1, 0, 2])
&gt;&gt;&gt; b = a + (torch.randn(50, 1) * 5).long()
&gt;&gt;&gt; torch.mode(b, 0)
torch.return_types.mode(values=tensor([6, 5, 1, 0, 2]), indices=tensor([2, 2, 2, 2, 2]))

</code></pre>
<hr />
<pre><code>torch.norm(input, p='fro', dim=None, keepdim=False, out=None, dtype=None)¶
</code></pre>
<p>返回给定张量的矩阵范数或向量范数。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–输入张量</p>
</li>
<li>
<p><strong>p</strong>  (<em>python：int</em> <em>，</em> <em>python：float</em> <em>，</em> <em>inf</em> <em>，</em> <em>-inf</em> <em>，</em> <em>'来回</em> <em>，</em> <em>'nuc'</em> <em>，</em> <em>可选</em>）–</p>
<p>规范的顺序。 默认值：<code>'fro'</code>可以计算以下规范：</p>
<p>| </p>
<p>奥德</p>
<p>| </p>
<p>矩阵范数</p>
<p>| </p>
<p>向量范数</p>
<p>|
| --- | --- | --- |
| 没有 | Frobenius 范数 | 2 范数 |
| 来回 | Frobenius norm | – |
| 'nuc' | 核规范 | – |
| 其他 | 当 dim 为 None 时作为 vec 规范 | sum(abs(x）<strong> ord）</strong>(1./ord） |</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> <em>，</em> <em>2 个元组的 python：ints</em> <em>，</em> <em>2 个列表 python：ints</em> <em>，</em> <em>可选</em>）–如果为 int，则将计算向量范数，如果为 int 的 2 元组，则将计算矩阵范数。 如果值为 None，则在输入张量只有二维时将计算矩阵范数，而在输入张量只有一维时将计算向量范数。 如果输入张量具有两个以上的维，则矢量范数将应用于最后一个维。</p>
</li>
<li>
<p><strong>keepdim</strong>  (<em>bool</em> <em>，</em> <em>可选</em>）–输出张量是否保留<code>dim</code>。 如果<code>dim</code> = <code>None</code>和<code>out</code> = <code>None</code>则忽略。 默认值：<code>False</code></p>
</li>
<li>
<p><strong>输出</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>，</em> <em>可选</em>）–输出张量。 如果<code>dim</code> = <code>None</code>和<code>out</code> = <code>None</code>则忽略。</p>
</li>
<li>
<p><strong>dtype</strong>  (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> ，可选）–返回张量的所需数据类型。 如果已指定，则在执行操作时将输入张量强制转换为：attr：“ dtype”。 默认值：无。</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; import torch
&gt;&gt;&gt; a = torch.arange(9, dtype= torch.float) - 4
&gt;&gt;&gt; b = a.reshape((3, 3))
&gt;&gt;&gt; torch.norm(a)
tensor(7.7460)
&gt;&gt;&gt; torch.norm(b)
tensor(7.7460)
&gt;&gt;&gt; torch.norm(a, float('inf'))
tensor(4.)
&gt;&gt;&gt; torch.norm(b, float('inf'))
tensor(4.)
&gt;&gt;&gt; c = torch.tensor([[ 1, 2, 3],[-1, 1, 4]] , dtype= torch.float)
&gt;&gt;&gt; torch.norm(c, dim=0)
tensor([1.4142, 2.2361, 5.0000])
&gt;&gt;&gt; torch.norm(c, dim=1)
tensor([3.7417, 4.2426])
&gt;&gt;&gt; torch.norm(c, p=1, dim=1)
tensor([6., 6.])
&gt;&gt;&gt; d = torch.arange(8, dtype= torch.float).reshape(2,2,2)
&gt;&gt;&gt; torch.norm(d, dim=(1,2))
tensor([ 3.7417, 11.2250])
&gt;&gt;&gt; torch.norm(d[0, :, :]), torch.norm(d[1, :, :])
(tensor(3.7417), tensor(11.2250))

</code></pre>
<hr />
<pre><code>torch.prod()¶
</code></pre>
<hr />
<pre><code>torch.prod(input, dtype=None) → Tensor
</code></pre>
<p>返回<code>input</code>张量中所有元素的乘积。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>dtype</strong>  (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> ，可选）–返回张量的所需数据类型。 如果指定，则在执行操作之前将输入张量转换为<code>dtype</code>。 这对于防止数据类型溢出很有用。 默认值：无。</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a
tensor([[-0.8020,  0.5428, -1.5854]])
&gt;&gt;&gt; torch.prod(a)
tensor(0.6902)

</code></pre>
<hr />
<pre><code>torch.prod(input, dim, keepdim=False, dtype=None) → Tensor
</code></pre>
<p>返回给定维度<code>dim</code>中<code>input</code>张量的每一行的乘积。</p>
<p>如果<code>keepdim</code>为<code>True</code>，则输出张量的大小与<code>input</code>相同，但尺寸为<code>dim</code>的大小为 1。否则，将压缩<code>dim</code>(请参见 <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a>)，导致输出张量的尺寸比<code>input</code>小 1。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>dim</strong> (<em>python:int</em>) – the dimension to reduce.</p>
</li>
<li>
<p><strong>keepdim</strong> (<em>bool</em>) – whether the output tensor has <code>dim</code> retained or not.</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. If specified, the input tensor is casted to <code>dtype</code> before the operation is performed. This is useful for preventing data type overflows. Default: None.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 2)
&gt;&gt;&gt; a
tensor([[ 0.5261, -0.3837],
        [ 1.1857, -0.2498],
        [-1.1646,  0.0705],
        [ 1.1131, -1.0629]])
&gt;&gt;&gt; torch.prod(a, 1)
tensor([-0.2018, -0.2962, -0.0821, -1.1831])

</code></pre>
<hr />
<pre><code>torch.std()¶
</code></pre>
<hr />
<pre><code>torch.std(input, unbiased=True) → Tensor
</code></pre>
<p>返回<code>input</code>张量中所有元素的标准偏差。</p>
<p>如果<code>unbiased</code>为<code>False</code>，则将通过有偏估计量计算标准偏差。 否则，将使用贝塞尔的更正。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>无偏</strong> (<em>bool</em> )–是否使用无偏估计</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a
tensor([[-0.8166, -1.3802, -0.3560]])
&gt;&gt;&gt; torch.std(a)
tensor(0.5130)

</code></pre>
<hr />
<pre><code>torch.std(input, dim, keepdim=False, unbiased=True, out=None) → Tensor
</code></pre>
<p>返回<code>input</code>张量的每一行在标准<code>dim</code>中的标准偏差。 如果<code>dim</code>是尺寸列表，请缩小所有尺寸。</p>
<p>If <code>keepdim</code> is <code>True</code>, the output tensor is of the same size as <code>input</code> except in the dimension(s) <code>dim</code> where it is of size 1. Otherwise, <code>dim</code> is squeezed (see <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a>), resulting in the output tensor having 1 (or <code>len(dim)</code>) fewer dimension(s).</p>
<p>If <code>unbiased</code> is <code>False</code>, then the standard-deviation will be calculated via the biased estimator. Otherwise, Bessel's correction will be used.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>dim</strong> (<em>python:int</em> <em>or</em> <em>tuple of python:ints</em>) – the dimension or dimensions to reduce.</p>
</li>
<li>
<p><strong>keepdim</strong> (<em>bool</em>) – whether the output tensor has <code>dim</code> retained or not.</p>
</li>
<li>
<p><strong>unbiased</strong> (<em>bool</em>) – whether to use the unbiased estimation or not</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a
tensor([[ 0.2035,  1.2959,  1.8101, -0.4644],
        [ 1.5027, -0.3270,  0.5905,  0.6538],
        [-1.5745,  1.3330, -0.5596, -0.6548],
        [ 0.1264, -0.5080,  1.6420,  0.1992]])
&gt;&gt;&gt; torch.std(a, dim=1)
tensor([ 1.0311,  0.7477,  1.2204,  0.9087])

</code></pre>
<hr />
<pre><code>torch.std_mean()¶
</code></pre>
<hr />
<pre><code>torch.std_mean(input, unbiased=True) -&gt; (Tensor, Tensor)
</code></pre>
<p>返回<code>input</code>张量中所有元素的标准差和均值。</p>
<p>If <code>unbiased</code> is <code>False</code>, then the standard-deviation will be calculated via the biased estimator. Otherwise, Bessel's correction will be used.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>unbiased</strong> (<em>bool</em>) – whether to use the unbiased estimation or not</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a
tensor([[0.3364, 0.3591, 0.9462]])
&gt;&gt;&gt; torch.std_mean(a)
(tensor(0.3457), tensor(0.5472))

</code></pre>
<hr />
<pre><code>torch.std(input, dim, keepdim=False, unbiased=True) -&gt; (Tensor, Tensor)
</code></pre>
<p>返回<code>dim</code>张量中<code>input</code>张量的每一行的标准偏差和均值。 如果<code>dim</code>是尺寸列表，请缩小所有尺寸。</p>
<p>If <code>keepdim</code> is <code>True</code>, the output tensor is of the same size as <code>input</code> except in the dimension(s) <code>dim</code> where it is of size 1. Otherwise, <code>dim</code> is squeezed (see <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a>), resulting in the output tensor having 1 (or <code>len(dim)</code>) fewer dimension(s).</p>
<p>If <code>unbiased</code> is <code>False</code>, then the standard-deviation will be calculated via the biased estimator. Otherwise, Bessel's correction will be used.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>dim</strong> (<em>python:int</em> <em>or</em> <em>tuple of python:ints</em>) – the dimension or dimensions to reduce.</p>
</li>
<li>
<p><strong>keepdim</strong> (<em>bool</em>) – whether the output tensor has <code>dim</code> retained or not.</p>
</li>
<li>
<p><strong>unbiased</strong> (<em>bool</em>) – whether to use the unbiased estimation or not</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a
tensor([[ 0.5648, -0.5984, -1.2676, -1.4471],
        [ 0.9267,  1.0612,  1.1050, -0.6014],
        [ 0.0154,  1.9301,  0.0125, -1.0904],
        [-1.9711, -0.7748, -1.3840,  0.5067]])
&gt;&gt;&gt; torch.std_mean(a, 1)
(tensor([0.9110, 0.8197, 1.2552, 1.0608]), tensor([-0.6871,  0.6229,  0.2169, -0.9058]))

</code></pre>
<hr />
<pre><code>torch.sum()¶
</code></pre>
<hr />
<pre><code>torch.sum(input, dtype=None) → Tensor
</code></pre>
<p>返回<code>input</code>张量中所有元素的总和。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. If specified, the input tensor is casted to <code>dtype</code> before the operation is performed. This is useful for preventing data type overflows. Default: None.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a
tensor([[ 0.1133, -0.9567,  0.2958]])
&gt;&gt;&gt; torch.sum(a)
tensor(-0.5475)

</code></pre>
<hr />
<pre><code>torch.sum(input, dim, keepdim=False, dtype=None) → Tensor
</code></pre>
<p>返回给定维度<code>dim</code>中<code>input</code>张量的每一行的总和。 如果<code>dim</code>是尺寸列表，请缩小所有尺寸。</p>
<p>If <code>keepdim</code> is <code>True</code>, the output tensor is of the same size as <code>input</code> except in the dimension(s) <code>dim</code> where it is of size 1. Otherwise, <code>dim</code> is squeezed (see <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a>), resulting in the output tensor having 1 (or <code>len(dim)</code>) fewer dimension(s).</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>dim</strong> (<em>python:int</em> <em>or</em> <em>tuple of python:ints</em>) – the dimension or dimensions to reduce.</p>
</li>
<li>
<p><strong>keepdim</strong> (<em>bool</em>) – whether the output tensor has <code>dim</code> retained or not.</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. If specified, the input tensor is casted to <code>dtype</code> before the operation is performed. This is useful for preventing data type overflows. Default: None.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a
tensor([[ 0.0569, -0.2475,  0.0737, -0.3429],
        [-0.2993,  0.9138,  0.9337, -1.6864],
        [ 0.1132,  0.7892, -0.1003,  0.5688],
        [ 0.3637, -0.9906, -0.4752, -1.5197]])
&gt;&gt;&gt; torch.sum(a, 1)
tensor([-0.4598, -0.1381,  1.3708, -2.6217])
&gt;&gt;&gt; b = torch.arange(4 * 5 * 6).view(4, 5, 6)
&gt;&gt;&gt; torch.sum(b, (2, 1))
tensor([  435.,  1335.,  2235.,  3135.])

</code></pre>
<hr />
<pre><code>torch.unique(input, sorted=True, return_inverse=False, return_counts=False, dim=None)¶
</code></pre>
<p>返回输入张量的唯一元素。</p>
<p>Note</p>
<p>此功能与 <a href="#torch.unique_consecutive" title="torch.unique_consecutive"><code>torch.unique_consecutive()</code></a> 不同，因为该功能还消除了非连续的重复值。</p>
<p>Note</p>
<p>当前，在 CUDA 实现和 CPU 实现中，当指定 dim 时，无论 &lt;cite&gt;sort&lt;/cite&gt; 参数如何， &lt;cite&gt;torch.unique&lt;/cite&gt; 始终在开始时对张量进行排序。 排序可能会很慢，因此如果您的输入张量已被排序，建议使用 <a href="#torch.unique_consecutive" title="torch.unique_consecutive"><code>torch.unique_consecutive()</code></a> 以避免排序。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p>
</li>
<li>
<p><strong>排序的</strong> (<em>bool</em> )–在返回为输出之前是否按升序对唯一元素进行排序。</p>
</li>
<li>
<p><strong>return_inverse</strong>  (<em>bool</em> )–是否还返回原始输入中元素在返回的唯一列表中所处位置的索引。</p>
</li>
<li>
<p><strong>return_counts</strong>  (<em>bool</em> )–是否还返回每个唯一元素的计数。</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> )–应用唯一尺寸。 如果<code>None</code>，则返回拼合输入的唯一性。 默认值：<code>None</code></p>
</li>
</ul>
<p>Returns</p>
<p>一个张量或张量的元组包含</p>
<blockquote>
<ul>
<li>
<p><strong>输出</strong>(<em>tensor</em>）：唯一标量元素的输出列表。</p>
</li>
<li>
<p><strong>inverse_indices</strong> (<em>tensor</em>）：(可选）如果<code>return_inverse</code>为 True，将有一个额外的返回张量(形状与输入相同）表示原始输入中元素所在位置的索引 映射到输出中； 否则，此函数将仅返回单个张量。</p>
</li>
<li>
<p><strong>计数为</strong>(<em>tensor</em>）：(可选）如果<code>return_counts</code>为 True，则将有一个额外的返回张量(与 output 或 output.size(dim）相同的形状，如果 dim 为 代表每个唯一值或张量的出现次数。</p>
</li>
</ul>
</blockquote>
<p>Return type</p>
<p>(<a href="tensors.html#torch.Tensor" title="torch.Tensor">张量</a>，<a href="tensors.html#torch.Tensor" title="torch.Tensor">张量</a>(可选），<a href="tensors.html#torch.Tensor" title="torch.Tensor">张量</a>(可选））</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; output = torch.unique(torch.tensor([1, 3, 2, 3], dtype=torch.long))
&gt;&gt;&gt; output
tensor([ 2,  3,  1])

&gt;&gt;&gt; output, inverse_indices = torch.unique(
        torch.tensor([1, 3, 2, 3], dtype=torch.long), sorted=True, return_inverse=True)
&gt;&gt;&gt; output
tensor([ 1,  2,  3])
&gt;&gt;&gt; inverse_indices
tensor([ 0,  2,  1,  2])

&gt;&gt;&gt; output, inverse_indices = torch.unique(
        torch.tensor([[1, 3], [2, 3]], dtype=torch.long), sorted=True, return_inverse=True)
&gt;&gt;&gt; output
tensor([ 1,  2,  3])
&gt;&gt;&gt; inverse_indices
tensor([[ 0,  2],
        [ 1,  2]])

</code></pre>
<hr />
<pre><code>torch.unique_consecutive(input, return_inverse=False, return_counts=False, dim=None)¶
</code></pre>
<p>从每个连续的等效元素组中除去除第一个元素外的所有元素。</p>
<p>Note</p>
<p>在此功能仅消除连续重复值的意义上，此功能与 <a href="#torch.unique" title="torch.unique"><code>torch.unique()</code></a> 不同。 此语义类似于 C ++中的 &lt;cite&gt;std :: unique&lt;/cite&gt; 。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p>
</li>
<li>
<p><strong>return_inverse</strong> (<em>bool</em>) – Whether to also return the indices for where elements in the original input ended up in the returned unique list.</p>
</li>
<li>
<p><strong>return_counts</strong> (<em>bool</em>) – Whether to also return the counts for each unique element.</p>
</li>
<li>
<p><strong>dim</strong> (<em>python:int</em>) – the dimension to apply unique. If <code>None</code>, the unique of the flattened input is returned. default: <code>None</code></p>
</li>
</ul>
<p>Returns</p>
<p>A tensor or a tuple of tensors containing</p>
<blockquote>
<ul>
<li>
<p><strong>output</strong> (<em>Tensor</em>): the output list of unique scalar elements.</p>
</li>
<li>
<p><strong>inverse_indices</strong> (<em>Tensor</em>): (optional) if <code>return_inverse</code> is True, there will be an additional returned tensor (same shape as input) representing the indices for where elements in the original input map to in the output; otherwise, this function will only return a single tensor.</p>
</li>
<li>
<p><strong>counts</strong> (<em>Tensor</em>): (optional) if <code>return_counts</code> is True, there will be an additional returned tensor (same shape as output or output.size(dim), if dim was specified) representing the number of occurrences for each unique value or tensor.</p>
</li>
</ul>
</blockquote>
<p>Return type</p>
<p>(<a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>, <a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a> (optional), <a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a> (optional))</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.tensor([1, 1, 2, 2, 3, 1, 1, 2])
&gt;&gt;&gt; output = torch.unique_consecutive(x)
&gt;&gt;&gt; output
tensor([1, 2, 3, 1, 2])

&gt;&gt;&gt; output, inverse_indices = torch.unique_consecutive(x, return_inverse=True)
&gt;&gt;&gt; output
tensor([1, 2, 3, 1, 2])
&gt;&gt;&gt; inverse_indices
tensor([0, 0, 1, 1, 2, 3, 3, 4])

&gt;&gt;&gt; output, counts = torch.unique_consecutive(x, return_counts=True)
&gt;&gt;&gt; output
tensor([1, 2, 3, 1, 2])
&gt;&gt;&gt; counts
tensor([2, 2, 1, 2, 1])

</code></pre>
<hr />
<pre><code>torch.var()¶
</code></pre>
<hr />
<pre><code>torch.var(input, unbiased=True) → Tensor
</code></pre>
<p>返回<code>input</code>张量中所有元素的方差。</p>
<p>如果<code>unbiased</code>为<code>False</code>，则将通过有偏估计量计算方差。 否则，将使用贝塞尔的更正。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>unbiased</strong> (<em>bool</em>) – whether to use the unbiased estimation or not</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a
tensor([[-0.3425, -1.2636, -0.4864]])
&gt;&gt;&gt; torch.var(a)
tensor(0.2455)

</code></pre>
<hr />
<pre><code>torch.var(input, dim, keepdim=False, unbiased=True, out=None) → Tensor
</code></pre>
<p>返回给定维度<code>dim</code>中<code>input</code>张量的每一行的方差。</p>
<p>If <code>keepdim</code> is <code>True</code>, the output tensor is of the same size as <code>input</code> except in the dimension(s) <code>dim</code> where it is of size 1. Otherwise, <code>dim</code> is squeezed (see <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a>), resulting in the output tensor having 1 (or <code>len(dim)</code>) fewer dimension(s).</p>
<p>If <code>unbiased</code> is <code>False</code>, then the variance will be calculated via the biased estimator. Otherwise, Bessel's correction will be used.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>dim</strong> (<em>python:int</em> <em>or</em> <em>tuple of python:ints</em>) – the dimension or dimensions to reduce.</p>
</li>
<li>
<p><strong>keepdim</strong> (<em>bool</em>) – whether the output tensor has <code>dim</code> retained or not.</p>
</li>
<li>
<p><strong>unbiased</strong> (<em>bool</em>) – whether to use the unbiased estimation or not</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a
tensor([[-0.3567,  1.7385, -1.3042,  0.7423],
        [ 1.3436, -0.1015, -0.9834, -0.8438],
        [ 0.6056,  0.1089, -0.3112, -1.4085],
        [-0.7700,  0.6074, -0.1469,  0.7777]])
&gt;&gt;&gt; torch.var(a, 1)
tensor([ 1.7444,  1.1363,  0.7356,  0.5112])

</code></pre>
<hr />
<pre><code>torch.var_mean()¶
</code></pre>
<hr />
<pre><code>torch.var_mean(input, unbiased=True) -&gt; (Tensor, Tensor)
</code></pre>
<p>返回<code>input</code>张量中所有元素的方差和均值。</p>
<p>If <code>unbiased</code> is <code>False</code>, then the variance will be calculated via the biased estimator. Otherwise, Bessel's correction will be used.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>unbiased</strong> (<em>bool</em>) – whether to use the unbiased estimation or not</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a
tensor([[0.0146, 0.4258, 0.2211]])
&gt;&gt;&gt; torch.var_mean(a)
(tensor(0.0423), tensor(0.2205))

</code></pre>
<hr />
<pre><code>torch.var_mean(input, dim, keepdim=False, unbiased=True) -&gt; (Tensor, Tensor)
</code></pre>
<p>返回给定维度<code>dim</code>中<code>input</code>张量的每一行的方差和均值。</p>
<p>If <code>keepdim</code> is <code>True</code>, the output tensor is of the same size as <code>input</code> except in the dimension(s) <code>dim</code> where it is of size 1. Otherwise, <code>dim</code> is squeezed (see <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a>), resulting in the output tensor having 1 (or <code>len(dim)</code>) fewer dimension(s).</p>
<p>If <code>unbiased</code> is <code>False</code>, then the variance will be calculated via the biased estimator. Otherwise, Bessel's correction will be used.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>dim</strong> (<em>python:int</em> <em>or</em> <em>tuple of python:ints</em>) – the dimension or dimensions to reduce.</p>
</li>
<li>
<p><strong>keepdim</strong> (<em>bool</em>) – whether the output tensor has <code>dim</code> retained or not.</p>
</li>
<li>
<p><strong>unbiased</strong> (<em>bool</em>) – whether to use the unbiased estimation or not</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a
tensor([[-1.5650,  2.0415, -0.1024, -0.5790],
        [ 0.2325, -2.6145, -1.6428, -0.3537],
        [-0.2159, -1.1069,  1.2882, -1.3265],
        [-0.6706, -1.5893,  0.6827,  1.6727]])
&gt;&gt;&gt; torch.var_mean(a, 1)
(tensor([2.3174, 1.6403, 1.4092, 2.0791]), tensor([-0.0512, -1.0946, -0.3403,  0.0239]))

</code></pre>
<h3 id="_14">比较行动</h3>
<hr />
<pre><code>torch.allclose(input, other, rtol=1e-05, atol=1e-08, equal_nan=False) → bool¶
</code></pre>
<p>此函数检查<code>input</code>和<code>other</code>是否都满足以下条件：</p>
<p><img alt="" src="../img/cbd6149aa59b9b03eadc2c023182fe68.jpg" /></p>
<p>对于<code>input</code>和<code>other</code>的所有元素，都是逐元素的。 此函数的行为类似于 <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.allclose.html">numpy.allclose</a></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–比较的第一个张量</p>
</li>
<li>
<p><strong>其他</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要比较的第二张量</p>
</li>
<li>
<p><strong>atol</strong>  (<em>python：float</em> <em>，</em> <em>可选</em>）–绝对公差。 默认值：1e-08</p>
</li>
<li>
<p><strong>rtol</strong>  (<em>python：float</em> <em>，</em> <em>可选</em>）–相对公差。 默认值：1e-05</p>
</li>
<li>
<p><strong>equal_nan</strong>  (<em>bool</em> <em>，</em> <em>可选</em>）–如果<code>True</code>，则将两个<code>NaN</code> s 相等。 默认值：<code>False</code></p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.allclose(torch.tensor([10000., 1e-07]), torch.tensor([10000.1, 1e-08]))
False
&gt;&gt;&gt; torch.allclose(torch.tensor([10000., 1e-08]), torch.tensor([10000.1, 1e-09]))
True
&gt;&gt;&gt; torch.allclose(torch.tensor([1.0, float('nan')]), torch.tensor([1.0, float('nan')]))
False
&gt;&gt;&gt; torch.allclose(torch.tensor([1.0, float('nan')]), torch.tensor([1.0, float('nan')]), equal_nan=True)
True

</code></pre>
<hr />
<pre><code>torch.argsort(input, dim=-1, descending=False, out=None) → LongTensor¶
</code></pre>
<p>返回按值升序对给定维度上的张量排序的索引。</p>
<p>这是 <a href="#torch.sort" title="torch.sort"><code>torch.sort()</code></a> 返回的第二个值。 有关此方法的确切语义，请参见其文档。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> <em>，</em> <em>可选</em>）–要排序的维度</p>
</li>
<li>
<p><strong>降序</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–控制排序顺序(升序或降序）</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a
tensor([[ 0.0785,  1.5267, -0.8521,  0.4065],
        [ 0.1598,  0.0788, -0.0745, -1.2700],
        [ 1.2208,  1.0722, -0.7064,  1.2564],
        [ 0.0669, -0.2318, -0.8229, -0.9280]])

&gt;&gt;&gt; torch.argsort(a, dim=1)
tensor([[2, 0, 3, 1],
        [3, 2, 1, 0],
        [2, 1, 0, 3],
        [3, 2, 1, 0]])

</code></pre>
<hr />
<pre><code>torch.eq(input, other, out=None) → Tensor¶
</code></pre>
<p>计算按元素相等</p>
<p>第二个参数可以是数字或张量，其形状可以与第一个参数一起广播为的<a href="notes/broadcasting.html#broadcasting-semantics">。</a></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要比较的张量</p>
</li>
<li>
<p><strong>其他</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>或</em> <em>python：float</em> )–要比较的张量或值</p>
</li>
<li>
<p><strong>输出</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>，</em> <em>可选</em>）–输出张量。 必须是 &lt;cite&gt;ByteTensor&lt;/cite&gt;</p>
</li>
</ul>
<p>Returns</p>
<p><code>torch.BoolTensor</code>在每个比较为真的位置包含一个真</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.eq(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))
tensor([[ 1,  0],
        [ 0,  1]], dtype=torch.uint8)

</code></pre>
<hr />
<pre><code>torch.equal(input, other) → bool¶
</code></pre>
<p>如果两个张量具有相同的大小和元素，则为<code>True</code>，否则为<code>False</code>。</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.equal(torch.tensor([1, 2]), torch.tensor([1, 2]))
True

</code></pre>
<hr />
<pre><code>torch.ge(input, other, out=None) → Tensor¶
</code></pre>
<p>逐元素计算<img alt="" src="../img/5807a6d54e9791f46e3bc29daa0ee7a1.jpg" />。</p>
<p>The second argument can be a number or a tensor whose shape is <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a> with the first argument.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor to compare</p>
</li>
<li>
<p><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a> <em>or</em> <em>python:float</em>) – the tensor or value to compare</p>
</li>
<li>
<p><strong>输出</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>，</em> <em>可选</em>）–输出张量必须为 &lt;cite&gt;BoolTensor&lt;/cite&gt;</p>
</li>
</ul>
<p>Returns</p>
<p>A <code>torch.BoolTensor</code> containing a True at each location where comparison is true</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.ge(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))
tensor([[True, True], [False, True]])

</code></pre>
<hr />
<pre><code>torch.gt(input, other, out=None) → Tensor¶
</code></pre>
<p>逐元素计算<img alt="" src="../img/0f05b585a740804201541c969a0fcf12.jpg" />。</p>
<p>The second argument can be a number or a tensor whose shape is <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a> with the first argument.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor to compare</p>
</li>
<li>
<p><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a> <em>or</em> <em>python:float</em>) – the tensor or value to compare</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor that must be a &lt;cite&gt;BoolTensor&lt;/cite&gt;</p>
</li>
</ul>
<p>Returns</p>
<p>A <code>torch.BoolTensor</code> containing a True at each location where comparison is true</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.gt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))
tensor([[False, True], [False, False]])

</code></pre>
<hr />
<pre><code>torch.isfinite()¶
</code></pre>
<p>返回带有布尔元素的新张量，布尔元素表示每个元素是否为&lt;cite&gt;有限&lt;/cite&gt;。</p>
<blockquote>
<p><code>Arguments:</code></p>
<p>张量(张量）：要检查的张量</p>
<p><code>Returns:</code></p>
<p>张量：<code>A torch.Tensor with dtype torch.bool</code>在有限元素的每个位置均包含 True，否则包含 False</p>
<p>Example:</p>
<p>```
&gt;&gt;&gt; torch.isfinite(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')]))
tensor([True,  False,  True,  False,  False])</p>
<p>```</p>
</blockquote>
<hr />
<pre><code>torch.isinf(tensor)¶
</code></pre>
<p>返回带有布尔元素的新张量，该布尔元素表示每个元素是否为 &lt;cite&gt;+/- INF&lt;/cite&gt; 。</p>
<p>Parameters</p>
<p><strong>张量</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要检查的张量</p>
<p>Returns</p>
<p><code>A torch.Tensor with dtype torch.bool</code>在 &lt;cite&gt;+/- INF&lt;/cite&gt; 元素的每个位置均包含 True，否则包含 False</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.isinf(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')]))
tensor([False,  True,  False,  True,  False])

</code></pre>
<hr />
<pre><code>torch.isnan()¶
</code></pre>
<p>返回带有布尔元素的新张量，布尔元素表示每个元素是否为 &lt;cite&gt;NaN&lt;/cite&gt; 。</p>
<p>Parameters</p>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要检查的张量</p>
<p>Returns</p>
<p>在 &lt;cite&gt;NaN&lt;/cite&gt; 元素的每个位置包含 True 的<code>torch.BoolTensor</code>。</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.isnan(torch.tensor([1, float('nan'), 2]))
tensor([False, True, False])

</code></pre>
<hr />
<pre><code>torch.kthvalue(input, k, dim=None, keepdim=False, out=None) -&gt; (Tensor, LongTensor)¶
</code></pre>
<p>返回一个命名元组<code>(values, indices)</code>，其中<code>values</code>是在给定维度<code>dim</code>中<code>input</code>张量的每一行的第<code>k</code>个最小元素。 <code>indices</code>是找到的每个元素的索引位置。</p>
<p>如果未提供<code>dim</code>，则选择&lt;cite&gt;输入&lt;/cite&gt;的最后尺寸。</p>
<p>如果<code>keepdim</code>为<code>True</code>，则<code>values</code>和<code>indices</code>张量与<code>input</code>的大小相同，但尺寸为<code>dim</code>的张量为 1。否则，<code>dim</code>会受到挤压 (参见 <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a>)，导致<code>values</code>和<code>indices</code>张量的尺寸都比<code>input</code>张量小 1。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>k</strong>  (<em>python：int</em> )–第 k 个最小元素的 k</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> <em>，</em> <em>可选</em>）–沿第 k 个值查找尺寸</p>
</li>
<li>
<p><strong>keepdim</strong> (<em>bool</em>) – whether the output tensor has <code>dim</code> retained or not.</p>
</li>
<li>
<p><strong>out</strong> (<em>元组</em> <em>，</em> <em>可选</em>）–(Tensor，LongTensor）的输出元组可以可选地用作输出缓冲区</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.arange(1., 6.)
&gt;&gt;&gt; x
tensor([ 1.,  2.,  3.,  4.,  5.])
&gt;&gt;&gt; torch.kthvalue(x, 4)
torch.return_types.kthvalue(values=tensor(4.), indices=tensor(3))

&gt;&gt;&gt; x=torch.arange(1.,7.).resize_(2,3)
&gt;&gt;&gt; x
tensor([[ 1.,  2.,  3.],
        [ 4.,  5.,  6.]])
&gt;&gt;&gt; torch.kthvalue(x, 2, 0, True)
torch.return_types.kthvalue(values=tensor([[4., 5., 6.]]), indices=tensor([[1, 1, 1]]))

</code></pre>
<hr />
<pre><code>torch.le(input, other, out=None) → Tensor¶
</code></pre>
<p>逐元素计算<img alt="" src="../img/155c37e00e2691f1d70e95a9eb0156f1.jpg" />。</p>
<p>The second argument can be a number or a tensor whose shape is <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a> with the first argument.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor to compare</p>
</li>
<li>
<p><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a> <em>or</em> <em>python:float</em>) – the tensor or value to compare</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor that must be a &lt;cite&gt;BoolTensor&lt;/cite&gt;</p>
</li>
</ul>
<p>Returns</p>
<p>A <code>torch.BoolTensor</code> containing a True at each location where comparison is true</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.le(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))
tensor([[True, False], [True, True]])

</code></pre>
<hr />
<pre><code>torch.lt(input, other, out=None) → Tensor¶
</code></pre>
<p>逐元素计算<img alt="" src="../img/6953e508ba80b2b7609d4e21f205f593.jpg" />。</p>
<p>The second argument can be a number or a tensor whose shape is <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a> with the first argument.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor to compare</p>
</li>
<li>
<p><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a> <em>or</em> <em>python:float</em>) – the tensor or value to compare</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor that must be a &lt;cite&gt;BoolTensor&lt;/cite&gt;</p>
</li>
</ul>
<p>Returns</p>
<p>在每个比较为真的位置处包含“真”的 &lt;cite&gt;Torch.BoolTensor&lt;/cite&gt;</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.lt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))
tensor([[False, False], [True, False]])

</code></pre>
<hr />
<pre><code>torch.max()¶
</code></pre>
<hr />
<pre><code>torch.max(input) → Tensor
</code></pre>
<p>返回<code>input</code>张量中所有元素的最大值。</p>
<p>Parameters</p>
<p><strong>{input}</strong> –</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a
tensor([[ 0.6763,  0.7445, -2.2369]])
&gt;&gt;&gt; torch.max(a)
tensor(0.7445)

</code></pre>
<hr />
<pre><code>torch.max(input, dim, keepdim=False, out=None) -&gt; (Tensor, LongTensor)
</code></pre>
<p>返回一个命名元组<code>(values, indices)</code>，其中<code>values</code>是在给定维度<code>dim</code>中<code>input</code>张量的每一行的最大值。 <code>indices</code>是找到的每个最大值(argmax）的索引位置。</p>
<p>如果<code>keepdim</code>为<code>True</code>，则输出张量的大小与<code>input</code>相同，只是尺寸为 1 的尺寸为<code>dim</code>。否则，将压缩<code>dim</code>(请参见 <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a>)，导致输出张量的尺寸比<code>input</code>小 1。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>{input}</strong> –</p>
</li>
<li>
<p><strong>{dim}</strong> –</p>
</li>
<li>
<p><strong>默认</strong> (<em>{keepdim}</em> )– <code>False</code>。</p>
</li>
<li>
<p><strong>输出</strong>(<em>元组</em> <em>，</em> <em>可选</em>）–两个输出张量的结果元组(max，max_indices）</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a
tensor([[-1.2360, -0.2942, -0.1222,  0.8475],
        [ 1.1949, -1.1127, -2.2379, -0.6702],
        [ 1.5717, -0.9207,  0.1297, -1.8768],
        [-0.6172,  1.0036, -0.6060, -0.2432]])
&gt;&gt;&gt; torch.max(a, 1)
torch.return_types.max(values=tensor([0.8475, 1.1949, 1.5717, 1.0036]), indices=tensor([3, 0, 0, 1]))

</code></pre>
<hr />
<pre><code>torch.max(input, other, out=None) → Tensor
</code></pre>
<p>将张量<code>input</code>的每个元素与张量<code>other</code>的对应元素进行比较，并获得逐个元素的最大值。</p>
<p><code>input</code>和<code>other</code>的形状不需要匹配，但它们必须是<a href="notes/broadcasting.html#broadcasting-semantics">可广播的</a>。</p>
<p><img alt="" src="../img/6b8f532893b7a5323f14fb2e70fe152e.jpg" /></p>
<p>Note</p>
<p>当形状不匹配时，返回的输出张量的形状遵循<a href="notes/broadcasting.html#broadcasting-semantics">广播规则</a>。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the second input tensor</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([ 0.2942, -0.7416,  0.2653, -0.1584])
&gt;&gt;&gt; b = torch.randn(4)
&gt;&gt;&gt; b
tensor([ 0.8722, -1.7421, -0.4141, -0.5055])
&gt;&gt;&gt; torch.max(a, b)
tensor([ 0.8722, -0.7416,  0.2653, -0.1584])

</code></pre>
<hr />
<pre><code>torch.min()¶
</code></pre>
<hr />
<pre><code>torch.min(input) → Tensor
</code></pre>
<p>返回<code>input</code>张量中所有元素的最小值。</p>
<p>Parameters</p>
<p><strong>{input}</strong> –</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a
tensor([[ 0.6750,  1.0857,  1.7197]])
&gt;&gt;&gt; torch.min(a)
tensor(0.6750)

</code></pre>
<hr />
<pre><code>torch.min(input, dim, keepdim=False, out=None) -&gt; (Tensor, LongTensor)
</code></pre>
<p>返回一个命名元组<code>(values, indices)</code>，其中<code>values</code>是在给定维度<code>dim</code>中<code>input</code>张量的每一行的最小值。 <code>indices</code>是找到的每个最小值的索引位置(argmin）。</p>
<p>如果<code>keepdim</code>为<code>True</code>，则输出张量的大小与<code>input</code>相同，只是尺寸为 1 的尺寸为<code>dim</code>。否则，将压缩<code>dim</code>(请参见 <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a>)，导致输出张量的尺寸比<code>input</code>小 1。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>{input}</strong> –</p>
</li>
<li>
<p><strong>{dim}</strong> –</p>
</li>
<li>
<p><strong>{keepdim}</strong> –</p>
</li>
<li>
<p><strong>输出</strong>(<em>元组</em> <em>，</em> <em>可选</em>）–两个输出张量的元组(min，min_indices）</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a
tensor([[-0.6248,  1.1334, -1.1899, -0.2803],
        [-1.4644, -0.2635, -0.3651,  0.6134],
        [ 0.2457,  0.0384,  1.0128,  0.7015],
        [-0.1153,  2.9849,  2.1458,  0.5788]])
&gt;&gt;&gt; torch.min(a, 1)
torch.return_types.min(values=tensor([-1.1899, -1.4644,  0.0384, -0.1153]), indices=tensor([2, 0, 1, 0]))

</code></pre>
<hr />
<pre><code>torch.min(input, other, out=None) → Tensor
</code></pre>
<p>将张量<code>input</code>的每个元素与张量<code>other</code>的对应元素进行比较，并按元素取最小值。 返回结果张量。</p>
<p>The shapes of <code>input</code> and <code>other</code> don't need to match, but they must be <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a>.</p>
<p><img alt="" src="../img/f033730f350678f87f2f08ee833a82fe.jpg" /></p>
<p>Note</p>
<p>When the shapes do not match, the shape of the returned output tensor follows the <a href="notes/broadcasting.html#broadcasting-semantics">broadcasting rules</a>.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the second input tensor</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([ 0.8137, -1.1740, -0.6460,  0.6308])
&gt;&gt;&gt; b = torch.randn(4)
&gt;&gt;&gt; b
tensor([-0.1369,  0.1555,  0.4019, -0.1929])
&gt;&gt;&gt; torch.min(a, b)
tensor([-0.1369, -1.1740, -0.6460, -0.1929])

</code></pre>
<hr />
<pre><code>torch.ne(input, other, out=None) → Tensor¶
</code></pre>
<p>逐元素计算<img alt="" src="../img/efbfcdce474b4ae10bb5dacb46481266.jpg" />。</p>
<p>The second argument can be a number or a tensor whose shape is <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a> with the first argument.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor to compare</p>
</li>
<li>
<p><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a> <em>or</em> <em>python:float</em>) – the tensor or value to compare</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor that must be a &lt;cite&gt;BoolTensor&lt;/cite&gt;</p>
</li>
</ul>
<p>Returns</p>
<p><code>torch.BoolTensor</code>在比较为真的每个位置都包含“真”。</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.ne(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))
tensor([[False, True], [True, False]])

</code></pre>
<hr />
<pre><code>torch.sort(input, dim=-1, descending=False, out=None) -&gt; (Tensor, LongTensor)¶
</code></pre>
<p>沿给定维度按值升序对<code>input</code>张量的元素进行排序。</p>
<p>If <code>dim</code> is not given, the last dimension of the &lt;cite&gt;input&lt;/cite&gt; is chosen.</p>
<p>如果<code>descending</code>为<code>True</code>，则元素将按值降序排序。</p>
<p>返回一个(值，索引）的命名元组，其中&lt;cite&gt;值&lt;/cite&gt;是排序的值，&lt;cite&gt;索引&lt;/cite&gt;是原始&lt;cite&gt;输入&lt;/cite&gt;张量中元素的索引。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>dim</strong> (<em>python:int__,</em> <em>optional</em>) – the dimension to sort along</p>
</li>
<li>
<p><strong>descending</strong> (<em>bool__,</em> <em>optional</em>) – controls the sorting order (ascending or descending)</p>
</li>
<li>
<p><strong>输出</strong>(<em>元组</em> <em>，</em> <em>可选</em>）–(&lt;cite&gt;张量&lt;/cite&gt;， &lt;cite&gt;LongTensor&lt;/cite&gt; )，可以选择将其用作输出缓冲区</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(3, 4)
&gt;&gt;&gt; sorted, indices = torch.sort(x)
&gt;&gt;&gt; sorted
tensor([[-0.2162,  0.0608,  0.6719,  2.3332],
        [-0.5793,  0.0061,  0.6058,  0.9497],
        [-0.5071,  0.3343,  0.9553,  1.0960]])
&gt;&gt;&gt; indices
tensor([[ 1,  0,  2,  3],
        [ 3,  1,  0,  2],
        [ 0,  3,  1,  2]])

&gt;&gt;&gt; sorted, indices = torch.sort(x, 0)
&gt;&gt;&gt; sorted
tensor([[-0.5071, -0.2162,  0.6719, -0.5793],
        [ 0.0608,  0.0061,  0.9497,  0.3343],
        [ 0.6058,  0.9553,  1.0960,  2.3332]])
&gt;&gt;&gt; indices
tensor([[ 2,  0,  0,  1],
        [ 0,  1,  1,  2],
        [ 1,  2,  2,  0]])

</code></pre>
<hr />
<pre><code>torch.topk(input, k, dim=None, largest=True, sorted=True, out=None) -&gt; (Tensor, LongTensor)¶
</code></pre>
<p>返回沿给定维度的给定<code>input</code>张量的<code>k</code>最大元素。</p>
<p>If <code>dim</code> is not given, the last dimension of the &lt;cite&gt;input&lt;/cite&gt; is chosen.</p>
<p>如果<code>largest</code>为<code>False</code>，则返回 &lt;cite&gt;k&lt;/cite&gt; 个最小的元素。</p>
<p>返回&lt;cite&gt;(值，索引）&lt;/cite&gt;的命名元组，其中&lt;cite&gt;索引&lt;/cite&gt;是原始&lt;cite&gt;输入&lt;/cite&gt;张量中元素的索引。</p>
<p>布尔选项<code>sorted</code>如果为<code>True</code>，将确保返回的 &lt;cite&gt;k&lt;/cite&gt; 元素本身已排序</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>k</strong>  (<em>python：int</em> )–“ top-k”中的 k</p>
</li>
<li>
<p><strong>dim</strong> (<em>python:int__,</em> <em>optional</em>) – the dimension to sort along</p>
</li>
<li>
<p><strong>最大的</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–控制是返回最大还是最小元素</p>
</li>
<li>
<p><strong>排序的</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–控制是否按排序顺序返回元素</p>
</li>
<li>
<p><strong>out</strong> (<em>元组</em> <em>，</em> <em>可选</em>）–可以选择提供(Tensor，LongTensor）的输出元组 缓冲区</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.arange(1., 6.)
&gt;&gt;&gt; x
tensor([ 1.,  2.,  3.,  4.,  5.])
&gt;&gt;&gt; torch.topk(x, 3)
torch.return_types.topk(values=tensor([5., 4., 3.]), indices=tensor([4, 3, 2]))

</code></pre>
<h3 id="_15">光谱操作</h3>
<hr />
<pre><code>torch.fft(input, signal_ndim, normalized=False) → Tensor¶
</code></pre>
<p>复数到复数离散傅立叶变换</p>
<p>此方法计算复数到复数离散傅里叶变换。 忽略批次尺寸，它将计算以下表达式：</p>
<p><img alt="" src="../img/f1d658cd4fac30eca4c9293d39d0d2fe.jpg" /></p>
<p>其中<img alt="" src="../img/95d07e221a9b945feb93422a44ac3d42.jpg" /> = <code>signal_ndim</code>是信号尺寸的数量，<img alt="" src="../img/8d61ccc06c1a60ca6e3c19f12aee4f33.jpg" />是信号尺寸<img alt="" src="../img/db26d1a59be5965889bd4d5533b7be61.jpg" />的尺寸。</p>
<p>此方法支持<code>signal_ndim</code>指示的 1D，2D 和 3D 复数到复数转换。 <code>input</code>必须是张量，其最后一个尺寸为 2，代表复数的实部和虚部，并且至少应具有<code>signal_ndim + 1</code>个尺寸，并可以选择任意数量的前批尺寸。 如果<code>normalized</code>设置为<code>True</code>，则通过将结果除以<img alt="" src="../img/ed8d1a6498daf3d5b9174f9c1535b2bd.jpg" />来对结果进行归一化，以使运算符为一元。</p>
<p>将实部和虚部一起返回为<code>input</code>形状相同的一个张量。</p>
<p>此函数的反函数为 <a href="#torch.ifft" title="torch.ifft"><code>ifft()</code></a> 。</p>
<p>Note</p>
<p>对于 CUDA 张量，LRU 缓存用于 cuFFT 计划，以加快在具有相同配置的相同几何形状的张量上重复运行 FFT 方法的速度。 有关如何监视和控制缓存的更多详细信息，请参见 <a href="notes/cuda.html#cufft-plan-cache">cuFFT 计划缓存</a>。</p>
<p>Warning</p>
<p>对于 CPU 张量，此方法当前仅适用于 MKL。 使用<code>torch.backends.mkl.is_available()</code>检查是否安装了 MKL。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–至少<code>signal_ndim</code> <code>+ 1</code>尺寸的输入张量</p>
</li>
<li>
<p><strong>signal_ndim</strong>  (<em>python：int</em> )–每个信号中的维数。 <code>signal_ndim</code>只能是 1、2 或 3</p>
</li>
<li>
<p><strong>标准化的</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–控制是否返回标准化结果。 默认值：<code>False</code></p>
</li>
</ul>
<p>Returns</p>
<p>包含复数到复数傅里叶变换结果的张量</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; # unbatched 2D FFT
&gt;&gt;&gt; x = torch.randn(4, 3, 2)
&gt;&gt;&gt; torch.fft(x, 2)
tensor([[[-0.0876,  1.7835],
         [-2.0399, -2.9754],
         [ 4.4773, -5.0119]],

        [[-1.5716,  2.7631],
         [-3.8846,  5.2652],
         [ 0.2046, -0.7088]],

        [[ 1.9938, -0.5901],
         [ 6.5637,  6.4556],
         [ 2.9865,  4.9318]],

        [[ 7.0193,  1.1742],
         [-1.3717, -2.1084],
         [ 2.0289,  2.9357]]])
&gt;&gt;&gt; # batched 1D FFT
&gt;&gt;&gt; torch.fft(x, 1)
tensor([[[ 1.8385,  1.2827],
         [-0.1831,  1.6593],
         [ 2.4243,  0.5367]],

        [[-0.9176, -1.5543],
         [-3.9943, -2.9860],
         [ 1.2838, -2.9420]],

        [[-0.8854, -0.6860],
         [ 2.4450,  0.0808],
         [ 1.3076, -0.5768]],

        [[-0.1231,  2.7411],
         [-0.3075, -1.7295],
         [-0.5384, -2.0299]]])
&gt;&gt;&gt; # arbitrary number of batch dimensions, 2D FFT
&gt;&gt;&gt; x = torch.randn(3, 3, 5, 5, 2)
&gt;&gt;&gt; y = torch.fft(x, 2)
&gt;&gt;&gt; y.shape
torch.Size([3, 3, 5, 5, 2])

</code></pre>
<hr />
<pre><code>torch.ifft(input, signal_ndim, normalized=False) → Tensor¶
</code></pre>
<p>复数到逆离散傅立叶逆变换</p>
<p>此方法计算复杂到复杂的逆离散傅里叶变换。 忽略批次尺寸，它将计算以下表达式：</p>
<p><img alt="" src="../img/5040603503ecec3ee3de71bfe027769c.jpg" /></p>
<p>where <img alt="" src="../img/95d07e221a9b945feb93422a44ac3d42.jpg" /> = <code>signal_ndim</code> is number of dimensions for the signal, and <img alt="" src="../img/8d61ccc06c1a60ca6e3c19f12aee4f33.jpg" /> is the size of signal dimension <img alt="" src="../img/db26d1a59be5965889bd4d5533b7be61.jpg" />.</p>
<p>参数规范几乎与 <a href="#torch.fft" title="torch.fft"><code>fft()</code></a> 相同。 但是，如果将<code>normalized</code>设置为<code>True</code>，它将返回结果乘以<img alt="" src="../img/77177756d6e74e1e1a97289dfeea062c.jpg" />的结果，从而成为一元运算符。 因此，要反转 <a href="#torch.fft" title="torch.fft"><code>fft()</code></a> ，应为 <a href="#torch.fft" title="torch.fft"><code>fft()</code></a> 设置<code>normalized</code>自变量。</p>
<p>Returns the real and the imaginary parts together as one tensor of the same shape of <code>input</code>.</p>
<p>此函数的反函数为 <a href="#torch.fft" title="torch.fft"><code>fft()</code></a> 。</p>
<p>Note</p>
<p>For CUDA tensors, an LRU cache is used for cuFFT plans to speed up repeatedly running FFT methods on tensors of same geometry with same configuration. See <a href="notes/cuda.html#cufft-plan-cache">cuFFT plan cache</a> for more details on how to monitor and control the cache.</p>
<p>Warning</p>
<p>For CPU tensors, this method is currently only available with MKL. Use <code>torch.backends.mkl.is_available()</code> to check if MKL is installed.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor of at least <code>signal_ndim</code> <code>+ 1</code> dimensions</p>
</li>
<li>
<p><strong>signal_ndim</strong> (<em>python:int</em>) – the number of dimensions in each signal. <code>signal_ndim</code> can only be 1, 2 or 3</p>
</li>
<li>
<p><strong>normalized</strong> (<em>bool__,</em> <em>optional</em>) – controls whether to return normalized results. Default: <code>False</code></p>
</li>
</ul>
<p>Returns</p>
<p>包含复数到复数傅立叶逆变换结果的张量</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(3, 3, 2)
&gt;&gt;&gt; x
tensor([[[ 1.2766,  1.3680],
         [-0.8337,  2.0251],
         [ 0.9465, -1.4390]],

        [[-0.1890,  1.6010],
         [ 1.1034, -1.9230],
         [-0.9482,  1.0775]],

        [[-0.7708, -0.8176],
         [-0.1843, -0.2287],
         [-1.9034, -0.2196]]])
&gt;&gt;&gt; y = torch.fft(x, 2)
&gt;&gt;&gt; torch.ifft(y, 2)  # recover x
tensor([[[ 1.2766,  1.3680],
         [-0.8337,  2.0251],
         [ 0.9465, -1.4390]],

        [[-0.1890,  1.6010],
         [ 1.1034, -1.9230],
         [-0.9482,  1.0775]],

        [[-0.7708, -0.8176],
         [-0.1843, -0.2287],
         [-1.9034, -0.2196]]])

</code></pre>
<hr />
<pre><code>torch.rfft(input, signal_ndim, normalized=False, onesided=True) → Tensor¶
</code></pre>
<p>实数到复杂离散傅里叶变换</p>
<p>此方法计算实数到复杂的离散傅里叶变换。 它在数学上等效于 <a href="#torch.fft" title="torch.fft"><code>fft()</code></a> ，只是输入和输出格式不同。</p>
<p>此方法支持 1D，2D 和 3D 实数到复杂的变换，由<code>signal_ndim</code>指示。 <code>input</code>必须是至少具有<code>signal_ndim</code>尺寸且可以选择任意数量的前导批尺寸的张量。 如果<code>normalized</code>设置为<code>True</code>，则通过将结果除以<img alt="" src="../img/ed8d1a6498daf3d5b9174f9c1535b2bd.jpg" />来标准化结果，以使运算符为 the，其中<img alt="" src="../img/8d61ccc06c1a60ca6e3c19f12aee4f33.jpg" />是信号维度<img alt="" src="../img/db26d1a59be5965889bd4d5533b7be61.jpg" />的大小。</p>
<p>实数到复杂的傅立叶变换结果遵循共轭对称性：</p>
<p><img alt="" src="../img/eacabf6004703215e86dc3522380c6ed.jpg" /></p>
<p>其中索引算术是计算模量的相应尺寸的大小，<img alt="" src="../img/f1deebf6650afaf555a88b62fc3992f0.jpg" />是共轭算子，<img alt="" src="../img/95d07e221a9b945feb93422a44ac3d42.jpg" /> = <code>signal_ndim</code>。 <code>onesided</code>标志控制是否避免输出结果中的冗余。 如果设置为<code>True</code>(默认值），输出将不是形状<img alt="" src="../img/f6dde7278b3a3d1ca525c45b7ad2b155.jpg" />的完全复杂结果，其中<img alt="" src="../img/dcef98688866c0d5a21137cf53bf228d.jpg" />是<code>input</code>的形状，但是最后一个尺寸将是<img alt="" src="../img/ab88558f86cd940a2c036983b4a17bc0.jpg" />尺寸的一半 。</p>
<p>此函数的反函数为 <a href="#torch.irfft" title="torch.irfft"><code>irfft()</code></a> 。</p>
<p>Note</p>
<p>For CUDA tensors, an LRU cache is used for cuFFT plans to speed up repeatedly running FFT methods on tensors of same geometry with same configuration. See <a href="notes/cuda.html#cufft-plan-cache">cuFFT plan cache</a> for more details on how to monitor and control the cache.</p>
<p>Warning</p>
<p>For CPU tensors, this method is currently only available with MKL. Use <code>torch.backends.mkl.is_available()</code> to check if MKL is installed.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–至少<code>signal_ndim</code>尺寸的输入张量</p>
</li>
<li>
<p><strong>signal_ndim</strong> (<em>python:int</em>) – the number of dimensions in each signal. <code>signal_ndim</code> can only be 1, 2 or 3</p>
</li>
<li>
<p><strong>normalized</strong> (<em>bool__,</em> <em>optional</em>) – controls whether to return normalized results. Default: <code>False</code></p>
</li>
<li>
<p><strong>单面</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–控制是否返回一半结果以避免重复。 默认值：<code>True</code></p>
</li>
</ul>
<p>Returns</p>
<p>包含实数到复数傅立叶变换结果的张量</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(5, 5)
&gt;&gt;&gt; torch.rfft(x, 2).shape
torch.Size([5, 3, 2])
&gt;&gt;&gt; torch.rfft(x, 2, onesided=False).shape
torch.Size([5, 5, 2])

</code></pre>
<hr />
<pre><code>torch.irfft(input, signal_ndim, normalized=False, onesided=True, signal_sizes=None) → Tensor¶
</code></pre>
<p>复数到实数离散傅里叶逆变换</p>
<p>此方法计算复数到实数的离散傅里叶逆变换。 它在数学上等效于 <a href="#torch.ifft" title="torch.ifft"><code>ifft()</code></a> ，只是输入和输出格式不同。</p>
<p>参数规范几乎与 <a href="#torch.ifft" title="torch.ifft"><code>ifft()</code></a> 相同。 与 <a href="#torch.ifft" title="torch.ifft"><code>ifft()</code></a> 相似，如果<code>normalized</code>设置为<code>True</code>，则通过将结果与<img alt="" src="../img/ed8d1a6498daf3d5b9174f9c1535b2bd.jpg" />相乘来对结果进行归一化，以使运算符为 ary，其中<img alt="" src="../img/8d61ccc06c1a60ca6e3c19f12aee4f33.jpg" />是信号的大小 尺寸<img alt="" src="../img/db26d1a59be5965889bd4d5533b7be61.jpg" />。</p>
<p>Note</p>
<p>由于共轭对称性，<code>input</code>不需要包含完整的复数频率值。 大约一半的值就足够了，就像 <a href="#torch.rfft" title="torch.rfft"><code>rfft()</code></a> 和<code>rfft(signal, onesided=True)</code>给出<code>input</code>的情况一样。 在这种情况下，请将此方法的<code>onesided</code>参数设置为<code>True</code>。 此外，原始信号形状信息有时会丢失，可以选择将<code>signal_sizes</code>设置为原始信号的大小(如果处于批处理模式，则没有批处理尺寸），以恢复正确的形状。</p>
<p>因此，要反转 <a href="#torch.rfft" title="torch.rfft"><code>rfft()</code></a> ，应为 <a href="#torch.irfft" title="torch.irfft"><code>irfft()</code></a> 设置<code>normalized</code>和<code>onesided</code>自变量，并且最好使用<code>signal_sizes</code>以避免大小 不匹配。 有关大小不匹配的情况，请参见下面的示例。</p>
<p>有关共轭对称性的详细信息，请参见 <a href="#torch.rfft" title="torch.rfft"><code>rfft()</code></a> 。</p>
<p>此函数的反函数为 <a href="#torch.rfft" title="torch.rfft"><code>rfft()</code></a> 。</p>
<p>Warning</p>
<p>通常，此函数的输入应包含遵循共轭对称性的值。 请注意，即使<code>onesided</code>为<code>True</code>，仍然经常需要在某些部分上保持对称。 当不满足此要求时， <a href="#torch.irfft" title="torch.irfft"><code>irfft()</code></a> 的行为是不确定的。 由于 <a href="autograd.html#torch.autograd.gradcheck" title="torch.autograd.gradcheck"><code>torch.autograd.gradcheck()</code></a> 估计带有点扰动的数字雅可比行列式，因此 <a href="#torch.irfft" title="torch.irfft"><code>irfft()</code></a> 几乎肯定会失败。</p>
<p>Note</p>
<p>For CUDA tensors, an LRU cache is used for cuFFT plans to speed up repeatedly running FFT methods on tensors of same geometry with same configuration. See <a href="notes/cuda.html#cufft-plan-cache">cuFFT plan cache</a> for more details on how to monitor and control the cache.</p>
<p>Warning</p>
<p>For CPU tensors, this method is currently only available with MKL. Use <code>torch.backends.mkl.is_available()</code> to check if MKL is installed.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor of at least <code>signal_ndim</code> <code>+ 1</code> dimensions</p>
</li>
<li>
<p><strong>signal_ndim</strong> (<em>python:int</em>) – the number of dimensions in each signal. <code>signal_ndim</code> can only be 1, 2 or 3</p>
</li>
<li>
<p><strong>normalized</strong> (<em>bool__,</em> <em>optional</em>) – controls whether to return normalized results. Default: <code>False</code></p>
</li>
<li>
<p><strong>单面</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–控制<code>input</code>是否对半以避免冗余，例如通过 <a href="#torch.rfft" title="torch.rfft"><code>rfft()</code></a> 。 默认值：<code>True</code></p>
</li>
<li>
<p><strong>signal_sizes</strong> (列表或<code>torch.Size</code>，可选）–原始信号的大小(无批次尺寸）。 默认值：<code>None</code></p>
</li>
</ul>
<p>Returns</p>
<p>包含复数到实数傅立叶逆变换结果的张量</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(4, 4)
&gt;&gt;&gt; torch.rfft(x, 2, onesided=True).shape
torch.Size([4, 3, 2])
&gt;&gt;&gt;
&gt;&gt;&gt; # notice that with onesided=True, output size does not determine the original signal size
&gt;&gt;&gt; x = torch.randn(4, 5)

&gt;&gt;&gt; torch.rfft(x, 2, onesided=True).shape
torch.Size([4, 3, 2])
&gt;&gt;&gt;
&gt;&gt;&gt; # now we use the original shape to recover x
&gt;&gt;&gt; x
tensor([[-0.8992,  0.6117, -1.6091, -0.4155, -0.8346],
        [-2.1596, -0.0853,  0.7232,  0.1941, -0.0789],
        [-2.0329,  1.1031,  0.6869, -0.5042,  0.9895],
        [-0.1884,  0.2858, -1.5831,  0.9917, -0.8356]])
&gt;&gt;&gt; y = torch.rfft(x, 2, onesided=True)
&gt;&gt;&gt; torch.irfft(y, 2, onesided=True, signal_sizes=x.shape)  # recover x
tensor([[-0.8992,  0.6117, -1.6091, -0.4155, -0.8346],
        [-2.1596, -0.0853,  0.7232,  0.1941, -0.0789],
        [-2.0329,  1.1031,  0.6869, -0.5042,  0.9895],
        [-0.1884,  0.2858, -1.5831,  0.9917, -0.8356]])

</code></pre>
<hr />
<pre><code>torch.stft(input, n_fft, hop_length=None, win_length=None, window=None, center=True, pad_mode='reflect', normalized=False, onesided=True)¶
</code></pre>
<p>短时傅立叶变换(STFT）。</p>
<p>忽略可选的批处理维，此方法将计算以下表达式：</p>
<p><img alt="" src="../img/33a339c5713e9db708c9eed0a7969d63.jpg" /></p>
<p>其中<img alt="" src="../img/03327e7b697db30918e96b2209927929.jpg" />是滑动窗口的索引，<img alt="" src="../img/47bff0d0b1fe99458172eae026405291.jpg" />是<img alt="" src="../img/c913af4e6c1176b6e9846f5f85011461.jpg" />的频率。 当<code>onesided</code>为默认值<code>True</code>时，</p>
<ul>
<li>
<p><code>input</code>必须是一维时间序列或二维时间序列批次。</p>
</li>
<li>
<p>如果<code>hop_length</code>为<code>None</code>(默认值），则将其视为等于<code>floor(n_fft / 4)</code>。</p>
</li>
<li>
<p>如果<code>win_length</code>为<code>None</code>(默认值），则将其视为等于<code>n_fft</code>。</p>
</li>
<li>
<p><code>window</code>可以是大小为<code>win_length</code>的一维张量，例如来自 <a href="#torch.hann_window" title="torch.hann_window"><code>torch.hann_window()</code></a> 。 如果<code>window</code>为<code>None</code>(默认），则将其视为窗口中到处都有<img alt="" src="../img/21c5bc8d40ee5ab2e18d64aaba8359c7.jpg" />。 如果使用<img alt="" src="../img/3579be09fb723df705e009678d545d64.jpg" />，则将<code>window</code>的两面填充长度<code>n_fft</code>，然后再进行应用。</p>
</li>
<li>
<p>如果<code>center</code>为<code>True</code>(默认值），则将在两边都填充<code>input</code>，以使第<img alt="" src="../img/ea198b23d3d3f891bcc7684b8659b3e4.jpg" />帧位于时间<img alt="" src="../img/33e0acbd6c2b37b3ef3123f4dc18a1c0.jpg" />的中心。 否则，第<img alt="" src="../img/ea198b23d3d3f891bcc7684b8659b3e4.jpg" />帧在时间<img alt="" src="../img/33e0acbd6c2b37b3ef3123f4dc18a1c0.jpg" />开始。</p>
</li>
<li>
<p><code>pad_mode</code>确定当<code>center</code>为<code>True</code>时在<code>input</code>上使用的填充方法。 有关所有可用选项，请参见 <a href="nn.functional.html#torch.nn.functional.pad" title="torch.nn.functional.pad"><code>torch.nn.functional.pad()</code></a> 。 默认值为<code>"reflect"</code>。</p>
</li>
<li>
<p>如果<code>onesided</code>为<code>True</code>(默认值），则仅返回<img alt="" src="../img/c15755e95a374ed10ce009af8ac323f5.jpg" />中<img alt="" src="../img/47bff0d0b1fe99458172eae026405291.jpg" />的值，因为实数到复数傅里叶变换满足共轭对称性，即<img alt="" src="../img/da5553744a960ad6822ef967ba0ea46f.jpg" />。</p>
</li>
<li>
<p>如果<code>normalized</code>为<code>True</code>(默认为<code>False</code>），则该函数返回归一化的 STFT 结果，即乘以<img alt="" src="../img/3a83bac3c29283a2eb3fcc60e15f37aa.jpg" />。</p>
</li>
</ul>
<p>将实部和虚部一起返回为一个大小为<img alt="" src="../img/1d33f2bcea40c743acb3c7e78a69ca62.jpg" />的张量，其中<img alt="" src="../img/dcef98688866c0d5a21137cf53bf228d.jpg" />是可选的<code>input</code>批大小，<img alt="" src="../img/7ea0d6aa290d37c8bc57957206bafe80.jpg" />是应用 STFT 的频率数，<img alt="" src="../img/e76349865309323eafcc6a541e3073ae.jpg" />是总数 所使用的帧数，最后一维中的每一对代表一个复数，作为实部和虚部。</p>
<p>Warning</p>
<p>此功能在版本 0.4.1 更改了签名。 使用前一个签名进行调用可能会导致错误或返回错误的结果。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p>
</li>
<li>
<p><strong>n_fft</strong>  (<em>python：int</em> )–傅立叶变换的大小</p>
</li>
<li>
<p><strong>hop_length</strong>  (<em>python：int</em> <em>，</em> <em>可选</em>）–相邻滑动窗口框架之间的距离。 默认值：<code>None</code>(等同于<code>floor(n_fft / 4)</code>）</p>
</li>
<li>
<p><strong>win_length</strong>  (<em>python：int</em> <em>，</em> <em>可选</em>）–窗口框架和 STFT 过滤器的大小。 默认值：<code>None</code>(等同于<code>n_fft</code>）</p>
</li>
<li>
<p><strong>窗口</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>，</em> <em>可选</em>）–可选窗口功能。 默认值：<code>None</code>(被视为所有<img alt="" src="../img/21c5bc8d40ee5ab2e18d64aaba8359c7.jpg" />的窗口）</p>
</li>
<li>
<p><strong>中心</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–是否在两侧都填充<code>input</code>以便第<img alt="" src="../img/ea198b23d3d3f891bcc7684b8659b3e4.jpg" />个帧位于 集中在时间<img alt="" src="../img/33e0acbd6c2b37b3ef3123f4dc18a1c0.jpg" />上。 默认值：<code>True</code></p>
</li>
<li>
<p><strong>pad_mode</strong> (<em>字符串</em> <em>，</em> <em>可选</em>）–控制当<code>center</code>为<code>True</code>时使用的填充方法。 默认值：<code>"reflect"</code></p>
</li>
<li>
<p><strong>规范化的</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–控制是否返回规范化的 STFT 结果默认：<code>False</code></p>
</li>
<li>
<p><strong>单面</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–控制是否返回一半结果以避免冗余默认值：<code>True</code></p>
</li>
</ul>
<p>Returns</p>
<p>包含 STFT 结果的张量具有上述形状</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<hr />
<pre><code>torch.bartlett_window(window_length, periodic=True, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor¶
</code></pre>
<p>Bartlett 窗口功能。</p>
<p><img alt="" src="../img/7a8f3ed9f6703292e22f8787927206ae.jpg" /></p>
<p>其中<img alt="" src="../img/7ea0d6aa290d37c8bc57957206bafe80.jpg" />是整个窗口的大小。</p>
<p>输入<code>window_length</code>是控制返回的窗口大小的正整数。 <code>periodic</code>标志确定返回的窗口是否从对称窗口中修剪掉最后一个重复值，并准备好用作具有 <a href="#torch.stft" title="torch.stft"><code>torch.stft()</code></a> 之类的周期性窗口。 因此，如果<code>periodic</code>为真，则上式中的<img alt="" src="../img/7ea0d6aa290d37c8bc57957206bafe80.jpg" />实际上为<img alt="" src="../img/1cfea2e0a42e6ef5aee44b22f2bf2ab4.jpg" />。 另外，我们总是<code>torch.bartlett_window(L, periodic=True)</code>等于<code>torch.bartlett_window(L + 1, periodic=False)[:-1])</code>。</p>
<p>Note</p>
<p>如果<code>window_length</code> <img alt="" src="../img/193049f37e5b8049aa1c2f98d7bf7b2c.jpg" />，则返回的窗口包含单个值 1。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>window_length</strong>  (<em>python：int</em> )–返回窗口的大小</p>
</li>
<li>
<p><strong>周期性</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–如果为 True，则返回用作周期性函数的窗口。 如果为 False，则返回一个对称窗口。</p>
</li>
<li>
<p><strong>dtype</strong>  (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> ，可选）–返回张量的所需数据类型。 默认值：如果<code>None</code>使用全局默认值(请参见 <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>)。 仅支持浮点类型。</p>
</li>
<li>
<p><strong>布局</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a> ，可选）–返回的窗口张量的所需布局。 仅支持<code>torch.strided</code>(密集布局）。</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Returns</p>
<p>包含窗口的大小为<img alt="" src="../img/ba92c6cf7c07822f0e23d550a6cf1108.jpg" />的一维张量</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<hr />
<pre><code>torch.blackman_window(window_length, periodic=True, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor¶
</code></pre>
<p>布莱克曼窗口功能。</p>
<p><img alt="" src="../img/ee8023c13513826303e160aa8135b175.jpg" /></p>
<p>where <img alt="" src="../img/7ea0d6aa290d37c8bc57957206bafe80.jpg" /> is the full window size.</p>
<p>输入<code>window_length</code>是控制返回的窗口大小的正整数。 <code>periodic</code>标志确定返回的窗口是否从对称窗口中修剪掉最后一个重复值，并准备好用作具有 <a href="#torch.stft" title="torch.stft"><code>torch.stft()</code></a> 之类的周期性窗口。 因此，如果<code>periodic</code>为真，则上式中的<img alt="" src="../img/7ea0d6aa290d37c8bc57957206bafe80.jpg" />实际上为<img alt="" src="../img/1cfea2e0a42e6ef5aee44b22f2bf2ab4.jpg" />。 另外，我们总是<code>torch.blackman_window(L, periodic=True)</code>等于<code>torch.blackman_window(L + 1, periodic=False)[:-1])</code>。</p>
<p>Note</p>
<p>If <code>window_length</code> <img alt="" src="../img/193049f37e5b8049aa1c2f98d7bf7b2c.jpg" />, the returned window contains a single value 1.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>window_length</strong> (<em>python:int</em>) – the size of returned window</p>
</li>
<li>
<p><strong>periodic</strong> (<em>bool__,</em> <em>optional</em>) – If True, returns a window to be used as periodic function. If False, return a symmetric window.</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). Only floating point types are supported.</p>
</li>
<li>
<p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) – the desired layout of returned window tensor. Only <code>torch.strided</code> (dense layout) is supported.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Returns</p>
<p>A 1-D tensor of size <img alt="" src="../img/ba92c6cf7c07822f0e23d550a6cf1108.jpg" /> containing the window</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<hr />
<pre><code>torch.hamming_window(window_length, periodic=True, alpha=0.54, beta=0.46, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor¶
</code></pre>
<p>汉明窗功能。</p>
<p><img alt="" src="../img/5fe81b7bd5bd7c7e122962f6739b7c8e.jpg" /></p>
<p>where <img alt="" src="../img/7ea0d6aa290d37c8bc57957206bafe80.jpg" /> is the full window size.</p>
<p>输入<code>window_length</code>是控制返回的窗口大小的正整数。 <code>periodic</code>标志确定返回的窗口是否从对称窗口中修剪掉最后一个重复值，并准备好用作具有 <a href="#torch.stft" title="torch.stft"><code>torch.stft()</code></a> 之类的周期性窗口。 因此，如果<code>periodic</code>为真，则上式中的<img alt="" src="../img/7ea0d6aa290d37c8bc57957206bafe80.jpg" />实际上为<img alt="" src="../img/1cfea2e0a42e6ef5aee44b22f2bf2ab4.jpg" />。 另外，我们总是<code>torch.hamming_window(L, periodic=True)</code>等于<code>torch.hamming_window(L + 1, periodic=False)[:-1])</code>。</p>
<p>Note</p>
<p>If <code>window_length</code> <img alt="" src="../img/193049f37e5b8049aa1c2f98d7bf7b2c.jpg" />, the returned window contains a single value 1.</p>
<p>Note</p>
<p>这是 <a href="#torch.hann_window" title="torch.hann_window"><code>torch.hann_window()</code></a> 的通用版本。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>window_length</strong> (<em>python:int</em>) – the size of returned window</p>
</li>
<li>
<p><strong>periodic</strong> (<em>bool__,</em> <em>optional</em>) – If True, returns a window to be used as periodic function. If False, return a symmetric window.</p>
</li>
<li>
<p><strong>alpha</strong>  (<em>python：float</em> <em>，</em> <em>可选</em>）–上式中的系数<img alt="" src="../img/5b9866fb35b01c553ed3e738e3972ae9.jpg" /></p>
</li>
<li>
<p><strong>beta</strong>  (<em>python：float</em> <em>，</em> <em>可选</em>）–上式中的系数<img alt="" src="../img/53a496ec7d546e2af9595a7055dd6a7e.jpg" /></p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). Only floating point types are supported.</p>
</li>
<li>
<p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) – the desired layout of returned window tensor. Only <code>torch.strided</code> (dense layout) is supported.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Returns</p>
<p>A 1-D tensor of size <img alt="" src="../img/ba92c6cf7c07822f0e23d550a6cf1108.jpg" /> containing the window</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<hr />
<pre><code>torch.hann_window(window_length, periodic=True, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor¶
</code></pre>
<p>汉恩窗口功能。</p>
<p><img alt="" src="../img/05f672670a77d8bc009f743a19461201.jpg" /></p>
<p>where <img alt="" src="../img/7ea0d6aa290d37c8bc57957206bafe80.jpg" /> is the full window size.</p>
<p>输入<code>window_length</code>是控制返回的窗口大小的正整数。 <code>periodic</code>标志确定返回的窗口是否从对称窗口中修剪掉最后一个重复值，并准备好用作具有 <a href="#torch.stft" title="torch.stft"><code>torch.stft()</code></a> 之类的周期性窗口。 因此，如果<code>periodic</code>为真，则上式中的<img alt="" src="../img/7ea0d6aa290d37c8bc57957206bafe80.jpg" />实际上为<img alt="" src="../img/1cfea2e0a42e6ef5aee44b22f2bf2ab4.jpg" />。 另外，我们总是<code>torch.hann_window(L, periodic=True)</code>等于<code>torch.hann_window(L + 1, periodic=False)[:-1])</code>。</p>
<p>Note</p>
<p>If <code>window_length</code> <img alt="" src="../img/193049f37e5b8049aa1c2f98d7bf7b2c.jpg" />, the returned window contains a single value 1.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>window_length</strong> (<em>python:int</em>) – the size of returned window</p>
</li>
<li>
<p><strong>periodic</strong> (<em>bool__,</em> <em>optional</em>) – If True, returns a window to be used as periodic function. If False, return a symmetric window.</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). Only floating point types are supported.</p>
</li>
<li>
<p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) – the desired layout of returned window tensor. Only <code>torch.strided</code> (dense layout) is supported.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li>
<p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Returns</p>
<p>A 1-D tensor of size <img alt="" src="../img/ba92c6cf7c07822f0e23d550a6cf1108.jpg" /> containing the window</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<h3 id="_16">其他作业</h3>
<hr />
<pre><code>torch.bincount(input, weights=None, minlength=0) → Tensor¶
</code></pre>
<p>计算非负整数数组中每个值的频率。</p>
<p>除非<code>input</code>为空，否则 bin 的数量(大小 1）比<code>input</code>中的最大值大一个，在这种情况下，结果是大小为 0 的张量。如果指定<code>minlength</code>，则 bin 的数量为 至少<code>minlength</code>为空，如果<code>input</code>为空，则结果为大小为<code>minlength</code>的张量填充零。 如果<code>n</code>是位置<code>i</code>的值，则如果指定了<code>weights</code>则为<code>out[n] += weights[i]</code>，否则指定<code>out[n] += 1</code>。</p>
<p>Note</p>
<p>使用 CUDA 后端时，此操作可能会导致不确定的行为，不容易关闭。 有关背景，请参见<a href="notes/randomness.html">重现性</a>的注释。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)– 1-d int 张量</p>
</li>
<li>
<p><strong>权重</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–可选，输入张量中每个值的权重。 应具有与输入张量相同的大小。</p>
</li>
<li>
<p><strong>minlength</strong>  (<em>python：int</em> )–可选，最小存储箱数。 应该是非负的。</p>
</li>
</ul>
<p>Returns</p>
<p>如果<code>input</code>为非空，则为<code>Size([max(input) + 1])</code>形状的张量，否则为<code>Size(0)</code></p>
<p>Return type</p>
<p>输出(<a href="tensors.html#torch.Tensor" title="torch.Tensor">张量</a>）</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; input = torch.randint(0, 8, (5,), dtype=torch.int64)
&gt;&gt;&gt; weights = torch.linspace(0, 1, steps=5)
&gt;&gt;&gt; input, weights
(tensor([4, 3, 6, 3, 4]),
 tensor([ 0.0000,  0.2500,  0.5000,  0.7500,  1.0000])

&gt;&gt;&gt; torch.bincount(input)
tensor([0, 0, 0, 2, 2, 0, 1])

&gt;&gt;&gt; input.bincount(weights)
tensor([0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.5000])

</code></pre>
<hr />
<pre><code>torch.broadcast_tensors(*tensors) → List of Tensors¶
</code></pre>
<p>根据<a href="notes/broadcasting.html#broadcasting-semantics">广播语义</a>广播给定张量。</p>
<p>Parameters</p>
<p><strong>*张量</strong> –任意数量的相同类型的张量</p>
<p>Warning</p>
<p>广播张量的一个以上元素可以引用单个存储位置。 结果，就地操作(尤其是矢量化的操作）可能会导致错误的行为。 如果需要写张量，请先克隆它们。</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.arange(3).view(1, 3)
&gt;&gt;&gt; y = torch.arange(2).view(2, 1)
&gt;&gt;&gt; a, b = torch.broadcast_tensors(x, y)
&gt;&gt;&gt; a.size()
torch.Size([2, 3])
&gt;&gt;&gt; a
tensor([[0, 1, 2],
        [0, 1, 2]])

</code></pre>
<hr />
<pre><code>torch.cartesian_prod(*tensors)¶
</code></pre>
<p>给定张量序列的笛卡尔积。 行为类似于 python 的 &lt;cite&gt;itertools.product&lt;/cite&gt; 。</p>
<p>Parameters</p>
<p><strong>*张量</strong> –任意数量的一维张量。</p>
<p>Returns</p>
<pre><code>A tensor equivalent to converting all the input tensors into lists,
</code></pre>
<p>在这些列表上执行 &lt;cite&gt;itertools.product&lt;/cite&gt; ，最后将结果列表转换为张量。</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = [1, 2, 3]
&gt;&gt;&gt; b = [4, 5]
&gt;&gt;&gt; list(itertools.product(a, b))
[(1, 4), (1, 5), (2, 4), (2, 5), (3, 4), (3, 5)]
&gt;&gt;&gt; tensor_a = torch.tensor(a)
&gt;&gt;&gt; tensor_b = torch.tensor(b)
&gt;&gt;&gt; torch.cartesian_prod(tensor_a, tensor_b)
tensor([[1, 4],
        [1, 5],
        [2, 4],
        [2, 5],
        [3, 4],
        [3, 5]])

</code></pre>
<hr />
<pre><code>torch.cdist(x1, x2, p=2, compute_mode='use_mm_for_euclid_dist_if_necessary')¶
</code></pre>
<p>计算批处理行向量的两个集合的每对之间的 p 范数距离。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>x1</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–形状为<img alt="" src="../img/b34d73ac200b58cfa9bc0608c6e1658a.jpg" />的输入张量。</p>
</li>
<li>
<p><strong>x2</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–形状为<img alt="" src="../img/a77a440e5ab82d234d8ad3c6f8c5dc73.jpg" />的输入张量。</p>
</li>
<li>
<p><strong>p</strong> -p 范数距离的 p 值，以计算每个向量对<img alt="" src="../img/2ed3340d16bd283f671129abab392a0b.jpg" />之间的距离。</p>
</li>
<li>
<p><strong>compute_mode</strong> –'use_mm_for_euclid_dist_if_necessary'-如果 P &gt; 25 或 R &gt; 25'use_mm_for_euclid_dist'-将始终使用矩阵乘法方法来计算欧几里德距离(p = 2） 欧式距离(p = 2）'donot_use_mm_for_euclid_dist'-永远不会使用矩阵乘法方法来计算欧式距离(p = 2）默认值：use_mm_for_euclid_dist_if_necessary。</p>
</li>
</ul>
<p>如果 x1 具有形状<img alt="" src="../img/b34d73ac200b58cfa9bc0608c6e1658a.jpg" />，而 x2 具有形状<img alt="" src="../img/a77a440e5ab82d234d8ad3c6f8c5dc73.jpg" />，则输出将具有形状<img alt="" src="../img/48db404261f2466cb86616d6f920e894.jpg" />。</p>
<p>如果<img alt="" src="../img/5323d331671967548fee2dd052f584b9.jpg" />，则此函数等效于 &lt;cite&gt;scipy.spatial.distance.cdist(input，'minkowski'，p = p）&lt;/cite&gt;。 当<img alt="" src="../img/0ed455324c64d9801ef75fe54be6b565.jpg" />等于 &lt;cite&gt;scipy.spatial.distance.cdist(input，'hamming'）* M&lt;/cite&gt; 。 当<img alt="" src="../img/2a680efba9b8e37cfffde015324eb07e.jpg" />时，最接近的 scipy 函数是 &lt;cite&gt;scipy.spatial.distance.cdist(xn，lambda x，y：np.abs(x-y）.max(））&lt;/cite&gt;。</p>
<p>Example</p>
<pre><code>&gt;&gt;&gt; a = torch.tensor([[0.9041,  0.0196], [-0.3108, -2.4423], [-0.4821,  1.059]])
&gt;&gt;&gt; a
tensor([[ 0.9041,  0.0196],
        [-0.3108, -2.4423],
        [-0.4821,  1.0590]])
&gt;&gt;&gt; b = torch.tensor([[-2.1763, -0.4713], [-0.6986,  1.3702]])
&gt;&gt;&gt; b
tensor([[-2.1763, -0.4713],
        [-0.6986,  1.3702]])
&gt;&gt;&gt; torch.cdist(a, b, p=2)
tensor([[3.1193, 2.0959],
        [2.7138, 3.8322],
        [2.2830, 0.3791]])

</code></pre>
<hr />
<pre><code>torch.combinations(input, r=2, with_replacement=False) → seq¶
</code></pre>
<p>计算给定张量的长度<img alt="" src="../img/a892400c6ed0045af4aedcc03b3e5fd5.jpg" />的组合。 当 &lt;cite&gt;with_replacement&lt;/cite&gt; 设置为 &lt;cite&gt;False&lt;/cite&gt; 时，该行为类似于 python 的 &lt;cite&gt;itertools.combinations&lt;/cite&gt; ；当 &lt;cite&gt;with_replacement&lt;/cite&gt; 设置为时，该行为与 &lt;cite&gt;itertools.combinations_with_replacement&lt;/cite&gt; 相似。 HTG10]设置为 &lt;cite&gt;True&lt;/cite&gt; 。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–一维矢量。</p>
</li>
<li>
<p><strong>r</strong>  (<em>python：int</em> <em>，</em> <em>可选</em>）–要组合的元素数</p>
</li>
<li>
<p><strong>with_replacement</strong> (<em>布尔值</em> <em>，</em> <em>可选</em>）–是否允许重复复制</p>
</li>
</ul>
<p>Returns</p>
<p>等于将所有输入张量转换为列表的张量，对这些列表执行 &lt;cite&gt;itertools.combinations&lt;/cite&gt; 或 &lt;cite&gt;itertools.combinations_with_replacement&lt;/cite&gt; ，最后将结果列表转换为张量。</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = [1, 2, 3]
&gt;&gt;&gt; list(itertools.combinations(a, r=2))
[(1, 2), (1, 3), (2, 3)]
&gt;&gt;&gt; list(itertools.combinations(a, r=3))
[(1, 2, 3)]
&gt;&gt;&gt; list(itertools.combinations_with_replacement(a, r=2))
[(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)]
&gt;&gt;&gt; tensor_a = torch.tensor(a)
&gt;&gt;&gt; torch.combinations(tensor_a)
tensor([[1, 2],
        [1, 3],
        [2, 3]])
&gt;&gt;&gt; torch.combinations(tensor_a, r=3)
tensor([[1, 2, 3]])
&gt;&gt;&gt; torch.combinations(tensor_a, with_replacement=True)
tensor([[1, 1],
        [1, 2],
        [1, 3],
        [2, 2],
        [2, 3],
        [3, 3]])

</code></pre>
<hr />
<pre><code>torch.cross(input, other, dim=-1, out=None) → Tensor¶
</code></pre>
<p>返回向量在<code>input</code>和<code>other</code>的维度<code>dim</code>中的叉积。</p>
<p><code>input</code>和<code>other</code>的大小必须相同，并且<code>dim</code>尺寸的大小应为 3。</p>
<p>如果未提供<code>dim</code>，则默认为找到的第一个尺寸为 3 的尺寸。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the second input tensor</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> <em>，</em> <em>可选</em>）–取叉积的尺寸。</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 3)
&gt;&gt;&gt; a
tensor([[-0.3956,  1.1455,  1.6895],
        [-0.5849,  1.3672,  0.3599],
        [-1.1626,  0.7180, -0.0521],
        [-0.1339,  0.9902, -2.0225]])
&gt;&gt;&gt; b = torch.randn(4, 3)
&gt;&gt;&gt; b
tensor([[-0.0257, -1.4725, -1.2251],
        [-1.1479, -0.7005, -1.9757],
        [-1.3904,  0.3726, -1.1836],
        [-0.9688, -0.7153,  0.2159]])
&gt;&gt;&gt; torch.cross(a, b, dim=1)
tensor([[ 1.0844, -0.5281,  0.6120],
        [-2.4490, -1.5687,  1.9792],
        [-0.8304, -1.3037,  0.5650],
        [-1.2329,  1.9883,  1.0551]])
&gt;&gt;&gt; torch.cross(a, b)
tensor([[ 1.0844, -0.5281,  0.6120],
        [-2.4490, -1.5687,  1.9792],
        [-0.8304, -1.3037,  0.5650],
        [-1.2329,  1.9883,  1.0551]])

</code></pre>
<hr />
<pre><code>torch.cumprod(input, dim, out=None, dtype=None) → Tensor¶
</code></pre>
<p>返回维度为<code>dim</code>的<code>input</code>元素的累积积。</p>
<p>例如，如果<code>input</code>是大小为 N 的向量，则结果也将是大小为 N 的向量(带有元素）。</p>
<p><img alt="" src="../img/89cdfc25537850f458deb6fa7119b4b4.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> )–执行操作的尺寸</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. If specified, the input tensor is casted to <code>dtype</code> before the operation is performed. This is useful for preventing data type overflows. Default: None.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(10)
&gt;&gt;&gt; a
tensor([ 0.6001,  0.2069, -0.1919,  0.9792,  0.6727,  1.0062,  0.4126,
        -0.2129, -0.4206,  0.1968])
&gt;&gt;&gt; torch.cumprod(a, dim=0)
tensor([ 0.6001,  0.1241, -0.0238, -0.0233, -0.0157, -0.0158, -0.0065,
         0.0014, -0.0006, -0.0001])

&gt;&gt;&gt; a[5] = 0.0
&gt;&gt;&gt; torch.cumprod(a, dim=0)
tensor([ 0.6001,  0.1241, -0.0238, -0.0233, -0.0157, -0.0000, -0.0000,
         0.0000, -0.0000, -0.0000])

</code></pre>
<hr />
<pre><code>torch.cumsum(input, dim, out=None, dtype=None) → Tensor¶
</code></pre>
<p>返回维度为<code>dim</code>的<code>input</code>元素的累积和。</p>
<p>For example, if <code>input</code> is a vector of size N, the result will also be a vector of size N, with elements.</p>
<p><img alt="" src="../img/0a2d62fcc8f81c2e2b3da579ec8cbccb.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>dim</strong> (<em>python:int</em>) – the dimension to do the operation over</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. If specified, the input tensor is casted to <code>dtype</code> before the operation is performed. This is useful for preventing data type overflows. Default: None.</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(10)
&gt;&gt;&gt; a
tensor([-0.8286, -0.4890,  0.5155,  0.8443,  0.1865, -0.1752, -2.0595,
         0.1850, -1.1571, -0.4243])
&gt;&gt;&gt; torch.cumsum(a, dim=0)
tensor([-0.8286, -1.3175, -0.8020,  0.0423,  0.2289,  0.0537, -2.0058,
        -1.8209, -2.9780, -3.4022])

</code></pre>
<hr />
<pre><code>torch.diag(input, diagonal=0, out=None) → Tensor¶
</code></pre>
<ul>
<li>
<p>如果<code>input</code>是矢量(1-D 张量），则返回以<code>input</code>的元素为对角线的 2-D 方形张量。</p>
</li>
<li>
<p>如果<code>input</code>是矩阵(2-D 张量），则返回带有<code>input</code>的对角元素的 1-D 张量。</p>
</li>
</ul>
<p>参数 <a href="#torch.diagonal" title="torch.diagonal"><code>diagonal</code></a> 控制要考虑的对角线：</p>
<ul>
<li>
<p>如果 <a href="#torch.diagonal" title="torch.diagonal"><code>diagonal</code></a> = 0，则它是主对角线。</p>
</li>
<li>
<p>如果 <a href="#torch.diagonal" title="torch.diagonal"><code>diagonal</code></a> &gt; 0，则它在主对角线上方。</p>
</li>
<li>
<p>如果 <a href="#torch.diagonal" title="torch.diagonal"><code>diagonal</code></a> &lt; 0，则它位于主对角线下方。</p>
</li>
</ul>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>对角线</strong> (<em>python：int</em> <em>，</em> <em>可选</em>）–对角线</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>See also</p>
<p><a href="#torch.diagonal" title="torch.diagonal"><code>torch.diagonal()</code></a> 始终返回其输入的对角线。</p>
<p><a href="#torch.diagflat" title="torch.diagflat"><code>torch.diagflat()</code></a> 始终使用输入指定的对角线元素构造张量。</p>
<p>Examples:</p>
<p>获取输入向量为对角线的方阵：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(3)
&gt;&gt;&gt; a
tensor([ 0.5950,-0.0872, 2.3298])
&gt;&gt;&gt; torch.diag(a)
tensor([[ 0.5950, 0.0000, 0.0000],
        [ 0.0000,-0.0872, 0.0000],
        [ 0.0000, 0.0000, 2.3298]])
&gt;&gt;&gt; torch.diag(a, 1)
tensor([[ 0.0000, 0.5950, 0.0000, 0.0000],
        [ 0.0000, 0.0000,-0.0872, 0.0000],
        [ 0.0000, 0.0000, 0.0000, 2.3298],
        [ 0.0000, 0.0000, 0.0000, 0.0000]])

</code></pre>
<p>获取给定矩阵的第 k 个对角线：</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(3, 3)
&gt;&gt;&gt; a
tensor([[-0.4264, 0.0255,-0.1064],
        [ 0.8795,-0.2429, 0.1374],
        [ 0.1029,-0.6482,-1.6300]])
&gt;&gt;&gt; torch.diag(a, 0)
tensor([-0.4264,-0.2429,-1.6300])
&gt;&gt;&gt; torch.diag(a, 1)
tensor([ 0.0255, 0.1374])

</code></pre>
<hr />
<pre><code>torch.diag_embed(input, offset=0, dim1=-2, dim2=-1) → Tensor¶
</code></pre>
<p>创建一个张量，其某些 2D 平面(由<code>dim1</code>和<code>dim2</code>指定）的对角线由<code>input</code>填充。 为了便于创建批处理对角矩阵，默认情况下选择由返回张量的最后两个维构成的 2D 平面。</p>
<p>参数<code>offset</code>控制要考虑的对角线：</p>
<ul>
<li>
<p>如果<code>offset</code> = 0，则它是主对角线。</p>
</li>
<li>
<p>如果<code>offset</code> &gt;为 0，则它​​在主对角线上方。</p>
</li>
<li>
<p>如果<code>offset</code> &lt;为 0，则它​​在主对角线下方。</p>
</li>
</ul>
<p>将计算新矩阵的大小，以使指定的对角线成为最后一个输入尺寸的大小。 注意，对于<img alt="" src="../img/e647c6371faf8b6dd9327515ae95cbdb.jpg" />以外的<code>offset</code>，<code>dim1</code>和<code>dim2</code>的顺序很重要。 交换它们等效于更改<code>offset</code>的符号。</p>
<p>将 <a href="#torch.diagonal" title="torch.diagonal"><code>torch.diagonal()</code></a> 应用于具有相同参数的该函数的输出，将产生与输入相同的矩阵。 但是， <a href="#torch.diagonal" title="torch.diagonal"><code>torch.diagonal()</code></a> 具有不同的默认尺寸，因此需要明确指定这些尺寸。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–输入张量。 必须至少为一维。</p>
</li>
<li>
<p><strong>偏移量</strong> (<em>python：int</em> <em>，</em> <em>可选</em>）–要考虑的对角线。 默认值：0(主对角线）。</p>
</li>
<li>
<p><strong>dim1</strong>  (<em>python：int</em> <em>，</em> <em>可选</em>）–取对角线的第一维。 默认值：-2。</p>
</li>
<li>
<p><strong>dim2</strong>  (<em>python：int</em> <em>，</em> <em>可选</em>）–取对角线的第二维。 默认值：-1。</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(2, 3)
&gt;&gt;&gt; torch.diag_embed(a)
tensor([[[ 1.5410,  0.0000,  0.0000],
         [ 0.0000, -0.2934,  0.0000],
         [ 0.0000,  0.0000, -2.1788]],

        [[ 0.5684,  0.0000,  0.0000],
         [ 0.0000, -1.0845,  0.0000],
         [ 0.0000,  0.0000, -1.3986]]])

&gt;&gt;&gt; torch.diag_embed(a, offset=1, dim1=0, dim2=2)
tensor([[[ 0.0000,  1.5410,  0.0000,  0.0000],
         [ 0.0000,  0.5684,  0.0000,  0.0000]],

        [[ 0.0000,  0.0000, -0.2934,  0.0000],
         [ 0.0000,  0.0000, -1.0845,  0.0000]],

        [[ 0.0000,  0.0000,  0.0000, -2.1788],
         [ 0.0000,  0.0000,  0.0000, -1.3986]],

        [[ 0.0000,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  0.0000]]])

</code></pre>
<hr />
<pre><code>torch.diagflat(input, offset=0) → Tensor¶
</code></pre>
<ul>
<li>
<p>If <code>input</code> is a vector (1-D tensor), then returns a 2-D square tensor with the elements of <code>input</code> as the diagonal.</p>
</li>
<li>
<p>如果<code>input</code>是一维以上的张量，则返回一个二维张量，其对角线元素等于展平的<code>input</code>。</p>
</li>
</ul>
<p>The argument <code>offset</code> controls which diagonal to consider:</p>
<ul>
<li>
<p>If <code>offset</code> = 0, it is the main diagonal.</p>
</li>
<li>
<p>If <code>offset</code> &gt; 0, it is above the main diagonal.</p>
</li>
<li>
<p>If <code>offset</code> &lt; 0, it is below the main diagonal.</p>
</li>
</ul>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>偏移量</strong> (<em>python：int</em> <em>，</em> <em>可选</em>）–要考虑的对角线。 默认值：0(主对角线）。</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(3)
&gt;&gt;&gt; a
tensor([-0.2956, -0.9068,  0.1695])
&gt;&gt;&gt; torch.diagflat(a)
tensor([[-0.2956,  0.0000,  0.0000],
        [ 0.0000, -0.9068,  0.0000],
        [ 0.0000,  0.0000,  0.1695]])
&gt;&gt;&gt; torch.diagflat(a, 1)
tensor([[ 0.0000, -0.2956,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.9068,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.1695],
        [ 0.0000,  0.0000,  0.0000,  0.0000]])

&gt;&gt;&gt; a = torch.randn(2, 2)
&gt;&gt;&gt; a
tensor([[ 0.2094, -0.3018],
        [-0.1516,  1.9342]])
&gt;&gt;&gt; torch.diagflat(a)
tensor([[ 0.2094,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.3018,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.1516,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  1.9342]])

</code></pre>
<hr />
<pre><code>torch.diagonal(input, offset=0, dim1=0, dim2=1) → Tensor¶
</code></pre>
<p>返回<code>input</code>的局部视图，其对角线元素相对于<code>dim1</code>和<code>dim2</code>附加为尺寸的末端形状。</p>
<p>The argument <code>offset</code> controls which diagonal to consider:</p>
<ul>
<li>
<p>If <code>offset</code> = 0, it is the main diagonal.</p>
</li>
<li>
<p>If <code>offset</code> &gt; 0, it is above the main diagonal.</p>
</li>
<li>
<p>If <code>offset</code> &lt; 0, it is below the main diagonal.</p>
</li>
</ul>
<p>将 <a href="#torch.diag_embed" title="torch.diag_embed"><code>torch.diag_embed()</code></a> 应用于具有相同参数的此函数的输出，将产生一个带有输入对角线项的对角矩阵。 但是， <a href="#torch.diag_embed" title="torch.diag_embed"><code>torch.diag_embed()</code></a> 具有不同的默认尺寸，因此需要明确指定这些尺寸。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–输入张量。 必须至少为二维。</p>
</li>
<li>
<p><strong>offset</strong> (<em>python:int__,</em> <em>optional</em>) – which diagonal to consider. Default: 0 (main diagonal).</p>
</li>
<li>
<p><strong>dim1</strong>  (<em>python：int</em> <em>，</em> <em>可选</em>）–取对角线的第一维。 默认值：0</p>
</li>
<li>
<p><strong>dim2</strong>  (<em>python：int</em> <em>，</em> <em>可选</em>）–取对角线的第二维。 默认值：1。</p>
</li>
</ul>
<p>Note</p>
<p>要取一批对角线，请传入 dim1 = -2，dim2 = -1。</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(3, 3)
&gt;&gt;&gt; a
tensor([[-1.0854,  1.1431, -0.1752],
        [ 0.8536, -0.0905,  0.0360],
        [ 0.6927, -0.3735, -0.4945]])

&gt;&gt;&gt; torch.diagonal(a, 0)
tensor([-1.0854, -0.0905, -0.4945])

&gt;&gt;&gt; torch.diagonal(a, 1)
tensor([ 1.1431,  0.0360])

&gt;&gt;&gt; x = torch.randn(2, 5, 4, 2)
&gt;&gt;&gt; torch.diagonal(x, offset=-1, dim1=1, dim2=2)
tensor([[[-1.2631,  0.3755, -1.5977, -1.8172],
         [-1.1065,  1.0401, -0.2235, -0.7938]],

        [[-1.7325, -0.3081,  0.6166,  0.2335],
         [ 1.0500,  0.7336, -0.3836, -1.1015]]])

</code></pre>
<hr />
<pre><code>torch.einsum(equation, *operands) → Tensor¶
</code></pre>
<p>此函数提供了一种使用爱因斯坦求和约定来计算多线性表达式(即乘积和）的方法。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>公式</strong>(<em>字符串</em>）–该公式以与操作数和结果的每个维相关联的小写字母(索引）形式给出。 左侧列出了操作数维，以逗号分隔。 每个张量维应该有一个索引字母。 右侧紧随&lt;cite&gt;-&gt;&lt;/cite&gt; 之后，并给出输出的索引。 如果省略&lt;cite&gt;-&gt;&lt;/cite&gt; 和右侧，则将其隐式定义为所有索引的按字母顺序排序的列表，这些列表在左侧仅出现一次。 在将操作数条目相乘后，将输出中不等于的索引相加。 如果同一操作数的索引出现多次，则采用对角线。 椭圆&lt;cite&gt;…&lt;/cite&gt;代表固定数量的尺寸。 如果推断出右侧，则省略号尺寸位于输出的开头。</p>
</li>
<li>
<p><strong>操作数</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–计算爱因斯坦总和的操作数。</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(5)
&gt;&gt;&gt; y = torch.randn(4)
&gt;&gt;&gt; torch.einsum('i,j-&gt;ij', x, y)  # outer product
tensor([[-0.0570, -0.0286, -0.0231,  0.0197],
        [ 1.2616,  0.6335,  0.5113, -0.4351],
        [ 1.4452,  0.7257,  0.5857, -0.4984],
        [-0.4647, -0.2333, -0.1883,  0.1603],
        [-1.1130, -0.5588, -0.4510,  0.3838]])

&gt;&gt;&gt; A = torch.randn(3,5,4)
&gt;&gt;&gt; l = torch.randn(2,5)
&gt;&gt;&gt; r = torch.randn(2,4)
&gt;&gt;&gt; torch.einsum('bn,anm,bm-&gt;ba', l, A, r) # compare torch.nn.functional.bilinear
tensor([[-0.3430, -5.2405,  0.4494],
        [ 0.3311,  5.5201, -3.0356]])

&gt;&gt;&gt; As = torch.randn(3,2,5)
&gt;&gt;&gt; Bs = torch.randn(3,5,4)
&gt;&gt;&gt; torch.einsum('bij,bjk-&gt;bik', As, Bs) # batch matrix multiplication
tensor([[[-1.0564, -1.5904,  3.2023,  3.1271],
         [-1.6706, -0.8097, -0.8025, -2.1183]],

        [[ 4.2239,  0.3107, -0.5756, -0.2354],
         [-1.4558, -0.3460,  1.5087, -0.8530]],

        [[ 2.8153,  1.8787, -4.3839, -1.2112],
         [ 0.3728, -2.1131,  0.0921,  0.8305]]])

&gt;&gt;&gt; A = torch.randn(3, 3)
&gt;&gt;&gt; torch.einsum('ii-&gt;i', A) # diagonal
tensor([-0.7825,  0.8291, -0.1936])

&gt;&gt;&gt; A = torch.randn(4, 3, 3)
&gt;&gt;&gt; torch.einsum('...ii-&gt;...i', A) # batch diagonal
tensor([[-1.0864,  0.7292,  0.0569],
        [-0.9725, -1.0270,  0.6493],
        [ 0.5832, -1.1716, -1.5084],
        [ 0.4041, -1.1690,  0.8570]])

&gt;&gt;&gt; A = torch.randn(2, 3, 4, 5)
&gt;&gt;&gt; torch.einsum('...ij-&gt;...ji', A).shape # batch permute
torch.Size([2, 3, 5, 4])

</code></pre>
<hr />
<pre><code>torch.flatten(input, start_dim=0, end_dim=-1) → Tensor¶
</code></pre>
<p>展平张量中连续的暗淡范围。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>start_dim</strong>  (<em>python：int</em> )–第一个变暗的像素</p>
</li>
<li>
<p><strong>end_dim</strong>  (<em>python：int</em> )–最后一个变暗的像素</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; t = torch.tensor([[[1, 2],
                       [3, 4]],
                      [[5, 6],
                       [7, 8]]])
&gt;&gt;&gt; torch.flatten(t)
tensor([1, 2, 3, 4, 5, 6, 7, 8])
&gt;&gt;&gt; torch.flatten(t, start_dim=1)
tensor([[1, 2, 3, 4],
        [5, 6, 7, 8]])

</code></pre>
<hr />
<pre><code>torch.flip(input, dims) → Tensor¶
</code></pre>
<p>沿给定轴反转 n-D 张量的顺序，以暗淡表示。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>使</strong>变暗(<em>列表</em> <em>或</em> <em>元组</em>）–翻转轴</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.arange(8).view(2, 2, 2)
&gt;&gt;&gt; x
tensor([[[ 0,  1],
         [ 2,  3]],

        [[ 4,  5],
         [ 6,  7]]])
&gt;&gt;&gt; torch.flip(x, [0, 1])
tensor([[[ 6,  7],
         [ 4,  5]],

        [[ 2,  3],
         [ 0,  1]]])

</code></pre>
<hr />
<pre><code>torch.rot90(input, k, dims) → Tensor¶
</code></pre>
<p>在调光轴指定的平面中将 n-D 张量旋转 90 度。 如果 k &gt; 0，则旋转方向是从第一个轴到第二个轴，对于 k &lt; 0，旋转方向是从第二个轴到第一个轴。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>k</strong>  (<em>python：int</em> )–旋转次数</p>
</li>
<li>
<p><strong>使</strong>变暗(<em>列表</em> <em>或</em> <em>元组</em>）–旋转轴</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.arange(4).view(2, 2)
&gt;&gt;&gt; x
tensor([[0, 1],
        [2, 3]])
&gt;&gt;&gt; torch.rot90(x, 1, [0, 1])
tensor([[1, 3],
        [0, 2]])

&gt;&gt;&gt; x = torch.arange(8).view(2, 2, 2)
&gt;&gt;&gt; x
tensor([[[0, 1],
         [2, 3]],

        [[4, 5],
         [6, 7]]])
&gt;&gt;&gt; torch.rot90(x, 1, [1, 2])
tensor([[[1, 3],
         [0, 2]],

        [[5, 7],
         [4, 6]]])

</code></pre>
<hr />
<pre><code>torch.histc(input, bins=100, min=0, max=0, out=None) → Tensor¶
</code></pre>
<p>计算张量的直方图。</p>
<p>元素被分类为 <a href="#torch.min" title="torch.min"><code>min</code></a> 和 <a href="#torch.max" title="torch.max"><code>max</code></a> 之间的等宽单元。 如果 <a href="#torch.min" title="torch.min"><code>min</code></a> 和 <a href="#torch.max" title="torch.max"><code>max</code></a> 均为零，则使用数据的最小值和最大值。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>箱</strong> (<em>python：int</em> )–直方图箱数</p>
</li>
<li>
<p><strong>min</strong>  (<em>python：int</em> )–范围的下限(包括）</p>
</li>
<li>
<p><strong>最大</strong> (<em>python：int</em> )–范围的上限(包括）</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Returns</p>
<p>直方图表示为张量</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.histc(torch.tensor([1., 2, 1]), bins=4, min=0, max=3)
tensor([ 0.,  2.,  1.,  0.])

</code></pre>
<hr />
<pre><code>torch.meshgrid(*tensors, **kwargs)¶
</code></pre>
<p>取<img alt="" src="../img/7ea0d6aa290d37c8bc57957206bafe80.jpg" />张量(每个张量可以是标量或一维向量），并创建<img alt="" src="../img/7ea0d6aa290d37c8bc57957206bafe80.jpg" /> N 维网格，其中通过扩展<img alt="" src="../img/db26d1a59be5965889bd4d5533b7be61.jpg" />定义<img alt="" src="../img/db26d1a59be5965889bd4d5533b7be61.jpg" /> &lt;sup&gt;第&lt;/sup&gt;网格。 &lt;sup&gt;和&lt;/sup&gt;输入超出其他输入定义的尺寸。</p>
<blockquote>
<p><code>Args:</code></p>
<p>张量(张量列表）：标量或一维张量的列表。 标量将被自动视为大小为<img alt="" src="../img/fc55934714b3777971b760dd3cf42978.jpg" />的张量</p>
<p><code>Returns:</code></p>
<p>seq(张量序列）：如果输入的<img alt="" src="../img/678502bee29f9b13e7115b18864a5822.jpg" />张量为<img alt="" src="../img/dc2a28f0eb5d798e9f246215dbb10795.jpg" />，则输出也将具有<img alt="" src="../img/678502bee29f9b13e7115b18864a5822.jpg" />张量，其中所有张量均为<img alt="" src="../img/3a092c0d2d4874edbfe18b4364b8a48d.jpg" />。</p>
<p>Example:</p>
<p>```
&gt;&gt;&gt; x = torch.tensor([1, 2, 3])
&gt;&gt;&gt; y = torch.tensor([4, 5, 6])
&gt;&gt;&gt; grid_x, grid_y = torch.meshgrid(x, y)
&gt;&gt;&gt; grid_x
tensor([[1, 1, 1],
        [2, 2, 2],
        [3, 3, 3]])
&gt;&gt;&gt; grid_y
tensor([[4, 5, 6],
        [4, 5, 6],
        [4, 5, 6]])</p>
<p>```</p>
</blockquote>
<hr />
<pre><code>torch.renorm(input, p, dim, maxnorm, out=None) → Tensor¶
</code></pre>
<p>返回一个张量，其中<code>input</code>沿维度<code>dim</code>的每个子张量均被规范化，以使子张量的 &lt;cite&gt;p&lt;/cite&gt; -norm 小于值<code>maxnorm</code></p>
<p>Note</p>
<p>如果某行的范数低于 &lt;cite&gt;maxnorm&lt;/cite&gt; ，则该行不变</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>p</strong>  (<em>python：float</em> )–范数计算的能力</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> )–切片以获得子张量的维度</p>
</li>
<li>
<p><strong>maxnorm</strong>  (<em>python：float</em> )–保持每个子张量低于的最大规范</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.ones(3, 3)
&gt;&gt;&gt; x[1].fill_(2)
tensor([ 2.,  2.,  2.])
&gt;&gt;&gt; x[2].fill_(3)
tensor([ 3.,  3.,  3.])
&gt;&gt;&gt; x
tensor([[ 1.,  1.,  1.],
        [ 2.,  2.,  2.],
        [ 3.,  3.,  3.]])
&gt;&gt;&gt; torch.renorm(x, 1, 0, 5)
tensor([[ 1.0000,  1.0000,  1.0000],
        [ 1.6667,  1.6667,  1.6667],
        [ 1.6667,  1.6667,  1.6667]])

</code></pre>
<hr />
<pre><code>torch.repeat_interleave()¶
</code></pre>
<hr />
<pre><code>torch.repeat_interleave(input, repeats, dim=None) → Tensor
</code></pre>
<p>重复张量的元素。</p>
<p>Warning</p>
<p>这与<code>torch.repeat()</code>不同，但与 &lt;cite&gt;numpy.repeat&lt;/cite&gt; 相似。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>重复</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>或</em> <em>python：int</em> )–每个元素的重复次数。 重复播放以适合给定轴的形状。</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> <em>，</em> <em>可选</em>）–沿其重复值的尺寸。 默认情况下，使用展平的输入数组，并返回展平的输出数组。</p>
</li>
</ul>
<p>Returns</p>
<pre><code>Repeated tensor which has the same shape as input, except along the
</code></pre>
<p>给定的轴。</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.tensor([1, 2, 3])
&gt;&gt;&gt; x.repeat_interleave(2)
tensor([1, 1, 2, 2, 3, 3])
&gt;&gt;&gt; y = torch.tensor([[1, 2], [3, 4]])
&gt;&gt;&gt; torch.repeat_interleave(y, 2)
tensor([1, 1, 2, 2, 3, 3, 4, 4])
&gt;&gt;&gt; torch.repeat_interleave(y, 3, dim=1)
tensor([[1, 1, 1, 2, 2, 2],
        [3, 3, 3, 4, 4, 4]])
&gt;&gt;&gt; torch.repeat_interleave(y, torch.tensor([1, 2]), dim=0)
tensor([[1, 2],
        [3, 4],
        [3, 4]])

</code></pre>
<hr />
<pre><code>torch.repeat_interleave(repeats) → Tensor
</code></pre>
<p>如果&lt;cite&gt;重复&lt;/cite&gt;为&lt;cite&gt;张量([n1，n2，n3，…]）&lt;/cite&gt;，则输出将为&lt;cite&gt;张量([0，0，…，1，1， …，2，2，…，…]）&lt;/cite&gt;，其中 &lt;cite&gt;0&lt;/cite&gt; 出现 &lt;cite&gt;n1&lt;/cite&gt; 次， &lt;cite&gt;1&lt;/cite&gt; 出现 &lt;cite&gt;n2&lt;/cite&gt; 次，[ &lt;cite&gt;2&lt;/cite&gt; 出现 &lt;cite&gt;n3&lt;/cite&gt; 次，等等。</p>
<hr />
<pre><code>torch.roll(input, shifts, dims=None) → Tensor¶
</code></pre>
<p>沿给定尺寸滚动张量。 移出最后位置的元素将重新引入第一个位置。 如果未指定尺寸，则张量将在滚动之前变平，然后恢复为原始形状。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>移位</strong> (<em>python：int</em> <em>或</em> <em>python：ints</em> 的元组）–张量元素移位的位数 。 如果 shifts 是一个元组，则 dims 必须是相同大小的元组，并且每个维度将滚动相应的值</p>
</li>
<li>
<p><strong>变暗</strong> (<em>python：int</em> <em>或</em> <em>tuple of python：ints</em> )–滚动轴</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8]).view(4, 2)
&gt;&gt;&gt; x
tensor([[1, 2],
        [3, 4],
        [5, 6],
        [7, 8]])
&gt;&gt;&gt; torch.roll(x, 1, 0)
tensor([[7, 8],
        [1, 2],
        [3, 4],
        [5, 6]])
&gt;&gt;&gt; torch.roll(x, -1, 0)
tensor([[3, 4],
        [5, 6],
        [7, 8],
        [1, 2]])
&gt;&gt;&gt; torch.roll(x, shifts=(2, 1), dims=(0, 1))
tensor([[6, 5],
        [8, 7],
        [2, 1],
        [4, 3]])

</code></pre>
<hr />
<pre><code>torch.tensordot(a, b, dims=2)¶
</code></pre>
<p>返回 a 和 b 在多个维度上的收缩。</p>
<p><a href="#torch.tensordot" title="torch.tensordot"><code>tensordot</code></a> 实现了广义矩阵乘积。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>a</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–左张量收缩</p>
</li>
<li>
<p><strong>b</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–右张量收缩</p>
</li>
<li>
<p><strong>变暗</strong> (<em>python：int</em> <em>或</em> <em>python：integers</em> 的两个列表的元组）–要收缩的尺寸数或尺寸的显式列表 分别用于<code>a</code>和<code>b</code></p>
</li>
</ul>
<p>当使用整数参数<code>dims</code> = <img alt="" src="../img/95d07e221a9b945feb93422a44ac3d42.jpg" />调用并且<code>a</code>和<code>b</code>的维数分别为<img alt="" src="../img/03327e7b697db30918e96b2209927929.jpg" />和<img alt="" src="../img/5f0051b0454fa75ca446b59b47eff6f6.jpg" />时，它将计算</p>
<p><img alt="" src="../img/1a4ce18109f30700be1f1dea87661f3e.jpg" /></p>
<p>当使用列表形式的<code>dims</code>调用时，给定的尺寸将代替<code>a</code>的最后一个<img alt="" src="../img/95d07e221a9b945feb93422a44ac3d42.jpg" />和<img alt="" src="../img/23a2cc697e4e1a8adeb60eb42ab51a6e.jpg" />的第一个<img alt="" src="../img/95d07e221a9b945feb93422a44ac3d42.jpg" />收缩。 这些尺寸的尺寸必须匹配，但是 <a href="#torch.tensordot" title="torch.tensordot"><code>tensordot</code></a> 将处理广播的尺寸。</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; a = torch.arange(60.).reshape(3, 4, 5)
&gt;&gt;&gt; b = torch.arange(24.).reshape(4, 3, 2)
&gt;&gt;&gt; torch.tensordot(a, b, dims=([1, 0], [0, 1]))
tensor([[4400., 4730.],
        [4532., 4874.],
        [4664., 5018.],
        [4796., 5162.],
        [4928., 5306.]])

&gt;&gt;&gt; a = torch.randn(3, 4, 5, device='cuda')
&gt;&gt;&gt; b = torch.randn(4, 5, 6, device='cuda')
&gt;&gt;&gt; c = torch.tensordot(a, b, dims=2).cpu()
tensor([[ 8.3504, -2.5436,  6.2922,  2.7556, -1.0732,  3.2741],
        [ 3.3161,  0.0704,  5.0187, -0.4079, -4.3126,  4.8744],
        [ 0.8223,  3.9445,  3.2168, -0.2400,  3.4117,  1.7780]])

</code></pre>
<hr />
<pre><code>torch.trace(input) → Tensor¶
</code></pre>
<p>返回输入二维矩阵对角线元素的总和。</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.arange(1., 10.).view(3, 3)
&gt;&gt;&gt; x
tensor([[ 1.,  2.,  3.],
        [ 4.,  5.,  6.],
        [ 7.,  8.,  9.]])
&gt;&gt;&gt; torch.trace(x)
tensor(15.)

</code></pre>
<hr />
<pre><code>torch.tril(input, diagonal=0, out=None) → Tensor¶
</code></pre>
<p>返回矩阵(2-D 张量）或矩阵批次<code>input</code>的下三角部分，结果张量<code>out</code>的其他元素设置为 0。</p>
<p>矩阵的下三角部分定义为对角线之上和之下的元素。</p>
<p>参数 <a href="#torch.diagonal" title="torch.diagonal"><code>diagonal</code></a> 控制要考虑的对角线。 如果 <a href="#torch.diagonal" title="torch.diagonal"><code>diagonal</code></a> = 0，则保留主对角线上和下方的所有元素。 正值包括在主对角线上方的对角线，同样，负值排除在主对角线下方的对角线。 主对角线是<img alt="" src="../img/f1dac6cf1b08b41a3c94272830a3eaba.jpg" />的索引集<img alt="" src="../img/efdb6cba2dff4626df6ecbd8c9117b62.jpg" />，其中<img alt="" src="../img/a760ba0f4e82b4f6875dd8b8db1bd3c4.jpg" />是矩阵的维数。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>diagonal</strong> (<em>python:int__,</em> <em>optional</em>) – the diagonal to consider</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(3, 3)
&gt;&gt;&gt; a
tensor([[-1.0813, -0.8619,  0.7105],
        [ 0.0935,  0.1380,  2.2112],
        [-0.3409, -0.9828,  0.0289]])
&gt;&gt;&gt; torch.tril(a)
tensor([[-1.0813,  0.0000,  0.0000],
        [ 0.0935,  0.1380,  0.0000],
        [-0.3409, -0.9828,  0.0289]])

&gt;&gt;&gt; b = torch.randn(4, 6)
&gt;&gt;&gt; b
tensor([[ 1.2219,  0.5653, -0.2521, -0.2345,  1.2544,  0.3461],
        [ 0.4785, -0.4477,  0.6049,  0.6368,  0.8775,  0.7145],
        [ 1.1502,  3.2716, -1.1243, -0.5413,  0.3615,  0.6864],
        [-0.0614, -0.7344, -1.3164, -0.7648, -1.4024,  0.0978]])
&gt;&gt;&gt; torch.tril(b, diagonal=1)
tensor([[ 1.2219,  0.5653,  0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.4785, -0.4477,  0.6049,  0.0000,  0.0000,  0.0000],
        [ 1.1502,  3.2716, -1.1243, -0.5413,  0.0000,  0.0000],
        [-0.0614, -0.7344, -1.3164, -0.7648, -1.4024,  0.0000]])
&gt;&gt;&gt; torch.tril(b, diagonal=-1)
tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.4785,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
        [ 1.1502,  3.2716,  0.0000,  0.0000,  0.0000,  0.0000],
        [-0.0614, -0.7344, -1.3164,  0.0000,  0.0000,  0.0000]])

</code></pre>
<hr />
<pre><code>torch.tril_indices(row, col, offset=0, dtype=torch.long, device='cpu', layout=torch.strided) → Tensor¶
</code></pre>
<p>返回 2*N 张量中<code>row</code>-<code>col</code>矩阵的下三角部分的索引，其中第一行包含所有索引的行坐标，第二行包含列坐标。 索引是根据行然后按列排序的。</p>
<p>The lower triangular part of the matrix is defined as the elements on and below the diagonal.</p>
<p>参数<code>offset</code>控制要考虑的对角线。 如果<code>offset</code> = 0，则保留主对角线上和下方的所有元素。 正值包括在主对角线上方的对角线，同样，负值排除在主对角线下方的对角线。 主要对角线是<img alt="" src="../img/f1dac6cf1b08b41a3c94272830a3eaba.jpg" />的索引集<img alt="" src="../img/efdb6cba2dff4626df6ecbd8c9117b62.jpg" />，其中<img alt="" src="../img/a760ba0f4e82b4f6875dd8b8db1bd3c4.jpg" />是矩阵的尺寸。</p>
<p>注意：在“ cuda”上运行时，行* col 必须小于<img alt="" src="../img/62e45bcf8fe738ca1f051a4f7e9a7845.jpg" />，以防止计算期间溢出。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>行</strong>(<code>int</code>）–二维矩阵中的行数。</p>
</li>
<li>
<p><strong>col</strong> (<code>int</code>）–二维矩阵中的列数。</p>
</li>
<li>
<p><strong>偏移量</strong>(<code>int</code>）–与主对角线的对角线偏移。 默认值：如果未提供，则为 0。</p>
</li>
<li>
<p><strong>dtype</strong>  (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> ，可选）–返回张量的所需数据类型。 默认值：如果<code>None</code>，<code>torch.long</code>。</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li>
<p><strong>布局</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a> ，可选）–当前仅支持<code>torch.strided</code>。</p>
</li>
</ul>
<pre><code>Example::
</code></pre>
<pre><code>&gt;&gt;&gt; a = torch.tril_indices(3, 3)
&gt;&gt;&gt; a
tensor([[0, 1, 1, 2, 2, 2],
        [0, 0, 1, 0, 1, 2]])

</code></pre>
<pre><code>&gt;&gt;&gt; a = torch.tril_indices(4, 3, -1)
&gt;&gt;&gt; a
tensor([[1, 2, 2, 3, 3, 3],
        [0, 0, 1, 0, 1, 2]])

</code></pre>
<pre><code>&gt;&gt;&gt; a = torch.tril_indices(4, 3, 1)
&gt;&gt;&gt; a
tensor([[0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3],
        [0, 1, 0, 1, 2, 0, 1, 2, 0, 1, 2]])

</code></pre>
<hr />
<pre><code>torch.triu(input, diagonal=0, out=None) → Tensor¶
</code></pre>
<p>返回矩阵(2-D 张量）或矩阵批次<code>input</code>的上三角部分，结果张量<code>out</code>的其他元素设置为 0。</p>
<p>矩阵的上三角部分定义为对角线上方和上方的元素。</p>
<p>参数 <a href="#torch.diagonal" title="torch.diagonal"><code>diagonal</code></a> 控制要考虑的对角线。 如果 <a href="#torch.diagonal" title="torch.diagonal"><code>diagonal</code></a> = 0，则保留主对角线上和上方的所有元素。 正值排除主要对角线上方的对角线，同样，负值包括主要对角线下方的对角线。 主对角线是<img alt="" src="../img/f1dac6cf1b08b41a3c94272830a3eaba.jpg" />的索引集<img alt="" src="../img/efdb6cba2dff4626df6ecbd8c9117b62.jpg" />，其中<img alt="" src="../img/a760ba0f4e82b4f6875dd8b8db1bd3c4.jpg" />是矩阵的维数。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>diagonal</strong> (<em>python:int__,</em> <em>optional</em>) – the diagonal to consider</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(3, 3)
&gt;&gt;&gt; a
tensor([[ 0.2309,  0.5207,  2.0049],
        [ 0.2072, -1.0680,  0.6602],
        [ 0.3480, -0.5211, -0.4573]])
&gt;&gt;&gt; torch.triu(a)
tensor([[ 0.2309,  0.5207,  2.0049],
        [ 0.0000, -1.0680,  0.6602],
        [ 0.0000,  0.0000, -0.4573]])
&gt;&gt;&gt; torch.triu(a, diagonal=1)
tensor([[ 0.0000,  0.5207,  2.0049],
        [ 0.0000,  0.0000,  0.6602],
        [ 0.0000,  0.0000,  0.0000]])
&gt;&gt;&gt; torch.triu(a, diagonal=-1)
tensor([[ 0.2309,  0.5207,  2.0049],
        [ 0.2072, -1.0680,  0.6602],
        [ 0.0000, -0.5211, -0.4573]])

&gt;&gt;&gt; b = torch.randn(4, 6)
&gt;&gt;&gt; b
tensor([[ 0.5876, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],
        [-0.2447,  0.9556, -1.2919,  1.3378, -0.1768, -1.0857],
        [ 0.4333,  0.3146,  0.6576, -1.0432,  0.9348, -0.4410],
        [-0.9888,  1.0679, -1.3337, -1.6556,  0.4798,  0.2830]])
&gt;&gt;&gt; torch.triu(b, diagonal=1)
tensor([[ 0.0000, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],
        [ 0.0000,  0.0000, -1.2919,  1.3378, -0.1768, -1.0857],
        [ 0.0000,  0.0000,  0.0000, -1.0432,  0.9348, -0.4410],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.4798,  0.2830]])
&gt;&gt;&gt; torch.triu(b, diagonal=-1)
tensor([[ 0.5876, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],
        [-0.2447,  0.9556, -1.2919,  1.3378, -0.1768, -1.0857],
        [ 0.0000,  0.3146,  0.6576, -1.0432,  0.9348, -0.4410],
        [ 0.0000,  0.0000, -1.3337, -1.6556,  0.4798,  0.2830]])

</code></pre>
<hr />
<pre><code>torch.triu_indices(row, col, offset=0, dtype=torch.long, device='cpu', layout=torch.strided) → Tensor¶
</code></pre>
<p>返回 2*N 张量中<code>row</code> x <code>col</code>矩阵的上三角部分的索引，其中第一行包含所有索引的行坐标，第二行包含列坐标。 索引是根据行然后按列排序的。</p>
<p>The upper triangular part of the matrix is defined as the elements on and above the diagonal.</p>
<p>参数<code>offset</code>控制要考虑的对角线。 如果<code>offset</code> = 0，则保留主对角线上和上方的所有元素。 正值排除主要对角线上方的对角线，同样，负值包括主要对角线下方的对角线。 主要对角线是<img alt="" src="../img/f1dac6cf1b08b41a3c94272830a3eaba.jpg" />的索引集<img alt="" src="../img/efdb6cba2dff4626df6ecbd8c9117b62.jpg" />，其中<img alt="" src="../img/a760ba0f4e82b4f6875dd8b8db1bd3c4.jpg" />是矩阵的尺寸。</p>
<p>NOTE: when running on 'cuda', row * col must be less than <img alt="" src="../img/62e45bcf8fe738ca1f051a4f7e9a7845.jpg" /> to prevent overflow during calculation.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>row</strong> (<code>int</code>) – number of rows in the 2-D matrix.</p>
</li>
<li>
<p><strong>col</strong> (<code>int</code>) – number of columns in the 2-D matrix.</p>
</li>
<li>
<p><strong>offset</strong> (<code>int</code>) – diagonal offset from the main diagonal. Default: if not provided, 0.</p>
</li>
<li>
<p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. Default: if <code>None</code>, <code>torch.long</code>.</p>
</li>
<li>
<p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li>
<p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) – currently only support <code>torch.strided</code>.</p>
</li>
</ul>
<pre><code>Example::
</code></pre>
<pre><code>&gt;&gt;&gt; a = torch.triu_indices(3, 3)
&gt;&gt;&gt; a
tensor([[0, 0, 0, 1, 1, 2],
        [0, 1, 2, 1, 2, 2]])

</code></pre>
<pre><code>&gt;&gt;&gt; a = torch.triu_indices(4, 3, -1)
&gt;&gt;&gt; a
tensor([[0, 0, 0, 1, 1, 1, 2, 2, 3],
        [0, 1, 2, 0, 1, 2, 1, 2, 2]])

</code></pre>
<pre><code>&gt;&gt;&gt; a = torch.triu_indices(4, 3, 1)
&gt;&gt;&gt; a
tensor([[0, 0, 1],
        [1, 2, 2]])

</code></pre>
<h3 id="blas-lapack">BLAS 和 LAPACK 操作</h3>
<hr />
<pre><code>torch.addbmm(beta=1, input, alpha=1, batch1, batch2, out=None) → Tensor¶
</code></pre>
<p>执行存储在<code>batch1</code>和<code>batch2</code>中的矩阵的批矩阵矩阵乘积，并减少加法步骤(所有矩阵乘法沿第一维累积）。 <code>input</code>被添加到最终结果中。</p>
<p><code>batch1</code>和<code>batch2</code>必须是 3D 张量，每个张量包含相同数量的矩阵。</p>
<p>如果<code>batch1</code>是<img alt="" src="../img/6bcc61944ebb068b264d492e9d3a9e9c.jpg" />张量，<code>batch2</code>是<img alt="" src="../img/30e4202e8cde7f489d43fdfbf59e0c46.jpg" />张量，则<code>input</code>必须是<a href="notes/broadcasting.html#broadcasting-semantics">可广播的</a>，带有<img alt="" src="../img/80b2c8de6d028b93a22dfe571079ee9c.jpg" />张量，而<code>out</code>将是<img alt="" src="../img/80b2c8de6d028b93a22dfe571079ee9c.jpg" />张量。 。</p>
<p><img alt="" src="../img/2c2652d014606e2ab1a85bdd3c257494.jpg" /></p>
<p>对于类型为 &lt;cite&gt;FloatTensor&lt;/cite&gt; 或 &lt;cite&gt;DoubleTensor&lt;/cite&gt; 的输入，参数<code>beta</code>和<code>alpha</code>必须为实数，否则应为整数。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>beta</strong> (<em>数字</em> <em>，</em> <em>可选</em>）– <code>input</code>(<img alt="" src="../img/53a496ec7d546e2af9595a7055dd6a7e.jpg" />）的乘数</p>
</li>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要添加的矩阵</p>
</li>
<li>
<p><strong>alpha</strong> (<em>编号</em> <em>，</em> <em>可选</em>）– &lt;cite&gt;batch1 @ batch2&lt;/cite&gt; (<img alt="" src="../img/5b9866fb35b01c553ed3e738e3972ae9.jpg" />）的乘数</p>
</li>
<li>
<p><strong>batch1</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–第一批要相乘的矩阵</p>
</li>
<li>
<p><strong>batch2</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要相乘的第二批矩阵</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; M = torch.randn(3, 5)
&gt;&gt;&gt; batch1 = torch.randn(10, 3, 4)
&gt;&gt;&gt; batch2 = torch.randn(10, 4, 5)
&gt;&gt;&gt; torch.addbmm(M, batch1, batch2)
tensor([[  6.6311,   0.0503,   6.9768, -12.0362,  -2.1653],
        [ -4.8185,  -1.4255,  -6.6760,   8.9453,   2.5743],
        [ -3.8202,   4.3691,   1.0943,  -1.1109,   5.4730]])

</code></pre>
<hr />
<pre><code>torch.addmm(beta=1, input, alpha=1, mat1, mat2, out=None) → Tensor¶
</code></pre>
<p>对矩阵<code>mat1</code>和<code>mat2</code>进行矩阵乘法。 矩阵<code>input</code>被添加到最终结果中。</p>
<p>如果<code>mat1</code>是<img alt="" src="../img/95760d62046dcfa418c3b7ffea4caefc.jpg" />张量，<code>mat2</code>是<img alt="" src="../img/d145985cd9c2b23f68a55b0d5429c2ac.jpg" />张量，那么<code>input</code>必须是<a href="notes/broadcasting.html#broadcasting-semantics">可广播的</a>，带有<img alt="" src="../img/80b2c8de6d028b93a22dfe571079ee9c.jpg" />张量，而<code>out</code>将是<img alt="" src="../img/80b2c8de6d028b93a22dfe571079ee9c.jpg" /> 张量。</p>
<p><code>alpha</code>和<code>beta</code>分别是<code>mat1</code>和<code>mat2</code>与添加的矩阵<code>input</code>之间的矩阵向量乘积的比例因子。</p>
<p><img alt="" src="../img/9acb7c0c73f7a2456fcd58168e607d32.jpg" /></p>
<p>对于类型为 &lt;cite&gt;FloatTensor&lt;/cite&gt; 或 &lt;cite&gt;DoubleTensor&lt;/cite&gt; 的输入，参数<code>beta</code>和<code>alpha</code>必须为实数，否则应为整数。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>beta</strong> (<em>Number__,</em> <em>optional</em>) – multiplier for <code>input</code> (<img alt="" src="../img/53a496ec7d546e2af9595a7055dd6a7e.jpg" />)</p>
</li>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – matrix to be added</p>
</li>
<li>
<p><strong>alpha</strong> (<em>编号</em> <em>，</em> <em>可选</em>）– <img alt="" src="../img/6a8a2e3415f1a12e8299e3d32d94728d.jpg" />(<img alt="" src="../img/5b9866fb35b01c553ed3e738e3972ae9.jpg" />）的乘数</p>
</li>
<li>
<p><strong>mat1</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要相乘的第一个矩阵</p>
</li>
<li>
<p><strong>mat2</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要相乘的第二个矩阵</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; M = torch.randn(2, 3)
&gt;&gt;&gt; mat1 = torch.randn(2, 3)
&gt;&gt;&gt; mat2 = torch.randn(3, 3)
&gt;&gt;&gt; torch.addmm(M, mat1, mat2)
tensor([[-4.8716,  1.4671, -1.3746],
        [ 0.7573, -3.9555, -2.8681]])

</code></pre>
<hr />
<pre><code>torch.addmv(beta=1, input, alpha=1, mat, vec, out=None) → Tensor¶
</code></pre>
<p>执行矩阵<code>mat</code>与向量<code>vec</code>的矩阵向量积。 向量<code>input</code>被添加到最终结果中。</p>
<p>如果<code>mat</code>是<img alt="" src="../img/95760d62046dcfa418c3b7ffea4caefc.jpg" />张量，<code>vec</code>是大小 &lt;cite&gt;m&lt;/cite&gt; 的一维张量，则<code>input</code>必须是<a href="notes/broadcasting.html#broadcasting-semantics">可广播</a>，且一维张量为 &lt;cite&gt;n&lt;/cite&gt; 和<code>out</code>大小将是 &lt;cite&gt;n&lt;/cite&gt; 大小的一维张量。</p>
<p><code>alpha</code>和<code>beta</code>分别是<code>mat</code>和<code>vec</code>与添加的张量<code>input</code>之间的矩阵向量乘积的比例因子。</p>
<p><img alt="" src="../img/c267729def81e95d252835fd4b3c0885.jpg" /></p>
<p>对于类型为 &lt;cite&gt;FloatTensor&lt;/cite&gt; 或 &lt;cite&gt;DoubleTensor&lt;/cite&gt; 的输入，参数<code>beta</code>和<code>alpha</code>必须为实数，否则应为整数</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>beta</strong> (<em>Number__,</em> <em>optional</em>) – multiplier for <code>input</code> (<img alt="" src="../img/53a496ec7d546e2af9595a7055dd6a7e.jpg" />)</p>
</li>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要添加的向量</p>
</li>
<li>
<p><strong>alpha</strong> (<em>编号</em> <em>，</em> <em>可选</em>）– <img alt="" src="../img/7906de3cf6f1147a41fa843b63151dce.jpg" />(<img alt="" src="../img/5b9866fb35b01c553ed3e738e3972ae9.jpg" />）的乘数</p>
</li>
<li>
<p><strong>垫</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要相乘的矩阵</p>
</li>
<li>
<p><strong>vec</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要相乘的向量</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; M = torch.randn(2)
&gt;&gt;&gt; mat = torch.randn(2, 3)
&gt;&gt;&gt; vec = torch.randn(3)
&gt;&gt;&gt; torch.addmv(M, mat, vec)
tensor([-0.3768, -5.5565])

</code></pre>
<hr />
<pre><code>torch.addr(beta=1, input, alpha=1, vec1, vec2, out=None) → Tensor¶
</code></pre>
<p>执行向量<code>vec1</code>和<code>vec2</code>的外积并将其添加到矩阵<code>input</code>中。</p>
<p>可选值<code>beta</code>和<code>alpha</code>分别是<code>vec1</code>和<code>vec2</code>与添加矩阵<code>input</code>之间的外部乘积的比例因子。</p>
<p><img alt="" src="../img/d41c12ed61e56a7f779894121105461d.jpg" /></p>
<p>如果<code>vec1</code>是 &lt;cite&gt;n&lt;/cite&gt; 大小的向量，而<code>vec2</code>是 &lt;cite&gt;m&lt;/cite&gt; 大小的向量，则<code>input</code>必须是<a href="notes/broadcasting.html#broadcasting-semantics">可广播</a>且矩阵为 <img alt="" src="../img/95760d62046dcfa418c3b7ffea4caefc.jpg" />和<code>out</code>大小将是<img alt="" src="../img/95760d62046dcfa418c3b7ffea4caefc.jpg" />大小的矩阵。</p>
<p>For inputs of type &lt;cite&gt;FloatTensor&lt;/cite&gt; or &lt;cite&gt;DoubleTensor&lt;/cite&gt;, arguments <code>beta</code> and <code>alpha</code> must be real numbers, otherwise they should be integers</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>beta</strong> (<em>Number__,</em> <em>optional</em>) – multiplier for <code>input</code> (<img alt="" src="../img/53a496ec7d546e2af9595a7055dd6a7e.jpg" />)</p>
</li>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – matrix to be added</p>
</li>
<li>
<p><strong>alpha</strong> (<em>编号</em> <em>，</em> <em>可选</em>）– <img alt="" src="../img/8b4088eb8d72b6ef9fe78d97a3149b5b.jpg" />(<img alt="" src="../img/5b9866fb35b01c553ed3e738e3972ae9.jpg" />）的乘数</p>
</li>
<li>
<p><strong>vec1</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–外积的第一个向量</p>
</li>
<li>
<p><strong>vec2</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–外积的第二向量</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; vec1 = torch.arange(1., 4.)
&gt;&gt;&gt; vec2 = torch.arange(1., 3.)
&gt;&gt;&gt; M = torch.zeros(3, 2)
&gt;&gt;&gt; torch.addr(M, vec1, vec2)
tensor([[ 1.,  2.],
        [ 2.,  4.],
        [ 3.,  6.]])

</code></pre>
<hr />
<pre><code>torch.baddbmm(beta=1, input, alpha=1, batch1, batch2, out=None) → Tensor¶
</code></pre>
<p>在<code>batch1</code>和<code>batch2</code>中执行矩阵的批处理矩阵矩阵乘积。 <code>input</code>被添加到最终结果中。</p>
<p><code>batch1</code>和<code>batch2</code>必须是 3D 张量，每个张量包含相同数量的矩阵。</p>
<p>如果<code>batch1</code>是<img alt="" src="../img/6bcc61944ebb068b264d492e9d3a9e9c.jpg" />张量，<code>batch2</code>是<img alt="" src="../img/30e4202e8cde7f489d43fdfbf59e0c46.jpg" />张量，那么<code>input</code>必须是<a href="notes/broadcasting.html#broadcasting-semantics">可广播的</a>，带有<img alt="" src="../img/f827b00456c04e393fa94ba4df1c7e08.jpg" />张量，而<code>out</code>将是<img alt="" src="../img/f827b00456c04e393fa94ba4df1c7e08.jpg" /> 张量。 <code>alpha</code>和<code>beta</code>的含义均与 <a href="#torch.addbmm" title="torch.addbmm"><code>torch.addbmm()</code></a> 中使用的缩放因子相同。</p>
<p><img alt="" src="../img/63aed732a8b14d4e5804e77ade6d9340.jpg" /></p>
<p>For inputs of type &lt;cite&gt;FloatTensor&lt;/cite&gt; or &lt;cite&gt;DoubleTensor&lt;/cite&gt;, arguments <code>beta</code> and <code>alpha</code> must be real numbers, otherwise they should be integers.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>beta</strong> (<em>Number__,</em> <em>optional</em>) – multiplier for <code>input</code> (<img alt="" src="../img/53a496ec7d546e2af9595a7055dd6a7e.jpg" />)</p>
</li>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor to be added</p>
</li>
<li>
<p><strong>alpha</strong> (<em>编号</em> <em>，</em> <em>可选</em>）– <img alt="" src="../img/7b5e33d60c908d236e66d3aba3044f4f.jpg" />(<img alt="" src="../img/5b9866fb35b01c553ed3e738e3972ae9.jpg" />）的乘数</p>
</li>
<li>
<p><strong>batch1</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the first batch of matrices to be multiplied</p>
</li>
<li>
<p><strong>batch2</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the second batch of matrices to be multiplied</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; M = torch.randn(10, 3, 5)
&gt;&gt;&gt; batch1 = torch.randn(10, 3, 4)
&gt;&gt;&gt; batch2 = torch.randn(10, 4, 5)
&gt;&gt;&gt; torch.baddbmm(M, batch1, batch2).size()
torch.Size([10, 3, 5])

</code></pre>
<hr />
<pre><code>torch.bmm(input, mat2, out=None) → Tensor¶
</code></pre>
<p>对<code>input</code>和<code>mat2</code>中存储的矩阵执行批处理矩阵矩阵乘积。</p>
<p><code>input</code>和<code>mat2</code>必须是 3D 张量，每个张量包含相同数量的矩阵。</p>
<p>如果<code>input</code>是<img alt="" src="../img/6bcc61944ebb068b264d492e9d3a9e9c.jpg" />张量，<code>mat2</code>是<img alt="" src="../img/30e4202e8cde7f489d43fdfbf59e0c46.jpg" />张量，<code>out</code>将是<img alt="" src="../img/f827b00456c04e393fa94ba4df1c7e08.jpg" />张量。</p>
<p><img alt="" src="../img/582f347178ea996d728f2eb71d21ae1d.jpg" /></p>
<p>Note</p>
<p>该功能不<a href="notes/broadcasting.html#broadcasting-semantics">广播</a>。 有关广播矩阵产品，请参见 <a href="#torch.matmul" title="torch.matmul"><code>torch.matmul()</code></a> 。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要相乘的第一批矩阵</p>
</li>
<li>
<p><strong>mat2</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–第二批矩阵相乘</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; input = torch.randn(10, 3, 4)
&gt;&gt;&gt; mat2 = torch.randn(10, 4, 5)
&gt;&gt;&gt; res = torch.bmm(input, mat2)
&gt;&gt;&gt; res.size()
torch.Size([10, 3, 5])

</code></pre>
<hr />
<pre><code>torch.chain_matmul(*matrices)¶
</code></pre>
<p>返回<img alt="" src="../img/7ea0d6aa290d37c8bc57957206bafe80.jpg" /> 2-D 张量的矩阵乘积。 使用矩阵链顺序算法可以有效地计算该乘积，该算法选择以算术运算 (<a href="https://mitpress.mit.edu/books/introduction-algorithms-third-edition">[CLRS]</a>)产生最低成本的顺序。 注意，由于这是一个计算乘积的函数，因此<img alt="" src="../img/7ea0d6aa290d37c8bc57957206bafe80.jpg" />必须大于或等于 2；因此，<img alt="" src="../img/7ea0d6aa290d37c8bc57957206bafe80.jpg" />必须大于或等于 2。 如果等于 2，则返回平凡的矩阵矩阵乘积。 如果<img alt="" src="../img/7ea0d6aa290d37c8bc57957206bafe80.jpg" />为 1，则为空操作-原始矩阵按原样返回。</p>
<p>Parameters</p>
<p><strong>矩阵</strong>(<em>张量...</em> )–由 2 个或多个 2D 张量确定其乘积的序列。</p>
<p>Returns</p>
<p>如果<img alt="" src="../img/10b8c0924577b4e47a4e8b6537a6ac9a.jpg" />张量的尺寸为<img alt="" src="../img/9342f0f16c0cf8e65f9e7baf672bc083.jpg" />，则乘积将为尺寸<img alt="" src="../img/169badb20d1f38fab1c101fc7c5c674f.jpg" />。</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(3, 4)
&gt;&gt;&gt; b = torch.randn(4, 5)
&gt;&gt;&gt; c = torch.randn(5, 6)
&gt;&gt;&gt; d = torch.randn(6, 7)
&gt;&gt;&gt; torch.chain_matmul(a, b, c, d)
tensor([[ -2.3375,  -3.9790,  -4.1119,  -6.6577,   9.5609, -11.5095,  -3.2614],
        [ 21.4038,   3.3378,  -8.4982,  -5.2457, -10.2561,  -2.4684,   2.7163],
        [ -0.9647,  -5.8917,  -2.3213,  -5.2284,  12.8615, -12.2816,  -2.5095]])

</code></pre>
<hr />
<pre><code>torch.cholesky(input, upper=False, out=None) → Tensor¶
</code></pre>
<p>计算对称正定矩阵<img alt="" src="../img/8cab287af6acaf0838d1d67381a3716d.jpg" />或一批对称正定矩阵的 Cholesky 分解。</p>
<p>如果<code>upper</code>为<code>True</code>，则返回的矩阵<code>U</code>为上三角，分解形式为：</p>
<p><img alt="" src="../img/917cbfcd41dc365c78ee1bb4651d5a3a.jpg" /></p>
<p>如果<code>upper</code>为<code>False</code>，则返回的矩阵<code>L</code>为下三角，分解形式为：</p>
<p><img alt="" src="../img/3f3a6cdf9c276b261c15da651aff430e.jpg" /></p>
<p>如果<code>upper</code>为<code>True</code>，并且<img alt="" src="../img/8cab287af6acaf0838d1d67381a3716d.jpg" />为一批对称的正定矩阵，则返回的张量将由各个矩阵的上三角 Cholesky 因子组成。 同样，当<code>upper</code>为<code>False</code>时，返回的张量将由每个单独矩阵的下三角 Cholesky 因子组成。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–大小为<img alt="" src="../img/aa6a866e7977a9ee67a53687003d3821.jpg" />的输入张量<img alt="" src="../img/8cab287af6acaf0838d1d67381a3716d.jpg" />，其中 &lt;cite&gt;*&lt;/cite&gt; 为零或更多个批处理尺寸，包括 对称正定矩阵。</p>
</li>
<li>
<p><strong>上</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–指示是否返回上三角矩阵或下三角矩阵的标志。 默认值：<code>False</code></p>
</li>
<li>
<p><strong>out</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>，</em> <em>可选</em>）–输出矩阵</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(3, 3)
&gt;&gt;&gt; a = torch.mm(a, a.t()) # make symmetric positive-definite
&gt;&gt;&gt; l = torch.cholesky(a)
&gt;&gt;&gt; a
tensor([[ 2.4112, -0.7486,  1.4551],
        [-0.7486,  1.3544,  0.1294],
        [ 1.4551,  0.1294,  1.6724]])
&gt;&gt;&gt; l
tensor([[ 1.5528,  0.0000,  0.0000],
        [-0.4821,  1.0592,  0.0000],
        [ 0.9371,  0.5487,  0.7023]])
&gt;&gt;&gt; torch.mm(l, l.t())
tensor([[ 2.4112, -0.7486,  1.4551],
        [-0.7486,  1.3544,  0.1294],
        [ 1.4551,  0.1294,  1.6724]])
&gt;&gt;&gt; a = torch.randn(3, 2, 2)
&gt;&gt;&gt; a = torch.matmul(a, a.transpose(-1, -2)) + 1e-03 # make symmetric positive-definite
&gt;&gt;&gt; l = torch.cholesky(a)
&gt;&gt;&gt; z = torch.matmul(l, l.transpose(-1, -2))
&gt;&gt;&gt; torch.max(torch.abs(z - a)) # Max non-zero
tensor(2.3842e-07)

</code></pre>
<hr />
<pre><code>torch.cholesky_inverse(input, upper=False, out=None) → Tensor¶
</code></pre>
<p>使用其 Cholesky 因子<img alt="" src="../img/83ebacfa515af65e0d7e683595edbde1.jpg" />计算对称正定矩阵<img alt="" src="../img/8cab287af6acaf0838d1d67381a3716d.jpg" />的逆：返回矩阵<code>inv</code>。 使用 LAPACK 例程<code>dpotri</code>和<code>spotri</code>(以及相应的 MAGMA 例程）计算逆。</p>
<p>如果<code>upper</code>为<code>False</code>，则<img alt="" src="../img/83ebacfa515af65e0d7e683595edbde1.jpg" />为下三角，这样返回的张量为</p>
<p><img alt="" src="../img/68e4a9138b1537d38f0e11634c0f068b.jpg" /></p>
<p>如果<code>upper</code>为<code>True</code>，或未提供，则<img alt="" src="../img/83ebacfa515af65e0d7e683595edbde1.jpg" />为上三角，使得返回的张量为</p>
<p><img alt="" src="../img/c4e7afdf52ea3475cb80b4d812b87a16.jpg" /></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–输入二维张量<img alt="" src="../img/83ebacfa515af65e0d7e683595edbde1.jpg" />，上或下三角 Cholesky 因子</p>
</li>
<li>
<p><strong>上部</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–是否返回下部(默认）或上部三角矩阵</p>
</li>
<li>
<p><strong>out</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>，</em> <em>可选</em>）– &lt;cite&gt;inv&lt;/cite&gt; 的输出张量</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(3, 3)
&gt;&gt;&gt; a = torch.mm(a, a.t()) + 1e-05 * torch.eye(3) # make symmetric positive definite
&gt;&gt;&gt; u = torch.cholesky(a)
&gt;&gt;&gt; a
tensor([[  0.9935,  -0.6353,   1.5806],
        [ -0.6353,   0.8769,  -1.7183],
        [  1.5806,  -1.7183,  10.6618]])
&gt;&gt;&gt; torch.cholesky_inverse(u)
tensor([[ 1.9314,  1.2251, -0.0889],
        [ 1.2251,  2.4439,  0.2122],
        [-0.0889,  0.2122,  0.1412]])
&gt;&gt;&gt; a.inverse()
tensor([[ 1.9314,  1.2251, -0.0889],
        [ 1.2251,  2.4439,  0.2122],
        [-0.0889,  0.2122,  0.1412]])

</code></pre>
<hr />
<pre><code>torch.cholesky_solve(input, input2, upper=False, out=None) → Tensor¶
</code></pre>
<p>给定其 Cholesky 因子矩阵<img alt="" src="../img/83ebacfa515af65e0d7e683595edbde1.jpg" />，以正半定矩阵解线性方程组。</p>
<p>如果<code>upper</code>为<code>False</code>，则<img alt="" src="../img/83ebacfa515af65e0d7e683595edbde1.jpg" />为且下部三角形，并且返回 &lt;cite&gt;c&lt;/cite&gt; 使得：</p>
<p><img alt="" src="../img/fbdb7ce9ef686ee96dd7242ca863ee3c.jpg" /></p>
<p>如果<code>upper</code>为<code>True</code>，则不提供<img alt="" src="../img/83ebacfa515af65e0d7e683595edbde1.jpg" />为上三角形，并且返回 &lt;cite&gt;c&lt;/cite&gt; ，使得：</p>
<p><img alt="" src="../img/b63d89883b7ac400b9b862e2f05738f7.jpg" /></p>
<p>&lt;cite&gt;torch.cholesky_solve(b，u）&lt;/cite&gt;可以接受 2D 输入 &lt;cite&gt;b，u&lt;/cite&gt; 或一批 2D 矩阵的输入。 如果输入为批次，则返回成批输出 &lt;cite&gt;c&lt;/cite&gt;</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–大小为<img alt="" src="../img/8ebb9d35b764041761ddb4e3436e265d.jpg" />的输入矩阵<img alt="" src="../img/23a2cc697e4e1a8adeb60eb42ab51a6e.jpg" />，其中<img alt="" src="../img/dcef98688866c0d5a21137cf53bf228d.jpg" />为零或更大批处理尺寸</p>
</li>
<li>
<p><strong>input2</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–大小为<img alt="" src="../img/10ea424983a008dbf72dc86628a2813d.jpg" />的输入矩阵<img alt="" src="../img/83ebacfa515af65e0d7e683595edbde1.jpg" />，其中<img alt="" src="../img/dcef98688866c0d5a21137cf53bf228d.jpg" />为零个或多个由上或下三角组成的批处理尺寸 胆固醇系数</p>
</li>
<li>
<p><strong>上</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–是否考虑将 Cholesky 因子视为下三角矩阵还是上三角矩阵。 默认值：<code>False</code>。</p>
</li>
<li>
<p><strong>输出</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>，</em> <em>可选</em>）– &lt;cite&gt;c&lt;/cite&gt; 的输出张量</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(3, 3)
&gt;&gt;&gt; a = torch.mm(a, a.t()) # make symmetric positive definite
&gt;&gt;&gt; u = torch.cholesky(a)
&gt;&gt;&gt; a
tensor([[ 0.7747, -1.9549,  1.3086],
        [-1.9549,  6.7546, -5.4114],
        [ 1.3086, -5.4114,  4.8733]])
&gt;&gt;&gt; b = torch.randn(3, 2)
&gt;&gt;&gt; b
tensor([[-0.6355,  0.9891],
        [ 0.1974,  1.4706],
        [-0.4115, -0.6225]])
&gt;&gt;&gt; torch.cholesky_solve(b, u)
tensor([[ -8.1625,  19.6097],
        [ -5.8398,  14.2387],
        [ -4.3771,  10.4173]])
&gt;&gt;&gt; torch.mm(a.inverse(), b)
tensor([[ -8.1626,  19.6097],
        [ -5.8398,  14.2387],
        [ -4.3771,  10.4173]])

</code></pre>
<hr />
<pre><code>torch.dot(input, tensor) → Tensor¶
</code></pre>
<p>计算两个张量的点积(内积）。</p>
<p>Note</p>
<p>该功能不<a href="notes/broadcasting.html#broadcasting-semantics">广播</a>。</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.dot(torch.tensor([2, 3]), torch.tensor([2, 1]))
tensor(7)

</code></pre>
<hr />
<pre><code>torch.eig(input, eigenvectors=False, out=None) -&gt; (Tensor, Tensor)¶
</code></pre>
<p>计算实方矩阵的特征值和特征向量。</p>
<p>Note</p>
<p>由于特征值和特征向量可能很复杂，因此仅 <a href="#torch.symeig" title="torch.symeig"><code>torch.symeig()</code></a> 支持反向传递</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–形状为<img alt="" src="../img/c64d7c7677227e8b59da7d7cfb2466d8.jpg" />的方阵，将为其计算特征值和特征向量</p>
</li>
<li>
<p><strong>特征向量</strong> (<em>bool</em> )– <code>True</code>以计算特征值和特征向量； 否则，将仅计算特征值</p>
</li>
<li>
<p><strong>out</strong> (<em>元组</em> <em>，</em> <em>可选</em>）–输出张量</p>
</li>
</ul>
<p>Returns</p>
<p>包含的 namedtuple(特征值，特征向量）</p>
<blockquote>
<ul>
<li>
<p><strong>特征值</strong>(<em>tensor</em>）：形状<img alt="" src="../img/f274e719751329e2ef63ca92533b23da.jpg" />。 每行是<code>input</code>的特征值，其中第一个元素是实部，第二个元素是虚部。 特征值不一定是有序的。</p>
</li>
<li>
<p><strong>特征向量</strong>(<em>tensor</em>）：如果<code>eigenvectors=False</code>为空，则为张量。 否则，可以使用形状<img alt="" src="../img/c64d7c7677227e8b59da7d7cfb2466d8.jpg" />的张量来计算对应特征值的归一化(单位长度）特征向量，如下所示。 如果对应的&lt;cite&gt;特征值[j]&lt;/cite&gt; 是实数，则&lt;cite&gt;特征向量[：，j]&lt;/cite&gt; 列是与&lt;cite&gt;特征值[j]&lt;/cite&gt; 相对应的特征向量。 如果相应的&lt;cite&gt;特征值[j]&lt;/cite&gt; 和&lt;cite&gt;特征值[j + 1]&lt;/cite&gt; 形成复共轭对，则真实特征向量可以计算为<img alt="" src="../img/cf872428156c87dba5ec860dd64ff692.jpg" />，<img alt="" src="../img/3a205492a67d1307528fb311bfba7d2a.jpg" />。</p>
</li>
</ul>
</blockquote>
<p>Return type</p>
<p>(<a href="tensors.html#torch.Tensor" title="torch.Tensor">张量</a>，<a href="tensors.html#torch.Tensor" title="torch.Tensor">张量</a>）</p>
<hr />
<pre><code>torch.geqrf(input, out=None) -&gt; (Tensor, Tensor)¶
</code></pre>
<p>这是直接调用 LAPACK 的底层函数。 该函数返回[eqg0f] 的 <a href="https://software.intel.com/en-us/node/521004">LAPACK 文档中定义的 namedtuple(a，tau）。</a></p>
<p>通常，您通常要使用 <a href="#torch.qr" title="torch.qr"><code>torch.qr()</code></a> 。</p>
<p>计算<code>input</code>的 QR 分解，但不将<img alt="" src="../img/7f927fea3856ca4796aab74326229f61.jpg" />和<img alt="" src="../img/fd6855baddb0a56aeca293dd58a9758d.jpg" />构造为明确的单独矩阵。</p>
<p>而是直接调用基础的 LAPACK 函数&lt;cite&gt;？geqrf&lt;/cite&gt; ，该函数产生一系列“基本反射器”。</p>
<p>有关更多详细信息，请参见 geqrf 的 <a href="https://software.intel.com/en-us/node/521004">LAPACK 文档。</a></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–输入矩阵</p>
</li>
<li>
<p><strong>out</strong> (<em>元组</em> <em>，</em> <em>可选</em>）–(张量，张量）的输出元组</p>
</li>
</ul>
<hr />
<pre><code>torch.ger(input, vec2, out=None) → Tensor¶
</code></pre>
<p><code>input</code>和<code>vec2</code>的外部乘积。 如果<code>input</code>是大小为<img alt="" src="../img/5f0051b0454fa75ca446b59b47eff6f6.jpg" />的向量，而<code>vec2</code>是大小为<img alt="" src="../img/03327e7b697db30918e96b2209927929.jpg" />的向量，则<code>out</code>必须是大小为<img alt="" src="../img/95760d62046dcfa418c3b7ffea4caefc.jpg" />的矩阵。</p>
<p>Note</p>
<p>This function does not <a href="notes/broadcasting.html#broadcasting-semantics">broadcast</a>.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–一维输入向量</p>
</li>
<li>
<p><strong>vec2</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–一维输入向量</p>
</li>
<li>
<p><strong>输出</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>，</em> <em>可选</em>）–可选输出矩阵</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; v1 = torch.arange(1., 5.)
&gt;&gt;&gt; v2 = torch.arange(1., 4.)
&gt;&gt;&gt; torch.ger(v1, v2)
tensor([[  1.,   2.,   3.],
        [  2.,   4.,   6.],
        [  3.,   6.,   9.],
        [  4.,   8.,  12.]])

</code></pre>
<hr />
<pre><code>torch.inverse(input, out=None) → Tensor¶
</code></pre>
<p>取方阵<code>input</code>的逆。 <code>input</code>可以是 2D 方形张量的批处理，在这种情况下，此函数将返回由各个逆组成的张量。</p>
<p>Note</p>
<p>无论原始步幅如何，返回的张量都将被转置，即使用 &lt;cite&gt;input.contiguous(）。transpose(-2，-1）.stride(）&lt;/cite&gt;之类的步幅</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–大小为<img alt="" src="../img/aa6a866e7977a9ee67a53687003d3821.jpg" />的输入张量，其中 &lt;cite&gt;*&lt;/cite&gt; 为零或更大批处理尺寸</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.rand(4, 4)
&gt;&gt;&gt; y = torch.inverse(x)
&gt;&gt;&gt; z = torch.mm(x, y)
&gt;&gt;&gt; z
tensor([[ 1.0000, -0.0000, -0.0000,  0.0000],
        [ 0.0000,  1.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.0000,  0.0000],
        [ 0.0000, -0.0000, -0.0000,  1.0000]])
&gt;&gt;&gt; torch.max(torch.abs(z - torch.eye(4))) # Max non-zero
tensor(1.1921e-07)
&gt;&gt;&gt; # Batched inverse example
&gt;&gt;&gt; x = torch.randn(2, 3, 4, 4)
&gt;&gt;&gt; y = torch.inverse(x)
&gt;&gt;&gt; z = torch.matmul(x, y)
&gt;&gt;&gt; torch.max(torch.abs(z - torch.eye(4).expand_as(x))) # Max non-zero
tensor(1.9073e-06)

</code></pre>
<hr />
<pre><code>torch.det(input) → Tensor¶
</code></pre>
<p>计算平方矩阵或批次平方矩阵的行列式。</p>
<p>Note</p>
<p>当<code>input</code>不可逆时，向后通过 <a href="#torch.det" title="torch.det"><code>det()</code></a> 内部使用 SVD 结果。 在这种情况下，如果<code>input</code>没有不同的奇异值，则通过 <a href="#torch.det" title="torch.det"><code>det()</code></a> 向后翻倍将不稳定。 有关详细信息，请参见 <a href="#torch.svd" title="torch.svd"><code>svd()</code></a> 。</p>
<p>Parameters</p>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–大小为<code>(*, n, n)</code>的输入张量，其中<code>*</code>为零或更大的批量尺寸。</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; A = torch.randn(3, 3)
&gt;&gt;&gt; torch.det(A)
tensor(3.7641)

&gt;&gt;&gt; A = torch.randn(3, 2, 2)
&gt;&gt;&gt; A
tensor([[[ 0.9254, -0.6213],
         [-0.5787,  1.6843]],

        [[ 0.3242, -0.9665],
         [ 0.4539, -0.0887]],

        [[ 1.1336, -0.4025],
         [-0.7089,  0.9032]]])
&gt;&gt;&gt; A.det()
tensor([1.1990, 0.4099, 0.7386])

</code></pre>
<hr />
<pre><code>torch.logdet(input) → Tensor¶
</code></pre>
<p>计算平方矩阵或批次平方矩阵的对数行列式。</p>
<p>Note</p>
<p>如果<code>input</code>的对数行列式为 0，则结果为<code>-inf</code>；如果<code>input</code>的行列式为负数，则结果为<code>nan</code>。</p>
<p>Note</p>
<p>当<code>input</code>不可逆时，向后通过 <a href="#torch.logdet" title="torch.logdet"><code>logdet()</code></a> 内部使用 SVD 结果。 在这种情况下，如果<code>input</code>没有不同的奇异值，则通过 <a href="#torch.logdet" title="torch.logdet"><code>logdet()</code></a> 向后翻倍将不稳定。 有关详细信息，请参见 <a href="#torch.svd" title="torch.svd"><code>svd()</code></a> 。</p>
<p>Parameters</p>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor of size <code>(*, n, n)</code> where <code>*</code> is zero or more batch dimensions.</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; A = torch.randn(3, 3)
&gt;&gt;&gt; torch.det(A)
tensor(0.2611)
&gt;&gt;&gt; torch.logdet(A)
tensor(-1.3430)
&gt;&gt;&gt; A
tensor([[[ 0.9254, -0.6213],
         [-0.5787,  1.6843]],

        [[ 0.3242, -0.9665],
         [ 0.4539, -0.0887]],

        [[ 1.1336, -0.4025],
         [-0.7089,  0.9032]]])
&gt;&gt;&gt; A.det()
tensor([1.1990, 0.4099, 0.7386])
&gt;&gt;&gt; A.det().log()
tensor([ 0.1815, -0.8917, -0.3031])

</code></pre>
<hr />
<pre><code>torch.slogdet(input) -&gt; (Tensor, Tensor)¶
</code></pre>
<p>计算平方矩阵或一批平方矩阵的行列式的正负号和对数绝对值。</p>
<p>Note</p>
<p>如果<code>input</code>的行列式为零，则返回<code>(0, -inf)</code>。</p>
<p>Note</p>
<p>当<code>input</code>不可逆时，向后通过 <a href="#torch.slogdet" title="torch.slogdet"><code>slogdet()</code></a> 内部使用 SVD 结果。 在这种情况下，如果<code>input</code>没有不同的奇异值，则通过 <a href="#torch.slogdet" title="torch.slogdet"><code>slogdet()</code></a> 向后翻倍将不稳定。 有关详细信息，请参见 <a href="#torch.svd" title="torch.svd"><code>svd()</code></a> 。</p>
<p>Parameters</p>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor of size <code>(*, n, n)</code> where <code>*</code> is zero or more batch dimensions.</p>
<p>Returns</p>
<p>包含行列式的符号和绝对行列式的对数值的 namedtuple(符号，logabsdet）。</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; A = torch.randn(3, 3)
&gt;&gt;&gt; A
tensor([[ 0.0032, -0.2239, -1.1219],
        [-0.6690,  0.1161,  0.4053],
        [-1.6218, -0.9273, -0.0082]])
&gt;&gt;&gt; torch.det(A)
tensor(-0.7576)
&gt;&gt;&gt; torch.logdet(A)
tensor(nan)
&gt;&gt;&gt; torch.slogdet(A)
torch.return_types.slogdet(sign=tensor(-1.), logabsdet=tensor(-0.2776))

</code></pre>
<hr />
<pre><code>torch.lstsq(input, A, out=None) → Tensor¶
</code></pre>
<p>计算大小为<img alt="" src="../img/88bf1d4bce0ef2568c0d7c879f26ce08.jpg" />的满秩矩阵<img alt="" src="../img/8cab287af6acaf0838d1d67381a3716d.jpg" />和大小为<img alt="" src="../img/811a1fadb1118d551ad2e0f7515b3ab2.jpg" />的矩阵<img alt="" src="../img/041cf842180f19a622690f37ed9f70d2.jpg" />的最小二乘和最小范数问题的解。</p>
<p>如果<img alt="" src="../img/7cbd71a05144eeca3bb1f22a22c30c3a.jpg" />， <a href="#torch.lstsq" title="torch.lstsq"><code>lstsq()</code></a> 解决了最小二乘问题：</p>
<p><img alt="" src="../img/9d1018603f258178ba5a9db4739d49ff.jpg" /></p>
<p>如果<img alt="" src="../img/30ee994fb1857c1f8d6540a60056fe45.jpg" />， <a href="#torch.lstsq" title="torch.lstsq"><code>lstsq()</code></a> 解决了最小范数问题：</p>
<p><img alt="" src="../img/cb7af0f918cae78e16fd68223243e29c.jpg" /></p>
<p>返回的张量<img alt="" src="../img/1dc567019b272fda0c5051c472dac2b7.jpg" />具有形状<img alt="" src="../img/2e7f8c37553bcb25b33aa412a4d78219.jpg" />。 <img alt="" src="../img/1dc567019b272fda0c5051c472dac2b7.jpg" />的前<img alt="" src="../img/5f0051b0454fa75ca446b59b47eff6f6.jpg" />行包含解决方案。 如果为<img alt="" src="../img/7cbd71a05144eeca3bb1f22a22c30c3a.jpg" />，则每列中解决方案的剩余平方和由该列其余<img alt="" src="../img/6d15ee60528ade686ac8933bcac404a4.jpg" />行中元素的平方和得出。</p>
<p>Note</p>
<p>GPU 不支持<img alt="" src="../img/30ee994fb1857c1f8d6540a60056fe45.jpg" />的情况。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–矩阵<img alt="" src="../img/041cf842180f19a622690f37ed9f70d2.jpg" /></p>
</li>
<li>
<p><strong>A</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–由<img alt="" src="../img/5f0051b0454fa75ca446b59b47eff6f6.jpg" />矩阵<img alt="" src="../img/8cab287af6acaf0838d1d67381a3716d.jpg" />构成的<img alt="" src="../img/03327e7b697db30918e96b2209927929.jpg" /></p>
</li>
<li>
<p><strong>out</strong> (<em>元组</em> <em>，</em> <em>可选</em>）–可选目标张量</p>
</li>
</ul>
<p>Returns</p>
<p>一个命名元组(解决方案，QR），其中包含：</p>
<blockquote>
<ul>
<li>
<p><strong>解</strong>(<em>tensor</em>）：最小二乘解</p>
</li>
<li>
<p><strong>QR</strong>  (<em>Tensor</em> )：QR 因式分解的详细信息</p>
</li>
</ul>
</blockquote>
<p>Return type</p>
<p>(<a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>, <a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>)</p>
<p>Note</p>
<p>无论输入矩阵的跨度如何，返回的矩阵将始终进行转置。 即，他们将具有&lt;cite&gt;(1，m）&lt;/cite&gt;而不是&lt;cite&gt;(m，1）&lt;/cite&gt;的步幅。</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; A = torch.tensor([[1., 1, 1],
                      [2, 3, 4],
                      [3, 5, 2],
                      [4, 2, 5],
                      [5, 4, 3]])
&gt;&gt;&gt; B = torch.tensor([[-10., -3],
                      [ 12, 14],
                      [ 14, 12],
                      [ 16, 16],
                      [ 18, 16]])
&gt;&gt;&gt; X, _ = torch.lstsq(B, A)
&gt;&gt;&gt; X
tensor([[  2.0000,   1.0000],
        [  1.0000,   1.0000],
        [  1.0000,   2.0000],
        [ 10.9635,   4.8501],
        [  8.9332,   5.2418]])

</code></pre>
<hr />
<pre><code>torch.lu(A, pivot=True, get_infos=False, out=None)¶
</code></pre>
<p>计算矩阵或矩阵批次的 LU 分解<code>A</code>。 返回一个包含 LU 分解和<code>A</code>的枢轴的元组。 如果<code>pivot</code>设置为<code>True</code>，则完成旋转。</p>
<p>Note</p>
<p>该函数返回的枢轴为 1 索引。 如果<code>pivot</code>为<code>False</code>，则返回的枢轴是一个张量，该张量填充有适当大小的零。</p>
<p>Note</p>
<p><code>pivot</code> = <code>False</code>的 LU 分解不适用于 CPU，尝试这样做会引发错误。 但是，CUDA 可使用<code>pivot</code> = <code>False</code>的 LU 分解。</p>
<p>Note</p>
<p>该函数不会检查分解是否成功，因为<code>get_infos</code>为<code>True</code>，因为返回元组的第三个元素中存在分解的状态。</p>
<p>Note</p>
<p>在 CUDA 设备上批量处理大小小于或等于 32 的平方矩阵的情况下，由于 MAGMA 库中的错误，对奇异矩阵重复进行 LU 因式分解(请参见岩浆问题 13）。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>A</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–大小的张量<img alt="" src="../img/6217ff594f4af540b27d0cc551ab742c.jpg" /></p>
</li>
<li>
<p><strong>枢轴</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–控制是否完成枢轴。 默认值：<code>True</code></p>
</li>
<li>
<p><strong>get_infos</strong>  (<em>bool</em> <em>，</em> <em>可选</em>）–如果设置为<code>True</code>，则返回信息 IntTensor。 默认值：<code>False</code></p>
</li>
<li>
<p><strong>输出</strong>(<em>元组</em> <em>，</em> <em>可选</em>）–可选输出元组。 如果<code>get_infos</code>为<code>True</code>，则元组中的元素为 Tensor，IntTensor 和 IntTensor。 如果<code>get_infos</code>为<code>False</code>，则元组中的元素为 Tensor，IntTensor。 默认值：<code>None</code></p>
</li>
</ul>
<p>Returns</p>
<p>张量的元组包含</p>
<blockquote>
<ul>
<li>
<p><strong>分解</strong>(<em>tensor</em>）：大小<img alt="" src="../img/6217ff594f4af540b27d0cc551ab742c.jpg" />的分解</p>
</li>
<li>
<p><strong>枢轴</strong> (<em>IntTensor</em> )：大小为<img alt="" src="../img/74dd7138867888df79f198ead03a8f07.jpg" />的枢轴</p>
</li>
<li>
<p><strong>信息</strong> (<em>IntTensor</em> ，<em>可选</em>）：如果<code>get_infos</code>为<code>True</code>，则此张量为<img alt="" src="../img/39f05b69c0ea6b9b6b228f948944c4f6.jpg" />，其中非零值表示是否 矩阵分解或每个小批量成功或失败</p>
</li>
</ul>
</blockquote>
<p>Return type</p>
<p>(<a href="tensors.html#torch.Tensor" title="torch.Tensor">张量</a>，IntTensor，IntTensor(可选））</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; A = torch.randn(2, 3, 3)
&gt;&gt;&gt; A_LU, pivots = torch.lu(A)
&gt;&gt;&gt; A_LU
tensor([[[ 1.3506,  2.5558, -0.0816],
         [ 0.1684,  1.1551,  0.1940],
         [ 0.1193,  0.6189, -0.5497]],

        [[ 0.4526,  1.2526, -0.3285],
         [-0.7988,  0.7175, -0.9701],
         [ 0.2634, -0.9255, -0.3459]]])
&gt;&gt;&gt; pivots
tensor([[ 3,  3,  3],
        [ 3,  3,  3]], dtype=torch.int32)
&gt;&gt;&gt; A_LU, pivots, info = torch.lu(A, get_infos=True)
&gt;&gt;&gt; if info.nonzero().size(0) == 0:
...   print('LU factorization succeeded for all samples!')
LU factorization succeeded for all samples!

</code></pre>
<hr />
<pre><code>torch.lu_solve(input, LU_data, LU_pivots, out=None) → Tensor¶
</code></pre>
<p>使用 <a href="#torch.lu" title="torch.lu"><code>torch.lu()</code></a> 中 A 的部分枢轴 LU 分解，返回线性系统<img alt="" src="../img/328fdc7aa24f647110fc0900733a006f.jpg" />的 LU 解。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>b</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–尺寸为<img alt="" src="../img/8ebb9d35b764041761ddb4e3436e265d.jpg" />的 RHS 张量，其中<img alt="" src="../img/dcef98688866c0d5a21137cf53bf228d.jpg" />为零或更大的批量尺寸。</p>
</li>
<li>
<p><strong>LU_data</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)– A 从 <a href="#torch.lu" title="torch.lu"><code>torch.lu()</code></a> 大小为<img alt="" src="../img/10ea424983a008dbf72dc86628a2813d.jpg" />的 A 的透视 LU 分解，其中<img alt="" src="../img/dcef98688866c0d5a21137cf53bf228d.jpg" />为 零个或多个批次尺寸。</p>
</li>
<li>
<p><strong>LU_pivots</strong>  (<em>IntTensor</em> )– LU 分解的枢轴来自 <a href="#torch.lu" title="torch.lu"><code>torch.lu()</code></a> ，大小为<img alt="" src="../img/74dd7138867888df79f198ead03a8f07.jpg" />，其中<img alt="" src="../img/dcef98688866c0d5a21137cf53bf228d.jpg" />为零或更大的批生产尺寸。 <code>LU_pivots</code>的批次尺寸必须等于<code>LU_data</code>的批次尺寸。</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; A = torch.randn(2, 3, 3)
&gt;&gt;&gt; b = torch.randn(2, 3, 1)
&gt;&gt;&gt; A_LU = torch.lu(A)
&gt;&gt;&gt; x = torch.lu_solve(b, *A_LU)
&gt;&gt;&gt; torch.norm(torch.bmm(A, x) - b)
tensor(1.00000e-07 *
       2.8312)

</code></pre>
<hr />
<pre><code>torch.lu_unpack(LU_data, LU_pivots, unpack_data=True, unpack_pivots=True)¶
</code></pre>
<p>解压缩数据并从张量的 LU 分解中枢转。</p>
<p>返回张量的元组为<code>(the pivots, the L tensor, the U tensor)</code>。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>LU_data</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–打包 LU 分解数据</p>
</li>
<li>
<p><strong>LU_pivots</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–压缩 LU 分解枢轴</p>
</li>
<li>
<p><strong>unpack_data</strong>  (<em>bool</em> )–指示是否应拆包数据的标志</p>
</li>
<li>
<p><strong>unpack_pivots</strong>  (<em>bool</em> )–指示是否应拆开枢轴的标志</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; A = torch.randn(2, 3, 3)
&gt;&gt;&gt; A_LU, pivots = A.lu()
&gt;&gt;&gt; P, A_L, A_U = torch.lu_unpack(A_LU, pivots)
&gt;&gt;&gt;
&gt;&gt;&gt; # can recover A from factorization
&gt;&gt;&gt; A_ = torch.bmm(P, torch.bmm(A_L, A_U))

&gt;&gt;&gt; # LU factorization of a rectangular matrix:
&gt;&gt;&gt; A = torch.randn(2, 3, 2)
&gt;&gt;&gt; A_LU, pivots = A.lu()
&gt;&gt;&gt; P, A_L, A_U = torch.lu_unpack(A_LU, pivots)
&gt;&gt;&gt; P
tensor([[[1., 0., 0.],
         [0., 1., 0.],
         [0., 0., 1.]],

        [[0., 0., 1.],
         [0., 1., 0.],
         [1., 0., 0.]]])
&gt;&gt;&gt; A_L
tensor([[[ 1.0000,  0.0000],
         [ 0.4763,  1.0000],
         [ 0.3683,  0.1135]],

        [[ 1.0000,  0.0000],
         [ 0.2957,  1.0000],
         [-0.9668, -0.3335]]])
&gt;&gt;&gt; A_U
tensor([[[ 2.1962,  1.0881],
         [ 0.0000, -0.8681]],

        [[-1.0947,  0.3736],
         [ 0.0000,  0.5718]]])
&gt;&gt;&gt; A_ = torch.bmm(P, torch.bmm(A_L, A_U))
&gt;&gt;&gt; torch.norm(A_ - A)
tensor(2.9802e-08)

</code></pre>
<hr />
<pre><code>torch.matmul(input, other, out=None) → Tensor¶
</code></pre>
<p>两个张量的矩阵乘积。</p>
<p>行为取决于张量的维数，如下所示：</p>
<ul>
<li>
<p>如果两个张量都是一维的，则返回点积(标量）。</p>
</li>
<li>
<p>如果两个参数都是二维的，则返回矩阵矩阵乘积。</p>
</li>
<li>
<p>如果第一个自变量是一维的，第二个自变量是二维的，则为了矩阵乘法，会将 1 附加到其维上。 矩阵相乘后，将删除前置尺寸。</p>
</li>
<li>
<p>如果第一个参数为 2 维，第二个参数为 1 维，则返回矩阵向量乘积。</p>
</li>
<li>
<p>如果两个自变量至少为一维且至少一个自变量为 N 维(其中 N &gt; 2），则返回批处理矩阵乘法。 如果第一个自变量是一维的，则将 1 附加到其维的前面，以实现批量矩阵乘法并在之后将其删除。 如果第二个参数是一维的，则将 1 附加到其维上，以实现成批矩阵倍数的目的，然后将其删除。 非矩阵(即批处理）尺寸是<a href="notes/broadcasting.html#broadcasting-semantics">广播的</a>(因此必须是可广播的）。 例如，如果<code>input</code>是<img alt="" src="../img/48e81addd1b15307d101249324d93372.jpg" />张量，<code>other</code>是<img alt="" src="../img/561dfed20b3d8014c77c118b8904ec66.jpg" />张量，则<code>out</code>将是<img alt="" src="../img/cb41125b3953740f3a7f50204cd20d7f.jpg" />张量。</p>
</li>
</ul>
<p>Note</p>
<p>此功能的一维点积产品版本不支持<code>out</code>参数。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要相乘的第一个张量</p>
</li>
<li>
<p><strong>其他</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要相乘的第二张量</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; # vector x vector
&gt;&gt;&gt; tensor1 = torch.randn(3)
&gt;&gt;&gt; tensor2 = torch.randn(3)
&gt;&gt;&gt; torch.matmul(tensor1, tensor2).size()
torch.Size([])
&gt;&gt;&gt; # matrix x vector
&gt;&gt;&gt; tensor1 = torch.randn(3, 4)
&gt;&gt;&gt; tensor2 = torch.randn(4)
&gt;&gt;&gt; torch.matmul(tensor1, tensor2).size()
torch.Size([3])
&gt;&gt;&gt; # batched matrix x broadcasted vector
&gt;&gt;&gt; tensor1 = torch.randn(10, 3, 4)
&gt;&gt;&gt; tensor2 = torch.randn(4)
&gt;&gt;&gt; torch.matmul(tensor1, tensor2).size()
torch.Size([10, 3])
&gt;&gt;&gt; # batched matrix x batched matrix
&gt;&gt;&gt; tensor1 = torch.randn(10, 3, 4)
&gt;&gt;&gt; tensor2 = torch.randn(10, 4, 5)
&gt;&gt;&gt; torch.matmul(tensor1, tensor2).size()
torch.Size([10, 3, 5])
&gt;&gt;&gt; # batched matrix x broadcasted matrix
&gt;&gt;&gt; tensor1 = torch.randn(10, 3, 4)
&gt;&gt;&gt; tensor2 = torch.randn(4, 5)
&gt;&gt;&gt; torch.matmul(tensor1, tensor2).size()
torch.Size([10, 3, 5])

</code></pre>
<hr />
<pre><code>torch.matrix_power(input, n) → Tensor¶
</code></pre>
<p>返回平方矩阵乘幂<code>n</code>的矩阵。 对于一批矩阵，将每个单独的矩阵提高到幂<code>n</code>。</p>
<p>如果<code>n</code>为负，则矩阵的逆(如果是可逆的）提高到幂<code>n</code>。 对于一批矩阵，将成批的逆(如果可逆）提高到幂<code>n</code>。 如果<code>n</code>为 0，则返回一个单位矩阵。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor.</p>
</li>
<li>
<p><strong>n</strong>  (<em>python：int</em> )–将矩阵提升为</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(2, 2, 2)
&gt;&gt;&gt; a
tensor([[[-1.9975, -1.9610],
         [ 0.9592, -2.3364]],

        [[-1.2534, -1.3429],
         [ 0.4153, -1.4664]]])
&gt;&gt;&gt; torch.matrix_power(a, 3)
tensor([[[  3.9392, -23.9916],
         [ 11.7357,  -0.2070]],

        [[  0.2468,  -6.7168],
         [  2.0774,  -0.8187]]])

</code></pre>
<hr />
<pre><code>torch.matrix_rank(input, tol=None, symmetric=False) → Tensor¶
</code></pre>
<p>返回二维张量的数值等级。 默认情况下，使用 SVD 完成计算矩阵等级的方法。 如果<code>symmetric</code>为<code>True</code>，则假定<code>input</code>是对称的，并且通过获得特征值来完成秩的计算。</p>
<p><code>tol</code>是阈值，低于该阈值的奇异值(或当<code>symmetric</code>为<code>True</code>时的特征值）被视为 0。如果未指定<code>tol</code>，则<code>tol</code>设置为<code>S.max() * max(S.size()) * eps</code>，其中 &lt;cite&gt;S&lt;/cite&gt; 是奇异值(或<code>symmetric</code>是<code>True</code>时的特征值），<code>eps</code>是<code>input</code>数据类型的 epsilon 值。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–输入二维张量</p>
</li>
<li>
<p><strong>tol</strong>  (<em>python：float</em> <em>，</em> <em>可选</em>）–公差值。 默认值：<code>None</code></p>
</li>
<li>
<p><strong>对称</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–指示<code>input</code>是否对称。 默认值：<code>False</code></p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.eye(10)
&gt;&gt;&gt; torch.matrix_rank(a)
tensor(10)
&gt;&gt;&gt; b = torch.eye(10)
&gt;&gt;&gt; b[0, 0] = 0
&gt;&gt;&gt; torch.matrix_rank(b)
tensor(9)

</code></pre>
<hr />
<pre><code>torch.mm(input, mat2, out=None) → Tensor¶
</code></pre>
<p>对矩阵<code>input</code>和<code>mat2</code>进行矩阵乘法。</p>
<p>如果<code>input</code>是<img alt="" src="../img/95760d62046dcfa418c3b7ffea4caefc.jpg" />张量，<code>mat2</code>是<img alt="" src="../img/d145985cd9c2b23f68a55b0d5429c2ac.jpg" />张量，<code>out</code>将是<img alt="" src="../img/80b2c8de6d028b93a22dfe571079ee9c.jpg" />张量。</p>
<p>Note</p>
<p>This function does not <a href="notes/broadcasting.html#broadcasting-semantics">broadcast</a>. For broadcasting matrix products, see <a href="#torch.matmul" title="torch.matmul"><code>torch.matmul()</code></a>.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要相乘的第一个矩阵</p>
</li>
<li>
<p><strong>mat2</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the second matrix to be multiplied</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; mat1 = torch.randn(2, 3)
&gt;&gt;&gt; mat2 = torch.randn(3, 3)
&gt;&gt;&gt; torch.mm(mat1, mat2)
tensor([[ 0.4851,  0.5037, -0.3633],
        [-0.0760, -3.6705,  2.4784]])

</code></pre>
<hr />
<pre><code>torch.mv(input, vec, out=None) → Tensor¶
</code></pre>
<p>执行矩阵<code>input</code>与向量<code>vec</code>的矩阵向量积。</p>
<p>如果<code>input</code>是<img alt="" src="../img/95760d62046dcfa418c3b7ffea4caefc.jpg" />张量，<code>vec</code>是大小<img alt="" src="../img/03327e7b697db30918e96b2209927929.jpg" />的一维张量，则<code>out</code>将是大小<img alt="" src="../img/5f0051b0454fa75ca446b59b47eff6f6.jpg" />的一维。</p>
<p>Note</p>
<p>This function does not <a href="notes/broadcasting.html#broadcasting-semantics">broadcast</a>.</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要相乘的矩阵</p>
</li>
<li>
<p><strong>vec</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – vector to be multiplied</p>
</li>
<li>
<p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; mat = torch.randn(2, 3)
&gt;&gt;&gt; vec = torch.randn(3)
&gt;&gt;&gt; torch.mv(mat, vec)
tensor([ 1.0404, -0.6361])

</code></pre>
<hr />
<pre><code>torch.orgqr(input, input2) → Tensor¶
</code></pre>
<p>根据 <a href="#torch.geqrf" title="torch.geqrf"><code>torch.geqrf()</code></a> 返回的&lt;cite&gt;(输入，input2）&lt;/cite&gt;元组，计算 QR 分解的正交矩阵 &lt;cite&gt;Q&lt;/cite&gt; 。</p>
<p>这将直接调用基础的 LAPACK 函数&lt;cite&gt;？orgqr&lt;/cite&gt; 。 有关更多详细信息，请参见 orgqr 的 <a href="https://software.intel.com/en-us/mkl-developer-reference-c-orgqr">LAPACK 文档。</a></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–来自 <a href="#torch.geqrf" title="torch.geqrf"><code>torch.geqrf()</code></a> 的 &lt;cite&gt;a&lt;/cite&gt; 。</p>
</li>
<li>
<p><strong>input2</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–来自 <a href="#torch.geqrf" title="torch.geqrf"><code>torch.geqrf()</code></a> 的 &lt;cite&gt;tau&lt;/cite&gt; 。</p>
</li>
</ul>
<hr />
<pre><code>torch.ormqr(input, input2, input3, left=True, transpose=False) → Tensor¶
</code></pre>
<p>将&lt;cite&gt;垫&lt;/cite&gt;(由<code>input3</code>赋予）乘以 <a href="#torch.geqrf" title="torch.geqrf"><code>torch.geqrf()</code></a> 表示的 QR 因式分解的正交 &lt;cite&gt;Q&lt;/cite&gt; 矩阵，该矩阵由&lt;cite&gt;(a (tau）&lt;/cite&gt;(由[<code>input</code>，<code>input2</code>给予））。</p>
<p>这将直接调用基础的 LAPACK 函数&lt;cite&gt;？ormqr&lt;/cite&gt; 。 有关更多详细信息，请参见 ormqr 的 <a href="https://software.intel.com/en-us/mkl-developer-reference-c-ormqr">LAPACK 文档。</a></p>
<p>Parameters</p>
<ul>
<li>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the &lt;cite&gt;a&lt;/cite&gt; from <a href="#torch.geqrf" title="torch.geqrf"><code>torch.geqrf()</code></a>.</p>
</li>
<li>
<p><strong>input2</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the &lt;cite&gt;tau&lt;/cite&gt; from <a href="#torch.geqrf" title="torch.geqrf"><code>torch.geqrf()</code></a>.</p>
</li>
<li>
<p><strong>input3</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–要相乘的矩阵。</p>
</li>
</ul>
<hr />
<pre><code>torch.pinverse(input, rcond=1e-15) → Tensor¶
</code></pre>
<p>计算 2D 张量的伪逆(也称为 Moore-Penrose 逆）。 请查看 <a href="https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse">Moore-Penrose 逆</a>了解更多详细信息</p>
<p>Note</p>
<p>使用奇异值分解实现此方法。</p>
<p>Note</p>
<p>在矩阵 <a href="https://epubs.siam.org/doi/10.1137/0117004">[1]</a> 的元素中，伪逆不一定是连续函数。 因此，导数并不总是存在，并且仅以恒定等级存在 <a href="https://www.jstor.org/stable/2156365">[2]</a> 。 但是，由于使用 SVD 结果实现，因此该方法可向后传播，并且可能不稳定。 由于内部使用 SVD，因此双向后退也会变得不稳定。 有关更多详细信息，请参见 <a href="#torch.svd" title="torch.svd"><code>svd()</code></a> 。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–大小为<img alt="" src="../img/6217ff594f4af540b27d0cc551ab742c.jpg" />的输入张量，其中<img alt="" src="../img/dcef98688866c0d5a21137cf53bf228d.jpg" />为零或更大批处理尺寸</p>
</li>
<li>
<p><strong>rcond</strong>  (<em>python：float</em> )–一个浮点值，用于确定小的奇异值的截止值。 默认值：1e-15</p>
</li>
</ul>
<p>Returns</p>
<p>尺寸为<img alt="" src="../img/decace38e1e20419be05da9c4eac4b78.jpg" />的<code>input</code>的伪逆。</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; input = torch.randn(3, 5)
&gt;&gt;&gt; input
tensor([[ 0.5495,  0.0979, -1.4092, -0.1128,  0.4132],
        [-1.1143, -0.3662,  0.3042,  1.6374, -0.9294],
        [-0.3269, -0.5745, -0.0382, -0.5922, -0.6759]])
&gt;&gt;&gt; torch.pinverse(input)
tensor([[ 0.0600, -0.1933, -0.2090],
        [-0.0903, -0.0817, -0.4752],
        [-0.7124, -0.1631, -0.2272],
        [ 0.1356,  0.3933, -0.5023],
        [-0.0308, -0.1725, -0.5216]])
&gt;&gt;&gt; # Batched pinverse example
&gt;&gt;&gt; a = torch.randn(2,6,3)
&gt;&gt;&gt; b = torch.pinverse(a)
&gt;&gt;&gt; torch.matmul(b, a)
tensor([[[ 1.0000e+00,  1.6391e-07, -1.1548e-07],
        [ 8.3121e-08,  1.0000e+00, -2.7567e-07],
        [ 3.5390e-08,  1.4901e-08,  1.0000e+00]],

        [[ 1.0000e+00, -8.9407e-08,  2.9802e-08],
        [-2.2352e-07,  1.0000e+00,  1.1921e-07],
        [ 0.0000e+00,  8.9407e-08,  1.0000e+00]]])

</code></pre>
<hr />
<pre><code>torch.qr(input, some=True, out=None) -&gt; (Tensor, Tensor)¶
</code></pre>
<p>计算矩阵或一批矩阵<code>input</code>的 QR 分解，并返回张量的命名元组(Q，R），使得<img alt="" src="../img/41222a3dad86eb0798f47e735d4eb15b.jpg" />其中<img alt="" src="../img/7f927fea3856ca4796aab74326229f61.jpg" />是正交矩阵或一批正交矩阵，而<img alt="" src="../img/fd6855baddb0a56aeca293dd58a9758d.jpg" />是 上三角矩阵或一批上三角矩阵。</p>
<p>如果<code>some</code>为<code>True</code>，则此函数返回瘦(​​精简）QR 因式分解。 否则，如果<code>some</code>为<code>False</code>，则此函数返回完整的 QR 因式分解。</p>
<p>Note</p>
<p>如果<code>input</code>的元素的幅度较大，则可能会失去精度</p>
<p>Note</p>
<p>尽管它始终可以为您提供有效的分解，但在各个平台上可能不会给您相同的分解-这取决于您的 LAPACK 实现。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–大小为<img alt="" src="../img/6217ff594f4af540b27d0cc551ab742c.jpg" />的输入张量，其中 &lt;cite&gt;*&lt;/cite&gt; 为零个或多个批处理尺寸，包括尺寸矩阵 <img alt="" src="../img/5324b7a0d3b2c1767998bc499b6dfb11.jpg" />。</p>
</li>
<li>
<p><strong>一些</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–设置为<code>True</code>可减少 QR 分解，将<code>False</code>进行完全 QR 分解。</p>
</li>
<li>
<p><strong>out</strong> (<em>元组</em> <em>，</em> <em>可选</em>）– &lt;cite&gt;Q&lt;/cite&gt; 和 &lt;cite&gt;R&lt;/cite&gt; 张量的元组 <code>input = torch.matmul(Q, R)</code>。 &lt;cite&gt;Q&lt;/cite&gt; 和 &lt;cite&gt;R&lt;/cite&gt; 的尺寸分别为<img alt="" src="../img/8ebb9d35b764041761ddb4e3436e265d.jpg" />和<img alt="" src="../img/d158d0764e1a57ef7842c747aafc64dc.jpg" />，如果<code>some:</code>为<code>True</code>则为<img alt="" src="../img/c7ea35623a8622e24f04c76bdf3f47c5.jpg" />，否则为<img alt="" src="../img/632a48f07f9deba36cb4ed068721ae09.jpg" />。</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.tensor([[12., -51, 4], [6, 167, -68], [-4, 24, -41]])
&gt;&gt;&gt; q, r = torch.qr(a)
&gt;&gt;&gt; q
tensor([[-0.8571,  0.3943,  0.3314],
        [-0.4286, -0.9029, -0.0343],
        [ 0.2857, -0.1714,  0.9429]])
&gt;&gt;&gt; r
tensor([[ -14.0000,  -21.0000,   14.0000],
        [   0.0000, -175.0000,   70.0000],
        [   0.0000,    0.0000,  -35.0000]])
&gt;&gt;&gt; torch.mm(q, r).round()
tensor([[  12.,  -51.,    4.],
        [   6.,  167.,  -68.],
        [  -4.,   24.,  -41.]])
&gt;&gt;&gt; torch.mm(q.t(), q).round()
tensor([[ 1.,  0.,  0.],
        [ 0.,  1., -0.],
        [ 0., -0.,  1.]])
&gt;&gt;&gt; a = torch.randn(3, 4, 5)
&gt;&gt;&gt; q, r = torch.qr(a, some=False)
&gt;&gt;&gt; torch.allclose(torch.matmul(q, r), a)
True
&gt;&gt;&gt; torch.allclose(torch.matmul(q.transpose(-2, -1), q), torch.eye(5))
True

</code></pre>
<hr />
<pre><code>torch.solve(input, A, out=None) -&gt; (Tensor, Tensor)¶
</code></pre>
<p>此函数将求解返回到由<img alt="" src="../img/d8a7525921c25fbc9d6af3a53d546be8.jpg" />表示的线性方程组和 A 的 LU 分解，以便将其作为命名元&lt;cite&gt;解决方案 LU&lt;/cite&gt; 。</p>
<p>&lt;cite&gt;LU&lt;/cite&gt; 包含 &lt;cite&gt;L&lt;/cite&gt; 和 &lt;cite&gt;U&lt;/cite&gt; 因素，用于 &lt;cite&gt;A&lt;/cite&gt; 的 LU 分解。</p>
<p>&lt;cite&gt;torch.solve(B，A）&lt;/cite&gt;可以接受 2D 输入 &lt;cite&gt;B，A&lt;/cite&gt; 或一批 2D 矩阵的输入。 如果输入是批次，则返回批次输出&lt;cite&gt;解决方案 LU&lt;/cite&gt; 。</p>
<p>Note</p>
<p>不管原始步幅如何，返回的矩阵&lt;cite&gt;解决方案&lt;/cite&gt;和 &lt;cite&gt;LU&lt;/cite&gt; 都将转置，即，步幅类似 &lt;cite&gt;B.contiguous(）。transpose(-1，-2）。 stride(）&lt;/cite&gt;和 &lt;cite&gt;A.contiguous(）。transpose(-1，-2）.stride(）&lt;/cite&gt;。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–大小为<img alt="" src="../img/8ebb9d35b764041761ddb4e3436e265d.jpg" />的输入矩阵<img alt="" src="../img/041cf842180f19a622690f37ed9f70d2.jpg" />，其中<img alt="" src="../img/dcef98688866c0d5a21137cf53bf228d.jpg" />为零或更大的批次尺寸。</p>
</li>
<li>
<p><strong>A</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–大小为<img alt="" src="../img/10ea424983a008dbf72dc86628a2813d.jpg" />的输入方阵，其中<img alt="" src="../img/dcef98688866c0d5a21137cf53bf228d.jpg" />为零或更大的批处理尺寸。</p>
</li>
<li>
<p><strong>输出</strong>(<em>(</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>，</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>）</em> <em>，</em> <em>可选</em>）–可选输出元组。</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; A = torch.tensor([[6.80, -2.11,  5.66,  5.97,  8.23],
                      [-6.05, -3.30,  5.36, -4.44,  1.08],
                      [-0.45,  2.58, -2.70,  0.27,  9.04],
                      [8.32,  2.71,  4.35,  -7.17,  2.14],
                      [-9.67, -5.14, -7.26,  6.08, -6.87]]).t()
&gt;&gt;&gt; B = torch.tensor([[4.02,  6.19, -8.22, -7.57, -3.03],
                      [-1.56,  4.00, -8.67,  1.75,  2.86],
                      [9.81, -4.09, -4.57, -8.61,  8.99]]).t()
&gt;&gt;&gt; X, LU = torch.solve(B, A)
&gt;&gt;&gt; torch.dist(B, torch.mm(A, X))
tensor(1.00000e-06 *
       7.0977)

&gt;&gt;&gt; # Batched solver example
&gt;&gt;&gt; A = torch.randn(2, 3, 1, 4, 4)
&gt;&gt;&gt; B = torch.randn(2, 3, 1, 4, 6)
&gt;&gt;&gt; X, LU = torch.solve(B, A)
&gt;&gt;&gt; torch.dist(B, A.matmul(X))
tensor(1.00000e-06 *
   3.6386)

</code></pre>
<hr />
<pre><code>torch.svd(input, some=True, compute_uv=True, out=None) -&gt; (Tensor, Tensor, Tensor)¶
</code></pre>
<p>该函数返回一个命名元组<code>(U, S, V)</code>，它是输入实数矩阵或一批实数矩阵<code>input</code>这样<img alt="" src="../img/2a4f9c13d7085a30a536b8751c0e3e4c.jpg" />的奇异值分解。</p>
<p>如果<code>some</code>为<code>True</code>(默认值），则该方法返回简化后的奇异值分解，即，如果<code>input</code>的最后两个维为<code>m</code>和<code>n</code>，则返回 &lt;cite&gt;U&lt;/cite&gt; 和 &lt;cite&gt;V&lt;/cite&gt; 矩阵将仅包含<img alt="" src="../img/cbc4cab078465d304683e6b32e90ce2e.jpg" />正交列。</p>
<p>如果<code>compute_uv</code>为<code>False</code>，则返回的 &lt;cite&gt;U&lt;/cite&gt; 和 &lt;cite&gt;V&lt;/cite&gt; 矩阵将分别为形状为<img alt="" src="../img/b9c3d9c79c4cd656bf4c904908b8c189.jpg" />和<img alt="" src="../img/c64d7c7677227e8b59da7d7cfb2466d8.jpg" />的零矩阵。 <code>some</code>在这里将被忽略。</p>
<p>Note</p>
<p>奇异值以降序返回。 如果<code>input</code>是一批矩阵，则该批中每个矩阵的奇异值将按降序返回。</p>
<p>Note</p>
<p>SVD 在 CPU 上的实现使用 LAPACK 例程&lt;cite&gt;？gesdd&lt;/cite&gt; (分治算法）代替&lt;cite&gt;？gesvd&lt;/cite&gt; 来提高速度。 类似地，GPU 上的 SVD 也使用 MAGMA 例程 &lt;cite&gt;gesdd&lt;/cite&gt; 。</p>
<p>Note</p>
<p>无论原始步幅如何，返回的矩阵 &lt;cite&gt;U&lt;/cite&gt; 都将转置，即步幅为<code>U.contiguous().transpose(-2, -1).stride()</code></p>
<p>Note</p>
<p>向后通过 &lt;cite&gt;U&lt;/cite&gt; 和 &lt;cite&gt;V&lt;/cite&gt; 输出时，需要格外小心。 仅当<code>input</code>具有所有不同的奇异值的完整等级时，此类操作才真正稳定。 否则，由于未正确定义渐变，可能会出现<code>NaN</code>。 另外，请注意，即使原始后退仅出现在 &lt;cite&gt;S&lt;/cite&gt; 上，两次后退通常也会通过 &lt;cite&gt;U&lt;/cite&gt; 和 &lt;cite&gt;V&lt;/cite&gt; 进行额外的后退。</p>
<p>Note</p>
<p>当<code>some</code> = <code>False</code>时，<code>U[..., :, min(m, n):]</code>和<code>V[..., :, min(m, n):]</code>上的梯度将向后忽略，因为这些向量可以是子空间的任意基。</p>
<p>Note</p>
<p>当<code>compute_uv</code> = <code>False</code>时，由于向后操作需要来自正向的 &lt;cite&gt;U&lt;/cite&gt; 和 &lt;cite&gt;V&lt;/cite&gt; ，因此无法执行反向。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–大小为<img alt="" src="../img/6217ff594f4af540b27d0cc551ab742c.jpg" />的输入张量，其中 &lt;cite&gt;*&lt;/cite&gt; 是零个或多个由<img alt="" src="../img/5324b7a0d3b2c1767998bc499b6dfb11.jpg" />组成的批量 矩阵。</p>
</li>
<li>
<p><strong>一些</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–控制返回的 &lt;cite&gt;U&lt;/cite&gt; 和 &lt;cite&gt;V&lt;/cite&gt;</p>
</li>
<li>
<p><strong>compute_uv</strong>  (<em>bool</em> <em>，</em> <em>可选</em>）–选择是否计算 &lt;cite&gt;U&lt;/cite&gt; 和 &lt;cite&gt;V&lt;/cite&gt; 或不</p>
</li>
<li>
<p><strong>out</strong> (<em>元组</em> <em>，</em> <em>可选</em>）–张量的输出元组</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(5, 3)
&gt;&gt;&gt; a
tensor([[ 0.2364, -0.7752,  0.6372],
        [ 1.7201,  0.7394, -0.0504],
        [-0.3371, -1.0584,  0.5296],
        [ 0.3550, -0.4022,  1.5569],
        [ 0.2445, -0.0158,  1.1414]])
&gt;&gt;&gt; u, s, v = torch.svd(a)
&gt;&gt;&gt; u
tensor([[ 0.4027,  0.0287,  0.5434],
        [-0.1946,  0.8833,  0.3679],
        [ 0.4296, -0.2890,  0.5261],
        [ 0.6604,  0.2717, -0.2618],
        [ 0.4234,  0.2481, -0.4733]])
&gt;&gt;&gt; s
tensor([2.3289, 2.0315, 0.7806])
&gt;&gt;&gt; v
tensor([[-0.0199,  0.8766,  0.4809],
        [-0.5080,  0.4054, -0.7600],
        [ 0.8611,  0.2594, -0.4373]])
&gt;&gt;&gt; torch.dist(a, torch.mm(torch.mm(u, torch.diag(s)), v.t()))
tensor(8.6531e-07)
&gt;&gt;&gt; a_big = torch.randn(7, 5, 3)
&gt;&gt;&gt; u, s, v = torch.svd(a_big)
&gt;&gt;&gt; torch.dist(a_big, torch.matmul(torch.matmul(u, torch.diag_embed(s)), v.transpose(-2, -1)))
tensor(2.6503e-06)

</code></pre>
<hr />
<pre><code>torch.symeig(input, eigenvectors=False, upper=True, out=None) -&gt; (Tensor, Tensor)¶
</code></pre>
<p>此函数返回实数对称矩阵<code>input</code>或一批实数对称矩阵的特征值和特征向量，由一个命名元组(特征值，特征向量）表示。</p>
<p>此函数计算<code>input</code>的所有特征值(和向量），使得<img alt="" src="../img/cb16e8778bbe7f58582093ac9fd7dd36.jpg" />。</p>
<p>布尔参数<code>eigenvectors</code>定义特征向量和特征值或仅特征值的计算。</p>
<p>如果为<code>False</code>，则仅计算特征值。 如果为<code>True</code>，则同时计算特征值和特征向量。</p>
<p>由于假定输入矩阵<code>input</code>是对称的，因此默认情况下仅使用上三角部分。</p>
<p>如果<code>upper</code>为<code>False</code>，则使用下部三角形部分。</p>
<p>Note</p>
<p>特征值以升序返回。 如果<code>input</code>是一批矩阵，则该批中每个矩阵的特征值将以升序返回。</p>
<p>Note</p>
<p>无论原始步幅如何，返回的矩阵 &lt;cite&gt;V&lt;/cite&gt; 都将转置，即使用步幅 &lt;cite&gt;V.contiguous(）。transpose(-1，-2）.stride(）&lt;/cite&gt;。</p>
<p>Note</p>
<p>向后通过输出时，需要格外小心。 只有当所有特征值都不同时，这种操作才真正稳定。 否则，可能会出现<code>NaN</code>，因为未正确定义渐变。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–大小为<img alt="" src="../img/aa6a866e7977a9ee67a53687003d3821.jpg" />的输入张量，其中 &lt;cite&gt;*&lt;/cite&gt; 为零或更多由对称矩阵组成的批处理尺寸。</p>
</li>
<li>
<p><strong>特征向量</strong>(<em>布尔</em> <em>，</em> <em>可选</em>）–控制是否必须计算特征向量</p>
</li>
<li>
<p><strong>上部</strong>(<em>布尔</em> <em>，</em> <em>可选</em>）–控制是考虑上三角区域还是下三角区域</p>
</li>
<li>
<p><strong>out</strong> (<em>tuple__,</em> <em>optional</em>) – the output tuple of (Tensor, Tensor)</p>
</li>
</ul>
<p>Returns</p>
<p>A namedtuple (eigenvalues, eigenvectors) containing</p>
<blockquote>
<ul>
<li>
<p><strong>特征值</strong>(<em>tensor</em>）：形状<img alt="" src="../img/74dd7138867888df79f198ead03a8f07.jpg" />。 特征值按升序排列。</p>
</li>
<li>
<p><strong>特征向量</strong>(<em>tensor</em>）：形状<img alt="" src="../img/10ea424983a008dbf72dc86628a2813d.jpg" />。 如果<code>eigenvectors=False</code>，则为张量为空。 否则，该张量包含<code>input</code>的正交特征向量。</p>
</li>
</ul>
</blockquote>
<p>Return type</p>
<p>(<a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>, <a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>)</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(5, 5)
&gt;&gt;&gt; a = a + a.t()  # To make a symmetric
&gt;&gt;&gt; a
tensor([[-5.7827,  4.4559, -0.2344, -1.7123, -1.8330],
        [ 4.4559,  1.4250, -2.8636, -3.2100, -0.1798],
        [-0.2344, -2.8636,  1.7112, -5.5785,  7.1988],
        [-1.7123, -3.2100, -5.5785, -2.6227,  3.1036],
        [-1.8330, -0.1798,  7.1988,  3.1036, -5.1453]])
&gt;&gt;&gt; e, v = torch.symeig(a, eigenvectors=True)
&gt;&gt;&gt; e
tensor([-13.7012,  -7.7497,  -2.3163,   5.2477,   8.1050])
&gt;&gt;&gt; v
tensor([[ 0.1643,  0.9034, -0.0291,  0.3508,  0.1817],
        [-0.2417, -0.3071, -0.5081,  0.6534,  0.4026],
        [-0.5176,  0.1223, -0.0220,  0.3295, -0.7798],
        [-0.4850,  0.2695, -0.5773, -0.5840,  0.1337],
        [ 0.6415, -0.0447, -0.6381, -0.0193, -0.4230]])
&gt;&gt;&gt; a_big = torch.randn(5, 2, 2)
&gt;&gt;&gt; a_big = a_big + a_big.transpose(-2, -1)  # To make a_big symmetric
&gt;&gt;&gt; e, v = a_big.symeig(eigenvectors=True)
&gt;&gt;&gt; torch.allclose(torch.matmul(v, torch.matmul(e.diag_embed(), v.transpose(-2, -1))), a_big)
True

</code></pre>
<hr />
<pre><code>torch.trapz()¶
</code></pre>
<hr />
<pre><code>torch.trapz(y, x, *, dim=-1) → Tensor
</code></pre>
<p>使用梯形法则估计&lt;cite&gt;暗&lt;/cite&gt;的<img alt="" src="../img/610dd7b4997e45b33ba79e39c75e1b9d.jpg" />。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>y</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–积分函数的值</p>
</li>
<li>
<p><strong>x</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–函数 &lt;cite&gt;y&lt;/cite&gt; 的采样点。 如果 &lt;cite&gt;x&lt;/cite&gt; 不按升序排列，则其减小的时间间隔将对估计的积分产生负面影响(即遵循惯例<img alt="" src="../img/7368567bca3b8caffef0c7639ae1ebd5.jpg" />）。</p>
</li>
<li>
<p><strong>暗淡的</strong> (<em>python：int</em> )–集成所沿的维度。 默认情况下，使用最后一个尺寸。</p>
</li>
</ul>
<p>Returns</p>
<p>一个与输入形状相同的张量，除了删除了&lt;cite&gt;暗淡的&lt;/cite&gt;。 返回的张量的每个元素代表沿着&lt;cite&gt;暗淡&lt;/cite&gt;的估计积分<img alt="" src="../img/610dd7b4997e45b33ba79e39c75e1b9d.jpg" />。</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; y = torch.randn((2, 3))
&gt;&gt;&gt; y
tensor([[-2.1156,  0.6857, -0.2700],
        [-1.2145,  0.5540,  2.0431]])
&gt;&gt;&gt; x = torch.tensor([[1, 3, 4], [1, 2, 3]])
&gt;&gt;&gt; torch.trapz(y, x)
tensor([-1.2220,  0.9683])

</code></pre>
<hr />
<pre><code>torch.trapz(y, *, dx=1, dim=-1) → Tensor
</code></pre>
<p>如上所述，但是采样点以 &lt;cite&gt;dx&lt;/cite&gt; 的距离均匀间隔。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>y</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – The values of the function to integrate</p>
</li>
<li>
<p><strong>dx</strong>  (<em>python：float</em> )–采样 &lt;cite&gt;y&lt;/cite&gt; 的点之间的距离。</p>
</li>
<li>
<p><strong>dim</strong> (<em>python:int</em>) – The dimension along which to integrate. By default, use the last dimension.</p>
</li>
</ul>
<p>Returns</p>
<p>A Tensor with the same shape as the input, except with &lt;cite&gt;dim&lt;/cite&gt; removed. Each element of the returned tensor represents the estimated integral <img alt="" src="../img/610dd7b4997e45b33ba79e39c75e1b9d.jpg" /> along &lt;cite&gt;dim&lt;/cite&gt;.</p>
<hr />
<pre><code>torch.triangular_solve(input, A, upper=True, transpose=False, unitriangular=False) -&gt; (Tensor, Tensor)¶
</code></pre>
<p>用三角系数矩阵<img alt="" src="../img/8cab287af6acaf0838d1d67381a3716d.jpg" />和多个右侧<img alt="" src="../img/23a2cc697e4e1a8adeb60eb42ab51a6e.jpg" />求解方程组。</p>
<p>特别是，求解<img alt="" src="../img/998c4e0ada5e41efb8c632f644ba86f1.jpg" />并假定<img alt="" src="../img/8cab287af6acaf0838d1d67381a3716d.jpg" />为带有默认关键字参数的上三角。</p>
<p>&lt;cite&gt;torch.triangular_solve(b，A）&lt;/cite&gt;可以接受 2D 输入 &lt;cite&gt;b，A&lt;/cite&gt; 或一批 2D 矩阵的输入。 如果输入为批次，则返回成批输出 &lt;cite&gt;X&lt;/cite&gt;</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>输入</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–尺寸为<img alt="" src="../img/8ebb9d35b764041761ddb4e3436e265d.jpg" />的多个右侧，其中<img alt="" src="../img/dcef98688866c0d5a21137cf53bf228d.jpg" />是零个以上的批次尺寸(<img alt="" src="../img/23a2cc697e4e1a8adeb60eb42ab51a6e.jpg" />）</p>
</li>
<li>
<p><strong>A</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)–大小为<img alt="" src="../img/10ea424983a008dbf72dc86628a2813d.jpg" />的输入三角系数矩阵，其中<img alt="" src="../img/dcef98688866c0d5a21137cf53bf228d.jpg" />为零或更大的批处理尺寸</p>
</li>
<li>
<p><strong>上</strong> (<em>bool</em> <em>，</em> <em>可选</em>）–是求解方程的上三角系统(默认）还是下三角系统 方程组。 默认值：<code>True</code>。</p>
</li>
<li>
<p><strong>换位</strong>(<em>布尔</em> <em>，</em> <em>可选</em>）–在将<img alt="" src="../img/8cab287af6acaf0838d1d67381a3716d.jpg" />发送到求解器之前是否应该对其进行换位。 默认值：<code>False</code>。</p>
</li>
<li>
<p><strong>单边形</strong>(<em>布尔</em> <em>，</em> <em>可选</em>）– <img alt="" src="../img/8cab287af6acaf0838d1d67381a3716d.jpg" />是否为单位三角形。 如果为 True，则假定<img alt="" src="../img/8cab287af6acaf0838d1d67381a3716d.jpg" />的对角元素为 1，并且未从<img alt="" src="../img/8cab287af6acaf0838d1d67381a3716d.jpg" />引用。 默认值：<code>False</code>。</p>
</li>
</ul>
<p>Returns</p>
<p>一个命名元组&lt;cite&gt;(解决方案，cloned_coefficient）&lt;/cite&gt;其中 &lt;cite&gt;cloned_coefficient&lt;/cite&gt; 是<img alt="" src="../img/8cab287af6acaf0838d1d67381a3716d.jpg" />的克隆，而&lt;cite&gt;解决方案&lt;/cite&gt;是<img alt="" src="../img/1dc567019b272fda0c5051c472dac2b7.jpg" />到<img alt="" src="../img/998c4e0ada5e41efb8c632f644ba86f1.jpg" />的解决方案(或其他变体） 的方程组，具体取决于关键字参数。）</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; A = torch.randn(2, 2).triu()
&gt;&gt;&gt; A
tensor([[ 1.1527, -1.0753],
        [ 0.0000,  0.7986]])
&gt;&gt;&gt; b = torch.randn(2, 3)
&gt;&gt;&gt; b
tensor([[-0.0210,  2.3513, -1.5492],
        [ 1.5429,  0.7403, -1.0243]])
&gt;&gt;&gt; torch.triangular_solve(b, A)
torch.return_types.triangular_solve(
solution=tensor([[ 1.7841,  2.9046, -2.5405],
        [ 1.9320,  0.9270, -1.2826]]),
cloned_coefficient=tensor([[ 1.1527, -1.0753],
        [ 0.0000,  0.7986]]))

</code></pre>
<h2 id="_17">实用工具</h2>
<hr />
<pre><code>torch.compiled_with_cxx11_abi()¶
</code></pre>
<p>返回 PyTorch 是否使用 _GLIBCXX_USE_CXX11_ABI = 1 构建</p>
<hr />
<pre><code>torch.result_type(tensor1, tensor2) → dtype¶
</code></pre>
<p>返回 <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> ，这是对提供的输入张量执行算术运算得出的。 有关类型升级逻辑的更多信息，请参见类型升级<a href="tensor_attributes.html#type-promotion-doc">文档</a>。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>张量 1</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>或</em> <em>数字</em>）–输入张量或数字</p>
</li>
<li>
<p><strong>张量 2</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>或</em> <em>数字</em>）–输入张量或数字</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.result_type(torch.tensor([1, 2], dtype=torch.int), 1.0)
torch.float32
&gt;&gt;&gt; torch.result_type(torch.tensor([1, 2], dtype=torch.uint8), torch.tensor(1))
torch.uint8

</code></pre>
<hr />
<pre><code>torch.can_cast(from, to) → bool¶
</code></pre>
<p>确定在类型提升<a href="tensor_attributes.html#type-promotion-doc">文档</a>中描述的 PyTorch 转换规则下是否允许类型转换。</p>
<p>Parameters</p>
<ul>
<li>
<p>中的<strong> (<em>dpython：type</em> )–原始的 <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> 。</strong></p>
</li>
<li>
<p><strong>到</strong> (<em>dpython：type</em> )–目标 <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> 。</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.can_cast(torch.double, torch.float)
True
&gt;&gt;&gt; torch.can_cast(torch.float, torch.int)
False

</code></pre>
<hr />
<pre><code>torch.promote_types(type1, type2) → dtype¶
</code></pre>
<p>返回尺寸和标量种类最小的 <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> ，其大小不小于 &lt;cite&gt;type1&lt;/cite&gt; 或 &lt;cite&gt;type2&lt;/cite&gt; 。 有关类型升级逻辑的更多信息，请参见类型升级<a href="tensor_attributes.html#type-promotion-doc">文档</a>。</p>
<p>Parameters</p>
<ul>
<li>
<p><strong>type1</strong>  (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>)–</p>
</li>
<li>
<p><strong>type2</strong>  (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>)–</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.promote_types(torch.int32, torch.float32))
torch.float32
&gt;&gt;&gt; torch.promote_types(torch.uint8, torch.long)
torch.long

</code></pre>
<hr/>
<div align="center">
  <p><a href="https://www.apachecn.org/" target="_blank"><font face="KaiTi" size="6" color="red">我们一直在努力</font></a><p>
  <p><a href="https://github.com/apachecn/pytorch-doc-zh" target="_blank">apachecn/pytorch-doc-zh</a></p>
  <p><a target="_blank" href="https://qm.qq.com/cgi-bin/qm/qr?k=5u_aAU-YlY3fH-m8meXTJzBEo2boQIUs&jump_from=webapi&authKey=CVZcReMt/vKdTXZBQ8ly+jWncXiSzzWOlrx5hybX5pSrKu6s0fvGX54+vHHlgYNt"><img border="0" src="https://pub.idqqimg.com/wpa/images/group.png" alt="【布客】中文翻译组" title="【布客】中文翻译组"></a></p>
  <p><span id="cnzz_stat_icon_1275211409"></span></p>
  <!-- <p><a href="https://get.brightdata.com/apachecn" target="_blank"><img src="/assets/images/partnerstack.gif" /></a><p> -->
  <div class="wwads-cn wwads-horizontal" data-id="206" style="max-width:680px"></div>
  <div style="text-align:center;margin:0 0 10.5px;">
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-3565452474788507" crossorigin="anonymous"></script>
    <!-- ApacheCNWide -->
    <ins class="adsbygoogle"
        style="display:inline-block;width:680px;height:90px"
        data-ad-client="ca-pub-3565452474788507"
        data-ad-slot="2543897000"></ins>
    <script>
        (adsbygoogle = window.adsbygoogle || []).push({});
    </script>
  </div>
</div>
<hr/>
<!-- 来必力City版安装代码 -->
<div id="lv-container" data-id="city" data-uid="MTAyMC81ODA2NC8zNDUyNw==">
  <script type="text/javascript">
  (function(d, s) {
      var j, e = d.getElementsByTagName(s)[0];

      if (typeof LivereTower === 'function') { return; }

      j = d.createElement(s);
      j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
      j.async = true;

      e.parentNode.insertBefore(j, e);
  })(document, 'script');
  </script>
<noscript> 为正常使用来必力评论功能请激活JavaScript</noscript>
</div>
<!-- City版安装代码已完成 -->






                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../72/" class="md-footer__link md-footer__link--prev" aria-label="Previous: PyTorch Java API" rel="prev">
            <div class="md-footer__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                PyTorch Java API
              </div>
            </div>
          </a>
        
        
          
          <a href="../75/" class="md-footer__link md-footer__link--next" aria-label="Next: torch.nn" rel="next">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                torch.nn
              </div>
            </div>
            <div class="md-footer__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright" style="text-align: center; width: 100%;">
  
  
    <div>
      <div style="margin:0 0 10.5px;"><script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1275211409'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s5.cnzz.com/z_stat.php%3Fid%3D1275211409%26online%3D1%26show%3Dline' type='text/javascript'%3E%3C/script%3E"));</script></div>
      <p>Copyright © 2023 学习网站 <a href="http://beian.miit.gov.cn" target="_blank">京ICP备19016010号-1</a><br/>网站由 <a href="https://apachecn.org/cooperate/">@片刻小哥哥</a> 提供支持 | 联系QQ/微信: 529815144 请注明来意！</p>
    </div>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["content.code.copy", "content.action.edit", "content.action.view", "navigation.footer"], "search": "../../assets/javascripts/workers/search.208ed371.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
  
      <script src="../../assets/javascripts/bundle.b425cdc4.min.js"></script>
      
        
          <script src="../../javascripts/mathjax.js"></script>
        
      
        
          <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        
      
        
          <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        
      
    
  <script src="../../assets/javascripts/custom.a7283b5f.min.js"></script>

  </body>
</html>