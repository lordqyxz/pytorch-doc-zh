
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://pytorch.apachecn.org/0.3/nn/">
      
      
        <link rel="prev" href="../storage/">
      
      
        <link rel="next" href="../optim/">
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.4.3, mkdocs-material-9.1.17">
    
    
      
        <title>torch.nn - 【布客】PyTorch 中文翻译</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.26e3688c.min.css">
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  <link rel="stylesheet" href="../../assets/stylesheets/custom.bea7efe8.min.css">
  <!-- google ads -->
  <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-3565452474788507" crossorigin="anonymous"></script>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-8DP4GX97XY"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-8DP4GX97XY');
  </script>
  <!-- google webmaster -->
  <meta name="google-site-verification" content="pyo9N70ZWyh8JB43bIu633mhxesJ1IcwWCZlM3jUfFo" />

  <!-- wwads-cn union -->
  <meta name="wwads-cn-verify" content="03c6b06952c750899bb03d998e631860" />
  <script type="text/javascript" charset="UTF-8" src="https://cdn.wwads.cn/js/makemoney.js" async></script>

  </head>
  
  
    <body dir="ltr">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#torchnn" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
        <aside class="md-banner">
          <div class="md-banner__inner md-grid md-typeset">
            
            
  => 组织无偿提供 中文版本（免费，秒级响应）
  <a target="_blank" href="https://chat.ibooker.org.cn/chat" style="color: red;">
    <span class="twemoji mastodon">
      <img src="https://data.apachecn.org/img/icon/ROBOT_TXT.svg" alt="ChatGPT - ailake.top">
    </span>
    <strong>ChatGPT - ailake.top</strong>
  </a> 一起来白嫖叭～！

          </div>
          
        </aside>
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="【布客】PyTorch 中文翻译" class="md-header__button md-logo" aria-label="【布客】PyTorch 中文翻译" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            【布客】PyTorch 中文翻译
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              torch.nn
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/apachecn/pytorch-doc-zh" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    apachecn/pytorch-doc-zh
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="【布客】PyTorch 中文翻译" class="md-nav__button md-logo" aria-label="【布客】PyTorch 中文翻译" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    【布客】PyTorch 中文翻译
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/apachecn/pytorch-doc-zh" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    apachecn/pytorch-doc-zh
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        PyTorch 中文文档 & 教程
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
      
      
      
        <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
          PyTorch 2.0 中文文档 & 教程
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 2.0 中文文档 & 教程
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1" >
      
      
      
        <label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="0">
          中文教程
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_1">
          <span class="md-nav__icon md-icon"></span>
          中文教程
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/tutorials/README.md" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_2" >
      
      
      
        <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="0">
          中文文档
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2_2">
          <span class="md-nav__icon md-icon"></span>
          中文文档
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../2.0/docs/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
      
      
      
        <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
          PyTorch 1.7 中文文档
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 1.7 中文文档
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" >
      
      
      
        <label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="0">
          学习 PyTorch
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_1">
          <span class="md-nav__icon md-icon"></span>
          学习 PyTorch
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_1" >
      
      
      
        <label class="md-nav__link" for="__nav_3_1_1" id="__nav_3_1_1_label" tabindex="0">
          PyTorch 深度学习：60 分钟的突击
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_1_1">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 深度学习：60 分钟的突击
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/02/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/03/" class="md-nav__link">
        张量
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/04/" class="md-nav__link">
        torch.autograd的简要介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/05/" class="md-nav__link">
        神经网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/06/" class="md-nav__link">
        训练分类器
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_2" >
      
      
      
        <label class="md-nav__link" for="__nav_3_1_2" id="__nav_3_1_2_label" tabindex="0">
          通过示例学习 PyTorch
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_1_2">
          <span class="md-nav__icon md-icon"></span>
          通过示例学习 PyTorch
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/07/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/08/" class="md-nav__link">
        热身：NumPy
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/09/" class="md-nav__link">
        PyTorch：张量
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/10/" class="md-nav__link">
        PyTorch：张量和 Autograd
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/11/" class="md-nav__link">
        PyTorch：定义新的 Autograd 函数
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/12/" class="md-nav__link">
        PyTorch：nn
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/13/" class="md-nav__link">
        PyTorch：optim
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/14/" class="md-nav__link">
        PyTorch：自定义nn模块
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/15/" class="md-nav__link">
        PyTorch：控制流 - 权重共享
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/16/" class="md-nav__link">
        torch.nn到底是什么？
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/17/" class="md-nav__link">
        使用 TensorBoard 可视化模型，数据和训练
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
      
      
      
        <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
          图片/视频
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_2">
          <span class="md-nav__icon md-icon"></span>
          图片/视频
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/19/" class="md-nav__link">
        torchvision对象检测微调教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/20/" class="md-nav__link">
        计算机视觉的迁移学习教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/21/" class="md-nav__link">
        对抗示例生成
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/22/" class="md-nav__link">
        DCGAN 教程
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_3" >
      
      
      
        <label class="md-nav__link" for="__nav_3_3" id="__nav_3_3_label" tabindex="0">
          音频
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_3">
          <span class="md-nav__icon md-icon"></span>
          音频
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/24/" class="md-nav__link">
        音频 I/O 和torchaudio的预处理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/25/" class="md-nav__link">
        使用torchaudio的语音命令识别
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4" >
      
      
      
        <label class="md-nav__link" for="__nav_3_4" id="__nav_3_4_label" tabindex="0">
          文本
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_4">
          <span class="md-nav__icon md-icon"></span>
          文本
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/27/" class="md-nav__link">
        使用nn.Transformer和torchtext的序列到序列建模
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/28/" class="md-nav__link">
        从零开始的 NLP：使用字符级 RNN 分类名称
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/29/" class="md-nav__link">
        从零开始的 NLP：使用字符级 RNN 生成名称
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/30/" class="md-nav__link">
        从零开始的 NLP：使用序列到序列网络和注意力的翻译
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/31/" class="md-nav__link">
        使用torchtext的文本分类
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/32/" class="md-nav__link">
        torchtext语言翻译
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_5" >
      
      
      
        <label class="md-nav__link" for="__nav_3_5" id="__nav_3_5_label" tabindex="0">
          强化学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_5">
          <span class="md-nav__icon md-icon"></span>
          强化学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/34/" class="md-nav__link">
        强化学习（DQN）教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/35/" class="md-nav__link">
        训练玩马里奥的 RL 智能体
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_6" >
      
      
      
        <label class="md-nav__link" for="__nav_3_6" id="__nav_3_6_label" tabindex="0">
          在生产中部署 PyTorch 模型
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_6">
          <span class="md-nav__icon md-icon"></span>
          在生产中部署 PyTorch 模型
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/37/" class="md-nav__link">
        通过使用 Flask 的 REST API 在 Python 中部署 PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/38/" class="md-nav__link">
        TorchScript 简介
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/39/" class="md-nav__link">
        在 C-- 中加载 TorchScript 模型
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/40/" class="md-nav__link">
        将模型从 PyTorch 导出到 ONNX 并使用 ONNX 运行时运行它（可选）
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_7" >
      
      
      
        <label class="md-nav__link" for="__nav_3_7" id="__nav_3_7_label" tabindex="0">
          前端 API
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_7_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_7">
          <span class="md-nav__icon md-icon"></span>
          前端 API
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/42/" class="md-nav__link">
        PyTorch 中的命名张量简介（原型）
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/43/" class="md-nav__link">
        PyTorch 中通道在最后的内存格式（beta）
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/44/" class="md-nav__link">
        使用 PyTorch C-- 前端
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/45/" class="md-nav__link">
        自定义 C-- 和 CUDA 扩展
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/46/" class="md-nav__link">
        使用自定义 C-- 运算符扩展 TorchScript
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/47/" class="md-nav__link">
        使用自定义 C-- 类扩展 TorchScript
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/48/" class="md-nav__link">
        TorchScript 中的动态并行性
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/49/" class="md-nav__link">
        C-- 前端中的 Autograd
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/50/" class="md-nav__link">
        在 C-- 中注册调度运算符
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_8" >
      
      
      
        <label class="md-nav__link" for="__nav_3_8" id="__nav_3_8_label" tabindex="0">
          模型优化
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_8_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_8">
          <span class="md-nav__icon md-icon"></span>
          模型优化
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/52/" class="md-nav__link">
        分析您的 PyTorch 模块
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/53/" class="md-nav__link">
        使用 Ray Tune 的超参数调整
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/54/" class="md-nav__link">
        模型剪裁教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/55/" class="md-nav__link">
        LSTM 单词语言模型上的动态量化（beta）
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/56/" class="md-nav__link">
        BERT 上的动态量化（Beta）
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/57/" class="md-nav__link">
        PyTorch 中使用 Eager 模式的静态量化（beta）
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/58/" class="md-nav__link">
        计算机视觉的量化迁移学习教程（beta）
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_9" >
      
      
      
        <label class="md-nav__link" for="__nav_3_9" id="__nav_3_9_label" tabindex="0">
          并行和分布式训练
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_9_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_9">
          <span class="md-nav__icon md-icon"></span>
          并行和分布式训练
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/60/" class="md-nav__link">
        PyTorch 分布式概述
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/61/" class="md-nav__link">
        单机模型并行最佳实践
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/62/" class="md-nav__link">
        分布式数据并行入门
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/63/" class="md-nav__link">
        用 PyTorch 编写分布式应用
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/64/" class="md-nav__link">
        分布式 RPC 框架入门
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/65/" class="md-nav__link">
        使用分布式 RPC 框架实现参数服务器
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/66/" class="md-nav__link">
        使用 RPC 的分布式管道并行化
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/67/" class="md-nav__link">
        使用异步执行实现批量 RPC 处理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.7/68/" class="md-nav__link">
        将分布式DataParallel与分布式 RPC 框架相结合
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
      
      
      
        <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
          PyTorch 1.4 中文文档 & 教程
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 1.4 中文文档 & 教程
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1" >
      
      
      
        <label class="md-nav__link" for="__nav_4_1" id="__nav_4_1_label" tabindex="0">
          入门
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_1">
          <span class="md-nav__icon md-icon"></span>
          入门
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1_1" >
      
      
      
        <label class="md-nav__link" for="__nav_4_1_1" id="__nav_4_1_1_label" tabindex="0">
          使用 PyTorch 进行深度学习：60 分钟的闪电战
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_4_1_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_1_1">
          <span class="md-nav__icon md-icon"></span>
          使用 PyTorch 进行深度学习：60 分钟的闪电战
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/4/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/blitz/tensor_tutorial/" class="md-nav__link">
        什么是PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/blitz/autograd_tutorial/" class="md-nav__link">
        Autograd：自动求导
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/blitz/neural_networks_tutorial/" class="md-nav__link">
        神经网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/blitz/cifar10_tutorial/" class="md-nav__link">
        训练分类器
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/blitz/data_parallel_tutorial/" class="md-nav__link">
        可选：数据并行
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/5/" class="md-nav__link">
        编写自定义数据集，数据加载器和转换
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/6/" class="md-nav__link">
        使用 TensorBoard 可视化模型，数据和训练
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_2" >
      
      
      
        <label class="md-nav__link" for="__nav_4_2" id="__nav_4_2_label" tabindex="0">
          图片
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_2">
          <span class="md-nav__icon md-icon"></span>
          图片
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/8/" class="md-nav__link">
        TorchVision 对象检测微调教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/9/" class="md-nav__link">
        转移学习的计算机视觉教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/10/" class="md-nav__link">
        空间变压器网络教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/11/" class="md-nav__link">
        使用 PyTorch 进行神经传递
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/12/" class="md-nav__link">
        对抗示例生成
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/13/" class="md-nav__link">
        DCGAN 教程
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_3" >
      
      
      
        <label class="md-nav__link" for="__nav_4_3" id="__nav_4_3_label" tabindex="0">
          音频
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_3">
          <span class="md-nav__icon md-icon"></span>
          音频
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/15/" class="md-nav__link">
        torchaudio 教程
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_4" >
      
      
      
        <label class="md-nav__link" for="__nav_4_4" id="__nav_4_4_label" tabindex="0">
          文本
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_4">
          <span class="md-nav__icon md-icon"></span>
          文本
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/17/" class="md-nav__link">
        NLP From Scratch: 使用char-RNN对姓氏进行分类
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/18/" class="md-nav__link">
        NLP From Scratch: 生成名称与字符级RNN
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/19/" class="md-nav__link">
        NLP From Scratch: 基于注意力机制的 seq2seq 神经网络翻译
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/20/" class="md-nav__link">
        使用 TorchText 进行文本分类
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/21/" class="md-nav__link">
        使用 TorchText 进行语言翻译
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/22/" class="md-nav__link">
        使用 nn.Transformer 和 TorchText 进行序列到序列建模
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_5" >
      
      
      
        <label class="md-nav__link" for="__nav_4_5" id="__nav_4_5_label" tabindex="0">
          命名为 Tensor(实验性）
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_5">
          <span class="md-nav__icon md-icon"></span>
          命名为 Tensor(实验性）
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/24/" class="md-nav__link">
        (实验性)PyTorch 中的命名张量简介
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_6" >
      
      
      
        <label class="md-nav__link" for="__nav_4_6" id="__nav_4_6_label" tabindex="0">
          强化学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_6">
          <span class="md-nav__icon md-icon"></span>
          强化学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/26/" class="md-nav__link">
        强化学习(DQN)教程
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_7" >
      
      
      
        <label class="md-nav__link" for="__nav_4_7" id="__nav_4_7_label" tabindex="0">
          在生产中部署 PyTorch 模型
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_7_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_7">
          <span class="md-nav__icon md-icon"></span>
          在生产中部署 PyTorch 模型
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/28/" class="md-nav__link">
        通过带有 Flask 的 REST API 在 Python 中部署 PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/29/" class="md-nav__link">
        TorchScript 简介
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/30/" class="md-nav__link">
        在 C --中加载 TorchScript 模型
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/31/" class="md-nav__link">
        (可选）将模型从 PyTorch 导出到 ONNX 并使用 ONNX Runtime 运行
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_8" >
      
      
      
        <label class="md-nav__link" for="__nav_4_8" id="__nav_4_8_label" tabindex="0">
          并行和分布式训练
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_8_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_8">
          <span class="md-nav__icon md-icon"></span>
          并行和分布式训练
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/33/" class="md-nav__link">
        单机模型并行最佳实践
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/34/" class="md-nav__link">
        分布式数据并行入门
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/35/" class="md-nav__link">
        用 PyTorch 编写分布式应用程序
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/36/" class="md-nav__link">
        分布式 RPC 框架入门
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/37/" class="md-nav__link">
        (高级）带有 Amazon AWS 的 PyTorch 1.0 分布式训练师
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_9" >
      
      
      
        <label class="md-nav__link" for="__nav_4_9" id="__nav_4_9_label" tabindex="0">
          扩展 PyTorch
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_9_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_9">
          <span class="md-nav__icon md-icon"></span>
          扩展 PyTorch
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/39/" class="md-nav__link">
        使用自定义 C --运算符扩展 TorchScript
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/40/" class="md-nav__link">
        使用自定义 C --类扩展 TorchScript
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/41/" class="md-nav__link">
        使用 numpy 和 scipy 创建扩展
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/42/" class="md-nav__link">
        自定义 C --和 CUDA 扩展
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_10" >
      
      
      
        <label class="md-nav__link" for="__nav_4_10" id="__nav_4_10_label" tabindex="0">
          模型优化
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_10_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_10">
          <span class="md-nav__icon md-icon"></span>
          模型优化
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/44/" class="md-nav__link">
        LSTM Word 语言模型上的(实验）动态量化
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/45/" class="md-nav__link">
        (实验性）在 PyTorch 中使用 Eager 模式进行静态量化
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/46/" class="md-nav__link">
        (实验性）计算机视觉教程的量化转移学习
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/47/" class="md-nav__link">
        (实验）BERT 上的动态量化
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/48/" class="md-nav__link">
        修剪教程
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_11" >
      
      
      
        <label class="md-nav__link" for="__nav_4_11" id="__nav_4_11_label" tabindex="0">
          PyTorch 用其他语言
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_11_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_11">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 用其他语言
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/50/" class="md-nav__link">
        使用 PyTorch C --前端
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_12" >
      
      
      
        <label class="md-nav__link" for="__nav_4_12" id="__nav_4_12_label" tabindex="0">
          PyTorch 基础知识
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_12_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_12">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 基础知识
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/52/" class="md-nav__link">
        通过示例学习 PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/53/" class="md-nav__link">
        torch.nn 到底是什么？
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_13" >
      
      
      
        <label class="md-nav__link" for="__nav_4_13" id="__nav_4_13_label" tabindex="0">
          笔记
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_13_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_13">
          <span class="md-nav__icon md-icon"></span>
          笔记
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/56/" class="md-nav__link">
        自动求导机制
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/57/" class="md-nav__link">
        广播语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/58/" class="md-nav__link">
        CPU 线程和 TorchScript 推断
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/59/" class="md-nav__link">
        CUDA 语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/60/" class="md-nav__link">
        分布式 Autograd 设计
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/61/" class="md-nav__link">
        扩展 PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/62/" class="md-nav__link">
        经常问的问题
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/63/" class="md-nav__link">
        大规模部署的功能
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/64/" class="md-nav__link">
        并行处理最佳实践
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/65/" class="md-nav__link">
        重现性
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/66/" class="md-nav__link">
        远程参考协议
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/67/" class="md-nav__link">
        序列化语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/68/" class="md-nav__link">
        Windows 常见问题
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/69/" class="md-nav__link">
        XLA 设备上的 PyTorch
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_14" >
      
      
      
        <label class="md-nav__link" for="__nav_4_14" id="__nav_4_14_label" tabindex="0">
          语言绑定
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_14_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_14">
          <span class="md-nav__icon md-icon"></span>
          语言绑定
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/71/" class="md-nav__link">
        PyTorch C -- API
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/72/" class="md-nav__link">
        PyTorch Java API
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_15" >
      
      
      
        <label class="md-nav__link" for="__nav_4_15" id="__nav_4_15_label" tabindex="0">
          Python API
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_15_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_15">
          <span class="md-nav__icon md-icon"></span>
          Python API
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/74/" class="md-nav__link">
        torch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/75/" class="md-nav__link">
        torch.nn
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/76/" class="md-nav__link">
        torch功能
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/77/" class="md-nav__link">
        torch张量
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/78/" class="md-nav__link">
        张量属性
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/79/" class="md-nav__link">
        自动差分包-Torch.Autograd
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/80/" class="md-nav__link">
        torch.cuda
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/81/" class="md-nav__link">
        分布式通讯包-Torch.Distributed
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/82/" class="md-nav__link">
        概率分布-torch分布
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/83/" class="md-nav__link">
        torch.hub
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/84/" class="md-nav__link">
        torch脚本
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/85/" class="md-nav__link">
        torch.nn.init
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/86/" class="md-nav__link">
        torch.onnx
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/87/" class="md-nav__link">
        torch.optim
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/88/" class="md-nav__link">
        量化
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/89/" class="md-nav__link">
        分布式 RPC 框架
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/90/" class="md-nav__link">
        torch随机
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/91/" class="md-nav__link">
        torch稀疏
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/92/" class="md-nav__link">
        torch存储
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/93/" class="md-nav__link">
        torch.utils.bottleneck
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/94/" class="md-nav__link">
        torch.utils.checkpoint
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/95/" class="md-nav__link">
        torch.utils.cpp_extension
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/96/" class="md-nav__link">
        torch.utils.data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/97/" class="md-nav__link">
        torch.utils.dlpack
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/98/" class="md-nav__link">
        torch.utils.model_zoo
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/99/" class="md-nav__link">
        torch.utils.tensorboard
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/100/" class="md-nav__link">
        类型信息
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/101/" class="md-nav__link">
        命名张量
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/102/" class="md-nav__link">
        命名为 Tensors 操作员范围
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/103/" class="md-nav__link">
        糟糕！
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_16" >
      
      
      
        <label class="md-nav__link" for="__nav_4_16" id="__nav_4_16_label" tabindex="0">
          torchvision参考
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_16_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_16">
          <span class="md-nav__icon md-icon"></span>
          torchvision参考
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/105/" class="md-nav__link">
        torchvision
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_17" >
      
      
      
        <label class="md-nav__link" for="__nav_4_17" id="__nav_4_17_label" tabindex="0">
          音频参考
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_17_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_17">
          <span class="md-nav__icon md-icon"></span>
          音频参考
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/107/" class="md-nav__link">
        torchaudio
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_18" >
      
      
      
        <label class="md-nav__link" for="__nav_4_18" id="__nav_4_18_label" tabindex="0">
          torchtext参考
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_18_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_18">
          <span class="md-nav__icon md-icon"></span>
          torchtext参考
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/109/" class="md-nav__link">
        torchtext
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_19" >
      
      
      
        <label class="md-nav__link" for="__nav_4_19" id="__nav_4_19_label" tabindex="0">
          社区
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_19_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_19">
          <span class="md-nav__icon md-icon"></span>
          社区
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/111/" class="md-nav__link">
        PyTorch 贡献指南
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/112/" class="md-nav__link">
        PyTorch 治理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.4/113/" class="md-nav__link">
        PyTorch 治理| 感兴趣的人
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
      
      
      
        <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
          PyTorch 1.0 中文文档 & 教程
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 1.0 中文文档 & 教程
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/" class="md-nav__link">
        目录
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2" id="__nav_5_2_label" tabindex="0">
          中文教程
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2">
          <span class="md-nav__icon md-icon"></span>
          中文教程
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2_1" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2_1" id="__nav_5_2_1_label" tabindex="0">
          入门
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_2_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2_1">
          <span class="md-nav__icon md-icon"></span>
          入门
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2_1_1" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2_1_1" id="__nav_5_2_1_1_label" tabindex="0">
          PyTorch 深度学习: 60 分钟极速入门
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_5_2_1_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2_1_1">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 深度学习: 60 分钟极速入门
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/deep_learning_60min_blitz/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/blitz_tensor_tutorial/" class="md-nav__link">
        什么是 PyTorch？
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/blitz_autograd_tutorial/" class="md-nav__link">
        Autograd：自动求导
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/blitz_neural_networks_tutorial/" class="md-nav__link">
        神经网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/blitz_cifar10_tutorial/" class="md-nav__link">
        训练分类器
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/blitz_data_parallel_tutorial/" class="md-nav__link">
        可选：数据并行处理
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/data_loading_tutorial/" class="md-nav__link">
        数据加载和处理教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/pytorch_with_examples/" class="md-nav__link">
        用例子学习 PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/transfer_learning_tutorial/" class="md-nav__link">
        迁移学习教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/deploy_seq2seq_hybrid_frontend_tutorial/" class="md-nav__link">
        混合前端的 seq2seq 模型部署
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/saving_loading_models/" class="md-nav__link">
        Saving and Loading Models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/nn_tutorial/" class="md-nav__link">
        What is torch.nn really?
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2_2" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2_2" id="__nav_5_2_2_label" tabindex="0">
          图像
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_2_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2_2">
          <span class="md-nav__icon md-icon"></span>
          图像
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/finetuning_torchvision_models_tutorial/" class="md-nav__link">
        Torchvision 模型微调
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/spatial_transformer_tutorial/" class="md-nav__link">
        空间变换器网络教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/neural_style_tutorial/" class="md-nav__link">
        使用 PyTorch 进行图像风格转换
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/fgsm_tutorial/" class="md-nav__link">
        对抗性示例生成
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/super_resolution_with_caffe2/" class="md-nav__link">
        使用 ONNX 将模型从 PyTorch 传输到 Caffe2 和移动端
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2_3" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2_3" id="__nav_5_2_3_label" tabindex="0">
          文本
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_2_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2_3">
          <span class="md-nav__icon md-icon"></span>
          文本
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/chatbot_tutorial/" class="md-nav__link">
        聊天机器人教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/char_rnn_generation_tutorial/" class="md-nav__link">
        使用字符级别特征的 RNN 网络生成姓氏
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/char_rnn_classification_tutorial/" class="md-nav__link">
        使用字符级别特征的 RNN 网络进行姓氏分类
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2_3_4" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2_3_4" id="__nav_5_2_3_4_label" tabindex="0">
          Deep Learning for NLP with Pytorch
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_5_2_3_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2_3_4">
          <span class="md-nav__icon md-icon"></span>
          Deep Learning for NLP with Pytorch
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/deep_learning_nlp_tutorial/" class="md-nav__link">
        在深度学习和 NLP 中使用 Pytorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/nlp_pytorch_tutorial/" class="md-nav__link">
        PyTorch 介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/nlp_deep_learning_tutorial/" class="md-nav__link">
        使用 PyTorch 进行深度学习
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/nlp_word_embeddings_tutorial/" class="md-nav__link">
        Word Embeddings: Encoding Lexical Semantics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/nlp_sequence_models_tutorial/" class="md-nav__link">
        序列模型和 LSTM 网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/nlp_advanced_tutorial/" class="md-nav__link">
        Advanced: Making Dynamic Decisions and the Bi-LSTM CRF
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/seq2seq_translation_tutorial/" class="md-nav__link">
        基于注意力机制的 seq2seq 神经网络翻译
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2_4" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2_4" id="__nav_5_2_4_label" tabindex="0">
          生成
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_2_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2_4">
          <span class="md-nav__icon md-icon"></span>
          生成
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/dcgan_faces_tutorial/" class="md-nav__link">
        DCGAN Tutorial
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2_5" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2_5" id="__nav_5_2_5_label" tabindex="0">
          强化学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_2_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2_5">
          <span class="md-nav__icon md-icon"></span>
          强化学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/reinforcement_q_learning/" class="md-nav__link">
        Reinforcement Learning (DQN) Tutorial
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2_6" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2_6" id="__nav_5_2_6_label" tabindex="0">
          扩展 PyTorch
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_2_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2_6">
          <span class="md-nav__icon md-icon"></span>
          扩展 PyTorch
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/numpy_extensions_tutorial/" class="md-nav__link">
        用 numpy 和 scipy 创建扩展
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/cpp_extension/" class="md-nav__link">
        Custom C-- and CUDA Extensions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torch_script_custom_ops/" class="md-nav__link">
        Extending TorchScript with Custom C-- Operators
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2_7" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2_7" id="__nav_5_2_7_label" tabindex="0">
          生产性使用
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_2_7_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2_7">
          <span class="md-nav__icon md-icon"></span>
          生产性使用
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/dist_tuto/" class="md-nav__link">
        Writing Distributed Applications with PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/aws_distributed_training_tutorial/" class="md-nav__link">
        使用 Amazon AWS 进行分布式训练
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/ONNXLive/" class="md-nav__link">
        ONNX 现场演示教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/cpp_export/" class="md-nav__link">
        在 C-- 中加载 PYTORCH 模型
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2_8" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2_8" id="__nav_5_2_8_label" tabindex="0">
          其它语言中的 PyTorch
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_2_8_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2_8">
          <span class="md-nav__icon md-icon"></span>
          其它语言中的 PyTorch
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/cpp_frontend/" class="md-nav__link">
        使用 PyTorch C-- 前端
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3" >
      
      
      
        <label class="md-nav__link" for="__nav_5_3" id="__nav_5_3_label" tabindex="0">
          中文文档
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_3">
          <span class="md-nav__icon md-icon"></span>
          中文文档
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3_1" >
      
      
      
        <label class="md-nav__link" for="__nav_5_3_1" id="__nav_5_3_1_label" tabindex="0">
          注解
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_3_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_3_1">
          <span class="md-nav__icon md-icon"></span>
          注解
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/notes_autograd/" class="md-nav__link">
        自动求导机制
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/notes_broadcasting/" class="md-nav__link">
        广播语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/notes_cuda/" class="md-nav__link">
        CUDA 语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/notes_extending/" class="md-nav__link">
        Extending PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/notes_faq/" class="md-nav__link">
        Frequently Asked Questions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/notes_multiprocessing/" class="md-nav__link">
        Multiprocessing best practices
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/notes_randomness/" class="md-nav__link">
        Reproducibility
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/notes_serialization/" class="md-nav__link">
        Serialization semantics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/notes_windows/" class="md-nav__link">
        Windows FAQ
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3_2" >
      
      
      
        <label class="md-nav__link" for="__nav_5_3_2" id="__nav_5_3_2_label" tabindex="0">
          包参考
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_3_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_3_2">
          <span class="md-nav__icon md-icon"></span>
          包参考
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3_2_1" >
      
      
      
        <label class="md-nav__link" for="__nav_5_3_2_1" id="__nav_5_3_2_1_label" tabindex="0">
          torch
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_5_3_2_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_3_2_1">
          <span class="md-nav__icon md-icon"></span>
          torch
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torch/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torch_tensors/" class="md-nav__link">
        Tensors
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torch_random_sampling/" class="md-nav__link">
        Random sampling
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torch_serialization_parallelism_utilities/" class="md-nav__link">
        Serialization, Parallelism, Utilities
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3_2_1_5" >
      
      
      
        <label class="md-nav__link" for="__nav_5_3_2_1_5" id="__nav_5_3_2_1_5_label" tabindex="0">
          Math operations
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="5" aria-labelledby="__nav_5_3_2_1_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_3_2_1_5">
          <span class="md-nav__icon md-icon"></span>
          Math operations
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torch_math_operations_pointwise_ops/" class="md-nav__link">
        Pointwise Ops
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torch_math_operations_reduction_ops/" class="md-nav__link">
        Reduction Ops
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torch_math_operations_comparison_ops/" class="md-nav__link">
        Comparison Ops
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torch_math_operations_spectral_ops/" class="md-nav__link">
        Spectral Ops
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torch_math_operations_other_ops/" class="md-nav__link">
        Other Operations
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torch_math_operations_blas_lapack_ops/" class="md-nav__link">
        BLAS and LAPACK Operations
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/tensors/" class="md-nav__link">
        torch.Tensor
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/tensor_attributes/" class="md-nav__link">
        Tensor Attributes
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/type_info/" class="md-nav__link">
        数据类型信息
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/sparse/" class="md-nav__link">
        torch.sparse
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/cuda/" class="md-nav__link">
        torch.cuda
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/storage/" class="md-nav__link">
        torch.Storage
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/nn/" class="md-nav__link">
        torch.nn
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/nn_functional/" class="md-nav__link">
        torch.nn.functional
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/nn_init/" class="md-nav__link">
        torch.nn.init
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/optim/" class="md-nav__link">
        torch.optim
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/autograd/" class="md-nav__link">
        Automatic differentiation package - torch.autograd
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/distributed/" class="md-nav__link">
        Distributed communication package - torch.distributed
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/distributions/" class="md-nav__link">
        Probability distributions - torch.distributions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/jit/" class="md-nav__link">
        Torch Script
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/multiprocessing/" class="md-nav__link">
        多进程包 - torch.multiprocessing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/bottleneck/" class="md-nav__link">
        torch.utils.bottleneck
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/checkpoint/" class="md-nav__link">
        torch.utils.checkpoint
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/docs_cpp_extension/" class="md-nav__link">
        torch.utils.cpp_extension
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/data/" class="md-nav__link">
        torch.utils.data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/dlpack/" class="md-nav__link">
        torch.utils.dlpack
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/hub/" class="md-nav__link">
        torch.hub
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/model_zoo/" class="md-nav__link">
        torch.utils.model_zoo
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/onnx/" class="md-nav__link">
        torch.onnx
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/distributed_deprecated/" class="md-nav__link">
        Distributed communication package (deprecated) - torch.distributed.deprecated
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3_3" >
      
      
      
        <label class="md-nav__link" for="__nav_5_3_3" id="__nav_5_3_3_label" tabindex="0">
          torchvision 参考
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_3_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_3_3">
          <span class="md-nav__icon md-icon"></span>
          torchvision 参考
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/docs_torchvision_ref/" class="md-nav__link">
        目录
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torchvision_datasets/" class="md-nav__link">
        torchvision.datasets
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torchvision_models/" class="md-nav__link">
        torchvision.models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torchvision_transforms/" class="md-nav__link">
        torchvision.transforms
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1.0/torchvision_utils/" class="md-nav__link">
        torchvision.utils
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
      
      
      
        <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
          PyTorch 0.4 中文文档
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 0.4 中文文档
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_1" >
      
      
      
        <label class="md-nav__link" for="__nav_6_1" id="__nav_6_1_label" tabindex="0">
          笔记
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_6_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_6_1">
          <span class="md-nav__icon md-icon"></span>
          笔记
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/1/" class="md-nav__link">
        自动求导机制
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/2/" class="md-nav__link">
        广播语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/3/" class="md-nav__link">
        CUDA 语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/4/" class="md-nav__link">
        扩展 PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/5/" class="md-nav__link">
        常见问题
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/6/" class="md-nav__link">
        多进程最佳实践
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/7/" class="md-nav__link">
        序列化语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/8/" class="md-nav__link">
        Windows 常见问题
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_2" >
      
      
      
        <label class="md-nav__link" for="__nav_6_2" id="__nav_6_2_label" tabindex="0">
          包参考
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_6_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_6_2">
          <span class="md-nav__icon md-icon"></span>
          包参考
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/10/" class="md-nav__link">
        Torch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/11/" class="md-nav__link">
        torch.Tensor
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/12/" class="md-nav__link">
        Tensor Attributes
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/13/" class="md-nav__link">
        torch.sparse
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/14/" class="md-nav__link">
        torch.cuda
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/15/" class="md-nav__link">
        torch.Storage
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/16/" class="md-nav__link">
        torch.nn
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/17/" class="md-nav__link">
        torch.nn.functional
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/18/" class="md-nav__link">
        自动差异化包 - torch.autograd
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/19/" class="md-nav__link">
        torch.optim
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/20/" class="md-nav__link">
        torch.nn.init
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/21/" class="md-nav__link">
        torch.distributions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/22/" class="md-nav__link">
        Multiprocessing 包 - torch.multiprocessing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/23/" class="md-nav__link">
        分布式通讯包 - torch.distributed
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/24/" class="md-nav__link">
        torch.utils.bottleneck
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/25/" class="md-nav__link">
        torch.utils.checkpoint
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/26/" class="md-nav__link">
        torch.utils.cpp_extension
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/27/" class="md-nav__link">
        torch.utils.data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/28/" class="md-nav__link">
        torch.utils.ffi
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/29/" class="md-nav__link">
        torch.utils.model_zoo
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/30/" class="md-nav__link">
        torch.onnx
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/31/" class="md-nav__link">
        遗留包 - torch.legacy
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_3" >
      
      
      
        <label class="md-nav__link" for="__nav_6_3" id="__nav_6_3_label" tabindex="0">
          torchvision 参考
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_6_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_6_3">
          <span class="md-nav__icon md-icon"></span>
          torchvision 参考
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/33/" class="md-nav__link">
        torchvision
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/34/" class="md-nav__link">
        torchvision.datasets
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/35/" class="md-nav__link">
        torchvision.models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/36/" class="md-nav__link">
        torchvision.transform
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.4/37/" class="md-nav__link">
        torchvision.utils
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" checked>
      
      
      
        <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
          PyTorch 0.3 中文文档 & 教程
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="true">
        <label class="md-nav__title" for="__nav_7">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 0.3 中文文档 & 教程
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        目录
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_2" >
      
      
      
        <label class="md-nav__link" for="__nav_7_2" id="__nav_7_2_label" tabindex="0">
          中文教程
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_7_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_2">
          <span class="md-nav__icon md-icon"></span>
          中文教程
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_2_1" >
      
      
      
        <label class="md-nav__link" for="__nav_7_2_1" id="__nav_7_2_1_label" tabindex="0">
          初学者教程
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_7_2_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_2_1">
          <span class="md-nav__icon md-icon"></span>
          初学者教程
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_2_1_1" >
      
      
      
        <label class="md-nav__link" for="__nav_7_2_1_1" id="__nav_7_2_1_1_label" tabindex="0">
          PyTorch 深度学习: 60 分钟极速入门教程
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_7_2_1_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_2_1_1">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 深度学习: 60 分钟极速入门教程
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../deep_learning_60min_blitz/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../blitz_tensor_tutorial/" class="md-nav__link">
        PyTorch 是什么？
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../blitz_autograd_tutorial/" class="md-nav__link">
        自动求导: 自动微分
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../blitz_neural_networks_tutorial/" class="md-nav__link">
        神经网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../blitz_cifar10_tutorial/" class="md-nav__link">
        训练一个分类器
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../blitz_data_parallel_tutorial/" class="md-nav__link">
        可选: 数据并行
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_2_1_2" >
      
      
      
        <label class="md-nav__link" for="__nav_7_2_1_2" id="__nav_7_2_1_2_label" tabindex="0">
          PyTorch for former Torch users
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_7_2_1_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_2_1_2">
          <span class="md-nav__icon md-icon"></span>
          PyTorch for former Torch users
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../former_torchies_tutorial/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../former_torchies_tensor_tutorial/" class="md-nav__link">
        Tensors
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../former_torchies_autograd_tutorial/" class="md-nav__link">
        Autograd (自动求导)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../former_torchies_nn_tutorial/" class="md-nav__link">
        nn package
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../former_torchies_parallelism_tutorial/" class="md-nav__link">
        Multi-GPU examples
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_2_1_3" >
      
      
      
        <label class="md-nav__link" for="__nav_7_2_1_3" id="__nav_7_2_1_3_label" tabindex="0">
          跟着例子学习 PyTorch
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_7_2_1_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_2_1_3">
          <span class="md-nav__icon md-icon"></span>
          跟着例子学习 PyTorch
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../pytorch_with_examples/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../pytorch_with_examples_warm-up-numpy/" class="md-nav__link">
        Warm-up: numpy
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../pytorch_with_examples_pytorch-tensors/" class="md-nav__link">
        PyTorch: Tensors
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../pytorch_with_examples_pytorch-variables-and-autograd/" class="md-nav__link">
        PyTorch: 变量和autograd
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../pytorch_with_examples_pytorch-defining-new-autograd-functions/" class="md-nav__link">
        PyTorch: 定义新的autograd函数
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../pytorch_with_examples_tensorflow-static-graphs/" class="md-nav__link">
        TensorFlow: 静态图
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../pytorch_with_examples_pytorch-nn/" class="md-nav__link">
        PyTorch: nn包
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../pytorch_with_examples_pytorch-optim/" class="md-nav__link">
        PyTorch: optim包
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../pytorch_with_examples_pytorch-custom-nn-modules/" class="md-nav__link">
        PyTorch: 定制化nn模块
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../pytorch_with_examples_pytorch-control-flow-weight-sharing/" class="md-nav__link">
        PyTorch: 动态控制流程 - 权重共享
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../transfer_learning_tutorial/" class="md-nav__link">
        迁移学习教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../data_loading_tutorial/" class="md-nav__link">
        数据加载和处理教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_2_1_6" >
      
      
      
        <label class="md-nav__link" for="__nav_7_2_1_6" id="__nav_7_2_1_6_label" tabindex="0">
          针对NLP的Pytorch深度学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_7_2_1_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_2_1_6">
          <span class="md-nav__icon md-icon"></span>
          针对NLP的Pytorch深度学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../deep_learning_nlp_tutorial/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../nlp_pytorch_tutorial/" class="md-nav__link">
        PyTorch介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../nlp_deep_learning_tutorial/" class="md-nav__link">
        PyTorch深度学习
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../nlp_word_embeddings_tutorial/" class="md-nav__link">
        词汇嵌入:编码词汇语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../nlp_sequence_models_tutorial/" class="md-nav__link">
        序列模型和 LSTM 网络(长短记忆网络）
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../nlp_advanced_tutorial/" class="md-nav__link">
        高级教程: 作出动态决策和 Bi-LSTM CRF
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_2_2" >
      
      
      
        <label class="md-nav__link" for="__nav_7_2_2" id="__nav_7_2_2_label" tabindex="0">
          中级教程
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_7_2_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_2_2">
          <span class="md-nav__icon md-icon"></span>
          中级教程
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../char_rnn_classification_tutorial/" class="md-nav__link">
        用字符级RNN分类名称
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../char_rnn_generation_tutorial/" class="md-nav__link">
        基与字符级RNN(Char-RNN）的人名生成
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../seq2seq_translation_tutorial/" class="md-nav__link">
        用基于注意力机制的seq2seq神经网络进行翻译
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../reinforcement_q_learning/" class="md-nav__link">
        强化学习(DQN）教程
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../dist_tuto/" class="md-nav__link">
        Writing Distributed Applications with PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../spatial_transformer_tutorial/" class="md-nav__link">
        空间转换网络 (Spatial Transformer Networks) 教程
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_2_3" >
      
      
      
        <label class="md-nav__link" for="__nav_7_2_3" id="__nav_7_2_3_label" tabindex="0">
          高级教程
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_7_2_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_2_3">
          <span class="md-nav__icon md-icon"></span>
          高级教程
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../neural_style_tutorial/" class="md-nav__link">
        用 PyTorch 做 神经转换 (Neural Transfer)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../numpy_extensions_tutorial/" class="md-nav__link">
        使用 numpy 和 scipy 创建扩展
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../super_resolution_with_caffe2/" class="md-nav__link">
        使用 ONNX 将模型从 PyTorch 迁移到 Caffe2 和 Mobile
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../c_extension/" class="md-nav__link">
        为 pytorch 自定义 C 扩展
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_3" checked>
      
      
      
        <label class="md-nav__link" for="__nav_7_3" id="__nav_7_3_label" tabindex="0">
          中文文档
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_7_3_label" aria-expanded="true">
        <label class="md-nav__title" for="__nav_7_3">
          <span class="md-nav__icon md-icon"></span>
          中文文档
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_3_1" >
      
      
      
        <label class="md-nav__link" for="__nav_7_3_1" id="__nav_7_3_1_label" tabindex="0">
          介绍
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_7_3_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_3_1">
          <span class="md-nav__icon md-icon"></span>
          介绍
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../notes_autograd/" class="md-nav__link">
        自动求导机制
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../notes_broadcasting/" class="md-nav__link">
        广播语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../notes_cuda/" class="md-nav__link">
        CUDA 语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../notes_extending/" class="md-nav__link">
        扩展 PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../notes_multiprocessing/" class="md-nav__link">
        多进程的最佳实践
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../notes_serialization/" class="md-nav__link">
        序列化语义
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_3_2" checked>
      
      
      
        <label class="md-nav__link" for="__nav_7_3_2" id="__nav_7_3_2_label" tabindex="0">
          Package 参考
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_7_3_2_label" aria-expanded="true">
        <label class="md-nav__title" for="__nav_7_3_2">
          <span class="md-nav__icon md-icon"></span>
          Package 参考
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../torch/" class="md-nav__link">
        torch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../tensors/" class="md-nav__link">
        torch.Tensor
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../sparse/" class="md-nav__link">
        torch.sparse
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../storage/" class="md-nav__link">
        torch.Storage
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          torch.nn
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        torch.nn
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#parameters" class="md-nav__link">
    Parameters (参数)
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#containers" class="md-nav__link">
    Containers (容器)
  </a>
  
    <nav class="md-nav" aria-label="Containers (容器)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#module" class="md-nav__link">
    Module
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sequential" class="md-nav__link">
    Sequential
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#modulelist" class="md-nav__link">
    ModuleList
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#parameterlist" class="md-nav__link">
    ParameterList
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#convolution-layers" class="md-nav__link">
    Convolution Layers (卷积层)
  </a>
  
    <nav class="md-nav" aria-label="Convolution Layers (卷积层)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#conv1d" class="md-nav__link">
    Conv1d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#conv2d" class="md-nav__link">
    Conv2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#conv3d" class="md-nav__link">
    Conv3d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convtranspose1d" class="md-nav__link">
    ConvTranspose1d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convtranspose2d" class="md-nav__link">
    ConvTranspose2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convtranspose3d" class="md-nav__link">
    ConvTranspose3d
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pooling-layers" class="md-nav__link">
    Pooling Layers (池化层)
  </a>
  
    <nav class="md-nav" aria-label="Pooling Layers (池化层)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#maxpool1d" class="md-nav__link">
    MaxPool1d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#maxpool2d" class="md-nav__link">
    MaxPool2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#maxpool3d" class="md-nav__link">
    MaxPool3d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#maxunpool1d" class="md-nav__link">
    MaxUnpool1d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#maxunpool2d" class="md-nav__link">
    MaxUnpool2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#maxunpool3d" class="md-nav__link">
    MaxUnpool3d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#avgpool1d" class="md-nav__link">
    AvgPool1d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#avgpool2d" class="md-nav__link">
    AvgPool2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#avgpool3d" class="md-nav__link">
    AvgPool3d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fractionalmaxpool2d" class="md-nav__link">
    FractionalMaxPool2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lppool2d" class="md-nav__link">
    LPPool2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adaptivemaxpool1d" class="md-nav__link">
    AdaptiveMaxPool1d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adaptivemaxpool2d" class="md-nav__link">
    AdaptiveMaxPool2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adaptivemaxpool3d" class="md-nav__link">
    AdaptiveMaxPool3d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adaptiveavgpool1d" class="md-nav__link">
    AdaptiveAvgPool1d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adaptiveavgpool2d" class="md-nav__link">
    AdaptiveAvgPool2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adaptiveavgpool3d" class="md-nav__link">
    AdaptiveAvgPool3d
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#padding-layers" class="md-nav__link">
    Padding Layers (填充层)
  </a>
  
    <nav class="md-nav" aria-label="Padding Layers (填充层)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#reflectionpad2d" class="md-nav__link">
    ReflectionPad2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#replicationpad2d" class="md-nav__link">
    ReplicationPad2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#replicationpad3d" class="md-nav__link">
    ReplicationPad3d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#zeropad2d" class="md-nav__link">
    ZeroPad2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#constantpad2d" class="md-nav__link">
    ConstantPad2d
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#non-linear-activations" class="md-nav__link">
    Non-linear Activations (非线性层)
  </a>
  
    <nav class="md-nav" aria-label="Non-linear Activations (非线性层)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#relu" class="md-nav__link">
    ReLU
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#relu6" class="md-nav__link">
    ReLU6
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#elu" class="md-nav__link">
    ELU
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#selu" class="md-nav__link">
    SELU
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prelu" class="md-nav__link">
    PReLU
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#leakyrelu" class="md-nav__link">
    LeakyReLU
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#threshold" class="md-nav__link">
    Threshold
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hardtanh" class="md-nav__link">
    Hardtanh
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sigmoid" class="md-nav__link">
    Sigmoid
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tanh" class="md-nav__link">
    Tanh
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#logsigmoid" class="md-nav__link">
    LogSigmoid
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#softplus" class="md-nav__link">
    Softplus
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#softshrink" class="md-nav__link">
    Softshrink
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#softsign" class="md-nav__link">
    Softsign
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tanhshrink" class="md-nav__link">
    Tanhshrink
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#softmin" class="md-nav__link">
    Softmin
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#softmax" class="md-nav__link">
    Softmax
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#softmax2d" class="md-nav__link">
    Softmax2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#logsoftmax" class="md-nav__link">
    LogSoftmax
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#normalization-layers" class="md-nav__link">
    Normalization layers (归一化层)
  </a>
  
    <nav class="md-nav" aria-label="Normalization layers (归一化层)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#batchnorm1d" class="md-nav__link">
    BatchNorm1d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#batchnorm2d" class="md-nav__link">
    BatchNorm2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#batchnorm3d" class="md-nav__link">
    BatchNorm3d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instancenorm1d" class="md-nav__link">
    InstanceNorm1d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instancenorm2d" class="md-nav__link">
    InstanceNorm2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instancenorm3d" class="md-nav__link">
    InstanceNorm3d
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#recurrent-layers" class="md-nav__link">
    Recurrent layers (循环层)
  </a>
  
    <nav class="md-nav" aria-label="Recurrent layers (循环层)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#rnn" class="md-nav__link">
    RNN
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lstm" class="md-nav__link">
    LSTM
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gru" class="md-nav__link">
    GRU
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rnncell" class="md-nav__link">
    RNNCell
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lstmcell" class="md-nav__link">
    LSTMCell
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grucell" class="md-nav__link">
    GRUCell
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#linear-layers" class="md-nav__link">
    Linear layers (线性层)
  </a>
  
    <nav class="md-nav" aria-label="Linear layers (线性层)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#linear" class="md-nav__link">
    Linear
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bilinear" class="md-nav__link">
    Bilinear
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#dropout-layers" class="md-nav__link">
    Dropout layers
  </a>
  
    <nav class="md-nav" aria-label="Dropout layers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dropout" class="md-nav__link">
    Dropout
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dropout2d" class="md-nav__link">
    Dropout2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dropout3d" class="md-nav__link">
    Dropout3d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#alphadropout" class="md-nav__link">
    AlphaDropout
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse-layers" class="md-nav__link">
    Sparse layers (稀疏层)
  </a>
  
    <nav class="md-nav" aria-label="Sparse layers (稀疏层)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#embedding" class="md-nav__link">
    Embedding
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#embeddingbag" class="md-nav__link">
    EmbeddingBag
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#distance-functions" class="md-nav__link">
    Distance functions (距离函数)
  </a>
  
    <nav class="md-nav" aria-label="Distance functions (距离函数)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cosinesimilarity" class="md-nav__link">
    CosineSimilarity
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pairwisedistance" class="md-nav__link">
    PairwiseDistance
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#loss-functions" class="md-nav__link">
    Loss functions (损失函数)
  </a>
  
    <nav class="md-nav" aria-label="Loss functions (损失函数)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#l1loss" class="md-nav__link">
    L1Loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mseloss" class="md-nav__link">
    MSELoss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#crossentropyloss" class="md-nav__link">
    CrossEntropyLoss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nllloss" class="md-nav__link">
    NLLLoss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#poissonnllloss" class="md-nav__link">
    PoissonNLLLoss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nllloss2d" class="md-nav__link">
    NLLLoss2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kldivloss" class="md-nav__link">
    KLDivLoss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bceloss" class="md-nav__link">
    BCELoss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bcewithlogitsloss" class="md-nav__link">
    BCEWithLogitsLoss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#marginrankingloss" class="md-nav__link">
    MarginRankingLoss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hingeembeddingloss" class="md-nav__link">
    HingeEmbeddingLoss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multilabelmarginloss" class="md-nav__link">
    MultiLabelMarginLoss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#smoothl1loss" class="md-nav__link">
    SmoothL1Loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#softmarginloss" class="md-nav__link">
    SoftMarginLoss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multilabelsoftmarginloss" class="md-nav__link">
    MultiLabelSoftMarginLoss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cosineembeddingloss" class="md-nav__link">
    CosineEmbeddingLoss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multimarginloss" class="md-nav__link">
    MultiMarginLoss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tripletmarginloss" class="md-nav__link">
    TripletMarginLoss
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vision-layers" class="md-nav__link">
    Vision layers (视觉层)
  </a>
  
    <nav class="md-nav" aria-label="Vision layers (视觉层)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pixelshuffle" class="md-nav__link">
    PixelShuffle
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#upsample" class="md-nav__link">
    Upsample
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#upsamplingnearest2d" class="md-nav__link">
    UpsamplingNearest2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#upsamplingbilinear2d" class="md-nav__link">
    UpsamplingBilinear2d
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#dataparallel-layers-multi-gpu-distributed-gpu" class="md-nav__link">
    DataParallel layers (multi-GPU, distributed) (数据并行层, 多 GPU 的, 分布式的)
  </a>
  
    <nav class="md-nav" aria-label="DataParallel layers (multi-GPU, distributed) (数据并行层, 多 GPU 的, 分布式的)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dataparallel" class="md-nav__link">
    DataParallel
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#distributeddataparallel" class="md-nav__link">
    DistributedDataParallel
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#utilities" class="md-nav__link">
    Utilities (工具包)
  </a>
  
    <nav class="md-nav" aria-label="Utilities (工具包)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#clip_grad_norm" class="md-nav__link">
    clip_grad_norm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#weight_norm" class="md-nav__link">
    weight_norm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#remove_weight_norm" class="md-nav__link">
    remove_weight_norm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#packedsequence" class="md-nav__link">
    PackedSequence
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pack_padded_sequence" class="md-nav__link">
    pack_padded_sequence
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pad_packed_sequence" class="md-nav__link">
    pad_packed_sequence
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../optim/" class="md-nav__link">
        torch.optim
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../autograd/" class="md-nav__link">
        Automatic differentiation package - torch.autograd
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../distributions/" class="md-nav__link">
        Probability distributions - torch.distributions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../multiprocessing/" class="md-nav__link">
        Multiprocessing package - torch.multiprocessing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../distributed/" class="md-nav__link">
        Distributed communication package - torch.distributed
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../legacy/" class="md-nav__link">
        Legacy package - torch.legacy
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../cuda/" class="md-nav__link">
        torch.cuda
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../ffi/" class="md-nav__link">
        torch.utils.ffi
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../data/" class="md-nav__link">
        torch.utils.data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../model_zoo/" class="md-nav__link">
        torch.utils.model_zoo
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../onnx/" class="md-nav__link">
        torch.onnx
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_3_3" >
      
      
      
        <label class="md-nav__link" for="__nav_7_3_3" id="__nav_7_3_3_label" tabindex="0">
          torchvision 参考
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_7_3_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7_3_3">
          <span class="md-nav__icon md-icon"></span>
          torchvision 参考
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../torchvision/" class="md-nav__link">
        torchvision
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../datasets/" class="md-nav__link">
        torchvision.datasets
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../models/" class="md-nav__link">
        torchvision.models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../transforms/" class="md-nav__link">
        torchvision.transforms
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../utils/" class="md-nav__link">
        torchvision.utils
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8" >
      
      
      
        <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0">
          PyTorch 0.2 中文文档
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_8">
          <span class="md-nav__icon md-icon"></span>
          PyTorch 0.2 中文文档
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/" class="md-nav__link">
        介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8_2" >
      
      
      
        <label class="md-nav__link" for="__nav_8_2" id="__nav_8_2_label" tabindex="0">
          说明
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_8_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_8_2">
          <span class="md-nav__icon md-icon"></span>
          说明
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/notes/autograd/" class="md-nav__link">
        自动求导机制
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/notes/cuda/" class="md-nav__link">
        CUDA语义
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/notes/extending/" class="md-nav__link">
        扩展PyTorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/notes/multiprocessing/" class="md-nav__link">
        多进程最佳实践
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/notes/serialization/" class="md-nav__link">
        序列化语义
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8_3" >
      
      
      
        <label class="md-nav__link" for="__nav_8_3" id="__nav_8_3_label" tabindex="0">
          PACKAGE参考
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_8_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_8_3">
          <span class="md-nav__icon md-icon"></span>
          PACKAGE参考
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/package_references/torch/" class="md-nav__link">
        torch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/package_references/Tensor/" class="md-nav__link">
        torch.Tensor
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/package_references/Storage/" class="md-nav__link">
        torch.Storage
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/package_references/torch-nn/" class="md-nav__link">
        torch.nn
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/package_references/functional/" class="md-nav__link">
        torch.nn.functional
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/package_references/torch-autograd/" class="md-nav__link">
        torch.autograd
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/package_references/torch-optim/" class="md-nav__link">
        torch.optim
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/package_references/nn_init/" class="md-nav__link">
        torch.nn.init
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/package_references/torch-multiprocessing/" class="md-nav__link">
        torch.multiprocessing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/package_references/legacy/" class="md-nav__link">
        torch.legacy
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/package_references/torch-cuda/" class="md-nav__link">
        torch.cuda
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/package_references/ffi/" class="md-nav__link">
        torch.utils.ffi
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/package_references/data/" class="md-nav__link">
        torch.utils.data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/package_references/model_zoo/" class="md-nav__link">
        torch.utils.model_zoo
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8_4" >
      
      
      
        <label class="md-nav__link" for="__nav_8_4" id="__nav_8_4_label" tabindex="0">
          TORCHVISION参考
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_8_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_8_4">
          <span class="md-nav__icon md-icon"></span>
          TORCHVISION参考
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/torchvision/torchvision/" class="md-nav__link">
        torchvision
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/torchvision/torchvision-datasets/" class="md-nav__link">
        torchvision.datasets
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/torchvision/torchvision-models/" class="md-nav__link">
        torchvision.models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/torchvision/torchvision-transform/" class="md-nav__link">
        torchvision.transforms
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/torchvision/torchvision-utils/" class="md-nav__link">
        torchvision.utils
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0.2/acknowledgement/" class="md-nav__link">
        致谢
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../contrib/" class="md-nav__link">
        贡献者
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="https://www.apachecn.org/about/" class="md-nav__link">
        关于我们
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="https://docs.apachecn.org" class="md-nav__link">
        中文资源合集
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#parameters" class="md-nav__link">
    Parameters (参数)
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#containers" class="md-nav__link">
    Containers (容器)
  </a>
  
    <nav class="md-nav" aria-label="Containers (容器)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#module" class="md-nav__link">
    Module
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sequential" class="md-nav__link">
    Sequential
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#modulelist" class="md-nav__link">
    ModuleList
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#parameterlist" class="md-nav__link">
    ParameterList
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#convolution-layers" class="md-nav__link">
    Convolution Layers (卷积层)
  </a>
  
    <nav class="md-nav" aria-label="Convolution Layers (卷积层)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#conv1d" class="md-nav__link">
    Conv1d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#conv2d" class="md-nav__link">
    Conv2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#conv3d" class="md-nav__link">
    Conv3d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convtranspose1d" class="md-nav__link">
    ConvTranspose1d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convtranspose2d" class="md-nav__link">
    ConvTranspose2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convtranspose3d" class="md-nav__link">
    ConvTranspose3d
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pooling-layers" class="md-nav__link">
    Pooling Layers (池化层)
  </a>
  
    <nav class="md-nav" aria-label="Pooling Layers (池化层)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#maxpool1d" class="md-nav__link">
    MaxPool1d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#maxpool2d" class="md-nav__link">
    MaxPool2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#maxpool3d" class="md-nav__link">
    MaxPool3d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#maxunpool1d" class="md-nav__link">
    MaxUnpool1d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#maxunpool2d" class="md-nav__link">
    MaxUnpool2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#maxunpool3d" class="md-nav__link">
    MaxUnpool3d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#avgpool1d" class="md-nav__link">
    AvgPool1d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#avgpool2d" class="md-nav__link">
    AvgPool2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#avgpool3d" class="md-nav__link">
    AvgPool3d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fractionalmaxpool2d" class="md-nav__link">
    FractionalMaxPool2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lppool2d" class="md-nav__link">
    LPPool2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adaptivemaxpool1d" class="md-nav__link">
    AdaptiveMaxPool1d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adaptivemaxpool2d" class="md-nav__link">
    AdaptiveMaxPool2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adaptivemaxpool3d" class="md-nav__link">
    AdaptiveMaxPool3d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adaptiveavgpool1d" class="md-nav__link">
    AdaptiveAvgPool1d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adaptiveavgpool2d" class="md-nav__link">
    AdaptiveAvgPool2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adaptiveavgpool3d" class="md-nav__link">
    AdaptiveAvgPool3d
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#padding-layers" class="md-nav__link">
    Padding Layers (填充层)
  </a>
  
    <nav class="md-nav" aria-label="Padding Layers (填充层)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#reflectionpad2d" class="md-nav__link">
    ReflectionPad2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#replicationpad2d" class="md-nav__link">
    ReplicationPad2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#replicationpad3d" class="md-nav__link">
    ReplicationPad3d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#zeropad2d" class="md-nav__link">
    ZeroPad2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#constantpad2d" class="md-nav__link">
    ConstantPad2d
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#non-linear-activations" class="md-nav__link">
    Non-linear Activations (非线性层)
  </a>
  
    <nav class="md-nav" aria-label="Non-linear Activations (非线性层)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#relu" class="md-nav__link">
    ReLU
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#relu6" class="md-nav__link">
    ReLU6
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#elu" class="md-nav__link">
    ELU
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#selu" class="md-nav__link">
    SELU
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prelu" class="md-nav__link">
    PReLU
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#leakyrelu" class="md-nav__link">
    LeakyReLU
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#threshold" class="md-nav__link">
    Threshold
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hardtanh" class="md-nav__link">
    Hardtanh
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sigmoid" class="md-nav__link">
    Sigmoid
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tanh" class="md-nav__link">
    Tanh
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#logsigmoid" class="md-nav__link">
    LogSigmoid
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#softplus" class="md-nav__link">
    Softplus
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#softshrink" class="md-nav__link">
    Softshrink
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#softsign" class="md-nav__link">
    Softsign
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tanhshrink" class="md-nav__link">
    Tanhshrink
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#softmin" class="md-nav__link">
    Softmin
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#softmax" class="md-nav__link">
    Softmax
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#softmax2d" class="md-nav__link">
    Softmax2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#logsoftmax" class="md-nav__link">
    LogSoftmax
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#normalization-layers" class="md-nav__link">
    Normalization layers (归一化层)
  </a>
  
    <nav class="md-nav" aria-label="Normalization layers (归一化层)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#batchnorm1d" class="md-nav__link">
    BatchNorm1d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#batchnorm2d" class="md-nav__link">
    BatchNorm2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#batchnorm3d" class="md-nav__link">
    BatchNorm3d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instancenorm1d" class="md-nav__link">
    InstanceNorm1d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instancenorm2d" class="md-nav__link">
    InstanceNorm2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instancenorm3d" class="md-nav__link">
    InstanceNorm3d
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#recurrent-layers" class="md-nav__link">
    Recurrent layers (循环层)
  </a>
  
    <nav class="md-nav" aria-label="Recurrent layers (循环层)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#rnn" class="md-nav__link">
    RNN
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lstm" class="md-nav__link">
    LSTM
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gru" class="md-nav__link">
    GRU
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rnncell" class="md-nav__link">
    RNNCell
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lstmcell" class="md-nav__link">
    LSTMCell
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grucell" class="md-nav__link">
    GRUCell
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#linear-layers" class="md-nav__link">
    Linear layers (线性层)
  </a>
  
    <nav class="md-nav" aria-label="Linear layers (线性层)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#linear" class="md-nav__link">
    Linear
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bilinear" class="md-nav__link">
    Bilinear
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#dropout-layers" class="md-nav__link">
    Dropout layers
  </a>
  
    <nav class="md-nav" aria-label="Dropout layers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dropout" class="md-nav__link">
    Dropout
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dropout2d" class="md-nav__link">
    Dropout2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dropout3d" class="md-nav__link">
    Dropout3d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#alphadropout" class="md-nav__link">
    AlphaDropout
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse-layers" class="md-nav__link">
    Sparse layers (稀疏层)
  </a>
  
    <nav class="md-nav" aria-label="Sparse layers (稀疏层)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#embedding" class="md-nav__link">
    Embedding
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#embeddingbag" class="md-nav__link">
    EmbeddingBag
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#distance-functions" class="md-nav__link">
    Distance functions (距离函数)
  </a>
  
    <nav class="md-nav" aria-label="Distance functions (距离函数)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cosinesimilarity" class="md-nav__link">
    CosineSimilarity
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pairwisedistance" class="md-nav__link">
    PairwiseDistance
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#loss-functions" class="md-nav__link">
    Loss functions (损失函数)
  </a>
  
    <nav class="md-nav" aria-label="Loss functions (损失函数)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#l1loss" class="md-nav__link">
    L1Loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mseloss" class="md-nav__link">
    MSELoss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#crossentropyloss" class="md-nav__link">
    CrossEntropyLoss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nllloss" class="md-nav__link">
    NLLLoss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#poissonnllloss" class="md-nav__link">
    PoissonNLLLoss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nllloss2d" class="md-nav__link">
    NLLLoss2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kldivloss" class="md-nav__link">
    KLDivLoss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bceloss" class="md-nav__link">
    BCELoss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bcewithlogitsloss" class="md-nav__link">
    BCEWithLogitsLoss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#marginrankingloss" class="md-nav__link">
    MarginRankingLoss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hingeembeddingloss" class="md-nav__link">
    HingeEmbeddingLoss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multilabelmarginloss" class="md-nav__link">
    MultiLabelMarginLoss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#smoothl1loss" class="md-nav__link">
    SmoothL1Loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#softmarginloss" class="md-nav__link">
    SoftMarginLoss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multilabelsoftmarginloss" class="md-nav__link">
    MultiLabelSoftMarginLoss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cosineembeddingloss" class="md-nav__link">
    CosineEmbeddingLoss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multimarginloss" class="md-nav__link">
    MultiMarginLoss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tripletmarginloss" class="md-nav__link">
    TripletMarginLoss
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vision-layers" class="md-nav__link">
    Vision layers (视觉层)
  </a>
  
    <nav class="md-nav" aria-label="Vision layers (视觉层)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pixelshuffle" class="md-nav__link">
    PixelShuffle
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#upsample" class="md-nav__link">
    Upsample
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#upsamplingnearest2d" class="md-nav__link">
    UpsamplingNearest2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#upsamplingbilinear2d" class="md-nav__link">
    UpsamplingBilinear2d
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#dataparallel-layers-multi-gpu-distributed-gpu" class="md-nav__link">
    DataParallel layers (multi-GPU, distributed) (数据并行层, 多 GPU 的, 分布式的)
  </a>
  
    <nav class="md-nav" aria-label="DataParallel layers (multi-GPU, distributed) (数据并行层, 多 GPU 的, 分布式的)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dataparallel" class="md-nav__link">
    DataParallel
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#distributeddataparallel" class="md-nav__link">
    DistributedDataParallel
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#utilities" class="md-nav__link">
    Utilities (工具包)
  </a>
  
    <nav class="md-nav" aria-label="Utilities (工具包)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#clip_grad_norm" class="md-nav__link">
    clip_grad_norm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#weight_norm" class="md-nav__link">
    weight_norm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#remove_weight_norm" class="md-nav__link">
    remove_weight_norm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#packedsequence" class="md-nav__link">
    PackedSequence
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pack_padded_sequence" class="md-nav__link">
    pack_padded_sequence
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pad_packed_sequence" class="md-nav__link">
    pad_packed_sequence
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
    <a href="https://github.com/apachecn/pytorch-doc-zh/edit/master/docs/0.3/nn.md" title="Edit this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4v-2m10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1 2.1 2.1Z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/apachecn/pytorch-doc-zh/raw/master/docs/0.3/nn.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.15 8.15 0 0 1-1.23-2Z"/></svg>
    </a>
  


<div class="wwads-cn wwads-horizontal" data-id="206" style="max-width:680px"></div>
<h1 id="torchnn">torch.nn</h1>
<blockquote>
<p>译者：<a href="https://github.com/VPrincekin">@小王子</a>、<a href="https://github.com/wangyangting">@那伊抹微笑</a>、<a href="https://github.com/busyboxs">@Yang Shun</a>、<a href="https://github.com/zhuyansen">@Zhu Yansen</a>、<a href="https://github.com/woaichipinngguo">@woaichipinngguo</a>、<a href="https://github.com/buldajs">@buldajs</a>、<a href="https://github.com/swardsman">@吉思雨</a>、<a href="https://github.com/vra">@王云峰</a>、<a href="https://github.com/sawyer7246">@李雨龙</a>、<a href="https://github.com/Eadral">@Yucong Zhu</a>、<a href="https://github.com/garry1ng">@林嘉应</a>、<a href="https://github.com/QianFanCe">@QianFanCe</a>、<a href="https://github.com/dabney777">@dabney777</a>、<a href="https://github.com/jizg">@Alex</a>、<a href="https://github.com/Mabinogiysk">@SiKai Yao</a>、<a href="https://github.com/QiaoXie">@小乔</a> @laihongchang @噼里啪啦嘣 <a href="https://github.com/BarrettLi">@BarrettLi</a>、<a href="https://github.com/KrokYin">@KrokYin</a>、<a href="https://github.com/JoinsenQ">@MUSK1881</a></p>
<p>校对者：<a href="http://community.apachecn.org/?/people/clown9804">@clown9804</a>、<a href="https://github.com/wizardforcel">@飞龙</a></p>
</blockquote>
<h2 id="parameters">Parameters (参数)</h2>
<pre><code class="language-py">class torch.nn.Parameter
</code></pre>
<p>Variable 的一种, 常被用于 module parameter(模块参数）.</p>
<p>Parameters 是 <code>Variable</code>](autograd.html#torch.autograd.Variable "torch.autograd.Variable") 的子类, 当它和 [<code>Module</code> 一起使用的时候会有一些特殊的属性 - 当它们被赋值给 Module 属性时, 它会自动的被加到 Module 的参数列表中, 并且会出现在 <code>parameters()</code> iterator 迭代器方法中. 将 Varibale 赋值给 Module 属性则不会有这样的影响. 这样做的原因是: 我们有时候会需要缓存一些临时的 state(状态）, 例如: 模型 RNN 中的最后一个隐藏状态. 如果没有 <code>Parameter</code> 这个类的话, 那么这些临时表也会注册为模型变量.</p>
<p>Variable 与 Parameter 的另一个不同之处在于, Parameter 不能被 volatile (即: 无法设置 volatile=True) 而且默认 requires_grad=True. Variable 默认 requires_grad=False.</p>
<p>参数：</p>
<ul>
<li><code>data (Tensor)</code> – parameter tensor.</li>
<li><code>requires_grad (bool, 可选)</code> – 如果参数需要梯度. 更多细节请参阅 <a href="notes/autograd.html#excluding-subgraphs">反向排除 subgraphs (子图)</a>.</li>
</ul>
<h2 id="containers">Containers (容器)</h2>
<h3 id="module">Module</h3>
<pre><code class="language-py">class torch.nn.Module
</code></pre>
<p>所有神经网络的基类.</p>
<p>你的模型应该也是该类的子类.</p>
<p>Modules 也可以包含其它 Modules, 允许使用树结构嵌入它们. 你可以将子模块赋值给模型属性</p>
<pre><code class="language-py">import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
       x = F.relu(self.conv1(x))
       return F.relu(self.conv2(x))

</code></pre>
<p>以这种方式分配的子模块将被注册, 并且在调用 .cuda() 等等方法时也将转换它们的参数.</p>
<pre><code class="language-py">add_module(name, module)
</code></pre>
<p>添加一个 child module(子模块）到当前的 module(模块）中.</p>
<p>被添加的 module 还可以通过指定的 name 属性来获取它.</p>
<p>参数：</p>
<ul>
<li><code>name (string)</code> – 子模块的名称. 可以使用指定的 name 从该模块访问子模块</li>
<li><code>parameter (Module)</code> – 被添加到模块的子模块.</li>
</ul>
<pre><code class="language-py">apply(fn)
</code></pre>
<p>将 <code>fn</code> 函数递归的应用到每一个子模块 (由 <code>.children()</code> 方法所返回的) 以及 self. 典型的用于包括初始化模型的参数 (也可参阅 torch-nn-init).</p>
<p>参数：<code>fn (Module -&gt; None)</code> – 要被应用到每一个子模块上的函数</p>
<p>返回值：<code>self</code></p>
<p>返回类型：<code>Module</code></p>
<p>示例：</p>
<pre><code class="language-py">&gt;&gt;&gt; def init_weights(m):
&gt;&gt;&gt;     print(m)
&gt;&gt;&gt;     if type(m) == nn.Linear:
&gt;&gt;&gt;         m.weight.data.fill_(1.0)
&gt;&gt;&gt;         print(m.weight)
&gt;&gt;&gt;
&gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))
&gt;&gt;&gt; net.apply(init_weights)
Linear (2 -&gt; 2)
Parameter containing:
 1  1
 1  1
[torch.FloatTensor of size 2x2]
Linear (2 -&gt; 2)
Parameter containing:
 1  1
 1  1
[torch.FloatTensor of size 2x2]
Sequential (
 (0): Linear (2 -&gt; 2)
 (1): Linear (2 -&gt; 2)
)

</code></pre>
<pre><code class="language-py">children()
</code></pre>
<p>返回一个最近子模块的 iterator(迭代器）.</p>
<p>Yields: <code>Module</code> – 一个子模块</p>
<pre><code class="language-py">cpu()
</code></pre>
<p>将所有的模型参数和缓冲区移动到 CPU.</p>
<p>返回值：<code>self</code></p>
<p>返回类型：<code>Module</code></p>
<pre><code class="language-py">cuda(device=None)
</code></pre>
<p>将所有的模型参数和缓冲区移动到 GPU.</p>
<p>这将会关联一些参数并且缓存不同的对象. 所以在构建优化器之前应该调用它, 如果模块在优化的情况下会生存在 GPU 上.</p>
<p>参数：<code>device (int, 可选)</code> – 如果指定, 所有参数将被复制到指定的设备上</p>
<p>返回值：<code>self</code></p>
<p>返回类型：<code>Module</code></p>
<pre><code class="language-py">double()
</code></pre>
<p>将所有的 parameters 和 buffers 的数据类型转换成 double.</p>
<p>返回值：<code>self</code></p>
<p>返回类型：<code>Module</code></p>
<pre><code class="language-py">eval()
</code></pre>
<p>将模块设置为评估模式.</p>
<p>这种方式只对 Dropout 或 BatchNorm 等模块有效.</p>
<pre><code class="language-py">float()
</code></pre>
<p>将所有的 parameters 和 buffers 的数据类型转换成float.</p>
<p>返回值：<code>self</code></p>
<p>返回类型：<code>Module</code></p>
<pre><code class="language-py">forward(*input)
</code></pre>
<p>定义每次调用时执行的计算.</p>
<p>应该被所有的子类重写.</p>
<p>注解：</p>
<p>尽管需要在此函数中定义正向传递的方式, 但是应该事后尽量调用 <code>Module</code> 实例, 因为前者负责运行已注册的钩子, 而后者静默的忽略它们.</p>
<pre><code class="language-py">half()
</code></pre>
<p>将所有的 parameters 和 buffers 的数据类型转换成 half.</p>
<p>返回值：<code>self</code></p>
<p>返回类型：<code>Module</code></p>
<pre><code class="language-py">load_state_dict(state_dict, strict=True)
</code></pre>
<p>将 <code>state_dict</code> 中的 parameters 和 buffers 复制到此模块和它的子后代中. 如果 <code>strict</code> 为 <code>True</code>, 则 <code>state_dict</code> 的 key 必须和模块的 <code>state_dict()</code> 函数返回的 key 一致.</p>
<p>参数：</p>
<ul>
<li><code>state_dict (dict)</code> – 一个包含 parameters 和 persistent buffers(持久化缓存的）字典.</li>
<li><code>strict (bool)</code> – 严格的强制 <code>state_dict</code> 属性中的 key 与该模块的函数 <code>state_dict()</code> 返回的 keys 相匹配.</li>
</ul>
<pre><code class="language-py">modules()
</code></pre>
<p>返回一个覆盖神经网络中所有模块的 iterator(迭代器）.</p>
<p>Yields: <code>Module</code> – 网络中的一个模块</p>
<p>注解：</p>
<p>重复的模块只返回一次. 在下面的例子中, <code>1</code> 只会被返回一次. example, <code>l</code> will be returned only once.</p>
<pre><code class="language-py">&gt;&gt;&gt; l = nn.Linear(2, 2)
&gt;&gt;&gt; net = nn.Sequential(l, l)
&gt;&gt;&gt; for idx, m in enumerate(net.modules()):
&gt;&gt;&gt;     print(idx, '-&gt;', m)
0 -&gt; Sequential (
 (0): Linear (2 -&gt; 2)
 (1): Linear (2 -&gt; 2)
)
1 -&gt; Linear (2 -&gt; 2)

</code></pre>
<pre><code class="language-py">named_children()
</code></pre>
<p>返回一个 iterator(迭代器）, 而不是最接近的子模块, 产生模块的 name 以及模块本身.</p>
<p>Yields: <code>(string, Module)</code> – 包含名称和子模块的 Tuple(元组）</p>
<p>示例：</p>
<pre><code class="language-py">&gt;&gt;&gt; for name, module in model.named_children():
&gt;&gt;&gt;     if name in ['conv4', 'conv5']:
&gt;&gt;&gt;         print(module)

</code></pre>
<pre><code class="language-py">named_modules(memo=None, prefix='')
</code></pre>
<p>返回一个神经网络中所有模块的 iterator(迭代器）, 产生模块的 name 以及模块本身.</p>
<p>Yields: <code>(string, Module)</code> – 名字和模块的 Tuple(元组）</p>
<p>注解：</p>
<p>重复的模块只返回一次. 在下面的例子中, <code>1</code> 只会被返回一次.</p>
<pre><code class="language-py">&gt;&gt;&gt; l = nn.Linear(2, 2)
&gt;&gt;&gt; net = nn.Sequential(l, l)
&gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):
&gt;&gt;&gt;     print(idx, '-&gt;', m)
0 -&gt; ('', Sequential (
 (0): Linear (2 -&gt; 2)
 (1): Linear (2 -&gt; 2)
))
1 -&gt; ('0', Linear (2 -&gt; 2))

</code></pre>
<pre><code class="language-py">named_parameters(memo=None, prefix='')
</code></pre>
<p>返回模块参数的迭代器, 产生参数的名称以及参数本身</p>
<p>Yields: <code>(string, Parameter)</code> – Tuple 包含名称很参数的 Tuple(元组）</p>
<p>示例：</p>
<pre><code class="language-py">&gt;&gt;&gt; for name, param in self.named_parameters():
&gt;&gt;&gt;    if name in ['bias']:
&gt;&gt;&gt;        print(param.size())

</code></pre>
<pre><code class="language-py">parameters()
</code></pre>
<p>返回一个模块参数的迭代器.</p>
<p>这通常传递给优化器.</p>
<p>Yields: <code>Parameter</code> – 模型参数</p>
<p>示例：</p>
<pre><code class="language-py">&gt;&gt;&gt; for param in model.parameters():
&gt;&gt;&gt;     print(type(param.data), param.size())
&lt;class 'torch.FloatTensor'&gt; (20L,)
&lt;class 'torch.FloatTensor'&gt; (20L, 1L, 5L, 5L)

</code></pre>
<pre><code class="language-py">register_backward_hook(hook)
</code></pre>
<p>在模块上注册一个 backward hook(反向钩子）.</p>
<p>每次计算关于模块输入的梯度时, 都会调用该钩子. 钩子应该有以下结构:</p>
<pre><code class="language-py">hook(module, grad_input, grad_output) -&gt; Tensor or None

</code></pre>
<p>如果 module 有多个输入或输出的话, 那么 <code>grad_input</code> 和 <code>grad_output</code> 将会是个 tuple. hook 不应该修改它的参数, 但是它可以选择性地返回一个新的关于输入的梯度, 这个返回的梯度在后续的计算中会替代 <code>grad_input</code>.</p>
<p>返回值：通过调用 <code>handle.remove()</code> 方法可以删除添加钩子的句柄 <code>handle.remove()</code></p>
<p>返回类型：<code>torch.utils.hooks.RemovableHandle</code></p>
<pre><code class="language-py">register_buffer(name, tensor)
</code></pre>
<p>给模块添加一个持久化的 buffer.</p>
<p>持久化的 buffer 通常被用在这么一种情况: 我们需要保存一个状态, 但是这个状态不能看作成为模型参数. 例如: BatchNorm 的 <code>running_mean</code> 不是一个 parameter, 但是它也是需要保存的状态之一.</p>
<p>Buffers 可以使用指定的 name 作为属性访问.</p>
<p>参数：</p>
<ul>
<li><code>name (string)</code> – buffer 的名称. 可以使用指定的 name 从该模块访问 buffer</li>
<li><code>tensor (Tensor)</code> – 被注册的 buffer.</li>
</ul>
<p>示例：</p>
<pre><code class="language-py">&gt;&gt;&gt; self.register_buffer('running_mean', torch.zeros(num_features))

</code></pre>
<pre><code class="language-py">register_forward_hook(hook)
</code></pre>
<p>在模块上注册一个 forward hook(前向钩子）.</p>
<p>每一次 <code>forward()</code> 函数计算出一个输出后, 该钩子将会被调用. 它应该具有以下结构</p>
<pre><code class="language-py">hook(module, input, output) -&gt; None

</code></pre>
<p>该钩子应该不会修改输入或输出.</p>
<p>返回值：通过调用 <code>handle.remove()</code> 方法可以删除添加钩子的句柄</p>
<p>返回类型：<code>torch.utils.hooks.RemovableHandle</code></p>
<pre><code class="language-py">register_forward_pre_hook(hook)
</code></pre>
<p>在模块上注册一个预前向钩子.</p>
<p>每一次在调用 <code>forward()</code> 函数前都会调用该钩子. 它应该有以下结构:</p>
<pre><code class="language-py">hook(module, input) -&gt; None

</code></pre>
<p>该钩子不应该修改输入.</p>
<p>返回值：通过调用 <code>handle.remove()</code> 方法可以删除添加钩子的句柄 <code>handle.remove()</code></p>
<p>返回类型：<code>torch.utils.hooks.RemovableHandle</code></p>
<pre><code class="language-py">register_parameter(name, param)
</code></pre>
<p>添加一个参数到模块中.</p>
<p>可以使用指定的 name 属性来访问参数.</p>
<p>参数：</p>
<ul>
<li><code>name (string)</code> – 参数名. 可以使用指定的 name 来从该模块中访问参数</li>
<li><code>parameter (Parameter)</code> – 要被添加到模块的参数.</li>
</ul>
<pre><code class="language-py">state_dict(destination=None, prefix='', keep_vars=False)
</code></pre>
<p>返回一个字典, 它包含整个模块的状态.</p>
<p>包括参数和持久化的缓冲区 (例如. 运行中的平均值). Keys 是与之对应的参数和缓冲区的 name.</p>
<p>当 keep_vars 为 <code>True</code> 时, 它为每一个参数(而不是一个张量）返回一个 Variable.</p>
<p>参数：</p>
<ul>
<li><code>destination (dict, 可选)</code> – 如果不是 None, 该返回的字典应该被存储到 destination 中. Default: None</li>
<li><code>prefix (string, 可选)</code> – 向结果字典中的每个参数和缓冲区的 key(名称）添加一个前缀. Default: ''</li>
<li><code>keep_vars (bool, 可选)</code> – 如果为 <code>True</code>, 为每一个参数返回一个 Variable. 如果为 <code>False</code>, 为每一个参数返回一个 Tensor. Default: <code>False</code></li>
</ul>
<p>返回值：包含模块整体状态的字典</p>
<p>返回类型：<code>dict</code></p>
<p>示例：</p>
<pre><code class="language-py">&gt;&gt;&gt; module.state_dict().keys()
['bias', 'weight']

</code></pre>
<pre><code class="language-py">train(mode=True)
</code></pre>
<p>设置模块为训练模式.</p>
<p>这只对诸如 Dropout 或 BatchNorm 等模块时才会有影响.</p>
<p>返回值：<code>self</code></p>
<p>返回类型：<code>Module</code></p>
<pre><code class="language-py">type(dst_type)
</code></pre>
<p>转换所有参数和缓冲区为 dst_type.</p>
<p>参数：<code>dst_type (type 或 string)</code> – 理想的类型</p>
<p>返回值：<code>self</code></p>
<p>返回类型：<code>Module</code></p>
<pre><code class="language-py">zero_grad()
</code></pre>
<p>将所有模型参数的梯度设置为零.</p>
<h3 id="sequential">Sequential</h3>
<pre><code class="language-py">class torch.nn.Sequential(*args)
</code></pre>
<p>一个顺序的容器. 模块将按照它们在构造函数中传递的顺序添加到它. 或者, 也可以传入模块的有序字典.</p>
<p>为了更容易理解, 列举小例来说明</p>
<pre><code class="language-py"># 使用 Sequential 的例子
model = nn.Sequential(
          nn.Conv2d(1,20,5),
          nn.ReLU(),
          nn.Conv2d(20,64,5),
          nn.ReLU()
        )

# 与 OrderedDict 一起使用 Sequential 的例子
model = nn.Sequential(OrderedDict([
          ('conv1', nn.Conv2d(1,20,5)),
          ('relu1', nn.ReLU()),
          ('conv2', nn.Conv2d(20,64,5)),
          ('relu2', nn.ReLU())
        ]))

</code></pre>
<h3 id="modulelist">ModuleList</h3>
<pre><code class="language-py">class torch.nn.ModuleList(modules=None)
</code></pre>
<p>将子模块放入一个 list 中.</p>
<p>ModuleList 可以像普通的 Python list 一样被索引, 但是它包含的模块已经被正确的注册了, 并且所有的 Module 方法都是可见的.</p>
<p>参数：<code>modules (list, 可选)</code> – 要添加的模块列表</p>
<p>示例：</p>
<pre><code class="language-py">class MyModule(nn.Module):
    def __init__(self):
        super(MyModule, self).__init__()
        self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])

    def forward(self, x):
        # ModuleList can act as an iterable, or be indexed using ints
        for i, l in enumerate(self.linears):
            x = self.linears[i // 2](x) + l(x)
        return x

</code></pre>
<pre><code class="language-py">append(module)
</code></pre>
<p>添加一个指定的模块到 list 尾部.</p>
<p>参数：<code>module (nn.Module)</code> – 要被添加的模块</p>
<pre><code class="language-py">extend(modules)
</code></pre>
<p>在最后添加 Python list 中的模块.</p>
<p>参数：<code>modules (list)</code> – 要被添加的模块列表</p>
<h3 id="parameterlist">ParameterList</h3>
<pre><code class="language-py">class torch.nn.ParameterList(parameters=None)
</code></pre>
<p>保存 list 中的 parameter.</p>
<p>ParameterList 可以像普通的 Python list 那样被索引, 但是它所包含的参数被正确的注册了, 并且所有的 Module 方法都可见的.</p>
<p>参数：<code>modules (list, 可选)</code> – 要被添加的 <code>Parameter</code> 列表</p>
<p>示例：</p>
<pre><code class="language-py">class MyModule(nn.Module):
    def __init__(self):
        super(MyModule, self).__init__()
        self.params = nn.ParameterList([nn.Parameter(torch.randn(10, 10)) for i in range(10)])

    def forward(self, x):
        # ModuleList 可以充当 iterable(迭代器）, 或者可以使用整数进行索引
        for i, p in enumerate(self.params):
            x = self.params[i // 2].mm(x) + p.mm(x)
        return x

</code></pre>
<pre><code class="language-py">append(parameter)
</code></pre>
<p>添加一个指定的参数到 list 尾部.</p>
<p>参数：<code>parameter (nn.Parameter)</code> – parameter to append</p>
<pre><code class="language-py">extend(parameters)
</code></pre>
<p>在最后添加 Python list 中的参数.</p>
<p>参数：<code>parameters (list)</code> – list of parameters to append</p>
<h2 id="convolution-layers">Convolution Layers (卷积层)</h2>
<h3 id="conv1d">Conv1d</h3>
<pre><code class="language-py">class torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)
</code></pre>
<p>一维卷积层 输入矩阵的维度为 <img alt="(N, C_{in}, L)" src="../img/tex-198b2086ddb5510d9fc69c433c3ee55d.gif" />, 输出矩阵维度为 <img alt="(N, C_{out}, L_{out})" src="../img/tex-bb2bc07c1c0ec52a2e964a60175aac05.gif" />. 其中N为输入数量, C为每个输入样本的通道数量, L为样本中一个通道下的数据的长度. 算法如下:</p>
<p><img alt="\begin{array}{ll} out(N_i, C_{out_j}) = bias(C_{out_j}) + \sum_{k=0}^{C_{in}-1} weight(C_{out_j}, k) \star input(N_i, k) \end{array}" src="../img/tex-b482ae261c2a6c7f51be7da721fe7e54.gif" /></p>
<p><img alt="\star" src="../img/tex-4b4efc2fbe82a047fc08c83ea081f1d9.gif" /> 是互相关运算符, 上式带 <img alt="\star" src="../img/tex-4b4efc2fbe82a047fc08c83ea081f1d9.gif" /> 项为卷积项.</p>
<p><code>stride</code> 计算相关系数的步长, 可以为 tuple .<code>padding</code> 处理边界时在两侧补0数量<code>dilation</code> 采样间隔数量. 大于1时为非致密采样, 如对(a,b,c,d,e)采样时, 若池化规模为2,</p>
<p>dilation 为1时, 使用 (a,b);(b,c)… 进行池化, dilation 为1时, 使用 (a,c);(b,d)… 进行池化. | <code>groups</code> 控制输入和输出之间的连接, group=1, 输出是所有输入的卷积；group=2, 此时相当于 有并排的两个卷基层, 每个卷积层只在对应的输入通道和输出通道之间计算, 并且输出时会将所有 输出通道简单的首尾相接作为结果输出.</p>
<blockquote>
<p><code>in_channels</code> 和 <code>out_channels</code>都要可以被 groups 整除.</p>
</blockquote>
<p>注解：</p>
<p>数据的最后一列可能会因为 kernal 大小设定不当而被丢弃(大部分发生在 kernal 大小不能被输入 整除的时候, 适当的 padding 可以避免这个问题）.</p>
<p>参数：</p>
<ul>
<li><code>in_channels (-)</code> – 输入信号的通道数.</li>
<li><code>out_channels (-)</code> – 卷积后输出结果的通道数.</li>
<li><code>kernel_size (-)</code> – 卷积核的形状.</li>
<li><code>stride (-)</code> – 卷积每次移动的步长, 默认为1.</li>
<li><code>padding (-)</code> – 处理边界时填充0的数量, 默认为0(不填充).</li>
<li><code>dilation (-)</code> – 采样间隔数量, 默认为1, 无间隔采样.</li>
<li><code>groups (-)</code> – 输入与输出通道的分组数量. 当不为1时, 默认为1(全连接).</li>
<li><code>bias (-)</code> – 为 <code>True</code> 时, 添加偏置.</li>
</ul>
<p>形状：</p>
<ul>
<li>输入 Input: <img alt="(N, C_{in}, L_{in})" src="../img/tex-1fe4e158f3308cc1b9e22afd6f08957d.gif" /></li>
<li>输出 Output: <img alt="(N, C_{out}, L_{out})" src="../img/tex-bb2bc07c1c0ec52a2e964a60175aac05.gif" /> 其中 <img alt="L_{out} = floor((L_{in} + 2 * padding - dilation * (kernel_size - 1) - 1) / stride + 1)" src="../img/tex-1ef60f8e6f123a4369ccdc2117bd3f0d.gif" /></li>
</ul>
<p>变量：</p>
<ul>
<li><code>weight (Tensor)</code> – 卷积网络层间连接的权重, 是模型需要学习的变量, 形状为 (out_channels, in_channels, kernel_size)</li>
<li><code>bias (Tensor)</code> – 偏置, 是模型需要学习的变量, 形状为 (out_channels)</li>
</ul>
<p>Examples:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = nn.Conv1d(16, 33, 3, stride=2)
&gt;&gt;&gt; input = autograd.Variable(torch.randn(20, 16, 50))
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="conv2d">Conv2d</h3>
<pre><code class="language-py">class torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)
</code></pre>
<p>二维卷积层 输入矩阵的维度为 <img alt="(N, C_{in}, H, W)" src="../img/tex-120cc675d6ab67bb046f090d7be120a6.gif" /> , 输出矩阵维度为 <img alt="(N, C_{out}, H_{out}, W_{out})" src="../img/tex-ba3afaecc84f511d8c24e8605d528d35.gif" /> . 其中N为输入数量, C为每个输入样本的通道数量, H, W 分别为样本中一个通道下的数据的形状. 算法如下:</p>
<p><img alt="%\begin{array}{ll} out(N_i, C_{out_j}) = bias(C_{out_j}) + \sum_{ {k}=0}^{C_{in}-1} weight(C_{out_j}, k) \star input(N_i, k) \end{array}%" src="../img/tex-b482ae261c2a6c7f51be7da721fe7e54.gif" /></p>
<p><img alt="\star" src="../img/tex-4b4efc2fbe82a047fc08c83ea081f1d9.gif" /> 是互相关运算符, 上式带<code>*</code>项为卷积项.</p>
<p><code>stride</code> 计算相关系数的步长, 可以为 tuple .<code>padding</code> 处理边界时在每个维度首尾补0数量.<code>dilation</code> 采样间隔数量. 大于1时为非致密采样.<code>groups</code> 控制输入和输出之间的连接, group=1, 输出是所有输入的卷积； group=2, 此时</p>
<p>相当于有并排的两个卷基层, 每个卷积层只在对应的输入通道和输出通道之间计算, 并且输出时会将所有 输出通道简单的首尾相接作为结果输出.</p>
<blockquote>
<p><code>in_channels</code> 和 <code>out_channels</code>都要可以被 groups 整除.</p>
</blockquote>
<p><code>kernel_size</code>, <code>stride</code>, <code>padding</code>, <code>dilation</code> 可以为:</p>
<blockquote>
<ul>
<li>单个 <code>int</code> 值 – 宽和高均被设定为此值.</li>
<li>由两个 <code>int</code> 组成的 <code>tuple</code> – 第一个 <code>int</code> 为高, 第二个 <code>int</code> 为宽.</li>
</ul>
</blockquote>
<p>注解：</p>
<p>数据的最后一列可能会因为 kernal 大小设定不当而被丢弃(大部分发生在 kernal 大小不能被输入 整除的时候, 适当的 padding 可以避免这个问题）.</p>
<p>参数：</p>
<ul>
<li><code>in_channels (-)</code> – 输入信号的通道数.</li>
<li><code>out_channels (-)</code> – 卷积后输出结果的通道数.</li>
<li><code>kernel_size (-)</code> – 卷积核的形状.</li>
<li><code>stride (-)</code> – 卷积每次移动的步长, 默认为1.</li>
<li><code>padding (-)</code> – 处理边界时填充0的数量, 默认为0(不填充).</li>
<li><code>dilation (-)</code> – 采样间隔数量, 默认为1, 无间隔采样.</li>
<li><code>groups (-)</code> – 输入与输出通道的分组数量. 当不为1时, 默认为1(全连接).</li>
<li><code>bias (-)</code> – 为 <code>True</code> 时, 添加偏置.</li>
</ul>
<p>形状：</p>
<ul>
<li>输入 Input: <img alt="(N, C_{in}, H_{in}, W_{in})" src="../img/tex-a821595925aa8f87a7488c9e2b28cbe7.gif" /></li>
<li>输出 Output: <img alt="(N, C_{out}, H_{out}, W_{out})" src="../img/tex-ba3afaecc84f511d8c24e8605d528d35.gif" /> 其中 <img alt="H_{out} = floor((H_{in} + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) / stride[0] + 1)" src="../img/tex-79b3618ba1cd1e8e6e665aae1b4fc446.gif" /> <img alt="W_{out} = floor((W_{in} + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) / stride[1] + 1)" src="../img/tex-e9f44b9b5fc42bdb5991cfcd52e2dced.gif" /></li>
</ul>
<p>变量：</p>
<ul>
<li><code>weight (Tensor)</code> – 卷积网络层间连接的权重, 是模型需要学习的变量, 形状为 (out_channels, in_channels, kernel_size[0], kernel_size[1])</li>
<li><code>bias (Tensor)</code> – 偏置, 是模型需要学习的变量, 形状为 (out_channels)</li>
</ul>
<p>Examples:</p>
<pre><code class="language-py">&gt;&gt;&gt; # With square kernels and equal stride
&gt;&gt;&gt; m = nn.Conv2d(16, 33, 3, stride=2)
&gt;&gt;&gt; # non-square kernels and unequal stride and with padding
&gt;&gt;&gt; m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))
&gt;&gt;&gt; # non-square kernels and unequal stride and with padding and dilation
&gt;&gt;&gt; m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))
&gt;&gt;&gt; input = autograd.Variable(torch.randn(20, 16, 50, 100))
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="conv3d">Conv3d</h3>
<pre><code class="language-py">class torch.nn.Conv3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)
</code></pre>
<p>三维卷基层 输入矩阵的维度为 <img alt="(N, C_{in}, D, H, W)" src="../img/tex-98729c804361218eea500df06cd60c8b.gif" />, 输出矩阵维度为:<img alt="(N, C_{out}, D_{out}, H_{out}, W_{out})" src="../img/tex-5b2f3f7fabcd6ee5b9f89543b54d71e2.gif" />. 其中N为输入数量, C为每个输入样本的通道数量, D, H, W 分别为样本中一个通道下的数据的形状. 算法如下:</p>
<p><img alt="\begin{array}{ll} out(N_i, C_{out_j}) = bias(C_{out_j}) + \sum_{k=0}^{C_{in}-1} weight(C_{out_j}, k) \star input(N_i, k) \end{array}" src="../img/tex-b482ae261c2a6c7f51be7da721fe7e54.gif" /></p>
<p><img alt="\star" src="../img/tex-4b4efc2fbe82a047fc08c83ea081f1d9.gif" /> 是互相关运算符, 上式带<code>*</code>项为卷积项.</p>
<p><code>stride</code> 计算相关系数的步长, 可以为 tuple .<code>padding</code> 处理边界时在每个维度首尾补0数量.<code>dilation</code> 采样间隔数量. 大于1时为非致密采样.<code>groups</code> 控制输入和输出之间的连接, group=1, 输出是所有输入的卷积； group=2, 此时</p>
<p>相当于有并排的两个卷基层, 每个卷积层只在对应的输入通道和输出通道之间计算, 并且输出时会将所有 输出通道简单的首尾相接作为结果输出.</p>
<blockquote>
<p><code>in_channels</code> 和 <code>out_channels</code>都要可以被 groups 整除.</p>
</blockquote>
<p><code>kernel_size</code>, <code>stride</code>, <code>padding</code>, <code>dilation</code> 可以为:</p>
<blockquote>
<ul>
<li>单个 <code>int</code> 值 – 宽和高和深度均被设定为此值.</li>
<li>由三个 <code>int</code> 组成的 <code>tuple</code> – 第一个 <code>int</code> 为深度, 第二个 <code>int</code> 为高度, 第三个 <code>int</code> 为宽度.</li>
</ul>
</blockquote>
<p>注解：</p>
<p>数据的最后一列可能会因为 kernal 大小设定不当而被丢弃(大部分发生在 kernal 大小不能被输入 整除的时候, 适当的 padding 可以避免这个问题）.</p>
<p>参数：</p>
<ul>
<li><code>in_channels (-)</code> – 输入信号的通道数.</li>
<li><code>out_channels (-)</code> – 卷积后输出结果的通道数.</li>
<li><code>kernel_size (-)</code> – 卷积核的形状.</li>
<li><code>stride (-)</code> – 卷积每次移动的步长, 默认为1.</li>
<li><code>padding (-)</code> – 处理边界时填充0的数量, 默认为0(不填充).</li>
<li><code>dilation (-)</code> – 采样间隔数量, 默认为1, 无间隔采样.</li>
<li><code>groups (-)</code> – 输入与输出通道的分组数量. 当不为1时, 默认为1(全连接).</li>
<li><code>bias (-)</code> – 为 <code>True</code> 时, 添加偏置.</li>
</ul>
<p>形状：</p>
<ul>
<li>输入 Input: <img alt="(N, C_{in}, D_{in}, H_{in}, W_{in})" src="../img/tex-3284da09db0303b6608e4bc197b59361.gif" /></li>
<li>输出 Output: <img alt="(N, C_{out}, D_{out}, H_{out}, W_{out})" src="../img/tex-5b2f3f7fabcd6ee5b9f89543b54d71e2.gif" /> 其中 <img alt="D_{out} = floor((D_{in} + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) / stride[0] + 1)" src="../img/tex-ba168d43ee6e937903b387d0afce9a40.gif" /> <img alt="H_{out} = floor((H_{in} + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) / stride[1] + 1)" src="../img/tex-89213fb2d0850f4cc41af72bae650bd0.gif" /> <img alt="W_{out} = floor((W_{in} + 2 * padding[2] - dilation[2] * (kernel_size[2] - 1) - 1) / stride[2] + 1)" src="../img/tex-397d0048589e3a1a6644d6613e7d4722.gif" /></li>
</ul>
<p>变量：</p>
<ul>
<li><code>weight (Tensor)</code> – 卷积网络层间连接的权重, 是模型需要学习的变量, 形状为 (out_channels, in_channels, kernel_size[0], kernel_size[1], kernel_size[2])</li>
<li><code>bias (Tensor)</code> – 偏置, 是模型需要学习的变量, 形状为 (out_channels)</li>
</ul>
<p>Examples:</p>
<pre><code class="language-py">&gt;&gt;&gt; # With square kernels and equal stride
&gt;&gt;&gt; m = nn.Conv3d(16, 33, 3, stride=2)
&gt;&gt;&gt; # non-square kernels and unequal stride and with padding
&gt;&gt;&gt; m = nn.Conv3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(4, 2, 0))
&gt;&gt;&gt; input = autograd.Variable(torch.randn(20, 16, 10, 50, 100))
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="convtranspose1d">ConvTranspose1d</h3>
<pre><code class="language-py">class torch.nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1)
</code></pre>
<p>一维反卷积层 反卷积层可以理解为输入的数据和卷积核的位置反转的卷积操作. 反卷积有时候也会被翻译成解卷积.</p>
<p><code>stride</code> 计算相关系数的步长.<code>padding</code> 处理边界时在每个维度首尾补0数量.<code>output_padding</code> 输出时候在首尾补0的数量. (卷积时, 形状不同的输入数据</p>
<p>对相同的核函数可以产生形状相同的结果；反卷积时, 同一个输入对相同的核函数可以产生多 个形状不同的输出, 而输出结果只能有一个, 因此必须对输出形状进行约束）. | <code>dilation</code> 采样间隔数量. 大于1时为非致密采样. | <code>groups</code> 控制输入和输出之间的连接, group=1, 输出是所有输入的卷积； group=2, 此时 相当于有并排的两个卷基层, 每个卷积层只在对应的输入通道和输出通道之间计算, 并且输出时会将所有 输出通道简单的首尾相接作为结果输出.</p>
<blockquote>
<p><code>in_channels</code> 和 <code>out_channels</code>都要可以被 groups 整除.</p>
</blockquote>
<p>注解：</p>
<p>数据的最后一列可能会因为 kernal 大小设定不当而被丢弃(大部分发生在 kernal 大小不能被输入 整除的时候, 适当的 padding 可以避免这个问题）.</p>
<p>参数：</p>
<ul>
<li><code>in_channels (-)</code> – 输入信号的通道数.</li>
<li><code>out_channels (-)</code> – 卷积后输出结果的通道数.</li>
<li><code>kernel_size (-)</code> – 卷积核的形状.</li>
<li><code>stride (-)</code> – 卷积每次移动的步长, 默认为1.</li>
<li><code>padding (-)</code> – 处理边界时填充0的数量, 默认为0(不填充).</li>
<li><code>output_padding (-)</code> – 输出时候在首尾补值的数量, 默认为0. (卷积时, 形状不同的输入数据</li>
<li><code>同一个输入对相同的核函数可以产生多 (_对相同的核函数可以产生形状相同的结果；反卷积时_,)</code> –</li>
<li>而输出结果只能有一个, 因此必须对输出形状进行约束） (<em>个形状不同的输出</em>,) –</li>
<li><code>groups (-)</code> – 输入与输出通道的分组数量. 当不为1时, 默认为1(全连接).</li>
<li><code>bias (-)</code> – 为 <code>True</code> 时, 添加偏置.</li>
<li><code>dilation (-)</code> – 采样间隔数量, 默认为1, 无间隔采样.</li>
</ul>
<p>形状：</p>
<ul>
<li>输入 Input: <img alt="(N, C_{in}, L_{in})" src="../img/tex-1fe4e158f3308cc1b9e22afd6f08957d.gif" /></li>
<li>输出 Output: <img alt="(N, C_{out}, L_{out})" src="../img/tex-bb2bc07c1c0ec52a2e964a60175aac05.gif" /> 其中 <img alt="L_{out} = (L_{in} - 1) * stride - 2 * padding + kernel_size + output_padding" src="../img/tex-3358a4e917382beb03a6ef2132bb4018.gif" /></li>
</ul>
<p>变量：</p>
<ul>
<li><code>weight (Tensor)</code> – 卷积网络层间连接的权重, 是模型需要学习的变量, 形状为weight (Tensor): 卷积网络层间连接的权重, 是模型需要学习的变量, 形状为 (in_channels, out_channels, kernel_size[0], kernel_size[1])</li>
<li><code>bias (Tensor)</code> – 偏置, 是模型需要学习的变量, 形状为 (out_channels)</li>
</ul>
<h3 id="convtranspose2d">ConvTranspose2d</h3>
<pre><code class="language-py">class torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1)
</code></pre>
<p>二维反卷积层 反卷积层可以理解为输入的数据和卷积核的位置反转的卷积操作. 反卷积有时候也会被翻译成解卷积.</p>
<p><code>stride</code> 计算相关系数的步长.<code>padding</code> 处理边界时在每个维度首尾补0数量.<code>output_padding</code> 输出时候在每一个维度首尾补0的数量. (卷积时, 形状不同的输入数据</p>
<p>对相同的核函数可以产生形状相同的结果；反卷积时, 同一个输入对相同的核函数可以产生多 个形状不同的输出, 而输出结果只能有一个, 因此必须对输出形状进行约束）. | <code>dilation</code> 采样间隔数量. 大于1时为非致密采样. | <code>groups</code> 控制输入和输出之间的连接, group=1, 输出是所有输入的卷积； group=2, 此时 相当于有并排的两个卷基层, 每个卷积层只在对应的输入通道和输出通道之间计算, 并且输出时会将所有 输出通道简单的首尾相接作为结果输出.</p>
<blockquote>
<p><code>in_channels</code> 和 <code>out_channels</code>都应当可以被 groups 整除.</p>
</blockquote>
<p><code>kernel_size</code>, <code>stride</code>, <code>padding</code>, <code>output_padding</code> 可以为:</p>
<blockquote>
<ul>
<li>单个 <code>int</code> 值 – 宽和高均被设定为此值.</li>
<li>由两个 <code>int</code> 组成的 <code>tuple</code> – 第一个 <code>int</code> 为高度, 第二个 <code>int</code> 为宽度.</li>
</ul>
</blockquote>
<p>注解：</p>
<p>数据的最后一列可能会因为 kernal 大小设定不当而被丢弃(大部分发生在 kernal 大小不能被输入 整除的时候, 适当的 padding 可以避免这个问题）.</p>
<p>参数：</p>
<ul>
<li><code>in_channels (-)</code> – 输入信号的通道数.</li>
<li><code>out_channels (-)</code> – 卷积后输出结果的通道数.</li>
<li><code>kernel_size (-)</code> – 卷积核的形状.</li>
<li><code>stride (-)</code> – 卷积每次移动的步长, 默认为1.</li>
<li><code>padding (-)</code> – 处理边界时填充0的数量, 默认为0(不填充).</li>
<li><code>output_padding (-)</code> – 输出时候在首尾补值的数量, 默认为0. (卷积时, 形状不同的输入数据</li>
<li><code>同一个输入对相同的核函数可以产生多 (_对相同的核函数可以产生形状相同的结果；反卷积时_,)</code> –</li>
<li>而输出结果只能有一个, 因此必须对输出形状进行约束） (<em>个形状不同的输出</em>,) –</li>
<li><code>groups (-)</code> – 输入与输出通道的分组数量. 当不为1时, 默认为1(全连接).</li>
<li><code>bias (-)</code> – 为 <code>True</code> 时, 添加偏置.</li>
<li><code>dilation (-)</code> – 采样间隔数量, 默认为1, 无间隔采样.</li>
</ul>
<p>形状：</p>
<ul>
<li>输入 Input: <img alt="(N, C_{in}, H_{in}, W_{in})" src="../img/tex-a821595925aa8f87a7488c9e2b28cbe7.gif" /></li>
<li>输出 Output: <img alt="(N, C_{out}, H_{out}, W_{out})" src="../img/tex-ba3afaecc84f511d8c24e8605d528d35.gif" /> 其中 <img alt="H_{out} = (H_{in} - 1) * stride[0] - 2 * padding[0] + kernel_size[0] + output_padding[0]" src="../img/tex-7009a9216729c8c52e70b14ec732620d.gif" /> <img alt="W_{out} = (W_{in} - 1) * stride[1] - 2 * padding[1] + kernel_size[1] + output_padding[1]" src="../img/tex-bc45574b44fdf01856bacfcd4abdeeba.gif" /></li>
</ul>
<p>变量：</p>
<ul>
<li><code>weight (Tensor)</code> – 卷积网络层间连接的权重, 是模型需要学习的变量, 形状为weight (Tensor): 卷积网络层间连接的权重, 是模型需要学习的变量, 形状为 (in_channels, out_channels, kernel_size[0], kernel_size[1])</li>
<li><code>bias (Tensor)</code> – 偏置, 是模型需要学习的变量, 形状为 (out_channels)</li>
</ul>
<p>Examples:</p>
<pre><code class="language-py">&gt;&gt;&gt; # With square kernels and equal stride
&gt;&gt;&gt; m = nn.ConvTranspose2d(16, 33, 3, stride=2)
&gt;&gt;&gt; # non-square kernels and unequal stride and with padding
&gt;&gt;&gt; m = nn.ConvTranspose2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))
&gt;&gt;&gt; input = autograd.Variable(torch.randn(20, 16, 50, 100))
&gt;&gt;&gt; output = m(input)
&gt;&gt;&gt; # exact output size can be also specified as an argument
&gt;&gt;&gt; input = autograd.Variable(torch.randn(1, 16, 12, 12))
&gt;&gt;&gt; downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1)
&gt;&gt;&gt; upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1)
&gt;&gt;&gt; h = downsample(input)
&gt;&gt;&gt; h.size()
torch.Size([1, 16, 6, 6])
&gt;&gt;&gt; output = upsample(h, output_size=input.size())
&gt;&gt;&gt; output.size()
torch.Size([1, 16, 12, 12])

</code></pre>
<h3 id="convtranspose3d">ConvTranspose3d</h3>
<pre><code class="language-py">class torch.nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1)
</code></pre>
<p>三维反卷积层 反卷积层可以理解为输入的数据和卷积核的位置反转的卷积操作. 反卷积有时候也会被翻译成解卷积.</p>
<p><code>stride</code> 计算相关系数的步长.<code>padding</code> 处理边界时在每个维度首尾补0数量.<code>output_padding</code> 输出时候在每一个维度首尾补0的数量. (卷积时, 形状不同的输入数据</p>
<p>对相同的核函数可以产生形状相同的结果；反卷积时, 同一个输入对相同的核函数可以产生多 个形状不同的输出, 而输出结果只能有一个, 因此必须对输出形状进行约束） | <code>dilation</code> 采样间隔数量. 大于1时为非致密采样. | <code>groups</code> 控制输入和输出之间的连接, group=1, 输出是所有输入的卷积； group=2, 此时 相当于有并排的两个卷基层, 每个卷积层只在对应的输入通道和输出通道之间计算, 并且输出时会将所有 输出通道简单的首尾相接作为结果输出.</p>
<blockquote>
<p><code>in_channels</code> 和 <code>out_channels</code>都应当可以被 groups 整除.</p>
</blockquote>
<p><code>kernel_size</code>, <code>stride</code>, <code>padding</code>, <code>output_padding</code> 可以为:</p>
<blockquote>
<ul>
<li>单个 <code>int</code> 值 – 深和宽和高均被设定为此值.</li>
<li>由三个 <code>int</code> 组成的 <code>tuple</code> – 第一个 <code>int</code> 为深度, 第二个 <code>int</code> 为高度,第三个 <code>int</code> 为宽度.</li>
</ul>
</blockquote>
<p>注解：</p>
<p>数据的最后一列可能会因为 kernal 大小设定不当而被丢弃(大部分发生在 kernal 大小不能被输入 整除的时候, 适当的 padding 可以避免这个问题）.</p>
<p>参数：</p>
<ul>
<li><code>in_channels (-)</code> – 输入信号的通道数.</li>
<li><code>out_channels (-)</code> – 卷积后输出结果的通道数.</li>
<li><code>kernel_size (-)</code> – 卷积核的形状.</li>
<li><code>stride (-)</code> – 卷积每次移动的步长, 默认为1.</li>
<li><code>padding (-)</code> – 处理边界时填充0的数量, 默认为0(不填充).</li>
<li><code>output_padding (-)</code> – 输出时候在首尾补值的数量, 默认为0. (卷积时, 形状不同的输入数据</li>
<li><code>同一个输入对相同的核函数可以产生多 (_对相同的核函数可以产生形状相同的结果；反卷积时_,)</code> –</li>
<li>而输出结果只能有一个, 因此必须对输出形状进行约束） (<em>个形状不同的输出</em>,) –</li>
<li><code>groups (-)</code> – 输入与输出通道的分组数量. 当不为1时, 默认为1(全连接).</li>
<li><code>bias (-)</code> – 为 <code>True</code> 时, 添加偏置.</li>
<li><code>dilation (-)</code> – 采样间隔数量, 默认为1, 无间隔采样.</li>
</ul>
<p>形状：</p>
<ul>
<li>输入 Input: <img alt="(N, C_{in}, D_{in}, H_{in}, W_{in})" src="../img/tex-3284da09db0303b6608e4bc197b59361.gif" /></li>
<li>输出 Output: <img alt="(N, C_{out}, D_{out}, H_{out}, W_{out})" src="../img/tex-5b2f3f7fabcd6ee5b9f89543b54d71e2.gif" /> 其中 <img alt="D_{out} = (D_{in} - 1) * stride[0] - 2 * padding[0] + kernel_size[0] + output_padding[0]" src="../img/tex-02979cc7d8145e84e5beef17eed9af98.gif" /> <img alt="H_{out} = (H_{in} - 1) * stride[1] - 2 * padding[1] + kernel_size[1] + output_padding[1]" src="../img/tex-fd675a2fc6af8db9a43fc6aee6bba673.gif" /> <img alt="W_{out} = (W_{in} - 1) * stride[2] - 2 * padding[2] + kernel_size[2] + output_padding[2]" src="../img/tex-ed1eac6d6bea1843a9838431c595dcb5.gif" /></li>
</ul>
<p>变量：</p>
<ul>
<li>是模型需要学习的变量, 形状为weight (<em>卷积网络层间连接的权重</em>,) – 卷积网络层间连接的权重, 是模型需要学习的变量, 形状为 (in_channels, out_channels, kernel_size[0], kernel_size[1], kernel_size[2])</li>
<li><code>bias (Tensor)</code> – 偏置, 是模型需要学习的变量, 形状为 (out_channels)</li>
</ul>
<p>Examples:</p>
<pre><code class="language-py">&gt;&gt;&gt; # With square kernels and equal stride
&gt;&gt;&gt; m = nn.ConvTranspose3d(16, 33, 3, stride=2)
&gt;&gt;&gt; # non-square kernels and unequal stride and with padding
&gt;&gt;&gt; m = nn.Conv3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(0, 4, 2))
&gt;&gt;&gt; input = autograd.Variable(torch.randn(20, 16, 10, 50, 100))
&gt;&gt;&gt; output = m(input)

</code></pre>
<h2 id="pooling-layers">Pooling Layers (池化层)</h2>
<h3 id="maxpool1d">MaxPool1d</h3>
<pre><code class="language-py">class torch.nn.MaxPool1d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)
</code></pre>
<p>对于多个输入通道组成的输入信号,应用一维的最大池化 <code>max pooling</code> 操作</p>
<p>最简单的例子, 如果输入大小为 <img alt="(N, C, L)" src="../img/tex-543737499dba0095d9151b4fd440b509.gif" />, 输出大小为 <img alt="(N, C, L_{out})" src="../img/tex-5b25a4bc4c225a5e291c54a4166929b8.gif" />, 该层输出值可以用下式精确计算:</p>
<p><img alt="\begin{array}{ll} out(N_i, C_j, k) = \max_{m=0}^{kernel_size-1} input(N_i, C_j, stride * k + m) \end{array}" src="../img/tex-033027842cb7efcbf0cb915d541c69c5.gif" /></p>
<p>如果 <code>padding</code> 不是0,那么在输入数据的每条边上会隐式填补对应 <code>padding</code> 数量的0值点<code>dilation</code> 用于控制内核点之间的间隔, <a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> 很好地可视化展示了 <code>dilation</code> 的功能</p>
<p>参数：</p>
<ul>
<li><code>kernel_size</code> – 最大池化操作时的窗口大小</li>
<li><code>stride</code> – 最大池化操作时窗口移动的步长, 默认值是 <code>kernel_size</code></li>
<li><code>padding</code> – 输入的每条边隐式补0的数量</li>
<li><code>dilation</code> – 用于控制窗口中元素的步长的参数</li>
<li><code>return_indices</code> – 如果等于 <code>True</code>, 在返回 max pooling 结果的同时返回最大值的索引. 这在之后的 Unpooling 时很有用</li>
<li><code>ceil_mode</code> – 如果等于 <code>True</code>, 在计算输出大小时,将采用向上取整来代替默认的向下取整的方式</li>
</ul>
<p>形状：</p>
<ul>
<li>输入：<img alt="(N, C, L_{in})" src="../img/tex-6c8971de65a57273c075bc491fa6ba0a.gif" /></li>
<li>输出：<img alt="(N, C, L_{out})" src="../img/tex-5b25a4bc4c225a5e291c54a4166929b8.gif" /> 遵从如下关系 <img alt="L_{out} = floor((L_{in} + 2 * padding - dilation * (kernel_size - 1) - 1) / stride + 1)" src="../img/tex-1ef60f8e6f123a4369ccdc2117bd3f0d.gif" /></li>
</ul>
<p>Examples:</p>
<pre><code class="language-py">&gt;&gt;&gt; # pool of size=3, stride=2
&gt;&gt;&gt; m = nn.MaxPool1d(3, stride=2)
&gt;&gt;&gt; input = autograd.Variable(torch.randn(20, 16, 50))
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="maxpool2d">MaxPool2d</h3>
<pre><code class="language-py">class torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)
</code></pre>
<p>对于多个输入通道组成的输入信号,应用二维的最大池化 <code>max pooling</code> 操作</p>
<p>最简单的例子, 如果输入大小为 <img alt="(N, C, H, W)" src="../img/tex-38d00342060234da90e0c2c5493892cb.gif" />, 输出大小为 <img alt="(N, C, H_{out}, W_{out})" src="../img/tex-403ffd9231342159e36ba660b2bf3ff3.gif" />, 池化窗口大小 <code>kernel_size</code> 为 <img alt="(kH, kW)" src="../img/tex-11acc7e0901ba5158f843445d5bbdc92.gif" /> 该层输出值可以用下式精确计算:</p>
<p><img alt="\begin{array}{ll} out(N_i, C_j, h, w) = \max_{m=0}^{kH-1} \max_{n=0}^{kW-1} input(N_i, C_j, stride[0] * h + m, stride[1] * w + n) \end{array}" src="../img/tex-573dc90f741480b5e40bf216db293982.gif" /></p>
<p>如果 <code>padding</code> 不是0, 那么在输入数据的每条边上会隐式填补对应 <code>padding</code> 数量的0值点<code>dilation</code> 用于控制内核点之间的间隔, <a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> 很好地可视化展示了 <code>dilation</code> 的功能</p>
<p>参数 <code>kernel_size</code>, <code>stride</code>, <code>padding</code>, <code>dilation</code> 可以是以下任意一种数据类型:</p>
<blockquote>
<ul>
<li>单个 <code>int</code> 类型数据 – 此时在 height 和 width 维度上将使用相同的值</li>
<li>包含两个 int 类型数据的 <code>tuple</code> 元组 – 此时第一个 <code>int</code> 数据表示 height 维度上的数值, 第二个 <code>int</code> 数据表示 width 维度上的数值</li>
</ul>
</blockquote>
<p>参数：</p>
<ul>
<li><code>kernel_size</code> – 最大池化操作时的窗口大小</li>
<li><code>stride</code> – 最大池化操作时窗口移动的步长, 默认值是 <code>kernel_size</code></li>
<li><code>padding</code> – 输入的每条边隐式补0的数量</li>
<li><code>dilation</code> – 用于控制窗口中元素的步长的参数</li>
<li><code>return_indices</code> – 如果等于 <code>True</code>, 在返回 max pooling 结果的同时返回最大值的索引 这在之后的 Unpooling 时很有用</li>
<li><code>ceil_mode</code> – 如果等于 <code>True</code>, 在计算输出大小时,将采用向上取整来代替默认的向下取整的方式</li>
</ul>
<p>形状：</p>
<ul>
<li>输入：<img alt="(N, C, H_{in}, W_{in})" src="../img/tex-d5acb55d3d2dd49b5b0d71ab7cf3b2d1.gif" /></li>
<li>输出：<img alt="(N, C, H_{out}, W_{out})" src="../img/tex-403ffd9231342159e36ba660b2bf3ff3.gif" /> 遵从如下关系 <img alt="H_{out} = floor((H_{in} + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) / stride[0] + 1)" src="../img/tex-79b3618ba1cd1e8e6e665aae1b4fc446.gif" /> <img alt="W_{out} = floor((W_{in} + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) / stride[1] + 1)" src="../img/tex-e9f44b9b5fc42bdb5991cfcd52e2dced.gif" /></li>
</ul>
<p>Examples:</p>
<pre><code class="language-py">&gt;&gt;&gt; # pool of square window of size=3, stride=2
&gt;&gt;&gt; m = nn.MaxPool2d(3, stride=2)
&gt;&gt;&gt; # pool of non-square window
&gt;&gt;&gt; m = nn.MaxPool2d((3, 2), stride=(2, 1))
&gt;&gt;&gt; input = autograd.Variable(torch.randn(20, 16, 50, 32))
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="maxpool3d">MaxPool3d</h3>
<pre><code class="language-py">class torch.nn.MaxPool3d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)
</code></pre>
<p>对于多个输入通道组成的输入信号,应用三维的最大池化 <code>max pooling</code> 操作</p>
<p>最简单的例子, 如果输入大小为 <img alt="(N, C, D, H, W)" src="../img/tex-6d9465a2eb2377437689121f4915b6b4.gif" />,输出大小为 <img alt="(N, C, D_{out}, H_{out}, W_{out})" src="../img/tex-d14df6868b2b3f7ee945a69616a0b867.gif" /> 池化窗口大小 <code>kernel_size</code> 为 <img alt="(kD, kH, kW)" src="../img/tex-b53b272ea52997eb2ccf903b6d58b4bc.gif" /> 该层输出值可以用下式精确计算:</p>
<p><img alt="\begin{array}{ll} out(N_i, C_j, d, h, w) = \max_{k=0}^{kD-1} \max_{m=0}^{kH-1} \max_{n=0}^{kW-1} input(N_i, C_j, stride[0] * k + d, stride[1] * h + m, stride[2] * w + n) \end{array}" src="../img/tex-42f8d78c4f022c0857c8561088078429.gif" /></p>
<p>如果 <code>padding</code> 不是0, 那么在输入数据的每条边上会隐式填补对应 <code>padding</code> 数量的0值点<code>dilation</code> 用于控制内核点之间的间隔, <a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> 很好地可视化展示了 <code>dilation</code> 的功能</p>
<p>参数 <code>kernel_size</code>, <code>stride</code>, <code>padding</code>, <code>dilation</code> 可以是以下任意一种数据类型:</p>
<blockquote>
<ul>
<li>单个 <code>int</code> 类型数据 – 此时在 depth, height 和 width 维度上将使用相同的值</li>
<li>包含三个 int 类型数据的 <code>tuple</code> 元组 – 此时第一个 <code>int</code> 数据表示 depth 维度上的数值, 第二个 <code>int</code> 数据表示 height 维度上的数值,第三个 <code>int</code> 数据表示 width 维度上的数值</li>
</ul>
</blockquote>
<p>参数：</p>
<ul>
<li><code>kernel_size</code> – 最大池化操作时的窗口大小</li>
<li><code>stride</code> – 最大池化操作时窗口移动的步长, 默认值是 <code>kernel_size</code></li>
<li><code>padding</code> – 输入所有三条边上隐式补0的数量</li>
<li><code>dilation</code> – 用于控制窗口中元素的步长的参数</li>
<li><code>return_indices</code> – 如果等于 <code>True</code>, 在返回 max pooling 结果的同时返回最大值的索引 这在之后的 Unpooling 时很有用</li>
<li><code>ceil_mode</code> – 如果等于 <code>True</code>, 在计算输出大小时,将采用向上取整来代替默认的向下取整的方式</li>
</ul>
<p>形状：</p>
<ul>
<li>输入：<img alt="(N, C, D_{in}, H_{in}, W_{in})" src="../img/tex-ce19deda602cf16ded15c0fb9cd5d280.gif" /></li>
<li>输出：<img alt="(N, C, D_{out}, H_{out}, W_{out})" src="../img/tex-d14df6868b2b3f7ee945a69616a0b867.gif" /> 遵从如下关系 <img alt="D_{out} = floor((D_{in} + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) / stride[0] + 1)" src="../img/tex-ba168d43ee6e937903b387d0afce9a40.gif" /> <img alt="H_{out} = floor((H_{in} + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) / stride[1] + 1)" src="../img/tex-89213fb2d0850f4cc41af72bae650bd0.gif" /> <img alt="W_{out} = floor((W_{in} + 2 * padding[2] - dilation[2] * (kernel_size[2] - 1) - 1) / stride[2] + 1)" src="../img/tex-397d0048589e3a1a6644d6613e7d4722.gif" /></li>
</ul>
<p>Examples:</p>
<pre><code class="language-py">&gt;&gt;&gt; # pool of square window of size=3, stride=2
&gt;&gt;&gt; m = nn.MaxPool3d(3, stride=2)
&gt;&gt;&gt; # pool of non-square window
&gt;&gt;&gt; m = nn.MaxPool3d((3, 2, 2), stride=(2, 1, 2))
&gt;&gt;&gt; input = autograd.Variable(torch.randn(20, 16, 50,44, 31))
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="maxunpool1d">MaxUnpool1d</h3>
<pre><code class="language-py">class torch.nn.MaxUnpool1d(kernel_size, stride=None, padding=0)
</code></pre>
<p><code>MaxPool1d</code> 的逆过程</p>
<p>要注意的是 <code>MaxPool1d</code> 并不是完全可逆的, 因为在max pooling过程中非最大值已经丢失</p>
<p><code>MaxUnpool1d</code> 以 <code>MaxPool1d</code> 的输出, 包含最大值的索引作为输入 计算max poooling的部分逆过程(对于那些最大值区域), 对于那些非最大值区域将设置为0值</p>
<p>注解：</p>
<p><cite>MaxPool1d</cite> 可以将多个输入大小映射到相同的输出大小, 因此反演过程可能会模棱两可 为适应这一点, 在调用forward函数时可以将需要的输出大小作为额外的参数 <cite>output_size</cite> 传入.</p>
<p>� 具体用法,请参阅下面的输入和示例</p>
<p>参数：</p>
<ul>
<li><code>kernel_size (int 或 tuple)</code> – 最大池化操作时的窗口大小</li>
<li><code>stride (int 或 tuple)</code> – 最大池化操作时窗口移动的步长, 默认值是 <code>kernel_size</code></li>
<li><code>padding (int 或 tuple)</code> – 输入的每条边填充0值的个数</li>
</ul>
<p>Inputs:</p>
<ul>
<li><code>input</code>: 需要转化的输入的 Tensor</li>
<li><code>indices</code>: <code>MaxPool1d</code> 提供的最大值索引</li>
<li><code>output_size</code> (可选) : <code>torch.Size</code> 类型的数据指定输出的大小</li>
</ul>
<p>形状：</p>
<ul>
<li>输入：<img alt="(N, C, H_{in})" src="../img/tex-99d3564b110d4919681501387a6ddf09.gif" /></li>
<li>输出：<img alt="(N, C, H_{out})" src="../img/tex-ec05c6d8987b027dc39dd18a863a9e03.gif" /> 遵从如下关系 <img alt="H_{out} = (H_{in} - 1) * stride[0] - 2 * padding[0] + kernel_size[0]" src="../img/tex-3e4cc86575ff480ad4c4141a89f2b470.gif" /> 或者在调用时指定输出大小 <code>output_size</code></li>
</ul>
<p>示例：</p>
<pre><code class="language-py">&gt;&gt;&gt; pool = nn.MaxPool1d(2, stride=2, return_indices=True)
&gt;&gt;&gt; unpool = nn.MaxUnpool1d(2, stride=2)
&gt;&gt;&gt; input = Variable(torch.Tensor([[[1, 2, 3, 4, 5, 6, 7, 8]]]))
&gt;&gt;&gt; output, indices = pool(input)
&gt;&gt;&gt; unpool(output, indices)
Variable containing:
(0 ,.,.) =
 0   2   0   4   0   6   0   8
[torch.FloatTensor of size 1x1x8]

&gt;&gt;&gt; # Example showcasing the use of output_size
&gt;&gt;&gt; input = Variable(torch.Tensor([[[1, 2, 3, 4, 5, 6, 7, 8, 9]]]))
&gt;&gt;&gt; output, indices = pool(input)
&gt;&gt;&gt; unpool(output, indices, output_size=input.size())
Variable containing:
(0 ,.,.) =
 0   2   0   4   0   6   0   8   0
[torch.FloatTensor of size 1x1x9]

&gt;&gt;&gt; unpool(output, indices)
Variable containing:
(0 ,.,.) =
 0   2   0   4   0   6   0   8
[torch.FloatTensor of size 1x1x8]

</code></pre>
<h3 id="maxunpool2d">MaxUnpool2d</h3>
<pre><code class="language-py">class torch.nn.MaxUnpool2d(kernel_size, stride=None, padding=0)
</code></pre>
<p><code>MaxPool2d</code> 的逆过程</p>
<p>要注意的是 <code>MaxPool2d</code> 并不是完全可逆的, 因为在max pooling过程中非最大值已经丢失</p>
<p><code>MaxUnpool2d</code> 以 <code>MaxPool2d</code> 的输出, 包含最大值的索引作为输入 计算max poooling的部分逆过程(对于那些最大值区域), 对于那些非最大值区域将设置为0值</p>
<p>注解：</p>
<p><cite>MaxPool2d</cite> 可以将多个输入大小映射到相同的输出大小, 因此反演过程可能会模棱两可. 为适应这一点, 在调用forward函数时可以将需要的输出大小作为额外的参数 <cite>output_size</cite> 传入.</p>
<p>� 具体用法,请参阅下面的输入和示例</p>
<p>参数：</p>
<ul>
<li><code>kernel_size (int 或 tuple)</code> – 最大池化操作时的窗口大小</li>
<li><code>stride (int 或 tuple)</code> – 最大池化操作时窗口移动的步长, 默认值是 <code>kernel_size</code></li>
<li><code>padding (int 或 tuple)</code> – 输入的每条边填充0值的个数</li>
</ul>
<p>Inputs:</p>
<ul>
<li><code>input</code>: 需要转化的输入的 Tensor</li>
<li><code>indices</code>: <code>MaxPool2d</code> 提供的最大值索引</li>
<li><code>output_size</code> (可选) : <code>torch.Size</code> 类型的数据指定输出的大小</li>
</ul>
<p>形状：</p>
<ul>
<li>输入：<img alt="(N, C, H_{in}, W_{in})" src="../img/tex-d5acb55d3d2dd49b5b0d71ab7cf3b2d1.gif" /></li>
<li>输出：<img alt="(N, C, H_{out}, W_{out})" src="../img/tex-403ffd9231342159e36ba660b2bf3ff3.gif" /> 遵从如下关系 <img alt="H_{out} = (H_{in} - 1) * stride[0] -2 * padding[0] + kernel_size[0]" src="../img/tex-bc6952442952352a9c45fd1615b9c8ab.gif" /> <img alt="W_{out} = (W_{in} - 1) * stride[1] -2 * padding[1] + kernel_size[1]" src="../img/tex-271dbbdd3ca44e09a3a07b7353048057.gif" /> 或者在调用时指定输出大小 <code>output_size</code></li>
</ul>
<p>示例：</p>
<pre><code class="language-py">&gt;&gt;&gt; pool = nn.MaxPool2d(2, stride=2, return_indices=True)
&gt;&gt;&gt; unpool = nn.MaxUnpool2d(2, stride=2)
&gt;&gt;&gt; input = Variable(torch.Tensor([[[[ 1,  2,  3,  4],
...                                  [ 5,  6,  7,  8],
...                                  [ 9, 10, 11, 12],
...                                  [13, 14, 15, 16]]]]))
&gt;&gt;&gt; output, indices = pool(input)
&gt;&gt;&gt; unpool(output, indices)
Variable containing:
(0 ,0 ,.,.) =
 0   0   0   0
 0   6   0   8
 0   0   0   0
 0  14   0  16
[torch.FloatTensor of size 1x1x4x4]

&gt;&gt;&gt; # specify a different output size than input size
&gt;&gt;&gt; unpool(output, indices, output_size=torch.Size([1, 1, 5, 5]))
Variable containing:
(0 ,0 ,.,.) =
 0   0   0   0   0
 6   0   8   0   0
 0   0   0  14   0
 16   0   0   0   0
 0   0   0   0   0
[torch.FloatTensor of size 1x1x5x5]

</code></pre>
<h3 id="maxunpool3d">MaxUnpool3d</h3>
<pre><code class="language-py">class torch.nn.MaxUnpool3d(kernel_size, stride=None, padding=0)
</code></pre>
<p><code>MaxPool3d</code> 的逆过程</p>
<p>要注意的是 <code>MaxPool3d</code> 并不是完全可逆的, 因为在max pooling过程中非最大值已经丢失 <code>MaxUnpool3d</code> 以 <code>MaxPool3d</code> 的输出, 包含最大值的索引作为输入 计算max poooling的部分逆过程(对于那些最大值区域), 对于那些非最大值区域将设置为0值</p>
<p>注解：</p>
<p><cite>MaxPool3d</cite> 可以将多个输入大小映射到相同的输出大小, 因此反演过程可能会模棱两可. 为适应这一点, 在调用forward函数时可以将需要的输出大小作为额外的参数 <cite>output_size</cite> 传入.</p>
<p>� 具体用法,请参阅下面的输入和示例</p>
<p>参数：</p>
<ul>
<li><code>kernel_size (int 或 tuple)</code> – 最大池化操作时的窗口大小</li>
<li><code>stride (int 或 tuple)</code> – 最大池化操作时窗口移动的步长, 默认值是 <code>kernel_size</code></li>
<li><code>padding (int 或 tuple)</code> – 输入的每条边填充0值的个数</li>
</ul>
<p>Inputs:</p>
<ul>
<li><code>input</code>: 需要转化的输入的 Tensor</li>
<li><code>indices</code>: <code>MaxPool3d</code> 提供的最大值索引</li>
<li><code>output_size</code> (可选) : <code>torch.Size</code> 类型的数据指定输出的大小</li>
</ul>
<p>形状：</p>
<ul>
<li>输入：<img alt="(N, C, D_{in}, H_{in}, W_{in})" src="../img/tex-ce19deda602cf16ded15c0fb9cd5d280.gif" /></li>
<li>输出：<img alt="(N, C, D_{out}, H_{out}, W_{out})" src="../img/tex-d14df6868b2b3f7ee945a69616a0b867.gif" /> 遵从如下关系 <img alt="D_{out} = (D_{in} - 1) * stride[0] - 2 * padding[0] + kernel_size[0]" src="../img/tex-570c38500306b160a0747f548ed0f215.gif" /> <img alt="H_{out} = (H_{in} - 1) * stride[1] - 2 * padding[1] + kernel_size[1]" src="../img/tex-f359d5e863d259e3983a5b5c33f30f38.gif" /> <img alt="W_{out} = (W_{in} - 1) * stride[2] - 2 * padding[2] + kernel_size[2]" src="../img/tex-0aaee5bf5ff41e2d03d399eead71c93e.gif" /> 或者在调用时指定输出大小 <code>output_size</code></li>
</ul>
<p>示例：</p>
<pre><code class="language-py">&gt;&gt;&gt; # pool of square window of size=3, stride=2
&gt;&gt;&gt; pool = nn.MaxPool3d(3, stride=2, return_indices=True)
&gt;&gt;&gt; unpool = nn.MaxUnpool3d(3, stride=2)
&gt;&gt;&gt; output, indices = pool(Variable(torch.randn(20, 16, 51, 33, 15)))
&gt;&gt;&gt; unpooled_output = unpool(output, indices)
&gt;&gt;&gt; unpooled_output.size()
torch.Size([20, 16, 51, 33, 15])

</code></pre>
<h3 id="avgpool1d">AvgPool1d</h3>
<pre><code class="language-py">class torch.nn.AvgPool1d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True)
</code></pre>
<p>对于多个输入通道组成的输入信号,应用一维的平均池化 <code>average pooling</code> 操作</p>
<p>最简单的例子, 如果输入大小为 <img alt="(N, C, L)" src="../img/tex-543737499dba0095d9151b4fd440b509.gif" />, 输出大小为 <img alt="(N, C, L_{out})" src="../img/tex-5b25a4bc4c225a5e291c54a4166929b8.gif" />, 池化窗口大小 <code>kernel_size</code> 为 <img alt="k" src="../img/tex-8ce4b16b22b58894aa86c421e8759df3.gif" /> 该层输出值可以用下式精确计算:</p>
<p><img alt="\begin{array}{ll} out(N_i, C_j, l) = 1 / k * \sum_{m=0}^{k} input(N_i, C_j, stride * l + m) \end{array}" src="../img/tex-5fb224489269649392e58309a75afb8b.gif" /></p>
<p>如果 <code>padding</code> 不是0, 那么在输入数据的每条边上会隐式填补对应 <code>padding</code> 数量的0值点</p>
<p>参数 <code>kernel_size</code>, <code>stride</code>, <code>padding</code> 可以为单个 <code>int</code> 类型的数据 或者是一个单元素的tuple元组</p>
<p>参数：</p>
<ul>
<li><code>kernel_size</code> – 平均池化操作时取平均值的窗口的大小</li>
<li><code>stride</code> – 平均池化操作时窗口移动的步长, 默认值是 <code>kernel_size</code></li>
<li><code>padding</code> – 输入的每条边隐式补0的数量</li>
<li><code>ceil_mode</code> – 如果等于 <code>True</code>, 在计算输出大小时,将采用向上取整来代替默认的向下取整的方式</li>
<li><code>count_include_pad</code> – 如果等于 <code>True</code>, 在计算平均池化的值时,将考虑 <code>padding</code> 填充的0</li>
</ul>
<p>形状：</p>
<ul>
<li>输入：<img alt="(N, C, L_{in})" src="../img/tex-6c8971de65a57273c075bc491fa6ba0a.gif" /></li>
<li>输出：<img alt="(N, C, L_{out})" src="../img/tex-5b25a4bc4c225a5e291c54a4166929b8.gif" /> 遵从如下关系 <img alt="L_{out} = floor((L_{in} + 2 * padding - kernel_size) / stride + 1)" src="../img/tex-ff3e003837e50249919e7bf1fa948e5c.gif" /></li>
</ul>
<p>Examples:</p>
<pre><code class="language-py">&gt;&gt;&gt; # pool with window of size=3, stride=2
&gt;&gt;&gt; m = nn.AvgPool1d(3, stride=2)
&gt;&gt;&gt; m(Variable(torch.Tensor([[[1,2,3,4,5,6,7]]])))
Variable containing:
(0 ,.,.) =
 2  4  6
[torch.FloatTensor of size 1x1x3]

</code></pre>
<h3 id="avgpool2d">AvgPool2d</h3>
<pre><code class="language-py">class torch.nn.AvgPool2d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True)
</code></pre>
<p>对于多个输入通道组成的输入信号,应用二维的平均池化 <code>average pooling</code> 操作</p>
<p>最简单的例子,如果输入大小为 <img alt="(N, C, H, W)" src="../img/tex-38d00342060234da90e0c2c5493892cb.gif" />,输出大小为 <img alt="(N, C, H_{out}, W_{out})" src="../img/tex-403ffd9231342159e36ba660b2bf3ff3.gif" />, 池化窗口大小 <code>kernel_size</code> 为 <img alt="(kH, kW)" src="../img/tex-11acc7e0901ba5158f843445d5bbdc92.gif" /> 该层输出值可以用下式精确计算:</p>
<p><img alt="\begin{array}{ll} out(N_i, C_j, h, w) = 1 / (kH * kW) * \sum_{m=0}^{kH-1} \sum_{n=0}^{kW-1} input(N_i, C_j, stride[0] * h + m, stride[1] * w + n) \end{array}" src="../img/tex-83e390b13d2c73927b15f35344142d36.gif" /></p>
<p>如果 <code>padding</code> 不是0, 那么在输入数据的每条边上会隐式填补对应 <code>padding</code> 数量的0值点</p>
<p>参数 <code>kernel_size</code>, <code>stride</code>, <code>padding</code> 可以是以下任意一种数据类型:</p>
<blockquote>
<ul>
<li>单个 <code>int</code> 类型数据 – 此时在 height 和 width 维度上将使用相同的值</li>
<li>包含两个 int 类型数据的 <code>tuple</code> 元组 – 此时第一个 <code>int</code> 数据表示 height 维度上的数值, 第二个 <code>int</code> 数据表示 width 维度上的数值</li>
</ul>
</blockquote>
<p>参数：</p>
<ul>
<li><code>kernel_size</code> – 平均池化操作时取平均值的窗口的大小</li>
<li><code>stride</code> – 平均池化操作时窗口移动的步长, 默认值是 <code>kernel_size</code></li>
<li><code>padding</code> – 输入的每条边隐式补0的数量</li>
<li><code>ceil_mode</code> – 如果等于 <code>True</code>, 在计算输出大小时,将采用向上取整来代替默认的向下取整的方式</li>
<li><code>count_include_pad</code> – 如果等于 <code>True</code>, 在计算平均池化的值时,将考虑 <code>padding</code> 填充的0</li>
</ul>
<p>形状：</p>
<ul>
<li>输入：<img alt="(N, C, H_{in}, W_{in})" src="../img/tex-d5acb55d3d2dd49b5b0d71ab7cf3b2d1.gif" /></li>
<li>输出：<img alt="(N, C, H_{out}, W_{out})" src="../img/tex-403ffd9231342159e36ba660b2bf3ff3.gif" /> 遵从如下关系 <img alt="H_{out} = floor((H_{in} + 2 * padding[0] - kernel_size[0]) / stride[0] + 1)" src="../img/tex-741bce32e0f25e0789dd133c8f7efabd.gif" /> <img alt="W_{out} = floor((W_{in} + 2 * padding[1] - kernel_size[1]) / stride[1] + 1)" src="../img/tex-e529efd000cb956703fed84faea15127.gif" /></li>
</ul>
<p>Examples:</p>
<pre><code class="language-py">&gt;&gt;&gt; # pool of square window of size=3, stride=2
&gt;&gt;&gt; m = nn.AvgPool2d(3, stride=2)
&gt;&gt;&gt; # pool of non-square window
&gt;&gt;&gt; m = nn.AvgPool2d((3, 2), stride=(2, 1))
&gt;&gt;&gt; input = autograd.Variable(torch.randn(20, 16, 50, 32))
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="avgpool3d">AvgPool3d</h3>
<pre><code class="language-py">class torch.nn.AvgPool3d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True)
</code></pre>
<p>对于多个输入通道组成的输入信号,应用三维的平均池化 <code>average pooling</code> 操作</p>
<p>最简单的例子, 如果输入大小为 <img alt="(N, C, D, H, W)" src="../img/tex-6d9465a2eb2377437689121f4915b6b4.gif" />,输出大小为 <img alt="(N, C, D_{out}, H_{out}, W_{out})" src="../img/tex-d14df6868b2b3f7ee945a69616a0b867.gif" /> 池化窗口大小 <code>kernel_size</code> 为 <img alt="(kD, kH, kW)" src="../img/tex-b53b272ea52997eb2ccf903b6d58b4bc.gif" /> 该层输出值可以用下式精确计算:</p>
<p><img alt="\begin{array}{ll} out(N_i, C_j, d, h, w) = 1 / (kD * kH * kW) * \sum_{k=0}^{kD-1} \sum_{m=0}^{kH-1} \sum_{n=0}^{kW-1} input(N_i, C_j, stride[0] * d + k, stride[1] * h + m, stride[2] * w + n) \end{array}" src="../img/tex-fbe2c38eff7c51172e8dab64682e8248.gif" /></p>
<p>如果 <code>padding</code> 不是0, 那么在输入数据的每条边上会隐式填补对应 <code>padding</code> 数量的0值点</p>
<p>参数 <code>kernel_size</code>, <code>stride</code> 可以是以下任意一种数据类型:</p>
<blockquote>
<ul>
<li>单个 <code>int</code> 类型数据 – 此时在 depth, height 和 width 维度上将使用相同的值</li>
<li>包含三个 int 类型数据的 <code>tuple</code> 元组 – 此时第一个 <code>int</code> 数据表示 depth 维度上的数值, 第二个 <code>int</code> 数据表示 height 维度上的数值,第三个 <code>int</code> 数据表示 width 维度上的数值</li>
</ul>
</blockquote>
<p>参数：</p>
<ul>
<li><code>kernel_size</code> – 平均池化操作时取平均值的窗口的大小</li>
<li><code>stride</code> – 平均池化操作时窗口移动的步长, 默认值是 <code>kernel_size</code></li>
<li><code>padding</code> – 输入的每条边隐式补0的数量</li>
<li><code>ceil_mode</code> – 如果等于 <code>True</code>, 在计算输出大小时,将采用向上取整来代替默认的向下取整的方式</li>
<li><code>count_include_pad</code> – 如果等于 <code>True</code>, 在计算平均池化的值时,将考虑 <code>padding</code> 填充的0</li>
</ul>
<p>形状：</p>
<ul>
<li>输入：<img alt="(N, C, D_{in}, H_{in}, W_{in})" src="../img/tex-ce19deda602cf16ded15c0fb9cd5d280.gif" /></li>
<li>输出：<img alt="(N, C, D_{out}, H_{out}, W_{out})" src="../img/tex-d14df6868b2b3f7ee945a69616a0b867.gif" /> 遵从如下关系 <img alt="D_{out} = floor((D_{in} + 2 * padding[0] - kernel_size[0]) / stride[0] + 1)" src="../img/tex-8b48af0f996fb14285ac4c971f261f39.gif" /> <img alt="H_{out} = floor((H_{in} + 2 * padding[1] - kernel_size[1]) / stride[1] + 1)" src="../img/tex-442ab69f414e2fff99841eec0edaa03e.gif" /> <img alt="W_{out} = floor((W_{in} + 2 * padding[2] - kernel_size[2]) / stride[2] + 1)" src="../img/tex-d5ddd94486a3777d830d45cb65de8dc5.gif" /></li>
</ul>
<p>Examples:</p>
<pre><code class="language-py">&gt;&gt;&gt; # pool of square window of size=3, stride=2
&gt;&gt;&gt; m = nn.AvgPool3d(3, stride=2)
&gt;&gt;&gt; # pool of non-square window
&gt;&gt;&gt; m = nn.AvgPool3d((3, 2, 2), stride=(2, 1, 2))
&gt;&gt;&gt; input = autograd.Variable(torch.randn(20, 16, 50,44, 31))
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="fractionalmaxpool2d">FractionalMaxPool2d</h3>
<pre><code class="language-py">class torch.nn.FractionalMaxPool2d(kernel_size, output_size=None, output_ratio=None, return_indices=False, _random_samples=None)
</code></pre>
<p>对于多个输入通道组成的输入信号,应用二维的分数最大池化 <code>fractional max pooling</code> 操作</p>
<p>分数最大池化 <code>Fractiona MaxPooling</code> 的具体细节描述,详见Ben Graham论文 <a href="http://arxiv.org/abs/1412.6071">Fractional MaxPooling</a></p>
<p>由目标输出大小确定随机步长,在 kH x kW 区域内进行最大池化的操作 输出特征的数量与输入通道的数量相同</p>
<p>参数：</p>
<ul>
<li><code>kernel_size</code> – 最大池化操作时窗口的大小. 可以是单个数字 k (等价于 k x k 的正方形窗口) 或者是 一个元组 tuple (kh x kw)</li>
<li><code>output_size</code> – oH x oW 形式的输出图像的尺寸. 可以用 一个 tuple 元组 (oH, oW) 表示 oH x oW 的输出尺寸, 或者是单个的数字 oH 表示 oH x oH 的输出尺寸</li>
<li><code>output_ratio</code> – 如果想用输入图像的百分比来指定输出图像的大小,可选用该选项. 使用范围在 (0,1) 之间的一个值来指定.</li>
<li><code>return_indices</code> – 如果等于 <code>True</code>,在返回输出结果的同时返回最大值的索引,该索引对 nn.MaxUnpool2d 有用. 默认情况下该值等于 <code>False</code></li>
</ul>
<p>示例：</p>
<pre><code class="language-py">&gt;&gt;&gt; # pool of square window of size=3, and target output size 13x12
&gt;&gt;&gt; m = nn.FractionalMaxPool2d(3, output_size=(13, 12))
&gt;&gt;&gt; # pool of square window and target output size being half of input image size
&gt;&gt;&gt; m = nn.FractionalMaxPool2d(3, output_ratio=(0.5, 0.5))
&gt;&gt;&gt; input = autograd.Variable(torch.randn(20, 16, 50, 32))
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="lppool2d">LPPool2d</h3>
<pre><code class="language-py">class torch.nn.LPPool2d(norm_type, kernel_size, stride=None, ceil_mode=False)
</code></pre>
<p>对于多个输入通道组成的输入信号,应用二维的幂平均池化 <code>power-average pooling</code> 操作</p>
<p>在每个窗口内, 输出的计算方式: <img alt="f(X) = pow(sum(pow(X, p)), 1/p)" src="../img/tex-c9c65ffb1f46231549f42a241f22e52d.gif" /></p>
<blockquote>
<ul>
<li>当 p 无穷大时,等价于最大池化 <code>Max Pooling</code> 操作</li>
<li>当 <code>p=1</code> 时, 等价于平均池化 <code>Average Pooling</code> 操作</li>
</ul>
</blockquote>
<p>参数 <code>kernel_size</code>, <code>stride</code> 可以是以下任意一种数据类型:</p>
<blockquote>
<ul>
<li>单个 <code>int</code> 类型数据 – 此时在height和width维度上将使用相同的值</li>
<li>包含两个 int 类型数据的 <code>tuple</code> 元组 – 此时第一个 <code>int</code> 数据表示 height 维度上的数值, 第二个 <code>int</code> 数据表示 width 维度上的数值</li>
</ul>
</blockquote>
<p>参数：</p>
<ul>
<li><code>kernel_size</code> – 幂平均池化时窗口的大小</li>
<li><code>stride</code> – 幂平均池化操作时窗口移动的步长, 默认值是 <code>kernel_size</code></li>
<li><code>ceil_mode</code> – 如果等于 <code>True</code>, 在计算输出大小时,将采用向上取整来代替默认的向下取整的方式</li>
</ul>
<p>形状：</p>
<ul>
<li>输入：<img alt="(N, C, H_{in}, W_{in})" src="../img/tex-d5acb55d3d2dd49b5b0d71ab7cf3b2d1.gif" /></li>
<li>输出：<img alt="(N, C, H_{out}, W_{out})" src="../img/tex-403ffd9231342159e36ba660b2bf3ff3.gif" /> 遵从如下关系 <img alt="H_{out} = floor((H_{in} + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) / stride[0] + 1)" src="../img/tex-79b3618ba1cd1e8e6e665aae1b4fc446.gif" /> <img alt="W_{out} = floor((W_{in} + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) / stride[1] + 1)" src="../img/tex-e9f44b9b5fc42bdb5991cfcd52e2dced.gif" /></li>
</ul>
<p>Examples:</p>
<pre><code class="language-py">&gt;&gt;&gt; # power-2 pool of square window of size=3, stride=2
&gt;&gt;&gt; m = nn.LPPool2d(2, 3, stride=2)
&gt;&gt;&gt; # pool of non-square window of power 1.2
&gt;&gt;&gt; m = nn.LPPool2d(1.2, (3, 2), stride=(2, 1))
&gt;&gt;&gt; input = autograd.Variable(torch.randn(20, 16, 50, 32))
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="adaptivemaxpool1d">AdaptiveMaxPool1d</h3>
<pre><code class="language-py">class torch.nn.AdaptiveMaxPool1d(output_size, return_indices=False)
</code></pre>
<p>对于多个输入通道组成的输入信号,应用一维的自适应最大池化 <code>adaptive max pooling</code> 操作</p>
<p>对于任意大小的输入,可以指定输出的尺寸为 H 输出特征的数量与输入通道的数量相同.</p>
<p>参数：</p>
<ul>
<li><code>output_size</code> – 目标输出的尺寸 H</li>
<li><code>return_indices</code> – 如果等于 <code>True</code>,在返回输出结果的同时返回最大值的索引,该索引对 nn.MaxUnpool1d 有用. 默认情况下该值等于 <code>False</code></li>
</ul>
<p>示例：</p>
<pre><code class="language-py">&gt;&gt;&gt; # target output size of 5
&gt;&gt;&gt; m = nn.AdaptiveMaxPool1d(5)
&gt;&gt;&gt; input = autograd.Variable(torch.randn(1, 64, 8))
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="adaptivemaxpool2d">AdaptiveMaxPool2d</h3>
<pre><code class="language-py">class torch.nn.AdaptiveMaxPool2d(output_size, return_indices=False)
</code></pre>
<p>对于多个输入通道组成的输入信号,应用二维的自适应最大池化 <code>adaptive max pooling</code> 操作</p>
<p>对于任意大小的输入,可以指定输出的尺寸为 H x W 输出特征的数量与输入通道的数量相同.</p>
<p>参数：</p>
<ul>
<li><code>output_size</code> – H x W 形式的输出图像的尺寸. 可以用 一个 tuple 元组 (H, W) 表示 H x W 的输出尺寸, 或者是单个的数字 H 表示 H x H 的输出尺寸</li>
<li><code>return_indices</code> – 如果等于 <code>True</code>,在返回输出结果的同时返回最大值的索引,该索引对 nn.MaxUnpool2d 有用. 默认情况下该值等于 <code>False</code></li>
</ul>
<p>示例：</p>
<pre><code class="language-py">&gt;&gt;&gt; # target output size of 5x7
&gt;&gt;&gt; m = nn.AdaptiveMaxPool2d((5,7))
&gt;&gt;&gt; input = autograd.Variable(torch.randn(1, 64, 8, 9))
&gt;&gt;&gt; output = m(input)
&gt;&gt;&gt; # target output size of 7x7 (square)
&gt;&gt;&gt; m = nn.AdaptiveMaxPool2d(7)
&gt;&gt;&gt; input = autograd.Variable(torch.randn(1, 64, 10, 9))
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="adaptivemaxpool3d">AdaptiveMaxPool3d</h3>
<pre><code class="language-py">class torch.nn.AdaptiveMaxPool3d(output_size, return_indices=False)
</code></pre>
<p>对于多个输入通道组成的输入信号,应用三维的自适应最大池化 <code>adaptive max pooling</code> 操作</p>
<p>对于任意大小的输入,可以指定输出的尺寸为 D x H x W 输出特征的数量与输入通道的数量相同.</p>
<p>参数：</p>
<ul>
<li><code>output_size</code> – D x H x W 形式的输出图像的尺寸. 可以用 一个 tuple 元组 (D, H, W) 表示 D x H x W 的输出尺寸, 或者是单个的数字 D 表示 D x D x D 的输出尺寸</li>
<li><code>return_indices</code> – 如果等于 <code>True</code>,在返回输出结果的同时返回最大值的索引,该索引对 nn.MaxUnpool3d 有用. 默认情况下该值等于 <code>False</code></li>
</ul>
<p>示例：</p>
<pre><code class="language-py">&gt;&gt;&gt; # target output size of 5x7x9
&gt;&gt;&gt; m = nn.AdaptiveMaxPool3d((5,7,9))
&gt;&gt;&gt; input = autograd.Variable(torch.randn(1, 64, 8, 9, 10))
&gt;&gt;&gt; output = m(input)
&gt;&gt;&gt; # target output size of 7x7x7 (cube)
&gt;&gt;&gt; m = nn.AdaptiveMaxPool3d(7)
&gt;&gt;&gt; input = autograd.Variable(torch.randn(1, 64, 10, 9, 8))
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="adaptiveavgpool1d">AdaptiveAvgPool1d</h3>
<pre><code class="language-py">class torch.nn.AdaptiveAvgPool1d(output_size)
</code></pre>
<p>对于多个输入通道组成的输入信号,应用一维的自适应平均池化 <code>adaptive average pooling</code> 操作</p>
<p>对于任意大小的输入,可以指定输出的尺寸为 H 输出特征的数量与输入通道的数量相同.</p>
<p>参数：output_size – 目标输出的尺寸 H</p>
<p>示例：</p>
<pre><code class="language-py">&gt;&gt;&gt; # target output size of 5
&gt;&gt;&gt; m = nn.AdaptiveAvgPool1d(5)
&gt;&gt;&gt; input = autograd.Variable(torch.randn(1, 64, 8))
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="adaptiveavgpool2d">AdaptiveAvgPool2d</h3>
<pre><code class="language-py">class torch.nn.AdaptiveAvgPool2d(output_size)
</code></pre>
<p>对于多个输入通道组成的输入信号,应用二维的自适应平均池化 <code>adaptive average pooling</code> 操作</p>
<p>对于任意大小的输入,可以指定输出的尺寸为 H x W 输出特征的数量与输入通道的数量相同.</p>
<p>参数：output_size – H x W 形式的输出图像的尺寸. 可以用 一个 tuple 元组 (H, W) 表示 H x W 的输出尺寸, 或者是单个的数字 H 表示 H x H 的输出尺寸</p>
<p>示例：</p>
<pre><code class="language-py">&gt;&gt;&gt; # target output size of 5x7
&gt;&gt;&gt; m = nn.AdaptiveAvgPool2d((5,7))
&gt;&gt;&gt; input = autograd.Variable(torch.randn(1, 64, 8, 9))
&gt;&gt;&gt; output = m(input)
&gt;&gt;&gt; # target output size of 7x7 (square)
&gt;&gt;&gt; m = nn.AdaptiveAvgPool2d(7)
&gt;&gt;&gt; input = autograd.Variable(torch.randn(1, 64, 10, 9))
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="adaptiveavgpool3d">AdaptiveAvgPool3d</h3>
<pre><code class="language-py">class torch.nn.AdaptiveAvgPool3d(output_size)
</code></pre>
<p>对于多个输入通道组成的输入信号,应用三维的自适应平均池化 <code>adaptive average pooling</code> 操作</p>
<p>对于任意大小的输入,可以指定输出的尺寸为 D x H x W 输出特征的数量与输入通道的数量相同.</p>
<p>参数：output_size – D x H x W 形式的输出图像的尺寸. 可以用 一个 tuple 元组 (D, H, W) 表示 D x H x W 的输出尺寸, 或者是单个的数字 D 表示 D x D x D 的输出尺寸</p>
<p>示例：</p>
<pre><code class="language-py">&gt;&gt;&gt; # target output size of 5x7x9
&gt;&gt;&gt; m = nn.AdaptiveAvgPool3d((5,7,9))
&gt;&gt;&gt; input = autograd.Variable(torch.randn(1, 64, 8, 9, 10))
&gt;&gt;&gt; output = m(input)
&gt;&gt;&gt; # target output size of 7x7x7 (cube)
&gt;&gt;&gt; m = nn.AdaptiveAvgPool3d(7)
&gt;&gt;&gt; input = autograd.Variable(torch.randn(1, 64, 10, 9, 8))
&gt;&gt;&gt; output = m(input)

</code></pre>
<h2 id="padding-layers">Padding Layers (填充层)</h2>
<h3 id="reflectionpad2d">ReflectionPad2d</h3>
<pre><code class="language-py">class torch.nn.ReflectionPad2d(padding)
</code></pre>
<p>使用输入边界的反射填充输入张量.</p>
<p>参数：</p>
<ul>
<li><code>padding (int, tuple)</code> – 填充的大小. 如果是int, 则在所有边界填充使用相同的.</li>
<li><code>则使用 (_如果是4个元组_,)</code> –</li>
</ul>
<p>形状：</p>
<ul>
<li>输入：<img alt="(N, C, H_{in}, W_{in})" src="../img/tex-d5acb55d3d2dd49b5b0d71ab7cf3b2d1.gif" /></li>
<li>输出：<img alt="(N, C, H_{out}, W_{out})" src="../img/tex-403ffd9231342159e36ba660b2bf3ff3.gif" /> where <img alt="H_{out} = H_{in} + paddingTop + paddingBottom" src="../img/tex-aadeb8a243fd73b11a90d3e81647b9ce.gif" /> <img alt="W_{out} = W_{in} + paddingLeft + paddingRight" src="../img/tex-b59826cca9d58f84da90f697b0482901.gif" /></li>
</ul>
<p>示例：</p>
<pre><code class="language-py">&gt;&gt;&gt; m = nn.ReflectionPad2d(3)
&gt;&gt;&gt; input = autograd.Variable(torch.randn(16, 3, 320, 480))
&gt;&gt;&gt; output = m(input)
&gt;&gt;&gt; # 使用不同的填充
&gt;&gt;&gt; m = nn.ReflectionPad2d((3, 3, 6, 6))
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="replicationpad2d">ReplicationPad2d</h3>
<pre><code class="language-py">class torch.nn.ReplicationPad2d(padding)
</code></pre>
<p>使用输入边界的复制填充输入张量.</p>
<p>参数：<code>padding (int, tuple)</code> – 填充的大小. 如果是int, 则在所有边界使用相同的填充. 如果是4个元组, 则使用(paddingLeft, paddingRight, paddingTop, paddingBottom)</p>
<p>形状：</p>
<ul>
<li>输入：<img alt="(N, C, H_{in}, W_{in})" src="../img/tex-d5acb55d3d2dd49b5b0d71ab7cf3b2d1.gif" /></li>
<li>输出：<img alt="(N, C, H_{out}, W_{out})" src="../img/tex-403ffd9231342159e36ba660b2bf3ff3.gif" /> where <img alt="H_{out} = H_{in} + paddingTop + paddingBottom" src="../img/tex-aadeb8a243fd73b11a90d3e81647b9ce.gif" /> <img alt="W_{out} = W_{in} + paddingLeft + paddingRight" src="../img/tex-b59826cca9d58f84da90f697b0482901.gif" /></li>
</ul>
<p>示例：</p>
<pre><code class="language-py">&gt;&gt;&gt; m = nn.ReplicationPad2d(3)
&gt;&gt;&gt; input = autograd.Variable(torch.randn(16, 3, 320, 480))
&gt;&gt;&gt; output = m(input)
&gt;&gt;&gt; # 使用不同的填充
&gt;&gt;&gt; m = nn.ReplicationPad2d((3, 3, 6, 6))
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="replicationpad3d">ReplicationPad3d</h3>
<pre><code class="language-py">class torch.nn.ReplicationPad3d(padding)
</code></pre>
<p>使用输入边界的复制填充输入张量.</p>
<p>参数：</p>
<ul>
<li><code>padding (int, tuple)</code> – 填充的大小. 如果是int, 则在所有边界使用相同的填充.</li>
<li><code>则使用 (paddingLeft, paddingRight, (_如果是四个元组_,)</code> –</li>
<li>paddingBottom, paddingFront, paddingBack) (<em>paddingTop</em>,) –</li>
</ul>
<p>形状：</p>
<ul>
<li>输入：<img alt="(N, C, D_{in}, H_{in}, W_{in})" src="../img/tex-ce19deda602cf16ded15c0fb9cd5d280.gif" /></li>
<li>输出：<img alt="(N, C, D_{out}, H_{out}, W_{out})" src="../img/tex-d14df6868b2b3f7ee945a69616a0b867.gif" /> where <img alt="D_{out} = D_{in} + paddingFront + paddingBack" src="../img/tex-1c73dcb800c79e8783388e4bd8318e9b.gif" /> <img alt="H_{out} = H_{in} + paddingTop + paddingBottom" src="../img/tex-aadeb8a243fd73b11a90d3e81647b9ce.gif" /> <img alt="W_{out} = W_{in} + paddingLeft + paddingRight" src="../img/tex-b59826cca9d58f84da90f697b0482901.gif" /></li>
</ul>
<p>示例：</p>
<pre><code class="language-py">&gt;&gt;&gt; m = nn.ReplicationPad3d(3)
&gt;&gt;&gt; input = autograd.Variable(torch.randn(16, 3, 8, 320, 480))
&gt;&gt;&gt; output = m(input)
&gt;&gt;&gt; # 使用不同的填充
&gt;&gt;&gt; m = nn.ReplicationPad3d((3, 3, 6, 6, 1, 1))
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="zeropad2d">ZeroPad2d</h3>
<pre><code class="language-py">class torch.nn.ZeroPad2d(padding)
</code></pre>
<p>用零填充输入张量边界.</p>
<p>参数：</p>
<ul>
<li><code>padding (int, tuple)</code> – 填充的大小. 如果是int, 则在所有边界使用相同的填充.</li>
</ul>
<p>形状：</p>
<ul>
<li>输入：<img alt="(N, C, H_{in}, W_{in})" src="../img/tex-d5acb55d3d2dd49b5b0d71ab7cf3b2d1.gif" /></li>
<li>输出：<img alt="(N, C, H_{out}, W_{out})" src="../img/tex-403ffd9231342159e36ba660b2bf3ff3.gif" /> where <img alt="H_{out} = H_{in} + paddingTop + paddingBottom" src="../img/tex-aadeb8a243fd73b11a90d3e81647b9ce.gif" /> <img alt="W_{out} = W_{in} + paddingLeft + paddingRight" src="../img/tex-b59826cca9d58f84da90f697b0482901.gif" /></li>
</ul>
<p>示例：</p>
<pre><code class="language-py">&gt;&gt;&gt; m = nn.ZeroPad2d(3)
&gt;&gt;&gt; input = autograd.Variable(torch.randn(16, 3, 320, 480))
&gt;&gt;&gt; output = m(input)
&gt;&gt;&gt; # 使用不同的填充
&gt;&gt;&gt; m = nn.ZeroPad2d((3, 3, 6, 6))
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="constantpad2d">ConstantPad2d</h3>
<pre><code class="language-py">class torch.nn.ConstantPad2d(padding, value)
</code></pre>
<p>用一个常数值填充输入张量边界.</p>
<p>对于 Nd-padding, 使用 nn.functional.pad().</p>
<p>参数：</p>
<ul>
<li><code>padding (int, tuple)</code> – 填充的大小. 如果是int, 则在所有边界使用相同的填充.</li>
<li><code>value</code> –</li>
</ul>
<p>形状：</p>
<ul>
<li>输入：<img alt="(N, C, H_{in}, W_{in})" src="../img/tex-d5acb55d3d2dd49b5b0d71ab7cf3b2d1.gif" /></li>
<li>输出：<img alt="(N, C, H_{out}, W_{out})" src="../img/tex-403ffd9231342159e36ba660b2bf3ff3.gif" /> where <img alt="H_{out} = H_{in} + paddingTop + paddingBottom" src="../img/tex-aadeb8a243fd73b11a90d3e81647b9ce.gif" /> <img alt="W_{out} = W_{in} + paddingLeft + paddingRight" src="../img/tex-b59826cca9d58f84da90f697b0482901.gif" /></li>
</ul>
<p>示例：</p>
<pre><code class="language-py">&gt;&gt;&gt; m = nn.ConstantPad2d(3, 3.5)
&gt;&gt;&gt; input = autograd.Variable(torch.randn(16, 3, 320, 480))
&gt;&gt;&gt; output = m(input)
&gt;&gt;&gt; # 使用不同的填充
&gt;&gt;&gt; m = nn.ConstantPad2d((3, 3, 6, 6), 3.5)
&gt;&gt;&gt; output = m(input)

</code></pre>
<h2 id="non-linear-activations">Non-linear Activations (非线性层)</h2>
<h3 id="relu">ReLU</h3>
<pre><code class="language-py">class torch.nn.ReLU(inplace=False)
</code></pre>
<p>对输入运用修正线性单元函数 <img alt="{ReLU}(x)= max(0, x)" src="../img/tex-98df823380c3c79a6f2651016f4e2d04.gif" /></p>
<p>参数：inplace – 选择是否进行覆盖运算 Default: <code>False</code></p>
<p>形状：</p>
<ul>
<li>输入：<img alt="(N, *)" src="../img/tex-b0400390724111bb8254d8d21c6a9f0d.gif" /> <code>*</code> 代表任意数目附加维度</li>
<li>输出：<img alt="(N, *)" src="../img/tex-b0400390724111bb8254d8d21c6a9f0d.gif" />, 与输入拥有同样的 shape 属性</li>
</ul>
<p>Examples:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = nn.ReLU()
&gt;&gt;&gt; input = autograd.Variable(torch.randn(2))
&gt;&gt;&gt; print(input)
&gt;&gt;&gt; print(m(input))

</code></pre>
<h3 id="relu6">ReLU6</h3>
<pre><code class="language-py">class torch.nn.ReLU6(inplace=False)
</code></pre>
<p>对输入的每一个元素运用函数 <img alt="{ReLU6}(x) = min(max(0,x), 6)" src="../img/tex-5c301d6fee27c04949bb9a71a985c404.gif" /></p>
<p>参数：inplace – 选择是否进行覆盖运算 默认值: <code>False</code></p>
<p>形状：</p>
<ul>
<li>输入：<img alt="(N, *)" src="../img/tex-b0400390724111bb8254d8d21c6a9f0d.gif" />, <code>*</code> 代表任意数目附加维度</li>
<li>输出：<img alt="(N, *)" src="../img/tex-b0400390724111bb8254d8d21c6a9f0d.gif" />, 与输入拥有同样的 shape 属性</li>
</ul>
<p>Examples:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = nn.ReLU6()
&gt;&gt;&gt; input = autograd.Variable(torch.randn(2))
&gt;&gt;&gt; print(input)
&gt;&gt;&gt; print(m(input))

</code></pre>
<h3 id="elu">ELU</h3>
<pre><code class="language-py">class torch.nn.ELU(alpha=1.0, inplace=False)
</code></pre>
<p>对输入的每一个元素运用函数, <img alt="f(x) = max(0,x) + min(0, alpha * (exp(x) - 1))" src="../img/tex-41cc8dcdf01c5ef5ba2584557d879fa6.gif" /></p>
<p>参数：</p>
<ul>
<li><code>alpha</code> – ELU 定义公式中的 alpha 值. 默认值: 1.0</li>
<li><code>inplace</code> – 选择是否进行覆盖运算 默认值: <code>False</code></li>
</ul>
<p>形状：</p>
<ul>
<li>输入：<img alt="(N, *)" src="../img/tex-b0400390724111bb8254d8d21c6a9f0d.gif" /> <code>*</code> 代表任意数目附加维度</li>
<li>输出：<img alt="(N, *)" src="../img/tex-b0400390724111bb8254d8d21c6a9f0d.gif" />, 与输入拥有同样的 shape 属性</li>
</ul>
<p>Examples:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = nn.ELU()
&gt;&gt;&gt; input = autograd.Variable(torch.randn(2))
&gt;&gt;&gt; print(input)
&gt;&gt;&gt; print(m(input))

</code></pre>
<h3 id="selu">SELU</h3>
<pre><code class="language-py">class torch.nn.SELU(inplace=False)
</code></pre>
<p>对输入的每一个元素运用函数, <img alt="f(x) = scale * (\max(0,x) + \min(0, alpha * (\exp(x) - 1)))" src="../img/tex-5b47266764559d96f29472d6fa7549cf.gif" />, <code>alpha=1.6732632423543772848170429916717</code>, <code>scale=1.0507009873554804934193349852946</code>.</p>
<p>更多地细节可以参阅论文 <a href="https://arxiv.org/abs/1706.02515">Self-Normalizing Neural Networks</a> .</p>
<p>参数：<code>inplace (bool, 可选)</code> – 选择是否进行覆盖运算. 默认值: <code>False</code></p>
<p>形状：</p>
<ul>
<li>输入：<img alt="(N, *)" src="../img/tex-b0400390724111bb8254d8d21c6a9f0d.gif" /> where <code>*</code> means, any number of additional dimensions</li>
<li>输出：<img alt="(N, *)" src="../img/tex-b0400390724111bb8254d8d21c6a9f0d.gif" />, same shape as the input</li>
</ul>
<p>Examples:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = nn.SELU()
&gt;&gt;&gt; input = autograd.Variable(torch.randn(2))
&gt;&gt;&gt; print(input)
&gt;&gt;&gt; print(m(input))

</code></pre>
<h3 id="prelu">PReLU</h3>
<pre><code class="language-py">class torch.nn.PReLU(num_parameters=1, init=0.25)
</code></pre>
<p>对输入的每一个元素运用函数 <img alt="PReLU(x) = max(0,x) + a * min(0,x)" src="../img/tex-1fdf83b42b80c315119cb53c14c86985.gif" /> 这里的 “a” 是自学习的参数. 当不带参数地调用时, nn.PReLU() 在所有输入通道中使用单个参数 “a” . 而如果用 nn.PReLU(nChannels) 调用, “a” 将应用到每个输入.</p>
<p>注解：</p>
<p>当为了表现更佳的模型而学习参数 “a” 时不要使用权重衰减 (weight decay)</p>
<p>参数：</p>
<ul>
<li><code>num_parameters</code> – 需要学习的 “a” 的个数. 默认等于1</li>
<li><code>init</code> – “a” 的初始值. 默认等于0.25</li>
</ul>
<p>形状：</p>
<ul>
<li>输入：<img alt="(N, *)" src="../img/tex-b0400390724111bb8254d8d21c6a9f0d.gif" /> 其中 <code>*</code> 代表任意数目的附加维度</li>
<li>输出：<img alt="(N, *)" src="../img/tex-b0400390724111bb8254d8d21c6a9f0d.gif" />, 和输入的格式 shape 一致</li>
</ul>
<p>例:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = nn.PReLU()
&gt;&gt;&gt; input = autograd.Variable(torch.randn(2))
&gt;&gt;&gt; print(input)
&gt;&gt;&gt; print(m(input))

</code></pre>
<h3 id="leakyrelu">LeakyReLU</h3>
<pre><code class="language-py">class torch.nn.LeakyReLU(negative_slope=0.01, inplace=False)
</code></pre>
<p>对输入的每一个元素运用, <img alt="f(x) = max(0, x) + {negative_slope} * min(0, x)" src="../img/tex-82e596913d879999b551a54f1e7b1d62.gif" /></p>
<p>参数：</p>
<ul>
<li><code>negative_slope</code> – 控制负斜率的角度, 默认值: 1e-2</li>
<li><code>inplace</code> – 选择是否进行覆盖运算 默认值: <code>False</code></li>
</ul>
<p>形状：</p>
<ul>
<li>输入：<img alt="(N, *)" src="../img/tex-b0400390724111bb8254d8d21c6a9f0d.gif" /> 其中 <code>*</code> 代表任意数目的附加维度</li>
<li>输出：<img alt="(N, *)" src="../img/tex-b0400390724111bb8254d8d21c6a9f0d.gif" />, 和输入的格式shape一致</li>
</ul>
<p>例:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = nn.LeakyReLU(0.1)
&gt;&gt;&gt; input = autograd.Variable(torch.randn(2))
&gt;&gt;&gt; print(input)
&gt;&gt;&gt; print(m(input))

</code></pre>
<h3 id="threshold">Threshold</h3>
<pre><code class="language-py">class torch.nn.Threshold(threshold, value, inplace=False)
</code></pre>
<p>基于 Tensor 中的每个元素创造阈值函数</p>
<p>Threshold 被定义为</p>
<pre><code class="language-py">y =  x        if x &gt;  threshold
     value    if x &lt;= threshold

</code></pre>
<p>参数：</p>
<ul>
<li><code>threshold</code> – 阈值</li>
<li><code>value</code> – 输入值小于阈值则会被 value 代替</li>
<li><code>inplace</code> – 选择是否进行覆盖运算. 默认值: <code>False</code></li>
</ul>
<p>形状：</p>
<ul>
<li>输入：<img alt="(N, *)" src="../img/tex-b0400390724111bb8254d8d21c6a9f0d.gif" /> 其中 <code>*</code> 代表任意数目的附加维度</li>
<li>输出：<img alt="(N, *)" src="../img/tex-b0400390724111bb8254d8d21c6a9f0d.gif" />, 和输入的格式 shape 一致</li>
</ul>
<p>例:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = nn.Threshold(0.1, 20)
&gt;&gt;&gt; input = Variable(torch.randn(2))
&gt;&gt;&gt; print(input)
&gt;&gt;&gt; print(m(input))

</code></pre>
<h3 id="hardtanh">Hardtanh</h3>
<pre><code class="language-py">class torch.nn.Hardtanh(min_val=-1, max_val=1, inplace=False, min_value=None, max_value=None)
</code></pre>
<p>对输入的每一个元素运用 HardTanh</p>
<p>HardTanh 被定义为:</p>
<pre><code class="language-py">f(x) = +1, if x  &gt;  1
f(x) = -1, if x  &lt; -1
f(x) =  x,  otherwise

</code></pre>
<p>线性区域的范围 <img alt="[-1, 1]" src="../img/tex-7dec1d46e68831c4eca28b020fcb1604.gif" /> 可以被调整</p>
<p>参数：</p>
<ul>
<li><code>min_val</code> – 线性区域范围最小值. 默认值: -1</li>
<li><code>max_val</code> – 线性区域范围最大值. 默认值: 1</li>
<li><code>inplace</code> – 选择是否进行覆盖运算. 默认值: <code>False</code></li>
</ul>
<p>关键字参数 <code>min_value</code> 以及 <code>max_value</code> 已被弃用. 更改为 <code>min_val</code> 和 <code>max_val</code></p>
<p>形状：</p>
<ul>
<li>输入：<img alt="(N, *)" src="../img/tex-b0400390724111bb8254d8d21c6a9f0d.gif" /> 其中 <code>*</code> 代表任意维度组合</li>
<li>输出：<img alt="(N, *)" src="../img/tex-b0400390724111bb8254d8d21c6a9f0d.gif" />, 与输入有相同的 shape 属性</li>
</ul>
<p>例</p>
<pre><code class="language-py">&gt;&gt;&gt; m = nn.Hardtanh(-2, 2)
&gt;&gt;&gt; input = autograd.Variable(torch.randn(2))
&gt;&gt;&gt; print(input)
&gt;&gt;&gt; print(m(input))

</code></pre>
<h3 id="sigmoid">Sigmoid</h3>
<pre><code class="language-py">class torch.nn.Sigmoid
</code></pre>
<p>对每个元素运用 Sigmoid 函数. Sigmoid 定义如下 <img alt="f(x) = 1 / ( 1 + exp(-x))" src="../img/tex-8d2c9846ab7f8067f6c3890e303e2a58.gif" /></p>
<p>形状：</p>
<ul>
<li>输入：<img alt="(N, *)" src="../img/tex-b0400390724111bb8254d8d21c6a9f0d.gif" /> <code>*</code> 表示任意维度组合</li>
<li>输出：<img alt="(N, *)" src="../img/tex-b0400390724111bb8254d8d21c6a9f0d.gif" />, 与输入有相同的 shape 属性</li>
</ul>
<p>Examples:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = nn.Sigmoid()
&gt;&gt;&gt; input = autograd.Variable(torch.randn(2))
&gt;&gt;&gt; print(input)
&gt;&gt;&gt; print(m(input))

</code></pre>
<h3 id="tanh">Tanh</h3>
<pre><code class="language-py">class torch.nn.Tanh
</code></pre>
<p>对输入的每个元素, <img alt="f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))" src="../img/tex-2e45b4a6182e5220661502b8945d0023.gif" /></p>
<p>形状：</p>
<ul>
<li>输入：<img alt="(N, *)" src="../img/tex-b0400390724111bb8254d8d21c6a9f0d.gif" /> <code>*</code> 表示任意维度组合</li>
<li>输出：<img alt="(N, *)" src="../img/tex-b0400390724111bb8254d8d21c6a9f0d.gif" />, 与输入有相同的 shape 属性</li>
</ul>
<p>Examples:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = nn.Tanh()
&gt;&gt;&gt; input = autograd.Variable(torch.randn(2))
&gt;&gt;&gt; print(input)
&gt;&gt;&gt; print(m(input))

</code></pre>
<h3 id="logsigmoid">LogSigmoid</h3>
<pre><code class="language-py">class torch.nn.LogSigmoid
</code></pre>
<p>对输入的每一个元素运用函数 <img alt="LogSigmoid(x) = log( 1 / (1 + exp(-x_i)))" src="../img/tex-d50941aafbaeba290ac473d7bad1177a.gif" /></p>
<p>形状：</p>
<ul>
<li>输入：<img alt="(N, *)" src="../img/tex-b0400390724111bb8254d8d21c6a9f0d.gif" /> 其中 <code>*</code> 代表任意数目的附加维度</li>
<li>输出：<img alt="(N, *)" src="../img/tex-b0400390724111bb8254d8d21c6a9f0d.gif" />, 和输入的格式shape一致</li>
</ul>
<p>例:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = nn.LogSigmoid()
&gt;&gt;&gt; input = autograd.Variable(torch.randn(2))
&gt;&gt;&gt; print(input)
&gt;&gt;&gt; print(m(input))

</code></pre>
<h3 id="softplus">Softplus</h3>
<pre><code class="language-py">class torch.nn.Softplus(beta=1, threshold=20)
</code></pre>
<p>对每个元素运用Softplus函数, Softplus 定义如下 :: <img alt="f(x) = 1/beta * log(1 + exp(beta * x_i))" src="../img/tex-6fe0507610989a3ed06095e1c518ba44.gif" /></p>
<p>Softplus 函数是ReLU函数的平滑逼近. Softplus 函数可以使得输出值限定为正数.</p>
<p>为了保证数值稳定性. 线性函数的转换可以使输出大于某个值.</p>
<p>参数：</p>
<ul>
<li><code>beta</code> – Softplus 公式中的 beta 值. 默认值: 1</li>
<li><code>threshold</code> – 阈值. 当输入到该值以上时我们的SoftPlus实现将还原为线性函数. 默认值: 20</li>
</ul>
<p>形状：</p>
<ul>
<li>输入：<img alt="(N, *)" src="../img/tex-b0400390724111bb8254d8d21c6a9f0d.gif" /> 其中 <code>*</code> 代表任意数目的附加维度 dimensions</li>
<li>输出：<img alt="(N, *)" src="../img/tex-b0400390724111bb8254d8d21c6a9f0d.gif" />, 和输入的格式shape一致</li>
</ul>
<p>例:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = nn.Softplus()
&gt;&gt;&gt; input = autograd.Variable(torch.randn(2))
&gt;&gt;&gt; print(input)
&gt;&gt;&gt; print(m(input))

</code></pre>
<h3 id="softshrink">Softshrink</h3>
<pre><code class="language-py">class torch.nn.Softshrink(lambd=0.5)
</code></pre>
<p>对输入的每一个元素运用 soft shrinkage 函数</p>
<p>SoftShrinkage 运算符定义为:</p>
<pre><code class="language-py">f(x) = x-lambda, if x &gt; lambda &gt;  f(x) = x+lambda, if x &lt; -lambda
f(x) = 0, otherwise

</code></pre>
<p>参数：lambd – Softshrink 公式中的 lambda 值. 默认值: 0.5</p>
<p>形状：</p>
<ul>
<li>输入：<img alt="(N, *)" src="../img/tex-b0400390724111bb8254d8d21c6a9f0d.gif" /> 其中 <code>*</code> 代表任意数目的附加维度</li>
<li>输出：<img alt="(N, *)" src="../img/tex-b0400390724111bb8254d8d21c6a9f0d.gif" />, 和输入的格式 shape 一致</li>
</ul>
<p>例:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = nn.Softshrink()
&gt;&gt;&gt; input = autograd.Variable(torch.randn(2))
&gt;&gt;&gt; print(input)
&gt;&gt;&gt; print(m(input))

</code></pre>
<h3 id="softsign">Softsign</h3>
<pre><code class="language-py">class torch.nn.Softsign
</code></pre>
<p>对输入的每一个元素运用函数 <img alt="f(x) = x / (1 + |x|)" src="../img/tex-7a038009b4e21f541255cead92ff31e1.gif" /></p>
<p>形状：</p>
<ul>
<li>输入：<img alt="(N, *)" src="../img/tex-b0400390724111bb8254d8d21c6a9f0d.gif" /> 其中 <code>*</code> 代表任意数目的附加维度</li>
<li>输出：<img alt="(N, *)" src="../img/tex-b0400390724111bb8254d8d21c6a9f0d.gif" />, 和输入的格式 shape 一致</li>
</ul>
<p>例:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = nn.Softsign()
&gt;&gt;&gt; input = autograd.Variable(torch.randn(2))
&gt;&gt;&gt; print(input)
&gt;&gt;&gt; print(m(input))

</code></pre>
<h3 id="tanhshrink">Tanhshrink</h3>
<pre><code class="language-py">class torch.nn.Tanhshrink
</code></pre>
<p>对输入的每一个元素运用函数, <img alt="Tanhshrink(x) = x - Tanh(x)" src="../img/tex-44c80efbc996ac6e387947091850f54b.gif" /></p>
<p>形状：</p>
<ul>
<li>输入：<img alt="(N, *)" src="../img/tex-b0400390724111bb8254d8d21c6a9f0d.gif" /> 其中 <code>*</code> 代表任意数目的附加维度</li>
<li>输出：<img alt="(N, *)" src="../img/tex-b0400390724111bb8254d8d21c6a9f0d.gif" />, 和输入的格式shape一致</li>
</ul>
<p>例:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = nn.Tanhshrink()
&gt;&gt;&gt; input = autograd.Variable(torch.randn(2))
&gt;&gt;&gt; print(input)
&gt;&gt;&gt; print(m(input))

</code></pre>
<h3 id="softmin">Softmin</h3>
<pre><code class="language-py">class torch.nn.Softmin(dim=None)
</code></pre>
<p>对n维输入张量运用 Softmin 函数, 将张量的每个元素缩放到 (0,1) 区间且和为 1.</p>
<p><img alt="f(x) = \frac{\exp(-x_i)}{\sum_j \exp(-x_j)}" src="../img/tex-4607c4c06b3ced41a4dfab06b8ac970a.gif" /></p>
<p>形状：</p>
<ul>
<li>输入：任意shape</li>
<li>输出：和输入相同</li>
</ul>
<p>参数：<code>dim (int)</code> – 这是将计算 Softmax 的维度 (所以每个沿着 dim 的切片和为 1).</p>
<p>返回值：返回结果是一个与输入维度相同的张量, 每个元素的取值范围在 [0, 1] 区间.</p>
<p>例:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = nn.Softmin()
&gt;&gt;&gt; input = autograd.Variable(torch.randn(2, 3))
&gt;&gt;&gt; print(input)
&gt;&gt;&gt; print(m(input))

</code></pre>
<h3 id="softmax">Softmax</h3>
<pre><code class="language-py">class torch.nn.Softmax(dim=None)
</code></pre>
<p>对n维输入张量运用 Softmax 函数, 将张量的每个元素缩放到 (0,1) 区间且和为 1. Softmax 函数定义如下 <img alt="f_i(x) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}" src="../img/tex-b1981394d33a113054724aea38de02f4.gif" /></p>
<p>形状：</p>
<ul>
<li>输入：任意shape</li>
<li>输出：和输入相同</li>
</ul>
<p>返回值：返回结果是一个与输入维度相同的张量, 每个元素的取值范围在 [0, 1] 区间.</p>
<p>参数：<code>dim (int)</code> – 这是将计算 Softmax 的那个维度 (所以每个沿着 dim 的切片和为 1).</p>
<p>注解：</p>
<p>如果你想对原始 Softmax 数据计算 Log 进行收缩, 并不能使该模块直接使用 NLLLoss 负对数似然损失函数. 取而代之, 应该使用 Logsoftmax (它有更快的运算速度和更好的数值性质).</p>
<p>例:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = nn.Softmax()
&gt;&gt;&gt; input = autograd.Variable(torch.randn(2, 3))
&gt;&gt;&gt; print(input)
&gt;&gt;&gt; print(m(input))

</code></pre>
<h3 id="softmax2d">Softmax2d</h3>
<pre><code class="language-py">class torch.nn.Softmax2d
</code></pre>
<p>把 SoftMax 应用于每个空间位置的特征.</p>
<p>给定图片的 通道数 Channels x 高 Height x 宽 Width, 它将对图片的每一个位置 使用 Softmax <img alt="(Channels, h_i, w_j)" src="../img/tex-a198a66fa501df7564f9b1e3564f34ca.gif" /></p>
<p>形状：</p>
<ul>
<li>输入：<img alt="(N, C, H, W)" src="../img/tex-38d00342060234da90e0c2c5493892cb.gif" /></li>
<li>输出：<img alt="(N, C, H, W)" src="../img/tex-38d00342060234da90e0c2c5493892cb.gif" /> (格式 shape 与输入相同)</li>
</ul>
<p>返回值：一个维度及格式 shape 都和输入相同的 Tensor, 取值范围在[0, 1]</p>
<p>例:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = nn.Softmax2d()
&gt;&gt;&gt; # you softmax over the 2nd dimension
&gt;&gt;&gt; input = autograd.Variable(torch.randn(2, 3, 12, 13))
&gt;&gt;&gt; print(input)
&gt;&gt;&gt; print(m(input))

</code></pre>
<h3 id="logsoftmax">LogSoftmax</h3>
<pre><code class="language-py">class torch.nn.LogSoftmax(dim=None)
</code></pre>
<p>对每个输入的 n 维 Tensor 使用 Log(Softmax(x)). LogSoftmax 公式可简化为</p>
<p><img alt="f_i(x) = log(exp(x_i) / sum_j exp(x_j))" src="../img/tex-e74abbc6578adf65f8ae7c421cafc170.gif" /></p>
<p>形状：</p>
<ul>
<li>输入：任意格式 shape</li>
<li>输出：和输入的格式 shape 一致</li>
</ul>
<p>参数：<code>dim (int)</code> – 这是将计算 Softmax 的那个维度 (所以每个沿着 dim 的切片和为1).</p>
<p>返回值：一个维度及格式 shape 都和输入相同的 Tensor, 取值范围在 [-inf, 0)</p>
<p>例:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = nn.LogSoftmax()
&gt;&gt;&gt; input = autograd.Variable(torch.randn(2, 3))
&gt;&gt;&gt; print(input)
&gt;&gt;&gt; print(m(input))

</code></pre>
<h2 id="normalization-layers">Normalization layers (归一化层)</h2>
<h3 id="batchnorm1d">BatchNorm1d</h3>
<pre><code class="language-py">class torch.nn.BatchNorm1d(num_features, eps=1e-05, momentum=0.1, affine=True)
</code></pre>
<p>对 2d 或者 3d 的小批量 (mini-batch) 数据进行批标准化 (Batch Normalization) 操作.</p>
<p><img alt="y = \frac{x - mean[x]}{ \sqrt{Var[x] + \epsilon}} * gamma + beta" src="../img/tex-7845c007673a63d2279ae8173ba805f4.gif" /></p>
<p>每个小批量数据中,计算各个维度的均值和标准差,并且 gamma 和 beta 是大小为 C 的可学习, 可改变的仿射参数向量( C 为输入大小).</p>
<p>在训练过程中,该层计算均值和方差,并进行平均移动,默认的平均移动动量值为 0.1.</p>
<p>在验证时,训练得到的均值/方差,用于标准化验证数据.</p>
<p>BatchNorm 在 'C' 维上处理,即 '(N,L)' 部分运行,被称作 'Temporal BatchNorm'</p>
<p>参数：</p>
<ul>
<li><code>num_features</code> – 预期输入的特征数,大小为 'batch_size x num_features [x width]'</li>
<li><code>eps</code> – 给分母加上的值,保证数值稳定(分母不能趋近0或取0),默认为 1e-5</li>
<li><code>momentum</code> – 动态均值和动态方差使用的移动动量值,默认为 0.1</li>
<li><code>affine</code> – 布尔值,设为 True 时,表示该层添加可学习,可改变的仿射参数,即 gamma 和 beta,默认为 True</li>
</ul>
<p>形状：</p>
<ul>
<li>输入：<img alt="(N, C)" src="../img/tex-0adbd266d78d43d0298b110b7b60ef70.gif" /> or <img alt="(N, C, L)" src="../img/tex-543737499dba0095d9151b4fd440b509.gif" /></li>
<li>输出：<img alt="(N, C)" src="../img/tex-0adbd266d78d43d0298b110b7b60ef70.gif" /> or <img alt="(N, C, L)" src="../img/tex-543737499dba0095d9151b4fd440b509.gif" /> (same shape as input)</li>
</ul>
<p>示例：</p>
<pre><code class="language-py">&gt;&gt;&gt; # With Learnable Parameters
&gt;&gt;&gt; m = nn.BatchNorm1d(100)
&gt;&gt;&gt; # Without Learnable Parameters
&gt;&gt;&gt; m = nn.BatchNorm1d(100, affine=False)
&gt;&gt;&gt; input = autograd.Variable(torch.randn(20, 100))
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="batchnorm2d">BatchNorm2d</h3>
<pre><code class="language-py">class torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True)
</code></pre>
<p>对小批量 (mini-batch) 3d 数据组成的 4d 输入进行标准化 (Batch Normalization) 操作.</p>
<p><img alt="y = \frac{x - mean[x]}{ \sqrt{Var[x] + \epsilon}} * gamma + beta" src="../img/tex-7845c007673a63d2279ae8173ba805f4.gif" /></p>
<p>每个小批量数据中,计算各个维度的均值和标准差, 并且 gamma 和 beta 是大小为 C 的可学习,可改变的仿射参数向量 (C 为输入大小).</p>
<p>在训练过程中,该层计算均值和方差,并进行平均移动.默认的平均移动动量值为 0.1.</p>
<p>在验证时,训练得到的均值/方差,用于标准化验证数据.</p>
<p>BatchNorm 在 'C' 维上处理,即 '(N, H, W)' 部分运行,被称作 'Spatial BatchNorm'.</p>
<p>参数：</p>
<ul>
<li><code>num_features</code> – 预期输入的特征数,大小为 'batch_size x num_features x height x width'</li>
<li><code>eps</code> – 给分母加上的值,保证数值稳定(分母不能趋近0或取0),默认为 1e-5</li>
<li><code>momentum</code> – 动态均值和动态方差使用的移动动量值,默认为 0.1</li>
<li><code>affine</code> – 布尔值,设为 True 时,表示该层添加可学习,可改变的仿射参数,即 gamma 和 beta,默认为 True</li>
</ul>
<p>形状：</p>
<ul>
<li>输入：<img alt="(N, C, H, W)" src="../img/tex-38d00342060234da90e0c2c5493892cb.gif" /></li>
<li>输出：<img alt="(N, C, H, W)" src="../img/tex-38d00342060234da90e0c2c5493892cb.gif" /> (same shape as input)</li>
</ul>
<p>示例：</p>
<pre><code class="language-py">&gt;&gt;&gt; # With Learnable Parameters
&gt;&gt;&gt; m = nn.BatchNorm2d(100)
&gt;&gt;&gt; # Without Learnable Parameters
&gt;&gt;&gt; m = nn.BatchNorm2d(100, affine=False)
&gt;&gt;&gt; input = autograd.Variable(torch.randn(20, 100, 35, 45))
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="batchnorm3d">BatchNorm3d</h3>
<pre><code class="language-py">class torch.nn.BatchNorm3d(num_features, eps=1e-05, momentum=0.1, affine=True)
</code></pre>
<p>对小批量 (mini-batch) 4d 数据组成的 5d 输入进行标准化 (Batch Normalization) 操作.</p>
<p><img alt="y = \frac{x - mean[x]}{ \sqrt{Var[x] + \epsilon}} * gamma + beta" src="../img/tex-7845c007673a63d2279ae8173ba805f4.gif" /></p>
<p>每个小批量数据中,计算各个维度的均值和标准差, 并且 gamma 和 beta 是大小为 C 的可学习,可改变的仿射参数向量 (C 为输入大小).</p>
<p>在训练过程中,该层计算均值和方差,并进行平均移动.默认的平均移动动量值为 0.1.</p>
<p>在验证时,训练得到的均值/方差,用于标准化验证数据.</p>
<p>BatchNorm 在 'C' 维上处理,即 '(N, D, H, W)' 部分运行,被称作 'Volumetric BatchNorm' 或者 'Spatio-temporal BatchNorm'</p>
<p>参数：</p>
<ul>
<li><code>num_features</code> – 预期输入的特征数,大小为 'batch_size x num_features x depth x height x width'</li>
<li><code>eps</code> – 给分母加上的值,保证数值稳定(分母不能趋近0或取0),默认为 1e-5</li>
<li><code>momentum</code> – 动态均值和动态方差使用的移动动量值,默认为 0.1</li>
<li><code>affine</code> – 布尔值,设为 True 时,表示该层添加可学习,可改变的仿射参数,即 gamma 和 beta,默认为 True</li>
</ul>
<p>形状：</p>
<ul>
<li>输入：<img alt="(N, C, D, H, W)" src="../img/tex-6d9465a2eb2377437689121f4915b6b4.gif" /></li>
<li>输出：<img alt="(N, C, D, H, W)" src="../img/tex-6d9465a2eb2377437689121f4915b6b4.gif" /> (same shape as input)</li>
</ul>
<p>示例：</p>
<pre><code class="language-py">&gt;&gt;&gt; # With Learnable Parameters
&gt;&gt;&gt; m = nn.BatchNorm3d(100)
&gt;&gt;&gt; # Without Learnable Parameters
&gt;&gt;&gt; m = nn.BatchNorm3d(100, affine=False)
&gt;&gt;&gt; input = autograd.Variable(torch.randn(20, 100, 35, 45, 10))
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="instancenorm1d">InstanceNorm1d</h3>
<pre><code class="language-py">class torch.nn.InstanceNorm1d(num_features, eps=1e-05, momentum=0.1, affine=False)
</code></pre>
<p>对 2d 或者 3d 的小批量 (mini-batch) 数据进行实例标准化 (Instance Normalization) 操作. .. math:</p>
<pre><code class="language-py">y = \frac{x - mean[x]}{ \sqrt{Var[x]} + \epsilon} * gamma + beta

</code></pre>
<p>对小批量数据中的每一个对象,计算其各个维度的均值和标准差,并且 gamma 和 beta 是大小为 C 的可学习, 可改变的仿射参数向量( C 为输入大小).</p>
<p>在训练过程中,该层计算均值和方差,并进行平均移动,默认的平均移动动量值为 0.1.</p>
<p>在验证时 (<code>.eval()</code>),InstanceNorm 模型默认保持不变,即求得的均值/方差不用于标准化验证数据, 但可以用 <code>.train(False)</code> 方法强制使用存储的均值和方差.</p>
<p>参数：</p>
<ul>
<li><code>num_features</code> – 预期输入的特征数,大小为 'batch_size x num_features x width'</li>
<li><code>eps</code> – 给分母加上的值,保证数值稳定(分母不能趋近0或取0),默认为 1e-5</li>
<li><code>momentum</code> – 动态均值和动态方差使用的移动动量值,默认为 0.1</li>
<li><code>affine</code> – 布尔值,设为 <code>True</code> 时,表示该层添加可学习,可改变的仿射参数,即 gamma 和 beta,默认为 <code>False</code></li>
</ul>
<p>形状：</p>
<ul>
<li>输入：<img alt="(N, C, L)" src="../img/tex-543737499dba0095d9151b4fd440b509.gif" /></li>
<li>输出：<img alt="(N, C, L)" src="../img/tex-543737499dba0095d9151b4fd440b509.gif" /> (same shape as input)</li>
</ul>
<p>示例：</p>
<pre><code class="language-py">&gt;&gt;&gt; # Without Learnable Parameters
&gt;&gt;&gt; m = nn.InstanceNorm1d(100)
&gt;&gt;&gt; # With Learnable Parameters
&gt;&gt;&gt; m = nn.InstanceNorm1d(100, affine=True)
&gt;&gt;&gt; input = autograd.Variable(torch.randn(20, 100, 40))
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="instancenorm2d">InstanceNorm2d</h3>
<pre><code class="language-py">class torch.nn.InstanceNorm2d(num_features, eps=1e-05, momentum=0.1, affine=False)
</code></pre>
<p>对小批量 (mini-batch) 3d 数据组成的 4d 输入进行实例标准化 (Batch Normalization) 操作. .. math:</p>
<pre><code class="language-py">y = \frac{x - mean[x]}{ \sqrt{Var[x]} + \epsilon} * gamma + beta

</code></pre>
<p>对小批量数据中的每一个对象,计算其各个维度的均值和标准差,并且 gamma 和 beta 是大小为 C 的可学习, 可改变的仿射参数向量( C 为输入大小).</p>
<p>在训练过程中,该层计算均值和方差,并进行平均移动,默认的平均移动动量值为 0.1.</p>
<p>在验证时 (<code>.eval()</code>),InstanceNorm 模型默认保持不变,即求得的均值/方差不用于标准化验证数据, 但可以用 <code>.train(False)</code> 方法强制使用存储的均值和方差.</p>
<p>参数：</p>
<ul>
<li><code>num_features</code> – 预期输入的特征数,大小为 'batch_size x num_features x height x width'</li>
<li><code>eps</code> – 给分母加上的值,保证数值稳定(分母不能趋近0或取0),默认为 1e-5</li>
<li><code>momentum</code> – 动态均值和动态方差使用的移动动量值,默认为 0.1</li>
<li><code>affine</code> – 布尔值,设为 <code>True</code> 时,表示该层添加可学习,可改变的仿射参数,即 gamma 和 beta,默认为 <code>False</code></li>
</ul>
<p>形状：</p>
<ul>
<li>输入：<img alt="(N, C, H, W)" src="../img/tex-38d00342060234da90e0c2c5493892cb.gif" /></li>
<li>输出：<img alt="(N, C, H, W)" src="../img/tex-38d00342060234da90e0c2c5493892cb.gif" /> (same shape as input)</li>
</ul>
<p>示例：</p>
<pre><code class="language-py">&gt;&gt;&gt; # Without Learnable Parameters
&gt;&gt;&gt; m = nn.InstanceNorm2d(100)
&gt;&gt;&gt; # With Learnable Parameters
&gt;&gt;&gt; m = nn.InstanceNorm2d(100, affine=True)
&gt;&gt;&gt; input = autograd.Variable(torch.randn(20, 100, 35, 45))
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="instancenorm3d">InstanceNorm3d</h3>
<pre><code class="language-py">class torch.nn.InstanceNorm3d(num_features, eps=1e-05, momentum=0.1, affine=False)
</code></pre>
<p>对小批量 (mini-batch) 4d 数据组成的 5d 输入进行实例标准化 (Batch Normalization) 操作. .. math:</p>
<pre><code class="language-py">y = \frac{x - mean[x]}{ \sqrt{Var[x]} + \epsilon} * gamma + beta

</code></pre>
<p>对小批量数据中的每一个对象,计算其各个维度的均值和标准差,并且 gamma 和 beta 是大小为 C 的可学习, 可改变的仿射参数向量( C 为输入大小).</p>
<p>在训练过程中,该层计算均值和方差,并进行平均移动,默认的平均移动动量值为 0.1.</p>
<p>在验证时 (<code>.eval()</code>),InstanceNorm 模型默认保持不变,即求得的均值/方差不用于标准化验证数据, 但可以用 <code>.train(False)</code> 方法强制使用存储的均值和方差.</p>
<p>参数：</p>
<ul>
<li><code>num_features</code> – 预期输入的特征数,大小为 'batch_size x num_features x depth x height x width'</li>
<li><code>eps</code> – 给分母加上的值,保证数值稳定(分母不能趋近0或取0),默认为 1e-5</li>
<li><code>momentum</code> – 动态均值和动态方差使用的移动动量值,默认为 0.1</li>
<li><code>affine</code> – 布尔值,设为 <code>True</code> 时,表示该层添加可学习,可改变的仿射参数,即 gamma 和 beta,默认为 <code>False</code></li>
</ul>
<p>形状：</p>
<ul>
<li>输入：<img alt="(N, C, D, H, W)" src="../img/tex-6d9465a2eb2377437689121f4915b6b4.gif" /></li>
<li>输出：<img alt="(N, C, D, H, W)" src="../img/tex-6d9465a2eb2377437689121f4915b6b4.gif" /> (same shape as input)</li>
</ul>
<p>示例：</p>
<pre><code class="language-py">&gt;&gt;&gt; # Without Learnable Parameters
&gt;&gt;&gt; m = nn.InstanceNorm3d(100)
&gt;&gt;&gt; # With Learnable Parameters
&gt;&gt;&gt; m = nn.InstanceNorm3d(100, affine=True)
&gt;&gt;&gt; input = autograd.Variable(torch.randn(20, 100, 35, 45, 10))
&gt;&gt;&gt; output = m(input)

</code></pre>
<h2 id="recurrent-layers">Recurrent layers (循环层)</h2>
<h3 id="rnn">RNN</h3>
<pre><code class="language-py">class torch.nn.RNN(*args, **kwargs)
</code></pre>
<p>对于输入序列使用一个多层的 <code>Elman RNN</code>, 它的激活函数为 <code>tanh</code> 或者 <code>ReLU</code> .</p>
<p>对输入序列中每个元素, 每层计算公式为:</p>
<p><img alt="h_t = \tanh(w_{ih} * x_t + b_{ih} + w_{hh} * h_{(t-1)} + b_{hh})" src="../img/tex-f8ac3a86ecb6d9bb814d9e166cdd6edb.gif" /></p>
<p>这里 <img alt="h_t" src="../img/tex-6c4ff69dbcc329835a33b80fe3a145c7.gif" /> 是当前在时刻 <code>t</code> 的隐状态, 并且 <img alt="x_t" src="../img/tex-cf7ee950cf61a6003c0ec4af7971d8a8.gif" /> 是之前一层在 <code>t</code> 时刻的隐状态, 或者是第一层的输入. 如果 <code>nonlinearity='relu'</code> ,那么将使用 relu 代替 tanh 作为激活函数.</p>
<p>参数：</p>
<ul>
<li><code>input_size</code> – 输入 x 的特征数量</li>
<li><code>hidden_size</code> – 隐状态 <code>h</code> 中的特征数量</li>
<li><code>num_layers</code> – RNN 的层数</li>
<li><code>nonlinearity</code> – 指定非线性函数使用 ['tanh'&#124;'relu']. 默认: 'tanh'</li>
<li><code>bias</code> – 如果是 <code>False</code> , 那么 RNN 层就不会使用偏置权重 b_ih 和 b_hh, 默认: <code>True</code></li>
<li><code>batch_first</code> – 如果 <code>True</code>, 那么输入 <code>Tensor</code> 的 shape 应该是 (batch, seq, feature),并且输出也是一样</li>
<li><code>dropout</code> – 如果值非零, 那么除了最后一层外, 其它层的输出都会套上一个 <code>dropout</code> 层</li>
<li><code>bidirectional</code> – 如果 <code>True</code> , 将会变成一个双向 RNN, 默认为 <code>False</code></li>
</ul>
<p>Inputs: input, h_0</p>
<ul>
<li><code>input (seq_len, batch, input_size)</code>: 包含输入序列特征的 <code>tensor</code> , <code>input</code> 可以是被填充的变长序列.细节请看 <code>torch.nn.utils.rnn.pack_padded_sequence()</code> .</li>
<li><code>h_0 (num_layers * num_directions, batch, hidden_size)</code>: 包含 <code>batch</code> 中每个元素保存着初始隐状态的 <code>tensor</code></li>
</ul>
<p>Outputs: output, h_n</p>
<ul>
<li><code>output (seq_len, batch, hidden_size * num_directions)</code>: 包含 RNN 最后一层输出特征 (h_k) 的 <code>tensor</code> 对于每个 k ,如果输入是一个 <code>torch.nn.utils.rnn.PackedSequence</code> , 那么输出也是一个可以是被填充的变长序列.</li>
<li><code>h_n (num_layers * num_directions, batch, hidden_size)</code>: 包含 k= seq_len 隐状态的 <code>tensor</code>.</li>
</ul>
<p>变量：</p>
<ul>
<li>weight_ih_l[k] – 第 k 层的 input-hidden 权重,可学习, shape 是 <code>(input_size x hidden_size)</code></li>
<li>weight_hh_l[k] – 第 k 层的 hidden-hidden 权重, 可学习, shape 是 <code>(hidden_size x hidden_size)</code></li>
<li>bias_ih_l[k] – 第 k 层的 input-hidden 偏置, 可学习, shape 是 <code>(hidden_size)</code></li>
<li>bias_hh_l[k] – 第 k 层的 hidden-hidden 偏置, 可学习, shape 是 <code>(hidden_size)</code></li>
</ul>
<p>Examples:</p>
<pre><code class="language-py">&gt;&gt;&gt; rnn = nn.RNN(10, 20, 2)
&gt;&gt;&gt; input = Variable(torch.randn(5, 3, 10))
&gt;&gt;&gt; h0 = Variable(torch.randn(2, 3, 20))
&gt;&gt;&gt; output, hn = rnn(input, h0)

</code></pre>
<h3 id="lstm">LSTM</h3>
<pre><code class="language-py">class torch.nn.LSTM(*args, **kwargs)
</code></pre>
<p>对于输入序列使用一个多层的 <code>LSTM</code> ( long short-term memory ).</p>
<p>对输入序列的每个元素, <code>LSTM</code> 的每层都会执行以下计算:</p>
<p><img alt="\begin{split}\begin{array}{ll} i_t = \mathrm{sigmoid}(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \ f_t = \mathrm{sigmoid}(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \ g_t = \tanh(W_{ig} x_t + b_{ig} + W_{hc} h_{(t-1)} + b_{hg}) \ o_t = \mathrm{sigmoid}(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \ c_t = f_t * c_{(t-1)} + i_t * g_t \ h_t = o_t * \tanh(c_t) \end{array}\end{split}" src="../img/tex-8d71c956f0fdd364faeb8fb4011edafe.gif" /></p>
<p>这里 <img alt="h_t" src="../img/tex-6c4ff69dbcc329835a33b80fe3a145c7.gif" /> 是在时刻 <code>t</code> 的隐状态, <img alt="c_t" src="../img/tex-da6834ea306c993ae190d8ac693a25f0.gif" /> 是在时刻 <code>t</code> 的细胞状态 (cell state), <img alt="x_t" src="../img/tex-cf7ee950cf61a6003c0ec4af7971d8a8.gif" /> 是上一层的在时刻 <code>t</code> 的隐状态或者是第一层的 <img alt="input_t" src="../img/tex-bbcb26ab6bab8f51269870e0abcec697.gif" /> , 而 <img alt="i_t" src="../img/tex-56e99db17308c13a71cfc5da5a3165df.gif" />, <img alt="f_t" src="../img/tex-bae155438877126b42cbddee193c048a.gif" />, <img alt="g_t" src="../img/tex-f752008a78c8519e9f8f178faf0b87ed.gif" />, <img alt="o_t" src="../img/tex-2b2a0a4b3fbc78a61cec549a40912932.gif" /> 分别代表 输入门,遗忘门,细胞和输出门.</p>
<p>参数：</p>
<ul>
<li><code>input_size</code> – 输入的特征维度</li>
<li><code>hidden_size</code> – 隐状态的特征维度</li>
<li><code>num_layers</code> – 层数(和时序展开要区分开)</li>
<li><code>bias</code> – 如果为 <code>False</code> ,那么 LSTM 将不会使用 b_ih 和 b_hh ,默认: <code>True</code></li>
<li><code>batch_first</code> – 如果为 <code>True</code> , 那么输入和输出 Tensor 的形状为 (batch, seq, feature)</li>
<li><code>dropout</code> – 如果非零的话, 将会在 RNN 的输出上加个 dropout , 最后一层除外</li>
<li><code>bidirectional</code> – 如果为 <code>True</code>,将会变成一个双向 RNN ,默认为 <code>False</code></li>
</ul>
<p>Inputs: input, (h_0, c_0)</p>
<ul>
<li><code>input (seq_len, batch, input_size)</code>: 包含输入序列特征的 <code>tensor</code> . 也可以是 <code>packed variable length sequence</code>, 详见 <code>torch.nn.utils.rnn.pack_padded_sequence()</code> .</li>
<li><code>h_0 (num_layers * num_directions, batch, hidden_size)</code>: 包含 batch 中每个元素的初始化隐状态的 <code>tensor</code> .</li>
<li><code>c_0 (num_layers * num_directions, batch, hidden_size)</code>: 包含 batch 中每个元素的初始化细胞状态的 <code>tensor</code> .</li>
</ul>
<p>Outputs: output, (h_n, c_n)</p>
<ul>
<li><code>output (seq_len, batch, hidden_size * num_directions)</code>: 包含 RNN 最后一层的输出特征 <code>(h_t)</code> 的 <code>tensor</code> , 对于每个 t . 如果输入是 <code>torch.nn.utils.rnn.PackedSequence</code> 那么输出也是一个可以是被填充的变长序列.</li>
<li><code>h_n (num_layers * num_directions, batch, hidden_size)</code>: 包含 t=seq_len 隐状态的 <code>tensor</code>.</li>
<li><code>c_n (num_layers * num_directions, batch, hidden_size)</code>: 包含 t=seq_len 细胞状态的 <code>tensor</code>.</li>
</ul>
<p>变量：</p>
<ul>
<li>weight_ih_l[k] – 第 k 层可学习的 input-hidden 权重 <code>(W_ii&amp;#124;W_if&amp;#124;W_ig&amp;#124;W_io)</code>, shape 是 <code>(4*hidden_size x input_size)</code></li>
<li>weight_hh_l[k] – 第 k 层可学习的 hidden-hidden 权重 <code>(W_hi&amp;#124;W_hf&amp;#124;W_hg&amp;#124;W_ho)</code>, shape 是 <code>(4*hidden_size x hidden_size)</code></li>
<li>bias_ih_l[k] – 第 k 层可学习的 input-hidden 偏置 <code>(b_ii&amp;#124;b_if&amp;#124;b_ig&amp;#124;b_io)</code>, shape 是 <code>(4*hidden_size)</code></li>
<li>bias_hh_l[k] – 第 k 层可学习的 hidden-hidden 偏置 <code>(b_hi&amp;#124;b_hf&amp;#124;b_hg&amp;#124;b_ho)</code>, shape 是 <code>(4*hidden_size)</code></li>
</ul>
<p>Examples:</p>
<pre><code class="language-py">&gt;&gt;&gt; rnn = nn.LSTM(10, 20, 2)
&gt;&gt;&gt; input = Variable(torch.randn(5, 3, 10))
&gt;&gt;&gt; h0 = Variable(torch.randn(2, 3, 20))
&gt;&gt;&gt; c0 = Variable(torch.randn(2, 3, 20))
&gt;&gt;&gt; output, hn = rnn(input, (h0, c0))

</code></pre>
<h3 id="gru">GRU</h3>
<pre><code class="language-py">class torch.nn.GRU(*args, **kwargs)
</code></pre>
<p>对于输入序列使用一个多层的 <code>GRU</code> (gated recurrent unit).</p>
<p>对输入序列的每个元素, 每层都会执行以下计算:</p>
<p><img alt="\begin{split}\begin{array}{ll} r_t = \mathrm{sigmoid}(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \ z_t = \mathrm{sigmoid}(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \ n_t = \tanh(W_{in} x_t + b_{in} + r_t * (W_{hn} h_{(t-1)}+ b_{hn})) \ h_t = (1 - z_t) * n_t + z_t * h_{(t-1)} \ \end{array}\end{split}" src="../img/tex-acd0dd125732ec681cab120f096f4a00.gif" /></p>
<p>这里 <img alt="h_t" src="../img/tex-6c4ff69dbcc329835a33b80fe3a145c7.gif" /> 是在时刻 <code>t</code> 的隐状态, <img alt="x_t" src="../img/tex-cf7ee950cf61a6003c0ec4af7971d8a8.gif" /> 是前一层在时刻 <code>t</code> 的隐状态或者是第一层的 <img alt="input_t" src="../img/tex-bbcb26ab6bab8f51269870e0abcec697.gif" /> , 而 <img alt="r_t" src="../img/tex-3d1dfe70cdc0d574aa6cf3e228a57166.gif" />, <img alt="z_t" src="../img/tex-0bcd91fb6432f991d5b4cbb079e562c7.gif" />, <img alt="n_t" src="../img/tex-871b354d265716a852eea9970e53ff5e.gif" /> 分别是重置门,输入门和新门.</p>
<p>参数：</p>
<ul>
<li><code>input_size</code> – 输入的特征维度</li>
<li><code>hidden_size</code> – 隐状态的特征维度</li>
<li><code>num_layers</code> – RNN 的层数</li>
<li><code>bias</code> – 如果为 <code>False</code>, 那么 RNN 层将不会使用偏置权重 b_ih 和 b_hh 默认: <code>True</code></li>
<li><code>batch_first</code> – 如果为 <code>True</code>, 那么输入和输出的 <code>tensor</code> 的形状是 (batch, seq, feature)</li>
<li><code>dropout</code> – 如果非零的话,将会在 RNN 的输出上加个 dropout ,最后一层除外</li>
<li><code>bidirectional</code> – 如果为 <code>True</code>, 将会变成一个双向 RNN . 默认: <code>False</code></li>
</ul>
<p>Inputs: input, h_0</p>
<ul>
<li><code>input (seq_len, batch, input_size)</code>: 包含输入序列特征的 <code>tensor</code> . 也可以是 <code>packed variable length sequence</code>, 详见 <code>torch.nn.utils.rnn.pack_padded_sequence()</code> .</li>
<li><code>h_0 (num_layers * num_directions, batch, hidden_size)</code>: 包含 batch 中每个元素的初始化隐状态的 <code>tensor</code></li>
</ul>
<p>Outputs: output, h_n</p>
<ul>
<li><code>output (seq_len, batch, hidden_size * num_directions)</code>: 包含 RNN 最后一层的输出特征 <code>(h_t)</code> 的 <code>tensor</code> , 对于每个 t . 如果输入是 <code>torch.nn.utils.rnn.PackedSequence</code> 那么输出也是一个可以是被填充的变长序列.</li>
<li><code>h_n (num_layers * num_directions, batch, hidden_size)</code>: 包含 t=seq_len 隐状态的 <code>tensor</code>.</li>
</ul>
<p>变量：</p>
<ul>
<li>weight_ih_l[k] – 第 k 层可学习的 input-hidden 权重 (W_ir&#124;W_iz&#124;W_in), shape 为 <code>(3*hidden_size x input_size)</code></li>
<li>weight_hh_l[k] – 第 k 层可学习的 hidden-hidden 权重 (W_hr&#124;W_hz&#124;W_hn), shape 为 <code>(3*hidden_size x hidden_size)</code></li>
<li>bias_ih_l[k] – 第 k 层可学习的 input-hidden 偏置 (b_ir&#124;b_iz&#124;b_in), shape 为 <code>(3*hidden_size)</code></li>
<li>bias_hh_l[k] – 第 k 层可学习的 hidden-hidden 偏置 (b_hr&#124;b_hz&#124;b_hn), shape 为 <code>(3*hidden_size)</code></li>
</ul>
<p>Examples:</p>
<pre><code class="language-py">&gt;&gt;&gt; rnn = nn.GRU(10, 20, 2)
&gt;&gt;&gt; input = Variable(torch.randn(5, 3, 10))
&gt;&gt;&gt; h0 = Variable(torch.randn(2, 3, 20))
&gt;&gt;&gt; output, hn = rnn(input, h0)

</code></pre>
<h3 id="rnncell">RNNCell</h3>
<pre><code class="language-py">class torch.nn.RNNCell(input_size, hidden_size, bias=True, nonlinearity='tanh')
</code></pre>
<p>一个 <code>Elan RNN cell</code> , 激活函数是 tanh 或 ReLU , 用于输入序列.</p>
<p><img alt="h' = \tanh(w_{ih} * x + b_{ih} + w_{hh} * h + b_{hh})" src="../img/tex-209461341961969b0f0807545f76affd.gif" /></p>
<p>如果 nonlinearity='relu', 那么将会使用 ReLU 来代替 tanh .</p>
<p>参数：</p>
<ul>
<li><code>input_size</code> – 输入的特征维度</li>
<li><code>hidden_size</code> – 隐状态的特征维度</li>
<li><code>bias</code> – 如果为 <code>False</code>, 那么RNN层将不会使用偏置权重 b_ih 和 b_hh. 默认: <code>True</code></li>
<li><code>nonlinearity</code> – 用于选择非线性激活函数 ['tanh'&#124;'relu']. 默认: 'tanh'</li>
</ul>
<p>Inputs: input, hidden</p>
<ul>
<li><code>input (batch, input_size)</code>: 包含输入特征的 <code>tensor</code> .</li>
<li><code>hidden (batch, hidden_size)</code>: 包含 batch 中每个元素的初始化隐状态的 <code>tensor</code>.</li>
</ul>
<p>Outputs: h'</p>
<ul>
<li>h' (batch, hidden_size): 保存着 batch 中每个元素的下一层隐状态的 <code>tensor</code> .</li>
</ul>
<p>变量：</p>
<ul>
<li><code>weight_ih</code> – <code>input-hidden</code> 权重, 可学习, shape 为 <code>(input_size x hidden_size)</code></li>
<li><code>weight_hh</code> – <code>hidden-hidden</code> 权重, 可学习, shape 为 <code>(hidden_size x hidden_size)</code></li>
<li><code>bias_ih</code> – <code>input-hidden</code> 偏置,可学习, shape 为 <code>(hidden_size)</code></li>
<li><code>bias_hh</code> – <code>hidden-hidden</code> 偏置,可学习, shape 为 <code>(hidden_size)</code></li>
</ul>
<p>Examples:</p>
<pre><code class="language-py">&gt;&gt;&gt; rnn = nn.RNNCell(10, 20)
&gt;&gt;&gt; input = Variable(torch.randn(6, 3, 10))
&gt;&gt;&gt; hx = Variable(torch.randn(3, 20))
&gt;&gt;&gt; output = []
&gt;&gt;&gt; for i in range(6):
...     hx = rnn(input[i], hx)
...     output.append(hx)

</code></pre>
<h3 id="lstmcell">LSTMCell</h3>
<pre><code class="language-py">class torch.nn.LSTMCell(input_size, hidden_size, bias=True)
</code></pre>
<p>LSTM 细胞.</p>
<p><img alt="\begin{split}\begin{array}{ll} i = \mathrm{sigmoid}(W_{ii} x + b_{ii} + W_{hi} h + b_{hi}) \ f = \mathrm{sigmoid}(W_{if} x + b_{if} + W_{hf} h + b_{hf}) \ g = \tanh(W_{ig} x + b_{ig} + W_{hc} h + b_{hg}) \ o = \mathrm{sigmoid}(W_{io} x + b_{io} + W_{ho} h + b_{ho}) \ c' = f * c + i * g \ h' = o * \tanh(c') \ \end{array}\end{split}" src="../img/tex-00f313e58f35fa58da4f06223a36c475.gif" /></p>
<p>参数：</p>
<ul>
<li><code>input_size</code> – 输入的特征维度</li>
<li><code>hidden_size</code> – 隐状态的维度</li>
<li><code>bias</code> – 如果为 <code>False</code>, 那么RNN层将不会使用偏置权重 b_ih 和 b_hh 默认: <code>True</code></li>
</ul>
<p>Inputs: input, (h_0, c_0)</p>
<ul>
<li><code>input (batch, input_size)</code>: 包含输入特征的 <code>tensor</code> .</li>
<li><code>h_0 (batch, hidden_size)</code>: 包含 batch 中每个元素的初始化隐状态的 <code>tensor</code>.</li>
<li><code>c_0 (batch. hidden_size)</code>: 包含 batch 中每个元素的初始化细胞状态的 <code>tensor</code></li>
</ul>
<p>Outputs: h_1, c_1</p>
<ul>
<li><code>h_1 (batch, hidden_size)</code>: 保存着 batch 中每个元素的下一层隐状态的 <code>tensor</code></li>
<li><code>c_1 (batch, hidden_size)</code>: 保存着 batch 中每个元素的下一细胞状态的 <code>tensor</code></li>
</ul>
<p>变量：</p>
<ul>
<li><code>weight_ih</code> – <code>input-hidden</code> 权重, 可学习, 形状为 <code>(4*hidden_size x input_size)</code></li>
<li><code>weight_hh</code> – <code>hidden-hidden</code> 权重, 可学习, 形状为 <code>(4*hidden_size x hidden_size)</code></li>
<li><code>bias_ih</code> – <code>input-hidden</code> 偏置, 可学习, 形状为 <code>(4*hidden_size)</code></li>
<li><code>bias_hh</code> – <code>hidden-hidden</code> 偏置, 可学习, 形状为 <code>(4*hidden_size)</code></li>
</ul>
<p>Examples:</p>
<pre><code class="language-py">&gt;&gt;&gt; rnn = nn.LSTMCell(10, 20)
&gt;&gt;&gt; input = Variable(torch.randn(6, 3, 10))
&gt;&gt;&gt; hx = Variable(torch.randn(3, 20))
&gt;&gt;&gt; cx = Variable(torch.randn(3, 20))
&gt;&gt;&gt; output = []
&gt;&gt;&gt; for i in range(6):
...     hx, cx = rnn(input[i], (hx, cx))
...     output.append(hx)

</code></pre>
<h3 id="grucell">GRUCell</h3>
<pre><code class="language-py">class torch.nn.GRUCell(input_size, hidden_size, bias=True)
</code></pre>
<p>GRU 细胞</p>
<p><img alt="\begin{split}\begin{array}{ll} r = \mathrm{sigmoid}(W_{ir} x + b_{ir} + W_{hr} h + b_{hr}) \ z = \mathrm{sigmoid}(W_{iz} x + b_{iz} + W_{hz} h + b_{hz}) \ n = \tanh(W_{in} x + b_{in} + r * (W_{hn} h + b_{hn})) \ h' = (1 - z) * n + z * h \end{array}\end{split}" src="../img/tex-16404bdd55736af51799dd91ad2bcbf4.gif" /></p>
<p>参数：</p>
<ul>
<li><code>input_size</code> – 输入的特征维度</li>
<li><code>hidden_size</code> – 隐状态的维度</li>
<li><code>bias</code> – 如果为 <code>False</code>, 那么RNN层将不会使用偏置权重 b_ih 和 b_hh 默认: <code>True</code></li>
</ul>
<p>Inputs: input, hidden</p>
<ul>
<li><code>input (batch, input_size)</code>: 包含输入特征的 <code>tensor</code> .</li>
<li><code>hidden (batch, hidden_size)</code>: 包含 batch 中每个元素的初始化隐状态的 <code>tensor</code>.</li>
</ul>
<p>Outputs: h'</p>
<ul>
<li>h': (batch, hidden_size): 保存着 batch 中每个元素的下一层隐状态的 <code>tensor</code></li>
</ul>
<p>变量：</p>
<ul>
<li><code>weight_ih</code> – <code>input-hidden</code> 权重, 可学习, shape 为, <code>(3*hidden_size x input_size)</code></li>
<li><code>weight_hh</code> – <code>hidden-hidden</code> 权重, 可学习, shape 为 <code>(3*hidden_size x hidden_size)</code></li>
<li><code>bias_ih</code> – <code>input-hidden</code> 偏置, 可学习, shape 为 <code>(3*hidden_size)</code></li>
<li><code>bias_hh</code> – <code>hidden-hidden</code> 偏置, 可学习, shape 为 <code>(3*hidden_size)</code></li>
</ul>
<p>Examples:</p>
<pre><code class="language-py">&gt;&gt;&gt; rnn = nn.GRUCell(10, 20)
&gt;&gt;&gt; input = Variable(torch.randn(6, 3, 10))
&gt;&gt;&gt; hx = Variable(torch.randn(3, 20))
&gt;&gt;&gt; output = []
&gt;&gt;&gt; for i in range(6):
...     hx = rnn(input[i], hx)
...     output.append(hx)

</code></pre>
<h2 id="linear-layers">Linear layers (线性层)</h2>
<h3 id="linear">Linear</h3>
<pre><code class="language-py">class torch.nn.Linear(in_features, out_features, bias=True)
</code></pre>
<p>对输入数据进行线性变换: <img alt="y = Ax + b" src="../img/tex-7b9f11d97a4a0531566a8b3ceb4b3cd2.gif" /></p>
<p>参数：</p>
<ul>
<li><code>in_features</code> – 每个输入样本的大小</li>
<li><code>out_features</code> – 每个输出样本的大小</li>
<li><code>bias</code> – 若设置为 False, 这层不会学习偏置. 默认值: True</li>
</ul>
<p>形状：</p>
<ul>
<li>输入：<img alt="(N, *, in_features)" src="../img/tex-473bd210940b2d1080beaaa69de08f28.gif" /> 这里 <code>*</code> 意味着可以添加任意数量的其他维度</li>
<li>输出：<img alt="(N, *, out_features)" src="../img/tex-5726e9a80713e6a886ae6140fcdba481.gif" /> 除了最后一个维度外, 其余的都与输入相同</li>
</ul>
<p>变量：</p>
<ul>
<li><code>weight</code> – 形状为 (out_features x in_features) 的模块中可学习的权值</li>
<li><code>bias</code> – 形状为 (out_features) 的模块中可学习的偏置</li>
</ul>
<p>Examples:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = nn.Linear(20, 30)
&gt;&gt;&gt; input = autograd.Variable(torch.randn(128, 20))
&gt;&gt;&gt; output = m(input)
&gt;&gt;&gt; print(output.size())

</code></pre>
<h3 id="bilinear">Bilinear</h3>
<pre><code class="language-py">class torch.nn.Bilinear(in1_features, in2_features, out_features, bias=True)
</code></pre>
<p>对输入数据进行双线性变换: <img alt="y = x_1 * A * x_2 + b" src="../img/tex-aaf171cac82685c3c340333937649cd0.gif" /></p>
<p>参数：</p>
<ul>
<li><code>in1_features</code> – 输入一的每个输入样本的大小</li>
<li><code>in2_features</code> – 输入二的每个输入样本的大小</li>
<li><code>out_features</code> – 每个输出样本的大小</li>
<li><code>bias</code> – 若设置为False, 这层不会学习偏置. 默认值: True</li>
</ul>
<p>形状：</p>
<ul>
<li>输入：<img alt="(N, in1_features)" src="../img/tex-cd06e008711b0ac8b3a44cfab48eb593.gif" />, <img alt="(N, in2_features)" src="../img/tex-91654b72e709cc4b218047fb1f65ece3.gif" /></li>
<li>输出：<img alt="(N, out_features)" src="../img/tex-899e6c8f98b5fd7650a52ead62f6c414.gif" /></li>
</ul>
<p>变量：</p>
<ul>
<li><code>weight</code> – 形状为 (out_features x in1_features x in2_features) 的模块中可学习的权值</li>
<li><code>bias</code> – 形状为 (out_features) 的模块中可学习的偏置</li>
</ul>
<p>Examples:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = nn.Bilinear(20, 30, 40)
&gt;&gt;&gt; input1 = autograd.Variable(torch.randn(128, 20))
&gt;&gt;&gt; input2 = autograd.Variable(torch.randn(128, 30))
&gt;&gt;&gt; output = m(input1, input2)
&gt;&gt;&gt; print(output.size())

</code></pre>
<h2 id="dropout-layers">Dropout layers</h2>
<h3 id="dropout">Dropout</h3>
<pre><code class="language-py">class torch.nn.Dropout(p=0.5, inplace=False)
</code></pre>
<p>Dropout 在训练期间, 按照伯努利概率分布, 以概率 p 随机地将输入张量中的部分元素</p>
<p>置为 0, 在每次调用时, 被置为 0 的元素是随机的.</p>
<p>Dropout 已被证明是正则化的一个行之有效的技术, 并且在防止神经元之间互适应问题上 也卓有成效.(神经元互适应问题详见论文 <a href="https://arxiv.org/abs/1207.0580">Improving neural networks by preventing co-adaptation of feature detectors</a>)</p>
<p>并且, Dropout 的输出均与 <em>1/(1-p)</em> 的比例系数进行了相乘, 保证了求值时函数是归一化的.</p>
<p>Args: p: 元素被置为0的概率, 默认值: 0.5 inplace: 如果为 True, 置0操作将直接发生在传入的元素上.默认值: false Shape:</p>
<ul>
<li>输入：any.输入数据可以是任何大小</li>
<li>输出：Same.输出数据大小与输入相同</li>
</ul>
<p>Examples:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = nn.Dropout(p=0.2)

</code></pre>
<pre><code class="language-py">&gt;&gt;&gt; input = autograd.Variable(torch.randn(20, 16))
&gt;&gt;&gt; output = m(input)

</code></pre>
<h3 id="dropout2d">Dropout2d</h3>
<pre><code class="language-py">class torch.nn.Dropout2d(p=0.5, inplace=False)
</code></pre>
<p>Dropout2d 将输入张量的所有通道随机地置为 0.被置为 0 的通道在每次调用时是随机的.</p>
<blockquote>
<p>通常输入数据来自 Conv2d 模块.</p>
<p>在论文 <a href="http://arxiv.org/abs/1411.4280">Efficient Object Localization Using Convolutional Networks</a> 中有如下 描述: 如果特征映射中的邻接像素是强相关的(在早期的卷积层中很常见）, 那么独立同分布 的 dropout 将不会正则化激活函数, 相反其会导致有效的学习率的下降.</p>
<p>在这样的情况下, 应该使用函数函数 nn.Dropout2d , 它能够提升特征映射之间的独立性.</p>
<p>Args: p (float,optional): 元素被置0的概率 inplace(bool, 可选）: 如果被设为'True', 置0操作将直接作用在输入元素上 Shape:</p>
<ul>
<li>输入：math:(N, C, H, W)</li>
<li>输出：math:(N, C, H, W) (与输入相同）</li>
</ul>
<p>Examples:</p>
<p>```py
&gt;&gt;&gt; m = nn.Dropout2d(p=0.2)</p>
<p>```</p>
<p>```py
&gt;&gt;&gt; input = autograd.Variable(torch.randn(20, 16, 32, 32))
&gt;&gt;&gt; output = m(input)</p>
<p>```</p>
</blockquote>
<h3 id="dropout3d">Dropout3d</h3>
<pre><code class="language-py">class torch.nn.Dropout3d(p=0.5, inplace=False)
</code></pre>
<p>Dropout3d 将输入张量的所有通道随机地置为 0.被置为 0 的通道在每次调用时是随机的.</p>
<blockquote>
<p>通常输入数据来自 Conv3d 模块.</p>
<p>在论文 <a href="http://arxiv.org/abs/1411.4280">Efficient Object Localization Using Convolutional Networks</a> 中有如下 描述: 如果特征映射中的邻接像素是强相关的(在早期的卷积层中很常见）, 那么独立同分布 的 dropout 将不会正则化激活函数, 相反其会导致有效的学习率的下降.</p>
<p>在这样的情况下, 应该使用函数函数 nn.Dropout3d , 它能够促进特征映射之间的独立性.</p>
<p>Args: p (float,optional): 元素被置0的概率 inplace(bool, 可选）: 如果被设为 True , 置0操作将直接作用在输入元素上 Shape:</p>
<ul>
<li>输入：math:(N, C, H, W)</li>
<li>输出：math:(N, C, H, W) (与输入相同）</li>
</ul>
<p>Examples:</p>
<p>```py
&gt;&gt;&gt; m = nn.Dropout3d(p=0.2)</p>
<p>```</p>
<p>```py
&gt;&gt;&gt; input = autograd.Variable(torch.randn(20, 16, 4, 32, 32))
&gt;&gt;&gt; output = m(input)</p>
<p>```</p>
</blockquote>
<h3 id="alphadropout">AlphaDropout</h3>
<pre><code class="language-py">class torch.nn.AlphaDropout(p=0.5)
</code></pre>
<p>在输入上应用 Alpha Dropout.</p>
<blockquote>
<p>Alpha Dropout 是一种维持自正交性质的 Dropout . 对于一个均值为 0 和标准差为 1 的输入 来说, Alpha Dropout 能保持原始数据的均值和标准差.Alpha Dropout 和 SELU 激活函数 携手同行, 后者也保证了输出拥有与输入相同的均值和标准差.</p>
<p>Alpha Dropout 在训练期间, 按照伯努利概率分布, 以概率 p 随机地将输入张量中的部分元素 置进行掩盖, 在每次调用中, 被掩盖的元素是随机的, 并且对输出会进行缩放、变换等操作 以保持均值为 0、标准差为 1.</p>
<p>在求值期间, 模块简单的计算一个归一化的函数.</p>
<p>更多信息请参考论文: Self-Normalizing Neural Networks</p>
<p>Args: p(float）: 元素被掩盖的概率, 默认值: 0.5 Shape:</p>
<ul>
<li>输入：any.输入数据可以是任何大小</li>
<li>输出：Same.输出数据大小与输入相同</li>
</ul>
<p>Examples:</p>
<p>```py
&gt;&gt;&gt; m = nn.AlphaDropout(p=0.2)</p>
<p>```</p>
<p>```py
&gt;&gt;&gt; input = autograd.Variable(torch.randn(20, 16))
&gt;&gt;&gt; output = m(input)</p>
<p>```</p>
</blockquote>
<h2 id="sparse-layers">Sparse layers (稀疏层)</h2>
<h3 id="embedding">Embedding</h3>
<pre><code class="language-py">class torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2, scale_grad_by_freq=False, sparse=False)
</code></pre>
<p>一个简单的查找表, 存储了固定字典和大小的 embedding.</p>
<p>这个模块经常用来存储 word embeddings, 并通过索引来检索, 模块的输入是索引构成的列表, 输出是对应的 word embeddings.</p>
<p>参数：</p>
<ul>
<li><code>num_embeddings (int)</code> – embeddings 字典的大小</li>
<li><code>embedding_dim (int)</code> – 每个 embedding 向量的大小</li>
<li><code>padding_idx (int, 可选)</code> – 如果给出, 在索引处, 输出补零</li>
<li><code>max_norm (float, 可选)</code> – 如果给出, 重新归一化 embeddings, 使其范数小于该值</li>
<li><code>norm_type (float, 可选)</code> – 为 max_norm 选项计算 p 范数时 P</li>
<li><code>scale_grad_by_freq (boolean, 可选)</code> – 如果给出, 会根据 words 在 mini-batch 中的频率缩放梯度</li>
<li><code>sparse (boolean, 可选)</code> – 如果为 <code>True</code>, 关于权重矩阵的梯度是一个稀疏张量, 详情请参考稀疏梯度</li>
</ul>
<table>
<thead>
<tr>
<th>Variables:</th>
<th><strong>weight</strong> (Tensor) – shape 为 (num_embeddings, embedding_dim) 的模块的可学习权重</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>形状：</p>
<ul>
<li>输入：LongTensor <code>(N, W)</code>, N = mini-batch, W = 每个 mini-batch 中用来提取的索引数</li>
<li>输出：<code>(N, W, embedding_dim)</code></li>
</ul>
<p>注解：</p>
<p>请注意, 只支持有限数量的优化器. 稀疏梯度: 当前是 (<code>cuda</code> 和 <code>cpu</code>) 版本的 <code>optim.SGD</code>, 和 (<code>cpu</code>) 版本的 <code>optim.Adagrad</code>.</p>
<p>Examples:</p>
<pre><code class="language-py">&gt;&gt;&gt; # an Embedding module containing 10 tensors of size 3
&gt;&gt;&gt; embedding = nn.Embedding(10, 3)
&gt;&gt;&gt; # a batch of 2 samples of 4 indices each
&gt;&gt;&gt; input = Variable(torch.LongTensor([[1,2,4,5],[4,3,2,9]]))
&gt;&gt;&gt; embedding(input)

Variable containing:
(0 ,.,.) =
 -1.0822  1.2522  0.2434
 0.8393 -0.6062 -0.3348
 0.6597  0.0350  0.0837
 0.5521  0.9447  0.0498

(1 ,.,.) =
 0.6597  0.0350  0.0837
 -0.1527  0.0877  0.4260
 0.8393 -0.6062 -0.3348
 -0.8738 -0.9054  0.4281
[torch.FloatTensor of size 2x4x3]

&gt;&gt;&gt; # example with padding_idx
&gt;&gt;&gt; embedding = nn.Embedding(10, 3, padding_idx=0)
&gt;&gt;&gt; input = Variable(torch.LongTensor([[0,2,0,5]]))
&gt;&gt;&gt; embedding(input)

Variable containing:
(0 ,.,.) =
 0.0000  0.0000  0.0000
 0.3452  0.4937 -0.9361
 0.0000  0.0000  0.0000
 0.0706 -2.1962 -0.6276
[torch.FloatTensor of size 1x4x3]

</code></pre>
<h3 id="embeddingbag">EmbeddingBag</h3>
<pre><code class="language-py">class torch.nn.EmbeddingBag(num_embeddings, embedding_dim, max_norm=None, norm_type=2, scale_grad_by_freq=False, mode='mean')
</code></pre>
<p>计算一 个'bags' 里的 embedding s的均值或和, 不用实例化中间的 embeddings</p>
<p>对于固定长度的 bags</p>
<ul>
<li>nn.EmbeddingBag 和 <code>mode=sum</code> 相当于 nn.Embedding 与之后的 <code>torch.sum(dim=1)</code></li>
<li>其与 <code>mode=mean</code> 相当于 nn.Embedding 与之后的 <code>torch.mean(dim=1)</code></li>
</ul>
<p>然而, 比起一连串这样的操作, nn.EmbeddingBag 在时间和内存上更加高效.</p>
<p>参数：</p>
<ul>
<li><code>num_embeddings (int)</code> – embeddings 字典的大小</li>
<li><code>embedding_dim (int)</code> – 每个 embedding 向量的大小</li>
<li><code>max_norm (float, 可选)</code> – 如果给出, 重新归一化 embeddings, 使其范数小于该值</li>
<li><code>norm_type (float, 可选)</code> – 为 max_norm 选项计算 p 范数时的 P</li>
<li><code>scale_grad_by_freq (boolean, 可选)</code> – 如果给出, 会根据 words 在 mini-batch 中的频率缩放梯度</li>
<li><code>mode (string, 可选)</code> – 'sum' &#124; 'mean'. 指定减少 bag 的方式. 默认: 'mean'</li>
</ul>
<table>
<thead>
<tr>
<th>Variables:</th>
<th><strong>weight</strong> (Tensor) – shape 为 (num_embeddings, embedding_dim) 的模块的可学习权重</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Inputs: input, offsets</p>
<ul>
<li><code>input (N or BxN)</code>: LongTensor, 包括要提取的 embeddings 的索引, 当 <code>input</code> 是形状为 <code>N</code> 的 1D 张量时, 一个给出的 <code>offsets</code> 张量中包括: mini-batch 中每个新序列的起始位置</li>
<li><code>offsets (B or None)</code>: LongTensor, 包括一个 mini-batch 的可变长度序列中的每个新样本的起始位置 如果 <code>input</code> 是 2D (BxN) 的, offset 就不用再给出; 如果 <code>input</code> 是一个 mini-batch 的固定长度的序列, 每个序列的长度为 <code>N</code></li>
</ul>
<p>形状：</p>
<ul>
<li>输入：LongTensor <code>N</code>, N = 要提取的 embeddings 的数量,</li>
</ul>
<blockquote>
<p>或者是 LongTensor <code>BxN</code>, B = mini-batch 中序列的数量, N = 每个序列中 embeddings 的数量</p>
</blockquote>
<ul>
<li>Offsets: LongTensor <code>B</code>, B = bags 的数量, 值为每个 bag 中 <code>input</code> 的 offset, i.e. 是长度的累加. Offsets 不会给出, 如果 Input是 2D 的<code>BxN</code> 张量, 输入被认为是固定长度的序列</li>
<li>输出：<code>(B, embedding_dim)</code></li>
</ul>
<p>Examples:</p>
<pre><code class="language-py">&gt;&gt;&gt; # an Embedding module containing 10 tensors of size 3
&gt;&gt;&gt; embedding_sum = nn.EmbeddingBag(10, 3, mode='sum')
&gt;&gt;&gt; # a batch of 2 samples of 4 indices each
&gt;&gt;&gt; input = Variable(torch.LongTensor([1,2,4,5,4,3,2,9]))
&gt;&gt;&gt; offsets = Variable(torch.LongTensor([0,4]))
&gt;&gt;&gt; embedding_sum(input, offsets)

Variable containing:
-0.7296 -4.6926  0.3295
-0.5186 -0.5631 -0.2792
[torch.FloatTensor of size 2x3]

</code></pre>
<h2 id="distance-functions">Distance functions (距离函数)</h2>
<h3 id="cosinesimilarity">CosineSimilarity</h3>
<pre><code class="language-py">class torch.nn.CosineSimilarity(dim=1, eps=1e-08)
</code></pre>
<p>返回沿着 dim 方向计算的 x1 与 x2 之间的余弦相似度.</p>
<p><img alt="\text{similarity} = \dfrac{x_1 \cdot x_2}{\max(\Vert x_1 \Vert _2 \cdot \Vert x_2 \Vert _2, \epsilon)}" src="../img/tex-ff1174b83879355a400c05f671e9b5f0.gif" /></p>
<p>参数：</p>
<ul>
<li><code>dim (int, 可选)</code> – 计算余弦相似度的维度. Default: 1</li>
<li><code>eps (float, 可选)</code> – 小的值以避免被零除. Default: 1e-8</li>
</ul>
<p>形状：</p>
<ul>
<li>Input1: <img alt="(\ast_1, D, \ast_2)" src="../img/tex-76ea2d76e7fbeacf40dc68145779869f.gif" />, 其中的 D 表示 <code>dim</code> 的位置</li>
<li>Input2: <img alt="(\ast_1, D, \ast_2)" src="../img/tex-76ea2d76e7fbeacf40dc68145779869f.gif" />, 与 Input1 一样的 shape</li>
<li>输出：<img alt="(\ast_1, \ast_2)" src="../img/tex-17b66a91a0da1de30c6ed32b4a64db01.gif" /></li>
</ul>
<p>Examples:</p>
<pre><code class="language-py">&gt;&gt;&gt; input1 = autograd.Variable(torch.randn(100, 128))
&gt;&gt;&gt; input2 = autograd.Variable(torch.randn(100, 128))
&gt;&gt;&gt; cos = nn.CosineSimilarity(dim=1, eps=1e-6)
&gt;&gt;&gt; output = cos(input1, input2)
&gt;&gt;&gt; print(output)

</code></pre>
<h3 id="pairwisedistance">PairwiseDistance</h3>
<pre><code class="language-py">class torch.nn.PairwiseDistance(p=2, eps=1e-06)
</code></pre>
<p>计算向量 v1, v2 之间的 batchwise pairwise distance(分批成对距离):</p>
<p><img alt="\Vert x \Vert _p := \left( \sum_{i=1}^n \vert x_i \vert ^ p \right) ^ {1/p}" src="../img/tex-917e152b0e5035ea50d39db268bf9fca.gif" /></p>
<p>参数：</p>
<ul>
<li><code>p (_real_)</code> – norm degree(规范程度). Default: 2</li>
<li><code>eps (float, 可选)</code> – 小的值以避免被零除. Default: 1e-6</li>
</ul>
<p>形状：</p>
<ul>
<li>Input1: <img alt="(N, D)" src="../img/tex-df68502f8935f32bcaef520a28eb3e21.gif" />, 其中的 <code>D = vector dimension(向量维度)</code></li>
<li>Input2: <img alt="(N, D)" src="../img/tex-df68502f8935f32bcaef520a28eb3e21.gif" />, 与 Input1 的 shape 一样</li>
<li>输出：<img alt="(N, 1)" src="../img/tex-eb4023c8eeb604a58f58c44e29f7924a.gif" /></li>
</ul>
<p>Examples:</p>
<pre><code class="language-py">&gt;&gt;&gt; pdist = nn.PairwiseDistance(p=2)
&gt;&gt;&gt; input1 = autograd.Variable(torch.randn(100, 128))
&gt;&gt;&gt; input2 = autograd.Variable(torch.randn(100, 128))
&gt;&gt;&gt; output = pdist(input1, input2)

</code></pre>
<h2 id="loss-functions">Loss functions (损失函数)</h2>
<h3 id="l1loss">L1Loss</h3>
<pre><code class="language-py">class torch.nn.L1Loss(size_average=True, reduce=True)
</code></pre>
<p>创建一个衡量输入 <code>x</code> 与目标 <code>y</code> 之间差的绝对值的平均值的标准, 该 函数会逐元素地求出 <code>x</code> 和 <code>y</code> 之间差的绝对值, 最后返回绝对值的平均值.</p>
<p><img alt="{loss}(x, y) = 1/n \sum |x_i - y_i|" src="../img/tex-811b7ac372ab6fb167b5d3302c959dd3.gif" /></p>
<p><code>x</code> 和 <code>y</code> 可以是任意维度的数组, 但需要有相同数量的n个元素.</p>
<p>求和操作会对n个元素求和, 最后除以 <code>n</code> .</p>
<p>在构造函数的参数中传入 <code>size_average=False</code>, 最后求出来的绝对值将不会除以 <code>n</code>.</p>
<p>参数：</p>
<ul>
<li><code>size_average (bool, 可选)</code> – 默认情况下, loss 会在每个 mini-batch(小批量） 上取平均值. 如果字段 size_average 被设置为 <code>False</code>, loss 将会在每个 mini-batch(小批量） 上累加, 而不会取平均值. 当 reduce 的值为 <code>False</code> 时该字段会被忽略. 默认值: <code>True</code></li>
<li><code>reduce (bool, 可选)</code> – 默认情况下, loss 会在每个 mini-batch(小批量）上求平均值或者 求和. 当 reduce 是 <code>False</code> 时, 损失函数会对每个 batch 元素都返回一个 loss 并忽 略 size_average 字段. 默认值: <code>True</code></li>
</ul>
<p>形状：</p>
<ul>
<li>输入: <img alt="(N, *)" src="../img/tex-b0400390724111bb8254d8d21c6a9f0d.gif" />, <code>*</code> 表示任意数量的额外维度</li>
<li>目标: <img alt="(N, *)" src="../img/tex-b0400390724111bb8254d8d21c6a9f0d.gif" />, 和输入的shape相同</li>
<li>输出: 标量. 如果 reduce 是 <code>False</code> , 则输出为 <img alt="(N, *)" src="../img/tex-b0400390724111bb8254d8d21c6a9f0d.gif" />, shape与输出相同</li>
</ul>
<p>Examples:</p>
<pre><code class="language-py">&gt;&gt;&gt; loss = nn.L1Loss()
&gt;&gt;&gt; input = autograd.Variable(torch.randn(3, 5), requires_grad=True)
&gt;&gt;&gt; target = autograd.Variable(torch.randn(3, 5))
&gt;&gt;&gt; output = loss(input, target)
&gt;&gt;&gt; output.backward()

</code></pre>
<h3 id="mseloss">MSELoss</h3>
<pre><code class="language-py">class torch.nn.MSELoss(size_average=True, reduce=True)
</code></pre>
<p>输入 <code>x</code> 和 目标 <code>y</code> 之间的均方差</p>
<p><img alt="{loss}(x, y) = 1/n \sum |x_i - y_i|^2" src="../img/tex-b6d841cf2185a4776e1b61740870a9bf.gif" /></p>
<p><code>x</code> 和 <code>y</code> 可以是任意维度的数组, 但需要有相同数量的n个元素.</p>
<p>求和操作会对n个元素求和, 最后除以 <code>n</code>.</p>
<p>在构造函数的参数中传入 <code>size_average=False</code> , 最后求出来的绝对值将不会除以 <code>n</code>.</p>
<p>要得到每个 batch 中每个元素的 loss, 设置 <code>reduce</code> 为 <code>False</code>. 返回的 loss 将不会 取平均值, 也不会被 <code>size_average</code> 影响.</p>
<p>参数：</p>
<ul>
<li><code>size_average (bool, 可选)</code> – 默认情况下, loss 会在每个 mini-batch(小批量） 上取平均值. 如果字段 size_average 被设置为 <code>False</code> , loss 会在每 个 mini-batch(小批量）上求和. 只有当 reduce 的值为 <code>True</code> 才会生效. 默认值: <code>True</code></li>
<li><code>reduce (bool, 可选)</code> – 默认情况下, loss 会根据 size_average 的值在每 个 mini-batch(小批量）上求平均值或者求和. 当 reduce 是 <code>False</code> 时, 损失函数会对每 个 batch 元素都返回一个 loss 并忽略 size_average字段. 默认值: <code>True</code></li>
</ul>
<p>形状：</p>
<ul>
<li>输入: <img alt="(N, *)" src="../img/tex-b0400390724111bb8254d8d21c6a9f0d.gif" />, 其中 <code>*</code> 表示任意数量的额外维度.</li>
<li>目标: <img alt="(N, *)" src="../img/tex-b0400390724111bb8254d8d21c6a9f0d.gif" />, shape 跟输入相同</li>
</ul>
<p>Examples:</p>
<pre><code class="language-py">&gt;&gt;&gt; loss = nn.MSELoss()
&gt;&gt;&gt; input = autograd.Variable(torch.randn(3, 5), requires_grad=True)
&gt;&gt;&gt; target = autograd.Variable(torch.randn(3, 5))
&gt;&gt;&gt; output = loss(input, target)
&gt;&gt;&gt; output.backward()

</code></pre>
<h3 id="crossentropyloss">CrossEntropyLoss</h3>
<pre><code class="language-py">class torch.nn.CrossEntropyLoss(weight=None, size_average=True, ignore_index=-100, reduce=True)
</code></pre>
<p>该类把 <code>LogSoftMax</code> 和 <code>NLLLoss</code> 结合到了一个类中</p>
<p>当训练有 <code>C</code> 个类别的分类问题时很有效. 可选参数 <code>weight</code> 必须是一个1维 Tensor, 权重将被分配给各个类别. 对于不平衡的训练集非常有效.</p>
<p><code>input</code> 含有每个类别的分数</p>
<p><code>input</code> 必须是一个2维的形如 <code>(minibatch, C)</code> 的 <code>Tensor</code>.</p>
<p><code>target</code> 是一个类别索引 (0 to C-1), 对应于 <code>minibatch</code> 中的每个元素</p>
<p>loss 可以描述为:</p>
<pre><code class="language-py">loss(x, class) = -log(exp(x[class]) / (\sum_j exp(x[j])))
               = -x[class] + log(\sum_j exp(x[j]))

</code></pre>
<p>当 <code>weight</code> 参数存在时:</p>
<pre><code class="language-py">loss(x, class) = weight[class] * (-x[class] + log(\sum_j exp(x[j])))

</code></pre>
<p>loss 在每个 mini-batch(小批量）上取平均值.</p>
<p>参数：</p>
<ul>
<li><code>weight (Tensor, 可选)</code> – 自定义的每个类别的权重. 必须是一个长度为 <code>C</code> 的 Tensor</li>
<li><code>size_average (bool, 可选)</code> – 默认情况下, loss 会在每个 mini-batch(小批量） 上取平均值. 如果字段 size_average 被设置为 <code>False</code>, loss 将会在每个 mini-batch(小批量） 上累加, 而不会取平均值. 当 reduce 的值为 <code>False</code> 时该字段会被忽略.</li>
<li><code>ignore_index (int, 可选)</code> – 设置一个目标值, 该目标值会被忽略, 从而不会影响到 输入的梯度. 当 size_average 字段为 <code>True</code> 时, loss 将会在没有被忽略的元素上 取平均.</li>
<li><code>reduce (bool, 可选)</code> – 默认情况下, loss 会根据 size_average 的值在每 个 mini-batch(小批量）上求平均值或者求和. 当 reduce 是 <code>False</code> 时, 损失函数会对 每个 batch 元素都返回一个 loss 并忽略 size_average 字段. 默认值: <code>True</code></li>
</ul>
<p>形状：</p>
<ul>
<li>输入: <img alt="(N, C)" src="../img/tex-0adbd266d78d43d0298b110b7b60ef70.gif" />, 其中 <code>C</code> 是类别的数量</li>
<li>目标: <img alt="(N)" src="../img/tex-6e622d72b90c249c3e04aebf0eb12ca4.gif" />, 其中的每个元素都满足 <code>0 &amp;lt;= targets[i] &amp;lt;= C-1</code></li>
<li>输出: 标量. 如果 reduce 是 <code>False</code>, 则输出为 <img alt="(N)" src="../img/tex-6e622d72b90c249c3e04aebf0eb12ca4.gif" />.</li>
</ul>
<p>Examples:</p>
<pre><code class="language-py">&gt;&gt;&gt; loss = nn.CrossEntropyLoss()
&gt;&gt;&gt; input = autograd.Variable(torch.randn(3, 5), requires_grad=True)
&gt;&gt;&gt; target = autograd.Variable(torch.LongTensor(3).random_(5))
&gt;&gt;&gt; output = loss(input, target)
&gt;&gt;&gt; output.backward()

</code></pre>
<h3 id="nllloss">NLLLoss</h3>
<pre><code class="language-py">class torch.nn.NLLLoss(weight=None, size_average=True, ignore_index=-100, reduce=True)
</code></pre>
<p>负对数似然损失. 用于训练 <code>C</code> 个类别的分类问题. 可选参数 <code>weight</code> 是 一个1维的 Tensor, 用来设置每个类别的权重. 当训练集不平衡时该参数十分有用.</p>
<p>由前向传播得到的输入应该含有每个类别的对数概率: 输入必须是形如 <code>(minibatch, C)</code> 的 2维 Tensor.</p>
<p>在一个神经网络的最后一层添加 <code>LogSoftmax</code> 层可以得到对数概率. 如果你不希望在神经网络中 加入额外的一层, 也可以使用 <code>CrossEntropyLoss</code> 函数.</p>
<p>该损失函数需要的目标值是一个类别索引 <code>(0 到 C-1, 其中 C 是类别数量)</code></p>
<p>该 loss 可以描述为:</p>
<pre><code class="language-py">loss(x, class) = -x[class]

</code></pre>
<p>或者当 weight 参数存在时可以描述为:</p>
<pre><code class="language-py">loss(x, class) = -weight[class] * x[class]

</code></pre>
<p>又或者当 ignore_index 参数存在时可以描述为:</p>
<pre><code class="language-py">loss(x, class) = class != ignoreIndex ? -weight[class] * x[class] : 0

</code></pre>
<p>参数：</p>
<ul>
<li><code>weight (Tensor, 可选)</code> – 自定义的每个类别的权重. 必须是一个长度为 <code>C</code> 的 Tensor</li>
<li><code>size_average (bool, 可选)</code> – 默认情况下, loss 会在每个 mini-batch(小批量） 上取平均值. 如果字段 size_average 被设置为 <code>False</code>, loss 将会在每个 mini-batch(小批量） 上累加, 而不会取平均值. 当 reduce 的值为 <code>False</code> 时该字段会被忽略. 默认值: <code>True</code></li>
<li><code>ignore_index (int, 可选)</code> – 设置一个目标值, 该目标值会被忽略, 从而不会影响到 输入的梯度. 当 size_average 为 <code>True</code> 时, loss 将会在没有被忽略的元素上 取平均值.</li>
<li><code>reduce (bool, 可选)</code> – 默认情况下, loss 会在每个 mini-batch(小批量）上求平均值或者 求和. 当 reduce 是 <code>False</code> 时, 损失函数会对每个 batch 元素都返回一个 loss 并忽 略 size_average 字段. 默认值: <code>True</code></li>
</ul>
<p>形状：</p>
<ul>
<li>输入: <img alt="(N, C)" src="../img/tex-0adbd266d78d43d0298b110b7b60ef70.gif" />, 其中 <code>C</code> 是类别的数量</li>
<li>目标: <img alt="(N)" src="../img/tex-6e622d72b90c249c3e04aebf0eb12ca4.gif" />, 其中的每个元素都满足 <code>0 &amp;lt;= targets[i] &amp;lt;= C-1</code></li>
<li>输出: 标量. 如果 reduce 是 <code>False</code>, 则输出为 <img alt="(N)" src="../img/tex-6e622d72b90c249c3e04aebf0eb12ca4.gif" />.</li>
</ul>
<p>Examples:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = nn.LogSoftmax()
&gt;&gt;&gt; loss = nn.NLLLoss()
&gt;&gt;&gt; # input is of size N x C = 3 x 5
&gt;&gt;&gt; input = autograd.Variable(torch.randn(3, 5), requires_grad=True)
&gt;&gt;&gt; # each element in target has to have 0 &lt;= value &lt; C
&gt;&gt;&gt; target = autograd.Variable(torch.LongTensor([1, 0, 4]))
&gt;&gt;&gt; output = loss(m(input), target)
&gt;&gt;&gt; output.backward()

</code></pre>
<h3 id="poissonnllloss">PoissonNLLLoss</h3>
<pre><code class="language-py">class torch.nn.PoissonNLLLoss(log_input=True, full=False, size_average=True, eps=1e-08)
</code></pre>
<p>目标值为泊松分布的负对数似然损失.</p>
<p>该损失可以描述为:</p>
<blockquote>
<p>target ~ Pois(input) loss(input, target) = input - target * log(input) + log(target!)</p>
</blockquote>
<p>最后一项可以被省略或者用 Stirling 公式来近似. 该近似用于大于1的目标值. 当目标值 小于或等于1时, 则将0加到 loss 中.</p>
<p>参数：</p>
<ul>
<li><code>log_input (bool, 可选)</code> – 如果设置为 <code>True</code> , loss 将会按照公 式 <code>exp(input) - target * input</code> 来计算, 如果设置为 <code>False</code> , loss 将会按照 <code>input - target * log(input+eps)</code> 计算.</li>
<li><code>full (bool, 可选)</code> – 是否计算全部的 loss, i. e. 加上 Stirling 近似项 <code>target * log(target) - target + 0.5 * log(2 * pi * target)</code>.</li>
<li><code>size_average (bool, 可选)</code> – 默认情况下, loss 会在每个 mini-batch(小批量） 上取平均值. 如果字段 size_average 被设置为 <code>False</code>, loss 将会在每个 mini-batch(小批量） 上累加, 而不会取平均值.</li>
<li><code>eps (float, 可选)</code> – 当 log_input==<code>False</code> 时, 取一个很小的值用来避免计算 log(0). 默认值: 1e-8</li>
</ul>
<p>Examples:</p>
<pre><code class="language-py">&gt;&gt;&gt; loss = nn.PoissonNLLLoss()
&gt;&gt;&gt; log_input = autograd.Variable(torch.randn(5, 2), requires_grad=True)
&gt;&gt;&gt; target = autograd.Variable(torch.randn(5, 2))
&gt;&gt;&gt; output = loss(log_input, target)
&gt;&gt;&gt; output.backward()

</code></pre>
<h3 id="nllloss2d">NLLLoss2d</h3>
<pre><code class="language-py">class torch.nn.NLLLoss2d(weight=None, size_average=True, ignore_index=-100, reduce=True)
</code></pre>
<p>对于图片输入的负对数似然损失. 它计算每个像素的负对数似然损失.</p>
<p>参数：</p>
<ul>
<li><code>weight (Tensor, 可选)</code> – 自定义的每个类别的权重. 必须是一个长度为 <code>C</code> 的 Tensor</li>
<li><code>size_average</code> – 默认情况下, loss 会在每个 mini-batch(小批量） 上取平均值. 如果字段 size_average 被设置为 <code>False</code>, loss 将会在每个 mini-batch(小批量） 上累加, 而不会取平均值. 当 reduce 的值为 <code>False</code> 时该字段会被忽略. 默认值: <code>True</code></li>
<li><code>reduce (bool, 可选)</code> – 默认情况下, loss 会在每个 mini-batch(小批量）上求平均值或者 求和. 当 reduce 是 <code>False</code> 时, 损失函数会对每个 batch 元素都返回一个 loss 并忽 略 size_average 字段. 默认值: <code>True</code></li>
</ul>
<p>形状：</p>
<ul>
<li>输入：<img alt="(N, C, H, W)" src="../img/tex-38d00342060234da90e0c2c5493892cb.gif" /> where <code>C = number of classes</code></li>
<li>Target: <img alt="(N, H, W)" src="../img/tex-54826f002c63d212644cea7b448d5816.gif" /> where each value is <code>0 &amp;lt;= targets[i] &amp;lt;= C-1</code></li>
<li>输出：scalar. If reduce is <code>False</code>, then <img alt="(N, H, W)" src="../img/tex-54826f002c63d212644cea7b448d5816.gif" /> instead.</li>
</ul>
<p>Examples:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = nn.Conv2d(16, 32, (3, 3)).float()
&gt;&gt;&gt; loss = nn.NLLLoss2d()
&gt;&gt;&gt; # input is of size N x C x height x width
&gt;&gt;&gt; input = autograd.Variable(torch.randn(3, 16, 10, 10))
&gt;&gt;&gt; # each element in target has to have 0 &lt;= value &lt; C
&gt;&gt;&gt; target = autograd.Variable(torch.LongTensor(3, 8, 8).random_(0, 4))
&gt;&gt;&gt; output = loss(m(input), target)
&gt;&gt;&gt; output.backward()

</code></pre>
<h3 id="kldivloss">KLDivLoss</h3>
<pre><code class="language-py">class torch.nn.KLDivLoss(size_average=True, reduce=True)
</code></pre>
<p><a href="https://en.wikipedia.org/wiki/Kullback-Leibler_divergence">Kullback-Leibler divergence</a> 损失</p>
<p>KL 散度可用于衡量不同的连续分布之间的距离, 在连续的输出分布的空间上(离散采样)上进行直接回归时 很有效.</p>
<p>跟 <code>NLLLoss</code> 一样, <code>input</code> 需要含有 <em>对数概率</em> , 不同于 <code>ClassNLLLoss</code>, <code>input</code> 可 以不是2维的 Tensor, 因为该函数会逐元素地求值.</p>
<p>该方法需要一个shape跟 <code>input</code> <code>Tensor</code> 一样的 <code>target</code> <code>Tensor</code>.</p>
<p>损失可以描述为:</p>
<p><img alt="loss(x, target) = 1/n \sum(target_i * (log(target_i) - x_i))" src="../img/tex-9263028fed946a64b024a39508fa4339.gif" /></p>
<p>默认情况下, loss 会在每个 mini-batch(小批量）上和 <strong>维度</strong> 上取平均值. 如果字段 <code>size_average</code> 设置为 <code>False</code>, 则 loss 不会取平均值.</p>
<p>参数：</p>
<ul>
<li><code>size_average (bool, 可选)</code> – 默认情况下, loss 会在每个 mini-batch(小批量）上 和 <strong>维度</strong> 上取平均值. 如果设置为 <code>False</code>, 则 loss 会累加, 而不是取平均值.</li>
<li><code>reduce (bool, 可选)</code> – 默认情况下, loss 会根据 size_average 在每 个 mini-batch(小批量）上求平均值或者求和. 当 reduce 是 <code>False</code> 时, 损失函数会对每 个 batch 元素都返回一个 loss 并忽略 size_average 字段. 默认值: <code>True</code></li>
</ul>
<p>形状：</p>
<ul>
<li>输入: <img alt="(N, *)" src="../img/tex-b0400390724111bb8254d8d21c6a9f0d.gif" />, 其中 <code>*</code> 表示任意数量的额外维度.</li>
<li>目标: <img alt="(N, *)" src="../img/tex-b0400390724111bb8254d8d21c6a9f0d.gif" />, shape 跟输入相同</li>
<li>输出: 标量. 如果 <code>reduce</code> 是 <code>True</code>, 则输出为 <img alt="(N, *)" src="../img/tex-b0400390724111bb8254d8d21c6a9f0d.gif" />, shape 跟输入相同.</li>
</ul>
<h3 id="bceloss">BCELoss</h3>
<pre><code class="language-py">class torch.nn.BCELoss(weight=None, size_average=True)
</code></pre>
<p>计算目标和输出之间的二进制交叉熵:</p>
<p><img alt="loss(o, t) = - 1/n \sum_i (t[i] * log(o[i]) + (1 - t[i]) * log(1 - o[i]))" src="../img/tex-c665b18ba021122b899fb17d4f215496.gif" /></p>
<p>当定义了 weight 参数时:</p>
<p><img alt="loss(o, t) = - 1/n \sum_i weight[i] * (t[i] * log(o[i]) + (1 - t[i]) * log(1 - o[i]))" src="../img/tex-a75feea82623649f822315604b9dff5a.gif" /></p>
<p>这可用于测量重构的误差, 例如自动编码机. 注意目标的值 <code>t[i]</code> 的范围为0到1之间.</p>
<p>参数：</p>
<ul>
<li><code>weight (Tensor, 可选)</code> – 自定义的每个 batch 元素的 loss 的权重. 必须是一个长度为 “nbatch” 的 的 Tensor</li>
<li><code>size_average (bool, 可选)</code> – 默认情况下, loss 会在每个 mini-batch(小批量） 上取平均值. 如果字段 size_average 被设置为 <code>False</code> , loss 会在每 个 mini-batch(小批量）上累加, 而不是取平均值. 默认值: <code>True</code></li>
</ul>
<p>形状：</p>
<ul>
<li>输入: <img alt="(N, *)" src="../img/tex-b0400390724111bb8254d8d21c6a9f0d.gif" />, 其中 <code>*</code> 表示任意数量的额外维度.</li>
<li>目标: <img alt="(N, *)" src="../img/tex-b0400390724111bb8254d8d21c6a9f0d.gif" />, shape 跟输入相同</li>
</ul>
<p>Examples:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = nn.Sigmoid()
&gt;&gt;&gt; loss = nn.BCELoss()
&gt;&gt;&gt; input = autograd.Variable(torch.randn(3), requires_grad=True)
&gt;&gt;&gt; target = autograd.Variable(torch.FloatTensor(3).random_(2))
&gt;&gt;&gt; output = loss(m(input), target)
&gt;&gt;&gt; output.backward()

</code></pre>
<h3 id="bcewithlogitsloss">BCEWithLogitsLoss</h3>
<pre><code class="language-py">class torch.nn.BCEWithLogitsLoss(weight=None, size_average=True)
</code></pre>
<p>该损失函数把 <code>Sigmoid</code> 层集成到了 <code>BCELoss</code> 类中. 该版比用一个简单的 <code>Sigmoid</code> 层和 <code>BCELoss</code> 在数值上更稳定, 因为把这两个操作合并为一个层之后, 可以利用 log-sum-exp 的 技巧来实现数值稳定.</p>
<p>目标和输出之间的二值交叉熵(不含sigmoid函数)是:</p>
<p><img alt="loss(o, t) = - 1/n \sum_i (t[i] * log(sigmoid(o[i])) + (1 - t[i]) * log(1 - sigmoid(o[i])))" src="../img/tex-f7f1487d3258a1beca2e4eb99261fbad.gif" /></p>
<p>当定义了 weight 参数之后可描述为:</p>
<p><img alt="loss(o, t) = - 1/n \sum_i weight[i] * (t[i] * log(sigmoid(o[i])) + (1 - t[i]) * log(1 - sigmoid(o[i])))" src="../img/tex-131660545fb84e407d9855bc8e08e7c1.gif" /></p>
<p>这可用于测量重构的误差, 例如自动编码机. 注意目标的值 <code>t[i]</code> 的范围为0到1之间.</p>
<p>参数：</p>
<ul>
<li><code>weight (Tensor, 可选)</code> – 自定义的每个 batch 元素的 loss 的权重. 必须是一个长度 为 “nbatch” 的 Tensor</li>
<li><code>size_average (bool, 可选)</code> – 默认情况下, loss 会在每个 mini-batch(小批量） 上取平均值. 如果字段 size_average 被设置为 <code>False</code> , loss 会在每 个 mini-batch(小批量）上累加, 而不是取平均值. 默认值: <code>True</code></li>
</ul>
<p>形状：</p>
<ul>
<li>输入: <img alt="(N, *)" src="../img/tex-b0400390724111bb8254d8d21c6a9f0d.gif" />, 其中 <code>*</code> 表示任意数量的额外维度.</li>
<li>目标: <img alt="(N, *)" src="../img/tex-b0400390724111bb8254d8d21c6a9f0d.gif" />, shape 跟输入相同</li>
</ul>
<p>Examples:</p>
<pre><code class="language-py">&gt;&gt;&gt; loss = nn.BCEWithLogitsLoss()
&gt;&gt;&gt; input = autograd.Variable(torch.randn(3), requires_grad=True)
&gt;&gt;&gt; target = autograd.Variable(torch.FloatTensor(3).random_(2))
&gt;&gt;&gt; output = loss(input, target)
&gt;&gt;&gt; output.backward()

</code></pre>
<h3 id="marginrankingloss">MarginRankingLoss</h3>
<pre><code class="language-py">class torch.nn.MarginRankingLoss(margin=0, size_average=True)
</code></pre>
<p>创建一个衡量 mini-batch(小批量) 中的2个1维 <code>Tensor</code> 的输入 <code>x1</code> 和 <code>x2</code>, 和1个1维 <code>Tensor</code> 的目标 <code>y</code>(<code>y</code> 的取值是 <code>1</code> 或者 <code>-1</code>) 之间损失的标准.</p>
<p>如果 <code>y == 1</code> 则认为第一个输入值应该排列在第二个输入值之上(即值更大), <code>y == -1</code> 时则相反.</p>
<p>对于 mini-batch(小批量) 中每个实例的损失函数如下:</p>
<pre><code class="language-py">loss(x, y) = max(0, -y * (x1 - x2) + margin)

</code></pre>
<p>如果内部变量 <code>size_average = True</code>, 则损失函数计算批次中所有实例的损失值的平均值; 如果 <code>size_average = False</code>, 则损失函数计算批次中所有实例的损失至的合计. <code>size_average</code> 默认值为 <code>True</code>.</p>
<h3 id="hingeembeddingloss">HingeEmbeddingLoss</h3>
<pre><code class="language-py">class torch.nn.HingeEmbeddingLoss(margin=1.0, size_average=True)
</code></pre>
<p>衡量输入 Tensor(张量) <code>x</code> 和 目标 Tensor(张量) <code>y</code> (取值为 <code>1</code> 和 <code>-1</code>) 之间的损失值. 此方法通常用来衡量两个输入值是否相似, 例如使用L1成对距离作为 <code>x</code>, 并且通常用来进行非线性嵌入学习或者 半监督学习:</p>
<pre><code class="language-py">                 { x_i,                  if y_i ==  1
loss(x, y) = 1/n {
                 { max(0, margin - x_i), if y_i == -1

</code></pre>
<p><code>x</code> 和 <code>y</code> 分别可以是具有 <code>n</code> 个元素的任意形状. 合计操作对所有元素进行计算.</p>
<p>如果 <code>size_average=False</code>, 则计算时不会除以 <code>n</code> 取平均值.</p>
<p><code>margin</code> 的默认值是 <code>1</code>, 或者可以通过构造函数来设置.</p>
<h3 id="multilabelmarginloss">MultiLabelMarginLoss</h3>
<pre><code class="language-py">class torch.nn.MultiLabelMarginLoss(size_average=True)
</code></pre>
<p>创建一个标准, 用以优化多元分类问题的合页损失函数 (基于空白的损失), 计算损失值时 需要2个参数分别为输入, <code>x</code> (一个2维小批量 <code>Tensor</code>) 和输出 <code>y</code> (一个2维 <code>Tensor</code>, 其值为 <code>x</code> 的索引值). 对于mini-batch(小批量) 中的每个样本按如下公式计算损失:</p>
<pre><code class="language-py">loss(x, y) = sum_ij(max(0, 1 - (x[y[j]] - x[i]))) / x.size(0)

</code></pre>
<p>其中 <code>i</code> 的取值范围是 <code>0</code> 到 <code>x.size(0)</code>, <code>j</code> 的取值范围是 <code>0</code> 到 <code>y.size(0)</code>, <code>y[j] &amp;gt;= 0</code>, 并且对于所有 <code>i</code> 和 <code>j</code> 有 <code>i != y[j]</code>.</p>
<p><code>y</code> 和 <code>x</code> 必须有相同的元素数量.</p>
<p>此标准仅考虑 <code>y[j]</code> 中最先出现的非零值.</p>
<p>如此可以允许每个样本可以有数量不同的目标类别.</p>
<h3 id="smoothl1loss">SmoothL1Loss</h3>
<pre><code class="language-py">class torch.nn.SmoothL1Loss(size_average=True, reduce=True)
</code></pre>
<p>创建一个标准, 当某个元素的错误值的绝对值小于1时使用平方项计算, 其他情况则使用L1范式计算. 此方法创建的标准对于异常值不如 <code>MSELoss</code>敏感, 但是同时在某些情况下可以防止梯度爆炸 (比如 参见论文 “Fast R-CNN” 作者 Ross Girshick). 也被称为 Huber 损失函数:</p>
<pre><code class="language-py">                      { 0.5 * (x_i - y_i)^2, if |x_i - y_i| &lt; 1
loss(x, y) = 1/n \sum {
                      { |x_i - y_i| - 0.5,   otherwise

</code></pre>
<p><code>x</code> 和 <code>y</code> 可以是任意形状只要都具备总计 <code>n</code> 个元素 合计仍然针对所有元素进行计算, 并且最后除以 <code>n</code>.</p>
<p>如果把内部变量 <code>size_average</code> 设置为 <code>False</code>, 则不会被除以 <code>n</code>.</p>
<p>参数：</p>
<ul>
<li><code>size_average (bool, 可选)</code> – 损失值默认会按照所有元素取平均值. 但是, 如果 size_average 被 设置为 <code>False</code>, 则损失值为所有元素的合计. 如果 reduce 参数设为 <code>False</code>, 则忽略此参数的值. 默认: <code>True</code></li>
<li><code>reduce (bool, 可选)</code> – 损失值默认会按照所有元素取平均值或者取合计值. 当 reduce 设置为 <code>False</code> 时, 损失函数对于每个元素都返回损失值并且忽略 size_average 参数. 默认: <code>True</code></li>
</ul>
<p>形状：</p>
<ul>
<li>输入: <img alt="(N, *)" src="../img/tex-b0400390724111bb8254d8d21c6a9f0d.gif" /> <code>*</code> 代表任意个其他维度</li>
<li>目标: <img alt="(N, *)" src="../img/tex-b0400390724111bb8254d8d21c6a9f0d.gif" />, 同输入</li>
<li>输出: 标量. 如果 reduce 设为 <code>False</code> 则为 <img alt="(N, *)" src="../img/tex-b0400390724111bb8254d8d21c6a9f0d.gif" />, 同输入</li>
</ul>
<h3 id="softmarginloss">SoftMarginLoss</h3>
<pre><code class="language-py">class torch.nn.SoftMarginLoss(size_average=True)
</code></pre>
<p>创建一个标准, 用以优化两分类的 logistic loss. 输入为 <code>x</code> (一个2维 mini-batch Tensor)和 目标 <code>y</code> (一个包含 <code>1</code> 或者 <code>-1</code> 的 Tensor).</p>
<pre><code class="language-py">loss(x, y) = sum_i (log(1 + exp(-y[i]*x[i]))) / x.nelement()

</code></pre>
<p>可以通过设置 <code>self.size_average</code> 为 <code>False</code> 来禁用按照元素数量取平均的正则化操作.</p>
<h3 id="multilabelsoftmarginloss">MultiLabelSoftMarginLoss</h3>
<pre><code class="language-py">class torch.nn.MultiLabelSoftMarginLoss(weight=None, size_average=True)
</code></pre>
<p>创建一个标准, 基于输入 <code>x</code> 和目标 <code>y</code>的 max-entropy(最大熵), 优化多标签 one-versus-all 损失. 输入 <code>x</code> 为一个2维 mini-batch <code>Tensor</code>, 目标 <code>y</code> 为2进制2维 <code>Tensor</code>. 对每个 mini-batch 中的样本, 对应的 loss 为:</p>
<pre><code class="language-py">loss(x, y) = - sum_i (y[i] * log( 1 / (1 + exp(-x[i])))
                  + ( (1-y[i]) * log(exp(-x[i]) / (1 + exp(-x[i]))) )

</code></pre>
<p>其中 <code>i == 0</code> 至 <code>x.nElement()-1</code>, <code>y[i] in {0,1}</code>. <code>y</code> 和 <code>x</code> 必须具有相同的维度.</p>
<h3 id="cosineembeddingloss">CosineEmbeddingLoss</h3>
<pre><code class="language-py">class torch.nn.CosineEmbeddingLoss(margin=0, size_average=True)
</code></pre>
<p>新建一个标准, 用以衡量输入 <code>Tensor</code> x1, x2 和取值为 1 或者 -1 的标签 <code>Tensor</code> <code>y</code>之间的 损失值. 此标准用 cosine 距离来衡量2个输入参数之间是否相似, 并且一般用来学习非线性 embedding 或者半监督 学习.</p>
<p><code>margin</code> 应该取 <code>-1</code> 到 <code>1</code> 之间的值, 建议取值范围是 <code>0</code> 到 <code>0.5</code>. 如果没有设置 <code>margin</code> 参数, 则默认值取 <code>0</code>.</p>
<p>每个样本的损失函数如下:</p>
<pre><code class="language-py">             { 1 - cos(x1, x2),              if y ==  1
loss(x, y) = {
             { max(0, cos(x1, x2) - margin), if y == -1

</code></pre>
<p>如果内部变量 <code>size_average</code> 设置为 <code>True</code>, 则损失函数以 batch 中所有的样本数取平均值; 如果 <code>size_average</code> 设置为 <code>False</code>, 则损失函数对 batch 中所有的样本求和. 默认情况下, <code>size_average = True</code>.</p>
<h3 id="multimarginloss">MultiMarginLoss</h3>
<pre><code class="language-py">class torch.nn.MultiMarginLoss(p=1, margin=1, weight=None, size_average=True)
</code></pre>
<p>创建一个标准, 用以优化多元分类问题的合页损失函数 (基于空白的损失), 计算损失值时 需要2个参数分别为输入, <code>x</code> (一个2维小批量 <code>Tensor</code>) 和输出 <code>y</code> (一个1维 <code>Tensor</code>, 其值为 <code>x</code> 的索引值, <code>0</code> &lt;= <code>y</code> &lt;= <code>x.size(1)</code>):</p>
<p>对于每个 mini-batch(小批量) 样本:</p>
<pre><code class="language-py">loss(x, y) = sum_i(max(0, (margin - x[y] + x[i]))^p) / x.size(0)
</code></pre>
<p>其中 <code>i == 0</code> 至 <code>x.size(0)</code> 并且 <code>i != y</code>.</p>
<p>可选择的, 如果您不想所有的类拥有同样的权重的话, 您可以通过在构造函数中传入 <code>weight</code> 参数来 解决这个问题, <code>weight</code> 是一个1维 Tensor.</p>
<p>传入 <code>weight</code> 后, 损失函数变为:</p>
<blockquote>
<p>loss(x, y) = sum_i(max(0, w[y] * (margin - x[y] - x[i]))^p) / x.size(0)</p>
</blockquote>
<p>默认情况下, 求出的损失值会对每个 minibatch 样本的结果取平均. 可以通过设置 <code>size_average</code> 为 <code>False</code> 来用合计操作取代取平均操作.</p>
<h3 id="tripletmarginloss">TripletMarginLoss</h3>
<pre><code class="language-py">class torch.nn.TripletMarginLoss(margin=1.0, p=2, eps=1e-06, swap=False)
</code></pre>
<p>创建一个标准, 用以衡量三元组合的损失值, 计算损失值时需要3个输入张量 <code>x1</code>, <code>x2</code>, <code>x3</code> 和 一个大于零的 <code>margin</code> 值. 此标准可以用来衡量输入样本间的相对相似性. 一个三元输入组合由 <code>a</code>, <code>p</code> 和 <code>n</code>: anchor, positive 样本 和 negative 样本组成. 所有输入变量的形式必须为 <img alt="(N, D)" src="../img/tex-df68502f8935f32bcaef520a28eb3e21.gif" />.</p>
<p>距离交换的详细说明请参考论文 <a href="http://www.iis.ee.ic.ac.uk/%7Evbalnt/shallow_descr/TFeat_paper.pdf">Learning shallow convolutional feature descriptors with triplet losses</a> by V. Balntas, E. Riba et al.</p>
<p><img alt="L(a, p, n) = \frac{1}{N} \left( \sum_{i=1}^N \max {d(a_i, p_i) - d(a_i, n_i) + {\rm margin}, 0} \right)" src="../img/tex-cf988cda62d3487317968d4fce873e69.gif" /></p>
<p>其中 <img alt="d(x_i, y_i) = \left\lVert {\bf x}_i - {\bf y}_i \right\rVert_p" src="../img/tex-58c638474fe46f788c74d4a7a62c5e1f.gif" />.</p>
<p>参数：</p>
<ul>
<li><code>anchor</code> – anchor 输入 tensor</li>
<li><code>positive</code> – positive 输入 tensor</li>
<li><code>negative</code> – negative 输入 tensor</li>
<li><code>p</code> – 正则化率. Default: 2</li>
</ul>
<p>形状：</p>
<ul>
<li>输入：<img alt="(N, D)" src="../img/tex-df68502f8935f32bcaef520a28eb3e21.gif" /> 其中 <code>D = vector dimension</code></li>
<li>输出：<img alt="(N, 1)" src="../img/tex-eb4023c8eeb604a58f58c44e29f7924a.gif" /></li>
</ul>
<pre><code class="language-py">&gt;&gt;&gt; triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)
&gt;&gt;&gt; input1 = autograd.Variable(torch.randn(100, 128))
&gt;&gt;&gt; input2 = autograd.Variable(torch.randn(100, 128))
&gt;&gt;&gt; input3 = autograd.Variable(torch.randn(100, 128))
&gt;&gt;&gt; output = triplet_loss(input1, input2, input3)
&gt;&gt;&gt; output.backward()

</code></pre>
<h2 id="vision-layers">Vision layers (视觉层)</h2>
<h3 id="pixelshuffle">PixelShuffle</h3>
<pre><code class="language-py">class torch.nn.PixelShuffle(upscale_factor)
</code></pre>
<p>对张量中形如 ![(*, C * r^2, H, W]](img/tex-92312e8331c8111c53ea986a22d9bfd2.gif) 的元素, 重新排列成 <img alt="(C, H * r, W * r)" src="../img/tex-fe414d0d481aa30dbc63aa33276b42d6.gif" />.</p>
<p>当使用 stride = <img alt="1/r" src="../img/tex-305837540b14c05ec56167e74aea3132.gif" /> 的高效子像素卷积很有用.</p>
<p>参考如下论文获得更多信息: <a href="https://arxiv.org/abs/1609.05158">Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network</a> Shi et. al (2016) .</p>
<p>参数：<code>upscale_factor (int)</code> – 增加空间分辨率的因子</p>
<p>形状：</p>
<ul>
<li>输入: <img alt="(N, C * {upscale_factor}^2, H, W)" src="../img/tex-35b4ae66acb21958d7890c5288813439.gif" /></li>
<li>输出: <img alt="(N, C, H * {upscale_factor}, W * {upscale_factor})" src="../img/tex-506e21c8cc042a71de4738e50912853e.gif" /></li>
</ul>
<p>Examples:</p>
<pre><code class="language-py">&gt;&gt;&gt; ps = nn.PixelShuffle(3)
&gt;&gt;&gt; input = autograd.Variable(torch.Tensor(1, 9, 4, 4))
&gt;&gt;&gt; output = ps(input)
&gt;&gt;&gt; print(output.size())
torch.Size([1, 1, 12, 12])

</code></pre>
<h3 id="upsample">Upsample</h3>
<pre><code class="language-py">class torch.nn.Upsample(size=None, scale_factor=None, mode='nearest')
</code></pre>
<p>对给定的多通道一维时序数据, 二维空间数据, 或三维容积数据进行上采样.</p>
<p>输入数据的格式为 <code>minibatch x channels x [depth] x [height] x width</code>. 因此, 对于2-D空间数据的输入, 期望得到一个4-D张量；对于3-D立体数据输入, 期望得到一个5-D张量.</p>
<p>对3D, 4D, 5D的输入张量进行最近邻、线性、双线性和三线性采样, 可用于该上采样方法.</p>
<p>可以提供 <code>scale_factor</code> 或目标输出的 <code>size</code> 来计算输出的大小. (不能同时都给, 因为这样做是含糊不清的. )</p>
<p>参数：</p>
<ul>
<li><code>size (tuple, 可选)</code> – 整型数的元组 ([D_out], [H_out], W_out) 输出大小</li>
<li><code>scale_factor (int / tuple[int...], 可选)</code> – 图像高度/宽度/深度的乘数</li>
<li><code>mode (string, 可选)</code> – 上采样算法: nearest &#124; linear &#124; bilinear &#124; trilinear. 默认为: nearest</li>
</ul>
<p>形状：</p>
<ul>
<li>输入: <img alt="(N, C, W_{in})" src="../img/tex-ea37c9cca622977d5bfa09dc10d5289e.gif" />, <img alt="(N, C, H_{in}, W_{in})" src="../img/tex-d5acb55d3d2dd49b5b0d71ab7cf3b2d1.gif" /> 或 <img alt="(N, C, D_{in}, H_{in}, W_{in})" src="../img/tex-ce19deda602cf16ded15c0fb9cd5d280.gif" /></li>
<li>输出: <img alt="(N, C, W_{out})" src="../img/tex-86a0834f28ea21dbf773dcde05f5a8a9.gif" />, <img alt="(N, C, H_{out}, W_{out})" src="../img/tex-403ffd9231342159e36ba660b2bf3ff3.gif" /> 或 <img alt="(N, C, D_{out}, H_{out}, W_{out})" src="../img/tex-d14df6868b2b3f7ee945a69616a0b867.gif" /> 其中: <img alt="D_{out} = floor(D_{in} * scale_factor)" src="../img/tex-ea21cae2b495cce278ead54dddcbd995.gif" /> 或 <code>size[-3]</code> <img alt="H_{out} = floor(H_{in} * scale_factor)" src="../img/tex-125c98960333dfccfe02ac897317e5c6.gif" /> 或 <code>size[-2]</code> <img alt="W_{out} = floor(W_{in} * scale_factor)" src="../img/tex-0debb495c4ef7738643087520b82f93c.gif" /> 或 <code>size[-1]</code></li>
</ul>
<p>示例:</p>
<pre><code class="language-py">&gt;&gt;&gt; inp
Variable containing:
(0 ,0 ,.,.) =
 1  2
 3  4
[torch.FloatTensor of size 1x1x2x2]

&gt;&gt;&gt; m = nn.Upsample(scale_factor=2, mode='bilinear')
&gt;&gt;&gt; m(inp)
Variable containing:
(0 ,0 ,.,.) =
 1.0000  1.3333  1.6667  2.0000
 1.6667  2.0000  2.3333  2.6667
 2.3333  2.6667  3.0000  3.3333
 3.0000  3.3333  3.6667  4.0000
[torch.FloatTensor of size 1x1x4x4]

&gt;&gt;&gt; inp
Variable containing:
(0 ,0 ,.,.) =
 1  2
 3  4
[torch.FloatTensor of size 1x1x2x2]

&gt;&gt;&gt; m = nn.Upsample(scale_factor=2, mode='nearest')
&gt;&gt;&gt; m(inp)
Variable containing:
(0 ,0 ,.,.) =
 1  1  2  2
 1  1  2  2
 3  3  4  4
 3  3  4  4
[torch.FloatTensor of size 1x1x4x4]

</code></pre>
<h3 id="upsamplingnearest2d">UpsamplingNearest2d</h3>
<pre><code class="language-py">class torch.nn.UpsamplingNearest2d(size=None, scale_factor=None)
</code></pre>
<p>对多个输入通道组成的输入信号进行2维最近邻上采样.</p>
<p>为了指定采样范围, 提供了 <code>size</code> 或 <code>scale_factor</code> 作为构造参数.</p>
<p>当给定 <code>size</code>, 输出图像的大小为 (h, w).</p>
<p>参数：</p>
<ul>
<li><code>size (tuple, 可选)</code> – 输出图片大小的整型元组(H_out, W_out)</li>
<li><code>scale_factor (int, 可选)</code> – 图像的 长和宽的乘子.</li>
</ul>
<p>形状：</p>
<ul>
<li>输入：<img alt="(N, C, H_{in}, W_{in})" src="../img/tex-d5acb55d3d2dd49b5b0d71ab7cf3b2d1.gif" /></li>
<li>输出：<img alt="(N, C, H_{out}, W_{out})" src="../img/tex-403ffd9231342159e36ba660b2bf3ff3.gif" /> 其中 <img alt="H_{out} = floor(H_{in} * scale_factor)" src="../img/tex-125c98960333dfccfe02ac897317e5c6.gif" /> <img alt="W_{out} = floor(W_{in} * scale_factor)" src="../img/tex-0debb495c4ef7738643087520b82f93c.gif" /></li>
</ul>
<p>示例:</p>
<pre><code class="language-py">&gt;&gt;&gt; inp
Variable containing:
(0 ,0 ,.,.) =
 1  2
 3  4
[torch.FloatTensor of size 1x1x2x2]

&gt;&gt;&gt; m = nn.UpsamplingNearest2d(scale_factor=2)
&gt;&gt;&gt; m(inp)
Variable containing:
(0 ,0 ,.,.) =
 1  1  2  2
 1  1  2  2
 3  3  4  4
 3  3  4  4
[torch.FloatTensor of size 1x1x4x4]

</code></pre>
<h3 id="upsamplingbilinear2d">UpsamplingBilinear2d</h3>
<pre><code class="language-py">class torch.nn.UpsamplingBilinear2d(size=None, scale_factor=None)
</code></pre>
<p>对多个输入通道组成的输入信号进行2维双线性上采样.</p>
<p>为了指定采样范围, 提供了 <code>size</code> 或 <code>scale_factor</code> 作为构造参数.</p>
<p>当给定 <code>size</code>, 输出图像的大小为 (h, w).</p>
<p>参数：</p>
<ul>
<li><code>size (tuple, 可选)</code> – 输出图片大小的整型元组(H_out, W_out)</li>
<li><code>scale_factor (int, 可选)</code> – 图像的 长和宽的乘子.</li>
</ul>
<p>形状：</p>
<ul>
<li>输入：<img alt="(N, C, H_{in}, W_{in})" src="../img/tex-d5acb55d3d2dd49b5b0d71ab7cf3b2d1.gif" /></li>
<li>输出：<img alt="(N, C, H_{out}, W_{out})" src="../img/tex-403ffd9231342159e36ba660b2bf3ff3.gif" /> 其中 <img alt="H_{out} = floor(H_{in} * scale_factor)" src="../img/tex-125c98960333dfccfe02ac897317e5c6.gif" /> <img alt="W_{out} = floor(W_{in} * scale_factor)" src="../img/tex-0debb495c4ef7738643087520b82f93c.gif" /></li>
</ul>
<p>示例：</p>
<pre><code class="language-py">&gt;&gt;&gt; inp
Variable containing:
(0 ,0 ,.,.) =
 1  2
 3  4
[torch.FloatTensor of size 1x1x2x2]

&gt;&gt;&gt; m = nn.UpsamplingBilinear2d(scale_factor=2)
&gt;&gt;&gt; m(inp)
Variable containing:
(0 ,0 ,.,.) =
 1.0000  1.3333  1.6667  2.0000
 1.6667  2.0000  2.3333  2.6667
 2.3333  2.6667  3.0000  3.3333
 3.0000  3.3333  3.6667  4.0000
[torch.FloatTensor of size 1x1x4x4]

</code></pre>
<h2 id="dataparallel-layers-multi-gpu-distributed-gpu">DataParallel layers (multi-GPU, distributed) (数据并行层, 多 GPU 的, 分布式的)</h2>
<h3 id="dataparallel">DataParallel</h3>
<pre><code class="language-py">class torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0)
</code></pre>
<p>在模块级别实现数据并行性.</p>
<p>此容器通过在批次维度中分块, 将输入分割到指定设备上, 从而并行化给定模块的应用程 序.在正向传递中, 模块被复制到每个设备上, 每个副本处理一部分输入.在向后传递期间, 来自每个副本的梯度变化被汇总到原始模块中.</p>
<p>batch size 应该大于 GPUs 的数量.同时也应该是 GPU 数量的整数倍, 以 便每个块大小相同(以便每个 GPU 处理相同数量的样本）.</p>
<p>引用 :<a href="notes/cuda.html#cuda-nn-dataparallel-instead">使用 nn.DataParallel 替代 multiprocessing</a></p>
<p>允许将任意位置和关键字输入传入 DataParallel EXCEPT Tensors. 所有的变量将被分 散在指定的维度(默认为0）.原始类型将被广播, 但所有其他类型将是一个浅层副本, 如 果写入模型的正向传递, 可能会被损坏.</p>
<p>Args : module: 并行的模型 device_ids: CUDA devices(CUDA 驱动） (default: all devices) output_device: 输出设备位置 (default: device_ids[0]) 示例 ::</p>
<pre><code class="language-py">&gt;&gt;&gt; net = torch.nn.DataParallel(model, device_ids=[0, 1, 2])
&gt;&gt;&gt; output = net(input_var)

</code></pre>
<h3 id="distributeddataparallel">DistributedDataParallel</h3>
<pre><code class="language-py">class torch.nn.parallel.DistributedDataParallel(module, device_ids=None, output_device=None, dim=0)
</code></pre>
<p>在模块级别实现分布式数据并行.</p>
<p>此容器通过在批次维度中分块, 将输入分割到指定设备上, 从而并行化给定模块的应用程序. 该模块被复制到每台机器和每个设备上, 每个这样的副本处理一部分输入.在向后传递期间, 来自每个节点的梯度被平均.</p>
<p>batch size 应该大于 GPUs 的数量.同时也应该是 GPU 数量的整数倍, 以便每个块大小 相同(以便每个 GPU 处理相同数量的样本）.</p>
<p>引用 :Basics](distributed.html#distributed-basics) 和 <a href="notes/cuda.html#cuda-nn-dataparallel-instead">使用 nn.DataParallel 替代 multiprocessing</a>. 对输入的约束和 [<code>torch.nn.DataParallel</code> 中一样.</p>
<p>创建这个类需要分布式包已经在 process group 模式下被初始化 (引用 <a href="distributed.html#torch.distributed.init_process_group" title="torch.distributed.init_process_group"><code>torch.distributed.init_process_group()</code></a>).</p>
<p>警告：</p>
<p>这个模块只能和<code>gloo</code>后端一起工作.</p>
<p>警告：</p>
<p>构造器, 转发方法和输出(或者这个模块的输出功能）的区分是分布式同步点.考虑到不同的 进程可能会执行不同的代码.</p>
<p>警告：</p>
<p>该模块假设所有参数在创建时都在模型中注册.之后不应该添加或删除参数.同样适用于缓冲区.</p>
<p>警告：</p>
<p>这个模块假定所有的缓冲区和梯度都是密集的.</p>
<p>警告：</p>
<p>这个模块不能用于 : func: <code>torch.autograd.grad</code> (即只有在参数的 <code>.grad</code> 属性中 累积梯度才能使用）.</p>
<p>注解：</p>
<p>参数永远不会在进程之间广播.模块在梯度上执行全部优化步骤, 并假定它们将以相同的方式在 所有进程中进行优化.缓冲区(e.g. BatchNorm stats）在等级0的过程中从模块广播到系统 中的每个迭代中的所有其他副本.</p>
<p>Args : module: 需要并行的模型 device_ids: CUDA devices (default: all devices) output_device: device location of output (default: device_ids[0]) 示例 ::</p>
<pre><code class="language-py">&gt;&gt;&gt; torch.distributed.init_process_group(world_size=4, init_method='...')
&gt;&gt;&gt; net = torch.nn.DistributedDataParallel(model)

</code></pre>
<h2 id="utilities">Utilities (工具包)</h2>
<h3 id="clip_grad_norm">clip_grad_norm</h3>
<pre><code class="language-py">torch.nn.utils.clip_grad_norm(parameters, max_norm, norm_type=2)
</code></pre>
<p>接收一个包含 Variable 的可迭代对象, 对 Variable 的梯度按范数进行裁剪.</p>
<p>范数是对所有梯度进行计算的, 等价于把所有输入变量的梯度连接成一个向量, 然后对这个向量按范数进行裁剪. 梯度将会被原地修改.</p>
<p>参数：</p>
<ul>
<li><code>parameters (Iterable[Variable])</code> – 一个可迭代对象, 其包含将要进行梯度正规化的 Variable</li>
<li><code>max_norm (float 或 int)</code> – 梯度的最大范数</li>
<li><code>norm_type (float 或 int)</code> – p 范数(指定 p ). 用 <code>'inf'</code> 表示无穷范数</li>
</ul>
<p>返回值：梯度的范数 (视为单个向量的).</p>
<h3 id="weight_norm">weight_norm</h3>
<pre><code class="language-py">torch.nn.utils.weight_norm(module, name='weight', dim=0)
</code></pre>
<p>将权重归一化应用于给定模块中的指定参数. .</p>
<p><img alt="\mathbf{w} = g \dfrac{\mathbf{v}}{|\mathbf{v}|}" src="../img/tex-5ad6c3a5cca3e461271c3498bc99a156.gif" /></p>
<p>权重归一化是将权重张量的大小和方向分离的再参数化. 该函数会用两个参数代替 <code>name</code> (e.g. “weight”)所指定的参数. 在新的参数中, 一个指定参数的大小 (e.g. “weight_g”), 一个指定参数的方向. 权重归一化是通过一个钩子实现的, 该钩子会在 <code>~Module.forward</code> 的每次调用之前根据大小和方向(两个新参数)重新计算权重张量.</p>
<p>默认情况下, <code>dim=0</code>, 范数会在每一个输出的 channel/plane 上分别计算. 若要对整个权重张量计算范数, 使用 <code>dim=None</code>.</p>
<p>参见 <a href="https://arxiv.org/abs/1602.07868">https://arxiv.org/abs/1602.07868</a></p>
<p>参数：</p>
<ul>
<li><code>module (nn.Module)</code> – 给定的 module</li>
<li><code>name (str, 可选)</code> – 权重参数的 name</li>
<li><code>dim (int, 可选)</code> – 进行范数计算的维度</li>
</ul>
<p>返回值：添加了权重归一化钩子的原 module</p>
<p>示例：</p>
<pre><code class="language-py">&gt;&gt;&gt; m = weight_norm(nn.Linear(20, 40), name='weight')
Linear (20 -&gt; 40)
&gt;&gt;&gt; m.weight_g.size()
torch.Size([40, 1])
&gt;&gt;&gt; m.weight_v.size()
torch.Size([40, 20])

</code></pre>
<h3 id="remove_weight_norm">remove_weight_norm</h3>
<pre><code class="language-py">torch.nn.utils.remove_weight_norm(module, name='weight')
</code></pre>
<p>从模块中移除权重归一化/再参数化.</p>
<p>参数：</p>
<ul>
<li><code>module (nn.Module)</code> – 给定的 module</li>
<li><code>name (str, 可选)</code> – 权重参数的 name</li>
</ul>
<p>示例：</p>
<pre><code class="language-py">&gt;&gt;&gt; m = weight_norm(nn.Linear(20, 40))
&gt;&gt;&gt; remove_weight_norm(m)

</code></pre>
<h3 id="packedsequence">PackedSequence</h3>
<pre><code class="language-py">torch.nn.utils.rnn.PackedSequence(_cls, data, batch_sizes)
</code></pre>
<p>保存一个打包序列的 data 和 batch_sizes.</p>
<p>所有的 RNN 模块都接收这种被包裹后的序列作为它们的输入.</p>
<p>注解：</p>
<p>永远不要手动创建这个类的实例. 它们应当被 <code>pack_padded_sequence()</code> 这样的函数实例化.</p>
<p>变量：</p>
<ul>
<li><code>data (Variable)</code> – 包含打包后序列的 Variable</li>
<li><code>batch_sizes (list[int])</code> – 包含每个序列步的 batch size 的列表</li>
</ul>
<h3 id="pack_padded_sequence">pack_padded_sequence</h3>
<pre><code class="language-py">torch.nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=False)
</code></pre>
<p>将填充过的变长序列打包(压紧).</p>
<p>输入的形状可以是 <code>TxBx*</code> . <code>T</code>是最长序列长度(等于 <code>lengths[0]</code>), <code>B</code>是批量大小, <code>*</code>代表任意维度(可以是 0). 如果 <code>batch_first=True</code> , 那么相应的输入大小就是 <code>BxTx*</code> .</p>
<p>Variable 中保存的序列, 应该按序列长度的长短排序, 长的在前, 短的在后. 即 input[:,0] 代表的是最长的序列, input[:, B-1] 保存的是最短的序列.</p>
<p>注解：</p>
<p>只要是维度大于等于2的 input 都可以作为这个函数的参数. 你可以用它来打包 labels, 然后用 RNN 的输出和打包后的 labels 来计算 loss. 通过 <code>PackedSequence</code> 对象的 <code>.data</code> 属性可以获取 Variable.</p>
<p>参数：</p>
<ul>
<li><code>input (Variable)</code> – 变长序列被填充后的 batch</li>
<li><code>lengths (list[int])</code> – Variable 中每个序列的长度.</li>
<li><code>batch_first (bool, 可选)</code> – 如果是 <code>True</code>, input 的形状应该是 <code>BxTx*</code>.</li>
</ul>
<p>返回值：一个 <code>PackedSequence</code> 对象.</p>
<h3 id="pad_packed_sequence">pad_packed_sequence</h3>
<pre><code class="language-py">torch.nn.utils.rnn.pad_packed_sequence(sequence, batch_first=False, padding_value=0.0)
</code></pre>
<p>填充打包过的变长序列.</p>
<p>这是 <code>pack_padded_sequence()</code> 的逆操作.</p>
<p>返回的 Varaible 的值的 size 是 <code>TxBx*</code>, T 是最长序列的长度, B 是 batch_size, 如果 <code>batch_first=True</code>, 那么返回值是 <code>BxTx*</code>.</p>
<p>Batch中的元素将会以它们长度的逆序排列.</p>
<p>参数：</p>
<ul>
<li><code>sequence (PackedSequence)</code> – 将要被填充的 batch</li>
<li><code>batch_first (bool, 可选)</code> – 如果为 <code>True</code> , 返回的数据的格式为 <code>BxTx*</code>.</li>
<li><code>padding_value (float, 可选)</code> – 用来填充元素的值</li>
</ul>
<p>返回值：一个 tuple, 包含被填充后的序列, 和 batch 中序列的长度列表.</p>
<hr/>
<div align="center">
  <p><a href="https://www.apachecn.org/" target="_blank"><font face="KaiTi" size="6" color="red">我们一直在努力</font></a><p>
  <p><a href="https://github.com/apachecn/pytorch-doc-zh" target="_blank">apachecn/pytorch-doc-zh</a></p>
  <p><a target="_blank" href="https://qm.qq.com/cgi-bin/qm/qr?k=5u_aAU-YlY3fH-m8meXTJzBEo2boQIUs&jump_from=webapi&authKey=CVZcReMt/vKdTXZBQ8ly+jWncXiSzzWOlrx5hybX5pSrKu6s0fvGX54+vHHlgYNt"><img border="0" src="https://pub.idqqimg.com/wpa/images/group.png" alt="【布客】中文翻译组" title="【布客】中文翻译组"></a></p>
  <p><span id="cnzz_stat_icon_1275211409"></span></p>
  <!-- <p><a href="https://get.brightdata.com/apachecn" target="_blank"><img src="/assets/images/partnerstack.gif" /></a><p> -->
  <div class="wwads-cn wwads-horizontal" data-id="206" style="max-width:680px"></div>
  <div style="text-align:center;margin:0 0 10.5px;">
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-3565452474788507" crossorigin="anonymous"></script>
    <!-- ApacheCNWide -->
    <ins class="adsbygoogle"
        style="display:inline-block;width:680px;height:90px"
        data-ad-client="ca-pub-3565452474788507"
        data-ad-slot="2543897000"></ins>
    <script>
        (adsbygoogle = window.adsbygoogle || []).push({});
    </script>
  </div>
</div>
<hr/>
<!-- 来必力City版安装代码 -->
<div id="lv-container" data-id="city" data-uid="MTAyMC81ODA2NC8zNDUyNw==">
  <script type="text/javascript">
  (function(d, s) {
      var j, e = d.getElementsByTagName(s)[0];

      if (typeof LivereTower === 'function') { return; }

      j = d.createElement(s);
      j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
      j.async = true;

      e.parentNode.insertBefore(j, e);
  })(document, 'script');
  </script>
<noscript> 为正常使用来必力评论功能请激活JavaScript</noscript>
</div>
<!-- City版安装代码已完成 -->






                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../storage/" class="md-footer__link md-footer__link--prev" aria-label="Previous: torch.Storage" rel="prev">
            <div class="md-footer__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                torch.Storage
              </div>
            </div>
          </a>
        
        
          
          <a href="../optim/" class="md-footer__link md-footer__link--next" aria-label="Next: torch.optim" rel="next">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                torch.optim
              </div>
            </div>
            <div class="md-footer__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright" style="text-align: center; width: 100%;">
  
  
    <div>
      <div style="margin:0 0 10.5px;"><script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1275211409'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s5.cnzz.com/z_stat.php%3Fid%3D1275211409%26online%3D1%26show%3Dline' type='text/javascript'%3E%3C/script%3E"));</script></div>
      <p>Copyright © 2023 学习网站 <a href="http://beian.miit.gov.cn" target="_blank">京ICP备19016010号-1</a><br/>网站由 <a href="https://apachecn.org/cooperate/">@片刻小哥哥</a> 提供支持 | 联系QQ/微信: 529815144 请注明来意！</p>
    </div>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["content.code.copy", "content.action.edit", "content.action.view", "navigation.footer"], "search": "../../assets/javascripts/workers/search.208ed371.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
  
      <script src="../../assets/javascripts/bundle.b425cdc4.min.js"></script>
      
        
          <script src="../../javascripts/mathjax.js"></script>
        
      
        
          <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        
      
        
          <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        
      
    
  <script src="../../assets/javascripts/custom.a7283b5f.min.js"></script>

  </body>
</html>